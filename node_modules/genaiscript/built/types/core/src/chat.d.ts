import OpenAI from "openai";
import { JSONLineCache } from "./cache";
import { MarkdownTrace } from "./trace";
import { PromptImage } from "./promptdom";
import { AICIRequest } from "./aici";
import { LanguageModelConfiguration } from "./host";
import { GenerationOptions } from "./promptcontext";
import { CancellationToken } from "./cancellation";
export type ChatCompletionContentPartText = OpenAI.Chat.Completions.ChatCompletionContentPartText;
export type ChatCompletionContentPart = OpenAI.Chat.Completions.ChatCompletionContentPart;
export type ChatCompletionTool = OpenAI.Chat.Completions.ChatCompletionTool;
export type ChatCompletionChunk = OpenAI.Chat.Completions.ChatCompletionChunk;
export type ChatCompletionSystemMessageParam = OpenAI.Chat.Completions.ChatCompletionSystemMessageParam;
export type ChatCompletionMessageParam = OpenAI.Chat.Completions.ChatCompletionMessageParam | AICIRequest;
export type CreateChatCompletionRequest = Omit<OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming, "messages"> & {
    /**
     * A list of messages comprising the conversation so far.
     * [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
     */
    messages: ChatCompletionMessageParam[];
};
export type ChatCompletionAssistantMessageParam = OpenAI.Chat.Completions.ChatCompletionAssistantMessageParam;
export type ChatCompletionUserMessageParam = OpenAI.Chat.Completions.ChatCompletionUserMessageParam;
export type ChatCompletionContentPartImage = OpenAI.Chat.Completions.ChatCompletionContentPartImage;
export interface ChatCompletionToolCall {
    id: string;
    name: string;
    arguments?: string;
}
export interface ChatCompletionResponse {
    text?: string;
    cached?: boolean;
    variables?: Record<string, string>;
    toolCalls?: ChatCompletionToolCall[];
    finishReason?: "stop" | "length" | "tool_calls" | "content_filter" | "cancel" | "fail";
}
export declare const ModelError: typeof import("openai/error.mjs").APIError;
export type ChatCompletionRequestCacheKey = CreateChatCompletionRequest & ModelOptions & Omit<LanguageModelConfiguration, "token" | "source">;
export type ChatCompletationRequestCacheValue = {
    text: string;
    finishReason: ChatCompletionResponse["finishReason"];
};
export type ChatCompletationRequestCache = JSONLineCache<ChatCompletionRequestCacheKey, ChatCompletationRequestCacheValue>;
export declare function getChatCompletionCache(name?: string): ChatCompletationRequestCache;
export interface ChatCompletionsProgressReport {
    tokensSoFar: number;
    responseSoFar: string;
    responseChunk: string;
}
export interface ChatCompletionsOptions {
    partialCb?: (progress: ChatCompletionsProgressReport) => void;
    requestOptions?: Partial<RequestInit>;
    maxCachedTemperature?: number;
    maxCachedTopP?: number;
    cache?: boolean;
    cacheName?: string;
    retry?: number;
    retryDelay?: number;
    maxDelay?: number;
}
export declare function toChatCompletionUserMessage(expanded: string, images?: PromptImage[]): ChatCompletionUserMessageParam;
export type ChatCompletionHandler = (req: CreateChatCompletionRequest, connection: LanguageModelConfiguration, options: ChatCompletionsOptions, trace: MarkdownTrace) => Promise<ChatCompletionResponse>;
export interface LanguageModelInfo {
    id: string;
    details?: string;
    url?: string;
}
export type ListModelsFunction = (cfg: LanguageModelConfiguration) => Promise<LanguageModelInfo[]>;
export interface LanguageModel {
    id: string;
    completer: ChatCompletionHandler;
    listModels?: ListModelsFunction;
}
export declare function mergeGenerationOptions(options: GenerationOptions, runOptions: ModelOptions): GenerationOptions;
export declare function executeChatSession(connectionToken: LanguageModelConfiguration, cancellationToken: CancellationToken, messages: ChatCompletionMessageParam[], vars: Partial<ExpansionVariables>, toolDefinitions: ToolCallback[], schemas: Record<string, JSONSchema>, completer: ChatCompletionHandler, chatParticipants: ChatParticipant[], genOptions: GenerationOptions): Promise<RunPromptResult>;
export declare function tracePromptResult(trace: MarkdownTrace, resp: RunPromptResult): void;
export declare function renderMessagesToMarkdown(messages: ChatCompletionMessageParam[], options?: {
    system?: boolean;
    user?: boolean;
    assistant?: boolean;
}): string;
//# sourceMappingURL=chat.d.ts.map