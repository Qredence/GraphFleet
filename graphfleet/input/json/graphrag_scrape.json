[
  {
    "content": "Get Started\n===========\n\nRequirements\n------------\n\n[Python 3.10-3.12](https://www.python.org/downloads/)\n\nTo get started with the GraphRAG system, you have a few options:\n\nðŸ‘‰ [Use the GraphRAG Accelerator solution](https://github.com/Azure-Samples/graphrag-accelerator)\n  \nðŸ‘‰ [Install from pypi](https://pypi.org/project/graphrag/)\n.  \nðŸ‘‰ [Use it from source](/graphrag/posts/developing)\n  \n\nQuickstart\n----------\n\nTo get started with the GraphRAG system we recommend trying the [Solution Accelerator](https://github.com/Azure-Samples/graphrag-accelerator)\n package. This provides a user-friendly end-to-end experience with Azure resources.\n\nTop-Level Modules\n=================\n\n[Indexing Pipeline Overview](/graphrag/posts/index/overview)\n  \n[Query Engine Overview](/graphrag/posts/query/overview)\n\nOverview\n========\n\nThe following is a simple end-to-end example for using the GraphRAG system. It shows how to use the system to index some text, and then use the indexed data to answer questions about the documents.\n\nInstall GraphRAG\n================\n\n    pip install graphrag\n\nRunning the Indexer\n===================\n\nNow we need to set up a data project and some initial configuration. Let's set that up. We're using the [default configuration mode](/graphrag/posts/config/overview/)\n, which you can customize as needed using a [config file](/graphrag/posts/config/json_yaml/)\n, which we recommend, or [environment variables](/graphrag/posts/config/env_vars/)\n.\n\nFirst let's get a sample dataset ready:\n\n    mkdir -p ./ragtest/input\n\nNow let's get a copy of A Christmas Carol by Charles Dickens from a trusted source\n\n    curl https://www.gutenberg.org/cache/epub/24022/pg24022.txt > ./ragtest/input/book.txt\n\nNext we'll inject some required config variables:\n\nSet Up Your Workspace Variables\n-------------------------------\n\nFirst let's make sure to setup the required environment variables. For details on these environment variables, and what environment variables are available, see the [variables documentation](/graphrag/posts/config/overview/)\n.\n\nTo initialize your workspace, let's first run the `graphrag.index --init` command. Since we have already configured a directory named .ragtest\\` in the previous step, we can run the following command:\n\n    python -m graphrag.index --init --root ./ragtest\n\nThis will create two files: `.env` and `settings.yaml` in the `./ragtest` directory.\n\n*   `.env` contains the environment variables required to run the GraphRAG pipeline. If you inspect the file, you'll see a single environment variable defined, `GRAPHRAG_API_KEY=<API_KEY>`. This is the API key for the OpenAI API or Azure OpenAI endpoint. You can replace this with your own API key.\n*   `settings.yaml` contains the settings for the pipeline. You can modify this file to change the settings for the pipeline.  \n    \n\n#### OpenAI and Azure OpenAI\n\nTo run in OpenAI mode, just make sure to update the value of `GRAPHRAG_API_KEY` in the `.env` file with your OpenAI API key.\n\n#### Azure OpenAI\n\nIn addition, Azure OpenAI users should set the following variables in the settings.yaml file. To find the appropriate sections, just search for the `llm:` configuration, you should see two sections, one for the chat endpoint and one for the embeddings endpoint. Here is an example of how to configure the chat endpoint:\n\n    type: azure_openai_chat # Or azure_openai_embedding for embeddings\n    api_base: https://<instance>.openai.azure.com\n    api_version: 2024-02-15-preview # You can customize this for other versions\n    deployment_name: <azure_model_deployment_name>\n\n*   For more details about configuring GraphRAG, see the [configuration documentation](/graphrag/posts/config/overview/)\n    .\n*   To learn more about Initialization, refer to the [Initialization documentation](/graphrag/posts/config/init/)\n    .\n*   For more details about using the CLI, refer to the [CLI documentation](/graphrag/posts/query/3-cli/)\n    .\n\nRunning the Indexing pipeline\n-----------------------------\n\nFinally we'll run the pipeline!\n\n    python -m graphrag.index --root ./ragtest\n\n![pipeline executing from the CLI](https://microsoft.github.io/graphrag/img/pipeline-running.png)\n\nThis process will take some time to run. This depends on the size of your input data, what model you're using, and the text chunk size being used (these can be configured in your `.env` file). Once the pipeline is complete, you should see a new folder called `./ragtest/output/<timestamp>/artifacts` with a series of parquet files.\n\nUsing the Query Engine\n======================\n\nRunning the Query Engine\n------------------------\n\nNow let's ask some questions using this dataset.\n\nHere is an example using Global search to ask a high-level question:\n\n    python -m graphrag.query \\\n    --root ./ragtest \\\n    --method global \\\n    \"What are the top themes in this story?\"\n\nHere is an example using Local search to ask a more specific question about a particular character:\n\n    python -m graphrag.query \\\n    --root ./ragtest \\\n    --method local \\\n    \"Who is Scrooge, and what are his main relationships?\"\n\nPlease refer to [Query Engine](/graphrag/posts/query/overview)\n docs for detailed information about how to leverage our Local and Global search mechanisms for extracting meaningful insights from data after the Indexer has wrapped up execution.",
    "markdown": "Get Started\n===========\n\nRequirements\n------------\n\n[Python 3.10-3.12](https://www.python.org/downloads/)\n\nTo get started with the GraphRAG system, you have a few options:\n\nðŸ‘‰ [Use the GraphRAG Accelerator solution](https://github.com/Azure-Samples/graphrag-accelerator)\n  \nðŸ‘‰ [Install from pypi](https://pypi.org/project/graphrag/)\n.  \nðŸ‘‰ [Use it from source](/graphrag/posts/developing)\n  \n\nQuickstart\n----------\n\nTo get started with the GraphRAG system we recommend trying the [Solution Accelerator](https://github.com/Azure-Samples/graphrag-accelerator)\n package. This provides a user-friendly end-to-end experience with Azure resources.\n\nTop-Level Modules\n=================\n\n[Indexing Pipeline Overview](/graphrag/posts/index/overview)\n  \n[Query Engine Overview](/graphrag/posts/query/overview)\n\nOverview\n========\n\nThe following is a simple end-to-end example for using the GraphRAG system. It shows how to use the system to index some text, and then use the indexed data to answer questions about the documents.\n\nInstall GraphRAG\n================\n\n    pip install graphrag\n\nRunning the Indexer\n===================\n\nNow we need to set up a data project and some initial configuration. Let's set that up. We're using the [default configuration mode](/graphrag/posts/config/overview/)\n, which you can customize as needed using a [config file](/graphrag/posts/config/json_yaml/)\n, which we recommend, or [environment variables](/graphrag/posts/config/env_vars/)\n.\n\nFirst let's get a sample dataset ready:\n\n    mkdir -p ./ragtest/input\n\nNow let's get a copy of A Christmas Carol by Charles Dickens from a trusted source\n\n    curl https://www.gutenberg.org/cache/epub/24022/pg24022.txt > ./ragtest/input/book.txt\n\nNext we'll inject some required config variables:\n\nSet Up Your Workspace Variables\n-------------------------------\n\nFirst let's make sure to setup the required environment variables. For details on these environment variables, and what environment variables are available, see the [variables documentation](/graphrag/posts/config/overview/)\n.\n\nTo initialize your workspace, let's first run the `graphrag.index --init` command. Since we have already configured a directory named .ragtest\\` in the previous step, we can run the following command:\n\n    python -m graphrag.index --init --root ./ragtest\n\nThis will create two files: `.env` and `settings.yaml` in the `./ragtest` directory.\n\n*   `.env` contains the environment variables required to run the GraphRAG pipeline. If you inspect the file, you'll see a single environment variable defined, `GRAPHRAG_API_KEY=<API_KEY>`. This is the API key for the OpenAI API or Azure OpenAI endpoint. You can replace this with your own API key.\n*   `settings.yaml` contains the settings for the pipeline. You can modify this file to change the settings for the pipeline.  \n    \n\n#### OpenAI and Azure OpenAI\n\nTo run in OpenAI mode, just make sure to update the value of `GRAPHRAG_API_KEY` in the `.env` file with your OpenAI API key.\n\n#### Azure OpenAI\n\nIn addition, Azure OpenAI users should set the following variables in the settings.yaml file. To find the appropriate sections, just search for the `llm:` configuration, you should see two sections, one for the chat endpoint and one for the embeddings endpoint. Here is an example of how to configure the chat endpoint:\n\n    type: azure_openai_chat # Or azure_openai_embedding for embeddings\n    api_base: https://<instance>.openai.azure.com\n    api_version: 2024-02-15-preview # You can customize this for other versions\n    deployment_name: <azure_model_deployment_name>\n\n*   For more details about configuring GraphRAG, see the [configuration documentation](/graphrag/posts/config/overview/)\n    .\n*   To learn more about Initialization, refer to the [Initialization documentation](/graphrag/posts/config/init/)\n    .\n*   For more details about using the CLI, refer to the [CLI documentation](/graphrag/posts/query/3-cli/)\n    .\n\nRunning the Indexing pipeline\n-----------------------------\n\nFinally we'll run the pipeline!\n\n    python -m graphrag.index --root ./ragtest\n\n![pipeline executing from the CLI](https://microsoft.github.io/graphrag/img/pipeline-running.png)\n\nThis process will take some time to run. This depends on the size of your input data, what model you're using, and the text chunk size being used (these can be configured in your `.env` file). Once the pipeline is complete, you should see a new folder called `./ragtest/output/<timestamp>/artifacts` with a series of parquet files.\n\nUsing the Query Engine\n======================\n\nRunning the Query Engine\n------------------------\n\nNow let's ask some questions using this dataset.\n\nHere is an example using Global search to ask a high-level question:\n\n    python -m graphrag.query \\\n    --root ./ragtest \\\n    --method global \\\n    \"What are the top themes in this story?\"\n\nHere is an example using Local search to ask a more specific question about a particular character:\n\n    python -m graphrag.query \\\n    --root ./ragtest \\\n    --method local \\\n    \"Who is Scrooge, and what are his main relationships?\"\n\nPlease refer to [Query Engine](/graphrag/posts/query/overview)\n docs for detailed information about how to leverage our Local and Global search mechanisms for extracting meaningful insights from data after the Indexer has wrapped up execution.",
    "metadata": {
      "title": "Get Started",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/get_started/",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "https://www.python.org/downloads/",
      "https://github.com/Azure-Samples/graphrag-accelerator",
      "https://pypi.org/project/graphrag/",
      "https://microsoft.github.io/graphrag/posts/developing",
      "https://microsoft.github.io/graphrag/posts/index/overview",
      "https://microsoft.github.io/graphrag/posts/query/overview",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml/",
      "https://microsoft.github.io/graphrag/posts/config/env_vars/",
      "https://microsoft.github.io/graphrag/posts/config/init/",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag"
    ]
  },
  {
    "content": "Developing GraphRAG\n===================\n\nRequirements\n============\n\n| Name | Installation | Purpose |\n| --- | --- | --- |\n| Python 3.10-3.12 | [Download](https://www.python.org/downloads/) | The library is Python-based. |\n| Poetry | [Instructions](https://python-poetry.org/docs/#installation) | Poetry is used for package management and virtualenv management in Python codebases |\n\nGetting Started\n===============\n\nInstall Dependencies\n--------------------\n\n    # Install Python dependencies.\n    poetry install\n\nExecute the Indexing Engine\n---------------------------\n\n    poetry run poe index <...args>\n\nExecuting Queries\n-----------------\n\n    poetry run poe query <...args>\n\nAzurite\n=======\n\nSome unit and smoke tests use Azurite to emulate Azure resources. This can be started by running:\n\n    ./scripts/start-azurite.sh\n\nor by simply running `azurite` in the terminal if already installed globally. See the [Azurite documentation](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azurite)\n for more information about how to install and use Azurite.\n\nLifecycle Scripts\n=================\n\nOur Python package utilizes Poetry to manage dependencies and [poethepoet](https://pypi.org/project/poethepoet/)\n to manage build scripts.\n\nAvailable scripts are:\n\n*   `poetry run poe index` - Run the Indexing CLI\n*   `poetry run poe query` - Run the Query CLI\n*   `poetry build` - This invokes `poetry build`, which will build a wheel file and other distributable artifacts.\n*   `poetry run poe test` - This will execute all tests.\n*   `poetry run poe test_unit` - This will execute unit tests.\n*   `poetry run poe test_integration` - This will execute integration tests.\n*   `poetry run poe test_smoke` - This will execute smoke tests.\n*   `poetry run poe check` - This will perform a suite of static checks across the package, including:\n    *   formatting\n    *   documentation formatting\n    *   linting\n    *   security patterns\n    *   type-checking\n*   `poetry run poe fix` - This will apply any available auto-fixes to the package. Usually this is just formatting fixes.\n*   `poetry run poe fix_unsafe` - This will apply any available auto-fixes to the package, including those that may be unsafe.\n*   `poetry run poe format` - Explicitly run the formatter across the package.\n\nTroubleshooting\n---------------\n\n### \"RuntimeError: llvm-config failed executing, please point LLVM\\_CONFIG to the path for llvm-config\" when running poetry install\n\nMake sure llvm-9 and llvm-9-dev are installed:\n\n`sudo apt-get install llvm-9 llvm-9-dev`\n\nand then in your bashrc, add\n\n`export LLVM_CONFIG=/usr/bin/llvm-config-9`\n\n### \"numba/\\_pymodule.h:6:10: fatal error: Python.h: No such file or directory\" when running poetry install\n\nMake sure you have python3.10-dev installed or more generally `python<version>-dev`\n\n`sudo apt-get install python3.10-dev`\n\n### LLM call constantly exceeds TPM, RPM or time limits\n\n`GRAPHRAG_LLM_THREAD_COUNT` and `GRAPHRAG_EMBEDDING_THREAD_COUNT` are both set to 50 by default. You can modify this values to reduce concurrency. Please refer to the [Configuration Documents](../config/overview)",
    "markdown": "Developing GraphRAG\n===================\n\nRequirements\n============\n\n| Name | Installation | Purpose |\n| --- | --- | --- |\n| Python 3.10-3.12 | [Download](https://www.python.org/downloads/) | The library is Python-based. |\n| Poetry | [Instructions](https://python-poetry.org/docs/#installation) | Poetry is used for package management and virtualenv management in Python codebases |\n\nGetting Started\n===============\n\nInstall Dependencies\n--------------------\n\n    # Install Python dependencies.\n    poetry install\n\nExecute the Indexing Engine\n---------------------------\n\n    poetry run poe index <...args>\n\nExecuting Queries\n-----------------\n\n    poetry run poe query <...args>\n\nAzurite\n=======\n\nSome unit and smoke tests use Azurite to emulate Azure resources. This can be started by running:\n\n    ./scripts/start-azurite.sh\n\nor by simply running `azurite` in the terminal if already installed globally. See the [Azurite documentation](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azurite)\n for more information about how to install and use Azurite.\n\nLifecycle Scripts\n=================\n\nOur Python package utilizes Poetry to manage dependencies and [poethepoet](https://pypi.org/project/poethepoet/)\n to manage build scripts.\n\nAvailable scripts are:\n\n*   `poetry run poe index` - Run the Indexing CLI\n*   `poetry run poe query` - Run the Query CLI\n*   `poetry build` - This invokes `poetry build`, which will build a wheel file and other distributable artifacts.\n*   `poetry run poe test` - This will execute all tests.\n*   `poetry run poe test_unit` - This will execute unit tests.\n*   `poetry run poe test_integration` - This will execute integration tests.\n*   `poetry run poe test_smoke` - This will execute smoke tests.\n*   `poetry run poe check` - This will perform a suite of static checks across the package, including:\n    *   formatting\n    *   documentation formatting\n    *   linting\n    *   security patterns\n    *   type-checking\n*   `poetry run poe fix` - This will apply any available auto-fixes to the package. Usually this is just formatting fixes.\n*   `poetry run poe fix_unsafe` - This will apply any available auto-fixes to the package, including those that may be unsafe.\n*   `poetry run poe format` - Explicitly run the formatter across the package.\n\nTroubleshooting\n---------------\n\n### \"RuntimeError: llvm-config failed executing, please point LLVM\\_CONFIG to the path for llvm-config\" when running poetry install\n\nMake sure llvm-9 and llvm-9-dev are installed:\n\n`sudo apt-get install llvm-9 llvm-9-dev`\n\nand then in your bashrc, add\n\n`export LLVM_CONFIG=/usr/bin/llvm-config-9`\n\n### \"numba/\\_pymodule.h:6:10: fatal error: Python.h: No such file or directory\" when running poetry install\n\nMake sure you have python3.10-dev installed or more generally `python<version>-dev`\n\n`sudo apt-get install python3.10-dev`\n\n### LLM call constantly exceeds TPM, RPM or time limits\n\n`GRAPHRAG_LLM_THREAD_COUNT` and `GRAPHRAG_EMBEDDING_THREAD_COUNT` are both set to 50 by default. You can modify this values to reduce concurrency. Please refer to the [Configuration Documents](../config/overview)",
    "metadata": {
      "title": "Developing GraphRAG",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/developing/",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "https://www.python.org/downloads/",
      "https://python-poetry.org/docs/#installation",
      "https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azurite",
      "https://pypi.org/project/poethepoet/",
      "https://microsoft.github.io/graphrag/posts/developing//../config/overview",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag",
      "https://github.com/Azure-Samples/graphrag-accelerator"
    ]
  },
  {
    "content": "GraphRAG Indexing ðŸ¤–\n====================\n\nThe GraphRAG indexing package is a data pipeline and transformation suite that is designed to extract meaningful, structured data from unstructured text using LLMs.\n\nIndexing Pipelines are configurable. They are composed of workflows, standard and custom steps, prompt templates, and input/output adapters. Our standard pipeline is designed to:\n\n*   extract entities, relationships and claims from raw text\n*   perform community detection in entities\n*   generate community summaries and reports at multiple levels of granularity\n*   embed entities into a graph vector space\n*   embed text chunks into a textual vector space\n\nThe outputs of the pipeline can be stored in a variety of formats, including JSON and Parquet - or they can be handled manually via the Python API.\n\nGetting Started\n---------------\n\n### Requirements\n\nSee the [requirements](/graphrag/posts/developing#requirements)\n section in [Get Started](/graphrag/posts/get_started)\n for details on setting up a development environment.\n\nThe Indexing Engine can be used in either a default configuration mode or with a custom pipeline. To configure GraphRAG, see the [configuration](/graphrag/posts/config/overview)\n documentation. After you have a config file you can run the pipeline using the CLI or the Python API.\n\nUsage\n-----\n\n### CLI\n\n    # Via Poetry\n    poetry run poe cli --root <data_root> # default config mode\n    poetry run poe cli --config your_pipeline.yml # custom config mode\n    \n    # Via Node\n    yarn run:index --root <data_root> # default config mode\n    yarn run:index --config your_pipeline.yml # custom config mode\n    \n\n### Python API\n\n    from graphrag.index import run_pipeline\n    from graphrag.index.config import PipelineWorkflowReference\n    \n    workflows: list[PipelineWorkflowReference] = [\\\n        PipelineWorkflowReference(\\\n            steps=[\\\n                {\\\n                    # built-in verb\\\n                    \"verb\": \"derive\",  # https://github.com/microsoft/datashaper/blob/main/python/datashaper/datashaper/engine/verbs/derive.py\\\n                    \"args\": {\\\n                        \"column1\": \"col1\",  # from above\\\n                        \"column2\": \"col2\",  # from above\\\n                        \"to\": \"col_multiplied\",  # new column name\\\n                        \"operator\": \"*\",  # multiply the two columns\\\n                    },\\\n                    # Since we're trying to act on the default input, we don't need explicitly to specify an input\\\n                }\\\n            ]\\\n        ),\\\n    ]\n    \n    dataset = pd.DataFrame([{\"col1\": 2, \"col2\": 4}, {\"col1\": 5, \"col2\": 10}])\n    outputs = []\n    async for output in await run_pipeline(dataset=dataset, workflows=workflows):\n        outputs.append(output)\n    pipeline_result = outputs[-1]\n    print(pipeline_result)\n\nFurther Reading\n---------------\n\n*   To start developing within the _GraphRAG_ project, see [getting started](/graphrag/posts/developing/)\n    \n*   To understand the underlying concepts and execution model of the indexing library, see [the architecture documentation](/graphrag/posts/index/0-architecture/)\n    \n*   To get running with a series of examples, see [the examples documentation](https://github.com/microsoft/graphrag/blob/main/examples/README.md)\n    \n*   To read more about configuring the indexing engine, see [the configuration documentation](/graphrag/posts/config/overview)",
    "markdown": "GraphRAG Indexing ðŸ¤–\n====================\n\nThe GraphRAG indexing package is a data pipeline and transformation suite that is designed to extract meaningful, structured data from unstructured text using LLMs.\n\nIndexing Pipelines are configurable. They are composed of workflows, standard and custom steps, prompt templates, and input/output adapters. Our standard pipeline is designed to:\n\n*   extract entities, relationships and claims from raw text\n*   perform community detection in entities\n*   generate community summaries and reports at multiple levels of granularity\n*   embed entities into a graph vector space\n*   embed text chunks into a textual vector space\n\nThe outputs of the pipeline can be stored in a variety of formats, including JSON and Parquet - or they can be handled manually via the Python API.\n\nGetting Started\n---------------\n\n### Requirements\n\nSee the [requirements](/graphrag/posts/developing#requirements)\n section in [Get Started](/graphrag/posts/get_started)\n for details on setting up a development environment.\n\nThe Indexing Engine can be used in either a default configuration mode or with a custom pipeline. To configure GraphRAG, see the [configuration](/graphrag/posts/config/overview)\n documentation. After you have a config file you can run the pipeline using the CLI or the Python API.\n\nUsage\n-----\n\n### CLI\n\n    # Via Poetry\n    poetry run poe cli --root <data_root> # default config mode\n    poetry run poe cli --config your_pipeline.yml # custom config mode\n    \n    # Via Node\n    yarn run:index --root <data_root> # default config mode\n    yarn run:index --config your_pipeline.yml # custom config mode\n    \n\n### Python API\n\n    from graphrag.index import run_pipeline\n    from graphrag.index.config import PipelineWorkflowReference\n    \n    workflows: list[PipelineWorkflowReference] = [\\\n        PipelineWorkflowReference(\\\n            steps=[\\\n                {\\\n                    # built-in verb\\\n                    \"verb\": \"derive\",  # https://github.com/microsoft/datashaper/blob/main/python/datashaper/datashaper/engine/verbs/derive.py\\\n                    \"args\": {\\\n                        \"column1\": \"col1\",  # from above\\\n                        \"column2\": \"col2\",  # from above\\\n                        \"to\": \"col_multiplied\",  # new column name\\\n                        \"operator\": \"*\",  # multiply the two columns\\\n                    },\\\n                    # Since we're trying to act on the default input, we don't need explicitly to specify an input\\\n                }\\\n            ]\\\n        ),\\\n    ]\n    \n    dataset = pd.DataFrame([{\"col1\": 2, \"col2\": 4}, {\"col1\": 5, \"col2\": 10}])\n    outputs = []\n    async for output in await run_pipeline(dataset=dataset, workflows=workflows):\n        outputs.append(output)\n    pipeline_result = outputs[-1]\n    print(pipeline_result)\n\nFurther Reading\n---------------\n\n*   To start developing within the _GraphRAG_ project, see [getting started](/graphrag/posts/developing/)\n    \n*   To understand the underlying concepts and execution model of the indexing library, see [the architecture documentation](/graphrag/posts/index/0-architecture/)\n    \n*   To get running with a series of examples, see [the examples documentation](https://github.com/microsoft/graphrag/blob/main/examples/README.md)\n    \n*   To read more about configuring the indexing engine, see [the configuration documentation](/graphrag/posts/config/overview)",
    "metadata": {
      "title": "GraphRAG Indexing ðŸ¤–",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/index/overview/",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "https://microsoft.github.io/graphrag/posts/developing#requirements",
      "https://microsoft.github.io/graphrag/posts/get_started",
      "https://microsoft.github.io/graphrag/posts/config/overview",
      "https://github.com/microsoft/graphrag/blob/main/examples/README.md",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag",
      "https://github.com/Azure-Samples/graphrag-accelerator"
    ]
  },
  {
    "content": "Indexing Architecture\n=====================\n\nKey Concepts\n------------\n\n### Knowledge Model\n\nIn order to support the GraphRAG system, the outputs of the indexing engine (in the Default Configuration Mode) are aligned to a knowledge model we call the _GraphRAG Knowledge Model_. This model is designed to be an abstraction over the underlying data storage technology, and to provide a common interface for the GraphRAG system to interact with. In normal use-cases the outputs of the GraphRAG Indexer would be loaded into a database system, and the GraphRAG's Query Engine would interact with the database using the knowledge model data-store types.\n\n### DataShaper Workflows\n\nGraphRAG's Indexing Pipeline is built on top of our open-source library, [DataShaper](https://github.com/microsoft/datashaper)\n. DataShaper is a data processing library that allows users to declaratively express data pipelines, schemas, and related assets using well-defined schemas. DataShaper has implementations in JavaScript and Python, and is designed to be extensible to other languages.\n\nOne of the core resource types within DataShaper is a [Workflow](https://github.com/microsoft/datashaper/blob/main/javascript/schema/src/workflow/WorkflowSchema.ts)\n. Workflows are expressed as sequences of steps, which we call [verbs](https://github.com/microsoft/datashaper/blob/main/javascript/schema/src/workflow/verbs.ts)\n. Each step has a verb name and a configuration object. In DataShaper, these verbs model relational concepts such as SELECT, DROP, JOIN, etc.. Each verb transforms an input data table, and that table is passed down the pipeline.\n\n### LLM-based Workflow Steps\n\nGraphRAG's Indexing Pipeline implements a handful of custom verbs on top of the standard, relational verbs that our DataShaper library provides. These verbs give us the ability to augment text documents with rich, structured data using the power of LLMs such as GPT-4. We utilize these verbs in our standard workflow to extract entities, relationships, claims, community structures, and community reports and summaries. This behavior is customizable and can be extended to support many kinds of AI-based data enrichment and extraction tasks.\n\n### Workflow Graphs\n\nBecause of the complexity of our data indexing tasks, we needed to be able to express our data pipeline as series of multiple, interdependent workflows. In the GraphRAG Indexing Pipeline, each workflow may define dependencies on other workflows, effectively forming a directed acyclic graph (DAG) of workflows, which is then used to schedule processing.\n\n\\---\ntitle: Sample Workflow DAG\n---\nstateDiagram-v2\n    \\[\\*\\] --> Prepare\n    Prepare --> Chunk\n    Chunk --> ExtractGraph\n    Chunk --> EmbedDocuments\n    ExtractGraph --> GenerateReports\n    ExtractGraph --> EmbedGraph\n    EntityResolution --> EmbedGraph\n    EntityResolution --> GenerateReports\n    ExtractGraph --> EntityResolution\n\n### Dataframe Message Format\n\nThe primary unit of communication between workflows, and between workflow steps is an instance of `pandas.DataFrame`. Although side-effects are possible, our goal is to be _data-centric_ and _table-centric_ in our approach to data processing. This allows us to easily reason about our data, and to leverage the power of dataframe-based ecosystems. Our underlying dataframe technology may change over time, but our primary goal is to support the DataShaper workflow schema while retaining single-machine ease of use and developer ergonomics.\n\n### LLM Caching\n\nThe GraphRAG library was designed with LLM interactions in mind, and a common setback when working with LLM APIs is various errors errors due to network latency, throttling, etc.. Because of these potential error cases, we've added a cache layer around LLM interactions. When completion requests are made using the same input set (prompt and tuning parameters), we return a cached result if one exists. This allows our indexer to be more resilient to network issues, to act idempotently, and to provide a more efficient end-user experience.",
    "markdown": "Indexing Architecture\n=====================\n\nKey Concepts\n------------\n\n### Knowledge Model\n\nIn order to support the GraphRAG system, the outputs of the indexing engine (in the Default Configuration Mode) are aligned to a knowledge model we call the _GraphRAG Knowledge Model_. This model is designed to be an abstraction over the underlying data storage technology, and to provide a common interface for the GraphRAG system to interact with. In normal use-cases the outputs of the GraphRAG Indexer would be loaded into a database system, and the GraphRAG's Query Engine would interact with the database using the knowledge model data-store types.\n\n### DataShaper Workflows\n\nGraphRAG's Indexing Pipeline is built on top of our open-source library, [DataShaper](https://github.com/microsoft/datashaper)\n. DataShaper is a data processing library that allows users to declaratively express data pipelines, schemas, and related assets using well-defined schemas. DataShaper has implementations in JavaScript and Python, and is designed to be extensible to other languages.\n\nOne of the core resource types within DataShaper is a [Workflow](https://github.com/microsoft/datashaper/blob/main/javascript/schema/src/workflow/WorkflowSchema.ts)\n. Workflows are expressed as sequences of steps, which we call [verbs](https://github.com/microsoft/datashaper/blob/main/javascript/schema/src/workflow/verbs.ts)\n. Each step has a verb name and a configuration object. In DataShaper, these verbs model relational concepts such as SELECT, DROP, JOIN, etc.. Each verb transforms an input data table, and that table is passed down the pipeline.\n\n### LLM-based Workflow Steps\n\nGraphRAG's Indexing Pipeline implements a handful of custom verbs on top of the standard, relational verbs that our DataShaper library provides. These verbs give us the ability to augment text documents with rich, structured data using the power of LLMs such as GPT-4. We utilize these verbs in our standard workflow to extract entities, relationships, claims, community structures, and community reports and summaries. This behavior is customizable and can be extended to support many kinds of AI-based data enrichment and extraction tasks.\n\n### Workflow Graphs\n\nBecause of the complexity of our data indexing tasks, we needed to be able to express our data pipeline as series of multiple, interdependent workflows. In the GraphRAG Indexing Pipeline, each workflow may define dependencies on other workflows, effectively forming a directed acyclic graph (DAG) of workflows, which is then used to schedule processing.\n\n\\---\ntitle: Sample Workflow DAG\n---\nstateDiagram-v2\n    \\[\\*\\] --> Prepare\n    Prepare --> Chunk\n    Chunk --> ExtractGraph\n    Chunk --> EmbedDocuments\n    ExtractGraph --> GenerateReports\n    ExtractGraph --> EmbedGraph\n    EntityResolution --> EmbedGraph\n    EntityResolution --> GenerateReports\n    ExtractGraph --> EntityResolution\n\n### Dataframe Message Format\n\nThe primary unit of communication between workflows, and between workflow steps is an instance of `pandas.DataFrame`. Although side-effects are possible, our goal is to be _data-centric_ and _table-centric_ in our approach to data processing. This allows us to easily reason about our data, and to leverage the power of dataframe-based ecosystems. Our underlying dataframe technology may change over time, but our primary goal is to support the DataShaper workflow schema while retaining single-machine ease of use and developer ergonomics.\n\n### LLM Caching\n\nThe GraphRAG library was designed with LLM interactions in mind, and a common setback when working with LLM APIs is various errors errors due to network latency, throttling, etc.. Because of these potential error cases, we've added a cache layer around LLM interactions. When completion requests are made using the same input set (prompt and tuning parameters), we return a cached result if one exists. This allows our indexer to be more resilient to network issues, to act idempotently, and to provide a more efficient end-user experience.",
    "metadata": {
      "title": "Indexing Architecture",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "https://github.com/microsoft/datashaper",
      "https://github.com/microsoft/datashaper/blob/main/javascript/schema/src/workflow/WorkflowSchema.ts",
      "https://github.com/microsoft/datashaper/blob/main/javascript/schema/src/workflow/verbs.ts",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag",
      "https://github.com/Azure-Samples/graphrag-accelerator"
    ]
  },
  {
    "content": "Indexing Dataflow\n=================\n\nThe GraphRAG Knowledge Model\n----------------------------\n\nThe knowledge model is a specification for data outputs that conform to our data-model definition. You can find these definitions in the python/graphrag/graphrag/model folder within the GraphRAG repository. The following entity types are provided. The fields here represent the fields that are text-embedded by default.\n\n*   `Document` - An input document into the system. These either represent individual rows in a CSV or individual .txt file.\n*   `TextUnit` - A chunk of text to analyze. The size of these chunks, their overlap, and whether they adhere to any data boundaries may be configured below. A common use case is to set `CHUNK_BY_COLUMNS` to `id` so that there is a 1-to-many relationship between documents and TextUnits instead of a many-to-many.\n*   `Entity` - An entity extracted from a TextUnit. These represent people, places, events, or some other entity-model that you provide.\n*   `Relationship` - A relationship between two entities. These are generated from the covariates.\n*   `Covariate` - Extracted claim information, which contains statements about entities which may be time-bound.\n*   `Community Report` - Once entities are generated, we perform hierarchical community detection on them and generate reports for each community in this hierarchy.\n*   `Node` - This table contains layout information for rendered graph-views of the Entities and Documents which have been embedded and clustered.\n\nThe Default Configuration Workflow\n----------------------------------\n\nLet's take a look at how the default-configuration workflow transforms text documents into the _GraphRAG Knowledge Model_. This page gives a general overview of the major steps in this process. To fully configure this workflow, check out the [configuration](/graphrag/posts/config/overview/)\n documentation.\n\nPhase 1: Compose TextUnits\n--------------------------\n\nThe first phase of the default-configuration workflow is to transform input documents into _TextUnits_. A _TextUnit_ is a chunk of text that is used for our graph extraction techniques. They are also used as source-references by extracted knowledge items in order to empower breadcrumbs and provenance by concepts back to their original source tex.\n\nThe chunk size (counted in tokens), is user-configurable. By default this is set to 300 tokens, although we've had positive experience with 1200-token chunks using a single \"glean\" step. (A \"glean\" step is a follow-on extraction). Larger chunks result in lower-fidelity output and less meaningful reference texts; however, using larger chunks can result in much faster processing time.\n\nThe group-by configuration is also user-configurable. By default, we align our chunks to document boundaries, meaning that there is a strict 1-to-many relationship between Documents and TextUnits. In rare cases, this can be turned into a many-to-many relationship. This is useful when the documents are very short and we need several of them to compose a meaningful analysis unit (e.g. Tweets or a chat log)\n\nEach of these text-units are text-embedded and passed into the next phase of the pipeline.\n\n\\---\ntitle: Documents into Text Chunks\n---\nflowchart LR\n    doc1\\[Document 1\\] --> tu1\\[TextUnit 1\\]\n    doc1 --> tu2\\[TextUnit 2\\]\n    doc2\\[Document 2\\] --> tu3\\[TextUnit 3\\]\n    doc2 --> tu4\\[TextUnit 4\\]\n\nPhase 2: Graph Extraction\n-------------------------\n\nIn this phase, we analyze each text unit and extract our graph primitives: _Entities_, _Relationships_, and _Claims_. Entities and Relationships are extracted at once in our _entity\\_extract_ verb, and claims are extracted in our _claim\\_extract_ verb. Results are then combined and passed into following phases of the pipeline.\n\n\\---\ntitle: Graph Extraction\n---\nflowchart LR\n    tu\\[TextUnit\\] --> ge\\[Graph Extraction\\] --> gs\\[Graph Summarization\\] --> er\\[Entity Resolution\\]\n    tu --> ce\\[Claim Extraction\\]\n\n### Entity & Relationship Extraction\n\nIn this first step of graph extraction, we process each text-unit in order to extract entities and relationships out of the raw text using the LLM. The output of this step is a subgraph-per-TextUnit containing a list of **entities** with a _name_, _type_, and _description_, and a list of **relationships** with a _source_, _target_, and _description_.\n\nThese subgraphs are merged together - any entities with the same _name_ and _type_ are merged by creating an array of their descriptions. Similarly, any relationships with the same _source_ and _target_ are merged by creating an array of their descriptions.\n\n### Entity & Relationship Summarization\n\nNow that we have a graph of entities and relationships, each with a list of descriptions, we can summarize these lists into a single description per entity and relationship. This is done by asking the LLM for a short summary that captures all of the distinct information from each description. This allows all of our entities and relationships to have a single concise description.\n\n### Entity Resolution (Not Enabled by Default)\n\nThe final step of graph extraction is to resolve any entities that represent the same real-world entity but but have different names. Since this is done via LLM, and we don't want to lose information, we want to take a conservative, non-destructive approach to this.\n\nOur current implementation of Entity Resolution, however, is destructive. It will provide the LLM with a series of entities and ask it to determine which ones should be merged. Those entities are then merged together into a single entity and their relationships are updated.\n\nWe are currently exploring other entity resolution techniques. In the near future, entity resolution will be executed by creating an edge between entity variants indicating that the entities have been resolved by the indexing engine. This will allow for end-users to undo indexing-side resolutions, and add their own non-destructive resolutions using a similar process.\n\n### Claim Extraction & Emission\n\nFinally, as an independent workflow, we extract claims from the source TextUnits. These claims represent positive factual statements with an evaluated status and time-bounds. These are emitted as a primary artifact called **Covariates**.\n\nPhase 3: Graph Augmentation\n---------------------------\n\nNow that we have a usable graph of entities and relationships, we want to understand their community structure and augment the graph with additional information. This is done in two steps: _Community Detection_ and _Graph Embedding_. These give us explicit (communities) and implicit (embeddings) ways of understanding the topological structure of our graph.\n\n\\---\ntitle: Graph Augmentation\n---\nflowchart LR\n    cd\\[Leiden Hierarchical Community Detection\\] --> ge\\[Node2Vec Graph Embedding\\] --> ag\\[Graph Table Emission\\]\n\n### Community Detection\n\nIn this step, we generate a hierarchy of entity communities using the Hierarchical Leiden Algorithm. This method will apply a recursive community-clustering to our graph until we reach a community-size threshold. This will allow us to understand the community structure of our graph and provide a way to navigate and summarize the graph at different levels of granularity.\n\n### Graph Embedding\n\nIn this step, we generate a vector representation of our graph using the Node2Vec algorithm. This will allow us to understand the implicit structure of our graph and provide an additional vector-space in which to search for related concepts during our query phase.\n\n### Graph Tables Emission\n\nOnce our graph augmentation steps are complete, the final **Entities** and **Relationships** tables are emitted after their text fields are text-embedded.\n\nPhase 4: Community Summarization\n--------------------------------\n\n\\---\ntitle: Community Summarization\n---\nflowchart LR\n    sc\\[Generate Community Reports\\] --> ss\\[Summarize Community Reports\\] --> ce\\[Community Embedding\\] --> co\\[Community Tables Emission\\]\n\nAt this point, we have a functional graph of entities and relationships, a hierarchy of communities for the entities, as well as node2vec embeddings.\n\nNow we want to build on the communities data and generate reports for each community. This gives us a high-level understanding of the graph at several points of graph granularity. For example, if community A is the top-level community, we'll get a report about the entire graph. If the community is lower-level, we'll get a report about a local cluster.\n\n### Generate Community Reports\n\nIn this step, we generate a summary of each community using the LLM. This will allow us to understand the distinct information contained within each community and provide a scoped understanding of the graph, from either a high-level or a low-level perspective. These reports contain an executive overview and reference the key entities, relationships, and claims within the community sub-structure.\n\n### Summarize Community Reports\n\nIn this step, each _community report_ is then summarized via the LLM for shorthand use.\n\n### Community Embedding\n\nIn this step, we generate a vector representation of our communities by generating text embeddings of the community report, the community report summary, and the title of the community report.\n\n### Community Tables Emission\n\nAt this point, some bookkeeping work is performed and we emit the **Communities** and **CommunityReports** tables.\n\nPhase 5: Document Processing\n----------------------------\n\nIn this phase of the workflow, we create the _Documents_ table for the knowledge model.\n\n\\---\ntitle: Document Processing\n---\nflowchart LR\n    aug\\[Augment\\] --> dp\\[Link to TextUnits\\] --> de\\[Avg. Embedding\\] --> dg\\[Document Table Emission\\]\n\n### Augment with Columns (CSV Only)\n\nIf the workflow is operating on CSV data, you may configure your workflow to add additional fields to Documents output. These fields should exist on the incoming CSV tables. Details about configuring this can be found in the [configuration documentation](/graphrag/posts/config/overview/)\n.\n\n### Link to TextUnits\n\nIn this step, we link each document to the text-units that were created in the first phase. This allows us to understand which documents are related to which text-units and vice-versa.\n\n### Document Embedding\n\nIn this step, we generate a vector representation of our documents using an average embedding of document slices. We re-chunk documents without overlapping chunks, and then generate an embedding for each chunk. We create an average of these chunks weighted by token-count and use this as the document embedding. This will allow us to understand the implicit relationship between documents, and will help us generate a network representation of our documents.\n\n### Documents Table Emission\n\nAt this point, we can emit the **Documents** table into the knowledge Model.\n\nPhase 6: Network Visualization\n------------------------------\n\nIn this phase of the workflow, we perform some steps to support network visualization of our high-dimensional vector spaces within our existing graphs. At this point there are two logical graphs at play: the _Entity-Relationship_ graph and the _Document_ graph.\n\n\\---\ntitle: Network Visualization Workflows\n---\nflowchart LR\n    nv\\[Umap Documents\\] --> ne\\[Umap Entities\\] --> ng\\[Nodes Table Emission\\]\n\nFor each of the logical graphs, we perform a UMAP dimensionality reduction to generate a 2D representation of the graph. This will allow us to visualize the graph in a 2D space and understand the relationships between the nodes in the graph. The UMAP embeddings are then emitted as a table of _Nodes_. The rows of this table include a discriminator indicating whether the node is a document or an entity, and the UMAP coordinates.",
    "markdown": "Indexing Dataflow\n=================\n\nThe GraphRAG Knowledge Model\n----------------------------\n\nThe knowledge model is a specification for data outputs that conform to our data-model definition. You can find these definitions in the python/graphrag/graphrag/model folder within the GraphRAG repository. The following entity types are provided. The fields here represent the fields that are text-embedded by default.\n\n*   `Document` - An input document into the system. These either represent individual rows in a CSV or individual .txt file.\n*   `TextUnit` - A chunk of text to analyze. The size of these chunks, their overlap, and whether they adhere to any data boundaries may be configured below. A common use case is to set `CHUNK_BY_COLUMNS` to `id` so that there is a 1-to-many relationship between documents and TextUnits instead of a many-to-many.\n*   `Entity` - An entity extracted from a TextUnit. These represent people, places, events, or some other entity-model that you provide.\n*   `Relationship` - A relationship between two entities. These are generated from the covariates.\n*   `Covariate` - Extracted claim information, which contains statements about entities which may be time-bound.\n*   `Community Report` - Once entities are generated, we perform hierarchical community detection on them and generate reports for each community in this hierarchy.\n*   `Node` - This table contains layout information for rendered graph-views of the Entities and Documents which have been embedded and clustered.\n\nThe Default Configuration Workflow\n----------------------------------\n\nLet's take a look at how the default-configuration workflow transforms text documents into the _GraphRAG Knowledge Model_. This page gives a general overview of the major steps in this process. To fully configure this workflow, check out the [configuration](/graphrag/posts/config/overview/)\n documentation.\n\nPhase 1: Compose TextUnits\n--------------------------\n\nThe first phase of the default-configuration workflow is to transform input documents into _TextUnits_. A _TextUnit_ is a chunk of text that is used for our graph extraction techniques. They are also used as source-references by extracted knowledge items in order to empower breadcrumbs and provenance by concepts back to their original source tex.\n\nThe chunk size (counted in tokens), is user-configurable. By default this is set to 300 tokens, although we've had positive experience with 1200-token chunks using a single \"glean\" step. (A \"glean\" step is a follow-on extraction). Larger chunks result in lower-fidelity output and less meaningful reference texts; however, using larger chunks can result in much faster processing time.\n\nThe group-by configuration is also user-configurable. By default, we align our chunks to document boundaries, meaning that there is a strict 1-to-many relationship between Documents and TextUnits. In rare cases, this can be turned into a many-to-many relationship. This is useful when the documents are very short and we need several of them to compose a meaningful analysis unit (e.g. Tweets or a chat log)\n\nEach of these text-units are text-embedded and passed into the next phase of the pipeline.\n\n\\---\ntitle: Documents into Text Chunks\n---\nflowchart LR\n    doc1\\[Document 1\\] --> tu1\\[TextUnit 1\\]\n    doc1 --> tu2\\[TextUnit 2\\]\n    doc2\\[Document 2\\] --> tu3\\[TextUnit 3\\]\n    doc2 --> tu4\\[TextUnit 4\\]\n\nPhase 2: Graph Extraction\n-------------------------\n\nIn this phase, we analyze each text unit and extract our graph primitives: _Entities_, _Relationships_, and _Claims_. Entities and Relationships are extracted at once in our _entity\\_extract_ verb, and claims are extracted in our _claim\\_extract_ verb. Results are then combined and passed into following phases of the pipeline.\n\n\\---\ntitle: Graph Extraction\n---\nflowchart LR\n    tu\\[TextUnit\\] --> ge\\[Graph Extraction\\] --> gs\\[Graph Summarization\\] --> er\\[Entity Resolution\\]\n    tu --> ce\\[Claim Extraction\\]\n\n### Entity & Relationship Extraction\n\nIn this first step of graph extraction, we process each text-unit in order to extract entities and relationships out of the raw text using the LLM. The output of this step is a subgraph-per-TextUnit containing a list of **entities** with a _name_, _type_, and _description_, and a list of **relationships** with a _source_, _target_, and _description_.\n\nThese subgraphs are merged together - any entities with the same _name_ and _type_ are merged by creating an array of their descriptions. Similarly, any relationships with the same _source_ and _target_ are merged by creating an array of their descriptions.\n\n### Entity & Relationship Summarization\n\nNow that we have a graph of entities and relationships, each with a list of descriptions, we can summarize these lists into a single description per entity and relationship. This is done by asking the LLM for a short summary that captures all of the distinct information from each description. This allows all of our entities and relationships to have a single concise description.\n\n### Entity Resolution (Not Enabled by Default)\n\nThe final step of graph extraction is to resolve any entities that represent the same real-world entity but but have different names. Since this is done via LLM, and we don't want to lose information, we want to take a conservative, non-destructive approach to this.\n\nOur current implementation of Entity Resolution, however, is destructive. It will provide the LLM with a series of entities and ask it to determine which ones should be merged. Those entities are then merged together into a single entity and their relationships are updated.\n\nWe are currently exploring other entity resolution techniques. In the near future, entity resolution will be executed by creating an edge between entity variants indicating that the entities have been resolved by the indexing engine. This will allow for end-users to undo indexing-side resolutions, and add their own non-destructive resolutions using a similar process.\n\n### Claim Extraction & Emission\n\nFinally, as an independent workflow, we extract claims from the source TextUnits. These claims represent positive factual statements with an evaluated status and time-bounds. These are emitted as a primary artifact called **Covariates**.\n\nPhase 3: Graph Augmentation\n---------------------------\n\nNow that we have a usable graph of entities and relationships, we want to understand their community structure and augment the graph with additional information. This is done in two steps: _Community Detection_ and _Graph Embedding_. These give us explicit (communities) and implicit (embeddings) ways of understanding the topological structure of our graph.\n\n\\---\ntitle: Graph Augmentation\n---\nflowchart LR\n    cd\\[Leiden Hierarchical Community Detection\\] --> ge\\[Node2Vec Graph Embedding\\] --> ag\\[Graph Table Emission\\]\n\n### Community Detection\n\nIn this step, we generate a hierarchy of entity communities using the Hierarchical Leiden Algorithm. This method will apply a recursive community-clustering to our graph until we reach a community-size threshold. This will allow us to understand the community structure of our graph and provide a way to navigate and summarize the graph at different levels of granularity.\n\n### Graph Embedding\n\nIn this step, we generate a vector representation of our graph using the Node2Vec algorithm. This will allow us to understand the implicit structure of our graph and provide an additional vector-space in which to search for related concepts during our query phase.\n\n### Graph Tables Emission\n\nOnce our graph augmentation steps are complete, the final **Entities** and **Relationships** tables are emitted after their text fields are text-embedded.\n\nPhase 4: Community Summarization\n--------------------------------\n\n\\---\ntitle: Community Summarization\n---\nflowchart LR\n    sc\\[Generate Community Reports\\] --> ss\\[Summarize Community Reports\\] --> ce\\[Community Embedding\\] --> co\\[Community Tables Emission\\]\n\nAt this point, we have a functional graph of entities and relationships, a hierarchy of communities for the entities, as well as node2vec embeddings.\n\nNow we want to build on the communities data and generate reports for each community. This gives us a high-level understanding of the graph at several points of graph granularity. For example, if community A is the top-level community, we'll get a report about the entire graph. If the community is lower-level, we'll get a report about a local cluster.\n\n### Generate Community Reports\n\nIn this step, we generate a summary of each community using the LLM. This will allow us to understand the distinct information contained within each community and provide a scoped understanding of the graph, from either a high-level or a low-level perspective. These reports contain an executive overview and reference the key entities, relationships, and claims within the community sub-structure.\n\n### Summarize Community Reports\n\nIn this step, each _community report_ is then summarized via the LLM for shorthand use.\n\n### Community Embedding\n\nIn this step, we generate a vector representation of our communities by generating text embeddings of the community report, the community report summary, and the title of the community report.\n\n### Community Tables Emission\n\nAt this point, some bookkeeping work is performed and we emit the **Communities** and **CommunityReports** tables.\n\nPhase 5: Document Processing\n----------------------------\n\nIn this phase of the workflow, we create the _Documents_ table for the knowledge model.\n\n\\---\ntitle: Document Processing\n---\nflowchart LR\n    aug\\[Augment\\] --> dp\\[Link to TextUnits\\] --> de\\[Avg. Embedding\\] --> dg\\[Document Table Emission\\]\n\n### Augment with Columns (CSV Only)\n\nIf the workflow is operating on CSV data, you may configure your workflow to add additional fields to Documents output. These fields should exist on the incoming CSV tables. Details about configuring this can be found in the [configuration documentation](/graphrag/posts/config/overview/)\n.\n\n### Link to TextUnits\n\nIn this step, we link each document to the text-units that were created in the first phase. This allows us to understand which documents are related to which text-units and vice-versa.\n\n### Document Embedding\n\nIn this step, we generate a vector representation of our documents using an average embedding of document slices. We re-chunk documents without overlapping chunks, and then generate an embedding for each chunk. We create an average of these chunks weighted by token-count and use this as the document embedding. This will allow us to understand the implicit relationship between documents, and will help us generate a network representation of our documents.\n\n### Documents Table Emission\n\nAt this point, we can emit the **Documents** table into the knowledge Model.\n\nPhase 6: Network Visualization\n------------------------------\n\nIn this phase of the workflow, we perform some steps to support network visualization of our high-dimensional vector spaces within our existing graphs. At this point there are two logical graphs at play: the _Entity-Relationship_ graph and the _Document_ graph.\n\n\\---\ntitle: Network Visualization Workflows\n---\nflowchart LR\n    nv\\[Umap Documents\\] --> ne\\[Umap Entities\\] --> ng\\[Nodes Table Emission\\]\n\nFor each of the logical graphs, we perform a UMAP dimensionality reduction to generate a 2D representation of the graph. This will allow us to visualize the graph in a 2D space and understand the relationships between the nodes in the graph. The UMAP embeddings are then emitted as a table of _Nodes_. The rows of this table include a discriminator indicating whether the node is a document or an entity, and the UMAP coordinates.",
    "metadata": {
      "title": "Indexing Dataflow",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag",
      "https://github.com/Azure-Samples/graphrag-accelerator"
    ]
  },
  {
    "content": "Indexer CLI\n===========\n\nThe GraphRAG indexer CLI allows for no-code usage of the GraphRAG Indexer.\n\n    python -m graphrag.index --verbose --root </workspace/project/root> --config <custom_config.yml>\n    --resume <timestamp> --reporter <rich|print|none> --emit json,csv,parquet\n    --nocache\n\nCLI Arguments\n-------------\n\n*   `--verbose` - Adds extra logging information during the run.\n*   `--root <data-project-dir>` - the data root directory. This should contain an `input` directory with the input data, and an `.env` file with environment variables. These are described below.\n*   `--init` - This will initialize the data project directory at the specified `root` with bootstrap configuration and prompt-overrides.\n*   `--resume <output-timestamp>` - if specified, the pipeline will attempt to resume a prior run. The parquet files from the prior run will be loaded into the system as inputs, and the workflows that generated those files will be skipped. The input value should be the timestamped output folder, e.g. \"20240105-143721\".\n*   `--config <config_file.yml>` - This will opt-out of the Default Configuration mode and execute a custom configuration. If this is used, then none of the environment-variables below will apply.\n*   `--reporter <reporter>` - This will specify the progress reporter to use. The default is `rich`. Valid values are `rich`, `print`, and `none`.\n*   `--emit <types>` - This specifies the table output formats the pipeline should emit. The default is `parquet`. Valid values are `parquet`, `csv`, and `json`, comma-separated.\n*   `--nocache` - This will disable the caching mechanism. This is useful for debugging and development, but should not be used in production.",
    "markdown": "Indexer CLI\n===========\n\nThe GraphRAG indexer CLI allows for no-code usage of the GraphRAG Indexer.\n\n    python -m graphrag.index --verbose --root </workspace/project/root> --config <custom_config.yml>\n    --resume <timestamp> --reporter <rich|print|none> --emit json,csv,parquet\n    --nocache\n\nCLI Arguments\n-------------\n\n*   `--verbose` - Adds extra logging information during the run.\n*   `--root <data-project-dir>` - the data root directory. This should contain an `input` directory with the input data, and an `.env` file with environment variables. These are described below.\n*   `--init` - This will initialize the data project directory at the specified `root` with bootstrap configuration and prompt-overrides.\n*   `--resume <output-timestamp>` - if specified, the pipeline will attempt to resume a prior run. The parquet files from the prior run will be loaded into the system as inputs, and the workflows that generated those files will be skipped. The input value should be the timestamped output folder, e.g. \"20240105-143721\".\n*   `--config <config_file.yml>` - This will opt-out of the Default Configuration mode and execute a custom configuration. If this is used, then none of the environment-variables below will apply.\n*   `--reporter <reporter>` - This will specify the progress reporter to use. The default is `rich`. Valid values are `rich`, `print`, and `none`.\n*   `--emit <types>` - This specifies the table output formats the pipeline should emit. The default is `parquet`. Valid values are `parquet`, `csv`, and `json`, comma-separated.\n*   `--nocache` - This will disable the caching mechanism. This is useful for debugging and development, but should not be used in production.",
    "metadata": {
      "title": "Indexer CLI",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag",
      "https://github.com/Azure-Samples/graphrag-accelerator"
    ]
  },
  {
    "content": "Configuring GraphRAG Indexing\n=============================\n\nThe GraphRAG system is highly configurable. This page provides an overview of the configuration options available for the GraphRAG indexing engine.\n\nDefault Configuration Mode\n--------------------------\n\nThe default configuration mode is the simplest way to get started with the GraphRAG system. It is designed to work out-of-the-box with minimal configuration. The primary configuration sections for the Indexing Engine pipelines are described below. The main ways to set up GraphRAG in Default Configuration mode are via:\n\n*   [Init command](/graphrag/posts/config/init)\n     (recommended)\n*   [Purely using environment variables](/graphrag/posts/config/env_vars)\n    \n*   [Using JSON or YAML for deeper control](/graphrag/posts/config/json_yaml)\n    \n\nCustom Configuration Mode\n-------------------------\n\nCustom configuration mode is an advanced use-case. Most users will want to use the Default Configuration instead. The primary configuration sections for Indexing Engine pipelines are described below. Details about how to use custom configuration are available in the [Custom Configuration Mode](/graphrag/posts/config/custom)\n documentation.",
    "markdown": "Configuring GraphRAG Indexing\n=============================\n\nThe GraphRAG system is highly configurable. This page provides an overview of the configuration options available for the GraphRAG indexing engine.\n\nDefault Configuration Mode\n--------------------------\n\nThe default configuration mode is the simplest way to get started with the GraphRAG system. It is designed to work out-of-the-box with minimal configuration. The primary configuration sections for the Indexing Engine pipelines are described below. The main ways to set up GraphRAG in Default Configuration mode are via:\n\n*   [Init command](/graphrag/posts/config/init)\n     (recommended)\n*   [Purely using environment variables](/graphrag/posts/config/env_vars)\n    \n*   [Using JSON or YAML for deeper control](/graphrag/posts/config/json_yaml)\n    \n\nCustom Configuration Mode\n-------------------------\n\nCustom configuration mode is an advanced use-case. Most users will want to use the Default Configuration instead. The primary configuration sections for Indexing Engine pipelines are described below. Details about how to use custom configuration are available in the [Custom Configuration Mode](/graphrag/posts/config/custom)\n documentation.",
    "metadata": {
      "title": "Configuring GraphRAG Indexing",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/config/overview/",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag",
      "https://github.com/Azure-Samples/graphrag-accelerator"
    ]
  },
  {
    "content": "Configuring GraphRAG Indexing\n=============================\n\nTo start using GraphRAG, you need to configure the system. The `init` command is the easiest way to get started. It will create a `.env` and `settings.yaml` files in the specified directory with the necessary configuration settings. It will also output the default LLM prompts used by GraphRAG.\n\nUsage\n-----\n\n    python -m graphrag.index [--init] [--root PATH]\n\nOptions\n-------\n\n*   `--init` - Initialize the directory with the necessary configuration files.\n*   `--root PATH` - The root directory to initialize. Default is the current directory.\n\nExample\n-------\n\n    python -m graphrag.index --init --root ./ragtest\n\nOutput\n------\n\nThe `init` command will create the following files in the specified directory:\n\n*   `settings.yaml` - The configuration settings file. This file contains the configuration settings for GraphRAG.\n*   `.env` - The environment variables file. These are referenced in the `settings.yaml` file.\n*   `prompts/` - The LLM prompts folder. This contains the default prompts used by GraphRAG, you can modify them or run the [Auto Prompt Tuning](/graphrag/posts/prompt_tuning/auto_prompt_tuning)\n     command to generate new prompts adapted to your data.\n\nNext Steps\n----------\n\nAfter initializing your workspace, you can either run the [Prompt Tuning](/graphrag/posts/prompt_tuning/auto_prompt_tuning)\n command to adapt the prompts to your data or even start running the [Indexing Pipeline](/graphrag/posts/index/overview)\n to index your data. For more information on configuring GraphRAG, see the [Configuration](/graphrag/posts/config/overview)\n documentation.",
    "markdown": "Configuring GraphRAG Indexing\n=============================\n\nTo start using GraphRAG, you need to configure the system. The `init` command is the easiest way to get started. It will create a `.env` and `settings.yaml` files in the specified directory with the necessary configuration settings. It will also output the default LLM prompts used by GraphRAG.\n\nUsage\n-----\n\n    python -m graphrag.index [--init] [--root PATH]\n\nOptions\n-------\n\n*   `--init` - Initialize the directory with the necessary configuration files.\n*   `--root PATH` - The root directory to initialize. Default is the current directory.\n\nExample\n-------\n\n    python -m graphrag.index --init --root ./ragtest\n\nOutput\n------\n\nThe `init` command will create the following files in the specified directory:\n\n*   `settings.yaml` - The configuration settings file. This file contains the configuration settings for GraphRAG.\n*   `.env` - The environment variables file. These are referenced in the `settings.yaml` file.\n*   `prompts/` - The LLM prompts folder. This contains the default prompts used by GraphRAG, you can modify them or run the [Auto Prompt Tuning](/graphrag/posts/prompt_tuning/auto_prompt_tuning)\n     command to generate new prompts adapted to your data.\n\nNext Steps\n----------\n\nAfter initializing your workspace, you can either run the [Prompt Tuning](/graphrag/posts/prompt_tuning/auto_prompt_tuning)\n command to adapt the prompts to your data or even start running the [Indexing Pipeline](/graphrag/posts/index/overview)\n to index your data. For more information on configuring GraphRAG, see the [Configuration](/graphrag/posts/config/overview)\n documentation.",
    "metadata": {
      "title": "Configuring GraphRAG Indexing",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/config/init",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning",
      "https://microsoft.github.io/graphrag/posts/index/overview",
      "https://microsoft.github.io/graphrag/posts/config/overview",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag",
      "https://github.com/Azure-Samples/graphrag-accelerator"
    ]
  },
  {
    "content": "Default Configuration Mode (using Env Vars)\n===========================================\n\nText-Embeddings Customization\n-----------------------------\n\nBy default, the GraphRAG indexer will only emit embeddings required for our query methods. However, the model has embeddings defined for all plaintext fields, and these can be generated by setting the `GRAPHRAG_EMBEDDING_TARGET` environment variable to `all`.\n\nIf the embedding target is `all`, and you want to only embed a subset of these fields, you may specify which embeddings to skip using the `GRAPHRAG_EMBEDDING_SKIP` argument described below.\n\n### Embedded Fields\n\n*   `text_unit.text`\n*   `document.raw_content`\n*   `entity.name`\n*   `entity.description`\n*   `relationship.description`\n*   `community.title`\n*   `community.summary`\n*   `community.full_content`\n\nInput Data\n----------\n\nOur pipeline can ingest .csv or .txt data from an input folder. These files can be nested within subfolders. To configure how input data is handled, what fields are mapped over, and how timestamps are parsed, look for configuration values starting with `GRAPHRAG_INPUT_` below. In general, CSV-based data provides the most customizeability. Each CSV should at least contain a `text` field (which can be mapped with environment variables), but it's helpful if they also have `title`, `timestamp`, and `source` fields. Additional fields can be included as well, which will land as extra fields on the `Document` table.\n\nBase LLM Settings\n-----------------\n\nThese are the primary settings for configuring LLM connectivity.\n\n| Parameter | Required? | Description | Type | Default Value |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_API_KEY` | **Yes for OpenAI. Optional for AOAI** | The API key. (Note: \\`OPENAI\\_API\\_KEY is also used as a fallback). If not defined when using AOAI, managed identity will be used. | `str` | `None` |\n| `GRAPHRAG_API_BASE` | **For AOAI** | The API Base URL | `str` | `None` |\n| `GRAPHRAG_API_VERSION` | **For AOAI** | The AOAI API version. | `str` | `None` |\n| `GRAPHRAG_API_ORGANIZATION` |     | The AOAI organization. | `str` | `None` |\n| `GRAPHRAG_API_PROXY` |     | The AOAI proxy. | `str` | `None` |\n\nText Generation Settings\n------------------------\n\nThese settings control the text generation model used by the pipeline. Any settings with a fallback will use the base LLM settings, if available.\n\n| Parameter | Required? | Description | Type | Default Value |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_LLM_TYPE` | **For AOAI** | The LLM operation type. Either `openai_chat` or `azure_openai_chat` | `str` | `openai_chat` |\n| `GRAPHRAG_LLM_DEPLOYMENT_NAME` | **For AOAI** | The AOAI model deployment name. | `str` | `None` |\n| `GRAPHRAG_LLM_API_KEY` | Yes (uses fallback) | The API key. If not defined when using AOAI, managed identity will be used. | `str` | `None` |\n| `GRAPHRAG_LLM_API_BASE` | For AOAI (uses fallback) | The API Base URL | `str` | `None` |\n| `GRAPHRAG_LLM_API_VERSION` | For AOAI (uses fallback) | The AOAI API version. | `str` | `None` |\n| `GRAPHRAG_LLM_API_ORGANIZATION` | For AOAI (uses fallback) | The AOAI organization. | `str` | `None` |\n| `GRAPHRAG_LLM_API_PROXY` |     | The AOAI proxy. | `str` | `None` |\n| `GRAPHRAG_LLM_MODEL` |     | The LLM model. | `str` | `gpt-4-turbo-preview` |\n| `GRAPHRAG_LLM_MAX_TOKENS` |     | The maximum number of tokens. | `int` | `4000` |\n| `GRAPHRAG_LLM_REQUEST_TIMEOUT` |     | The maximum number of seconds to wait for a response from the chat client. | `int` | `180` |\n| `GRAPHRAG_LLM_MODEL_SUPPORTS_JSON` |     | Indicates whether the given model supports JSON output mode. `True` to enable. | `str` | `None` |\n| `GRAPHRAG_LLM_THREAD_COUNT` |     | The number of threads to use for LLM parallelization. | `int` | 50  |\n| `GRAPHRAG_LLM_THREAD_STAGGER` |     | The time to wait (in seconds) between starting each thread. | `float` | 0.3 |\n| `GRAPHRAG_LLM_CONCURRENT_REQUESTS` |     | The number of concurrent requests to allow for the embedding client. | `int` | 25  |\n| `GRAPHRAG_LLM_TOKENS_PER_MINUTE` |     | The number of tokens per minute to allow for the LLM client. 0 = Bypass | `int` | 0   |\n| `GRAPHRAG_LLM_REQUESTS_PER_MINUTE` |     | The number of requests per minute to allow for the LLM client. 0 = Bypass | `int` | 0   |\n| `GRAPHRAG_LLM_MAX_RETRIES` |     | The maximum number of retries to attempt when a request fails. | `int` | 10  |\n| `GRAPHRAG_LLM_MAX_RETRY_WAIT` |     | The maximum number of seconds to wait between retries. | `int` | 10  |\n| `GRAPHRAG_LLM_SLEEP_ON_RATE_LIMIT_RECOMMENDATION` |     | Whether to sleep on rate limit recommendation. (Azure Only) | `bool` | `True` |\n| `GRAPHRAG_LLM_TEMPERATURE` |     | The temperature to use generation. | `float` | 0   |\n| `GRAPHRAG_LLM_TOP_P` |     | The top\\_p to use for sampling. | `float` | 1   |\n| `GRAPHRAG_LLM_N` |     | The number of responses to generate. | `int` | 1   |\n\nText Embedding Settings\n-----------------------\n\nThese settings control the text embedding model used by the pipeline. Any settings with a fallback will use the base LLM settings, if available.\n\n| Parameter | Required ? | Description | Type | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_EMBEDDING_TYPE` | **For AOAI** | The embedding client to use. Either `openai_embedding` or `azure_openai_embedding` | `str` | `openai_embedding` |\n| `GRAPHRAG_EMBEDDING_DEPLOYMENT_NAME` | **For AOAI** | The AOAI deployment name. | `str` | `None` |\n| `GRAPHRAG_EMBEDDING_API_KEY` | Yes (uses fallback) | The API key to use for the embedding client. If not defined when using AOAI, managed identity will be used. | `str` | `None` |\n| `GRAPHRAG_EMBEDDING_API_BASE` | For AOAI (uses fallback) | The API base URL. | `str` | `None` |\n| `GRAPHRAG_EMBEDDING_API_VERSION` | For AOAI (uses fallback) | The AOAI API version to use for the embedding client. | `str` | `None` |\n| `GRAPHRAG_EMBEDDING_API_ORGANIZATION` | For AOAI (uses fallback) | The AOAI organization to use for the embedding client. | `str` | `None` |\n| `GRAPHRAG_EMBEDDING_API_PROXY` |     | The AOAI proxy to use for the embedding client. | `str` | `None` |\n| `GRAPHRAG_EMBEDDING_MODEL` |     | The model to use for the embedding client. | `str` | `text-embedding-3-small` |\n| `GRAPHRAG_EMBEDDING_BATCH_SIZE` |     | The number of texts to embed at once. [(Azure limit is 16)](https://learn.microsoft.com/en-us/azure/ai-ce) | `int` | 16  |\n| `GRAPHRAG_EMBEDDING_BATCH_MAX_TOKENS` |     | The maximum tokens per batch [(Azure limit is 8191)](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference) | `int` | 8191 |\n| `GRAPHRAG_EMBEDDING_TARGET` |     | The target fields to embed. Either `required` or `all`. | `str` | `required` |\n| `GRAPHRAG_EMBEDDING_SKIP` |     | A comma-separated list of fields to skip embeddings for . (e.g. 'relationship.description') | `str` | `None` |\n| `GRAPHRAG_EMBEDDING_THREAD_COUNT` |     | The number of threads to use for parallelization for embeddings. | `int` |     |\n| `GRAPHRAG_EMBEDDING_THREAD_STAGGER` |     | The time to wait (in seconds) between starting each thread for embeddings. | `float` | 50  |\n| `GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS` |     | The number of concurrent requests to allow for the embedding client. | `int` | 25  |\n| `GRAPHRAG_EMBEDDING_TOKENS_PER_MINUTE` |     | The number of tokens per minute to allow for the embedding client. 0 = Bypass | `int` | 0   |\n| `GRAPHRAG_EMBEDDING_REQUESTS_PER_MINUTE` |     | The number of requests per minute to allow for the embedding client. 0 = Bypass | `int` | 0   |\n| `GRAPHRAG_EMBEDDING_MAX_RETRIES` |     | The maximum number of retries to attempt when a request fails. | `int` | 10  |\n| `GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT` |     | The maximum number of seconds to wait between retries. | `int` | 10  |\n| `GRAPHRAG_EMBEDDING_TARGET` |     | The target fields to embed. Either `required` or `all`. | `str` | `required` |\n| `GRAPHRAG_EMBEDDING_SLEEP_ON_RATE_LIMIT_RECOMMENDATION` |     | Whether to sleep on rate limit recommendation. (Azure Only) | `bool` | `True` |\n\nInput Settings\n--------------\n\nThese settings control the data input used by the pipeline. Any settings with a fallback will use the base LLM settings, if available.\n\n### Plaintext Input Data (`GRAPHRAG_INPUT_FILE_TYPE`\\=text)\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_INPUT_FILE_PATTERN` | The file pattern regexp to use when reading input files from the input directory. | `str` | optional | `.*\\.txt$` |\n\n### CSV Input Data (`GRAPHRAG_INPUT_FILE_TYPE`\\=csv)\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_INPUT_TYPE` | The input storage type to use when reading files. (`file` or `blob`) | `str` | optional | `file` |\n| `GRAPHRAG_INPUT_FILE_PATTERN` | The file pattern regexp to use when reading input files from the input directory. | `str` | optional | `.*\\.txt$` |\n| `GRAPHRAG_INPUT_SOURCE_COLUMN` | The 'source' column to use when reading CSV input files. | `str` | optional | `source` |\n| `GRAPHRAG_INPUT_TIMESTAMP_COLUMN` | The 'timestamp' column to use when reading CSV input files. | `str` | optional | `None` |\n| `GRAPHRAG_INPUT_TIMESTAMP_FORMAT` | The timestamp format to use when parsing timestamps in the timestamp column. | `str` | optional | `None` |\n| `GRAPHRAG_INPUT_TEXT_COLUMN` | The 'text' column to use when reading CSV input files. | `str` | optional | `text` |\n| `GRAPHRAG_INPUT_DOCUMENT_ATTRIBUTE_COLUMNS` | A list of CSV columns, comma-separated, to incorporate as document fields. | `str` | optional | `id` |\n| `GRAPHRAG_INPUT_TITLE_COLUMN` | The 'title' column to use when reading CSV input files. | `str` | optional | `title` |\n| `GRAPHRAG_INPUT_STORAGE_ACCOUNT_BLOB_URL` | The Azure Storage blob endpoint to use when in `blob` mode and using managed identity. Will have the format `https://<storage_account_name>.blob.core.windows.net` | `str` | optional | `None` |\n| `GRAPHRAG_INPUT_CONNECTION_STRING` | The connection string to use when reading CSV input files from Azure Blob Storage. | `str` | optional | `None` |\n| `GRAPHRAG_INPUT_CONTAINER_NAME` | The container name to use when reading CSV input files from Azure Blob Storage. | `str` | optional | `None` |\n| `GRAPHRAG_INPUT_BASE_DIR` | The base directory to read input files from. | `str` | optional | `None` |\n\nData Mapping Settings\n---------------------\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_INPUT_FILE_TYPE` | The type of input data, `csv` or `text` | `str` | optional | `text` |\n| `GRAPHRAG_INPUT_ENCODING` | The encoding to apply when reading CSV/text input files. | `str` | optional | `utf-8` |\n\nData Chunking\n-------------\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_CHUNK_SIZE` | The chunk size in tokens for text-chunk analysis windows. | `str` | optional | 1200 |\n| `GRAPHRAG_CHUNK_OVERLAP` | The chunk overlap in tokens for text-chunk analysis windows. | `str` | optional | 100 |\n| `GRAPHRAG_CHUNK_BY_COLUMNS` | A comma-separated list of document attributes to groupby when performing TextUnit chunking. | `str` | optional | `id` |\n| `GRAPHRAG_CHUNK_ENCODING_MODEL` | The encoding model to use for chunking. | `str` | optional | The top-level encoding model. |\n\nPrompting Overrides\n-------------------\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE` | The path (relative to the root) of an entity extraction prompt template text file. | `str` | optional | `None` |\n| `GRAPHRAG_ENTITY_EXTRACTION_MAX_GLEANINGS` | The maximum number of redrives (gleanings) to invoke when extracting entities in a loop. | `int` | optional | 1   |\n| `GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES` | A comma-separated list of entity types to extract. | `str` | optional | `organization,person,event,geo` |\n| `GRAPHRAG_ENTITY_EXTRACTION_ENCODING_MODEL` | The encoding model to use for entity extraction. | `str` | optional | The top-level encoding model. |\n| `GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE` | The path (relative to the root) of an description summarization prompt template text file. | `str` | optional | `None` |\n| `GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH` | The maximum number of tokens to generate per description summarization. | `int` | optional | 500 |\n| `GRAPHRAG_CLAIM_EXTRACTION_ENABLED` | Whether claim extraction is enabled for this pipeline. | `bool` | optional | `False` |\n| `GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION` | The claim\\_description prompting argument to utilize. | `string` | optional | \"Any claims or facts that could be relevant to threat analysis.\" |\n| `GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE` | The claim extraction prompt to utilize. | `string` | optional | `None` |\n| `GRAPHRAG_CLAIM_EXTRACTION_MAX_GLEANINGS` | The maximum number of redrives (gleanings) to invoke when extracting claims in a loop. | `int` | optional | 1   |\n| `GRAPHRAG_CLAIM_EXTRACTION_ENCODING_MODEL` | The encoding model to use for claim extraction. | `str` | optional | The top-level encoding model |\n| `GRAPHRAG_COMMUNITY_REPORTS_PROMPT_FILE` | The community reports extraction prompt to utilize. | `string` | optional | `None` |\n| `GRAPHRAG_COMMUNITY_REPORTS_MAX_LENGTH` | The maximum number of tokens to generate per community reports. | `int` | optional | 1500 |\n\nStorage\n-------\n\nThis section controls the storage mechanism used by the pipeline used for emitting output tables.\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_STORAGE_TYPE` | The type of reporter to use. Options are `file`, `memory`, or `blob` | `str` | optional | `file` |\n| `GRAPHRAG_STORAGE_STORAGE_ACCOUNT_BLOB_URL` | The Azure Storage blob endpoint to use when in `blob` mode and using managed identity. Will have the format `https://<storage_account_name>.blob.core.windows.net` | `str` | optional | None |\n| `GRAPHRAG_STORAGE_CONNECTION_STRING` | The Azure Storage connection string to use when in `blob` mode. | `str` | optional | None |\n| `GRAPHRAG_STORAGE_CONTAINER_NAME` | The Azure Storage container name to use when in `blob` mode. | `str` | optional | None |\n| `GRAPHRAG_STORAGE_BASE_DIR` | The base path to data outputs outputs. | `str` | optional | None |\n\nCache\n-----\n\nThis section controls the cache mechanism used by the pipeline. This is used to cache LLM invocation results.\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_CACHE_TYPE` | The type of cache to use. Options are `file`, `memory`, `none` or `blob` | `str` | optional | `file` |\n| `GRAPHRAG_CACHE_STORAGE_ACCOUNT_BLOB_URL` | The Azure Storage blob endpoint to use when in `blob` mode and using managed identity. Will have the format `https://<storage_account_name>.blob.core.windows.net` | `str` | optional | None |\n| `GRAPHRAG_CACHE_CONNECTION_STRING` | The Azure Storage connection string to use when in `blob` mode. | `str` | optional | None |\n| `GRAPHRAG_CACHE_CONTAINER_NAME` | The Azure Storage container name to use when in `blob` mode. | `str` | optional | None |\n| `GRAPHRAG_CACHE_BASE_DIR` | The base path to the reporting outputs. | `str` | optional | None |\n\nReporting\n---------\n\nThis section controls the reporting mechanism used by the pipeline, for common events and error messages. The default is to write reports to a file in the output directory. However, you can also choose to write reports to the console or to an Azure Blob Storage container.\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_REPORTING_TYPE` | The type of reporter to use. Options are `file`, `console`, or `blob` | `str` | optional | `file` |\n| `GRAPHRAG_REPORTING_STORAGE_ACCOUNT_BLOB_URL` | The Azure Storage blob endpoint to use when in `blob` mode and using managed identity. Will have the format `https://<storage_account_name>.blob.core.windows.net` | `str` | optional | None |\n| `GRAPHRAG_REPORTING_CONNECTION_STRING` | The Azure Storage connection string to use when in `blob` mode. | `str` | optional | None |\n| `GRAPHRAG_REPORTING_CONTAINER_NAME` | The Azure Storage container name to use when in `blob` mode. | `str` | optional | None |\n| `GRAPHRAG_REPORTING_BASE_DIR` | The base path to the reporting outputs. | `str` | optional | None |\n\nNode2Vec Parameters\n-------------------\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_NODE2VEC_ENABLED` | Whether to enable Node2Vec | `bool` | optional | False |\n| `GRAPHRAG_NODE2VEC_NUM_WALKS` | The Node2Vec number of walks to perform | `int` | optional | 10  |\n| `GRAPHRAG_NODE2VEC_WALK_LENGTH` | The Node2Vec walk length | `int` | optional | 40  |\n| `GRAPHRAG_NODE2VEC_WINDOW_SIZE` | The Node2Vec window size | `int` | optional | 2   |\n| `GRAPHRAG_NODE2VEC_ITERATIONS` | The number of iterations to run node2vec | `int` | optional | 3   |\n| `GRAPHRAG_NODE2VEC_RANDOM_SEED` | The random seed to use for node2vec | `int` | optional | 597832 |\n\nData Snapshotting\n-----------------\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_SNAPSHOT_GRAPHML` | Whether to enable GraphML snapshots. | `bool` | optional | False |\n| `GRAPHRAG_SNAPSHOT_RAW_ENTITIES` | Whether to enable raw entity snapshots. | `bool` | optional | False |\n| `GRAPHRAG_SNAPSHOT_TOP_LEVEL_NODES` | Whether to enable top-level node snapshots. | `bool` | optional | False |\n\nMiscellaneous Settings\n======================\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_ASYNC_MODE` | Which async mode to use. Either `asyncio` or `threaded`. | `str` | optional | `asyncio` |\n| `GRAPHRAG_ENCODING_MODEL` | The text encoding model, used in tiktoken, to encode text. | `str` | optional | `cl100k_base` |\n| `GRAPHRAG_MAX_CLUSTER_SIZE` | The maximum number of entities to include in a single Leiden cluster. | `int` | optional | 10  |\n| `GRAPHRAG_SKIP_WORKFLOWS` | A comma-separated list of workflow names to skip. | `str` | optional | `None` |\n| `GRAPHRAG_UMAP_ENABLED` | Whether to enable UMAP layouts | `bool` | optional | False |",
    "markdown": "Default Configuration Mode (using Env Vars)\n===========================================\n\nText-Embeddings Customization\n-----------------------------\n\nBy default, the GraphRAG indexer will only emit embeddings required for our query methods. However, the model has embeddings defined for all plaintext fields, and these can be generated by setting the `GRAPHRAG_EMBEDDING_TARGET` environment variable to `all`.\n\nIf the embedding target is `all`, and you want to only embed a subset of these fields, you may specify which embeddings to skip using the `GRAPHRAG_EMBEDDING_SKIP` argument described below.\n\n### Embedded Fields\n\n*   `text_unit.text`\n*   `document.raw_content`\n*   `entity.name`\n*   `entity.description`\n*   `relationship.description`\n*   `community.title`\n*   `community.summary`\n*   `community.full_content`\n\nInput Data\n----------\n\nOur pipeline can ingest .csv or .txt data from an input folder. These files can be nested within subfolders. To configure how input data is handled, what fields are mapped over, and how timestamps are parsed, look for configuration values starting with `GRAPHRAG_INPUT_` below. In general, CSV-based data provides the most customizeability. Each CSV should at least contain a `text` field (which can be mapped with environment variables), but it's helpful if they also have `title`, `timestamp`, and `source` fields. Additional fields can be included as well, which will land as extra fields on the `Document` table.\n\nBase LLM Settings\n-----------------\n\nThese are the primary settings for configuring LLM connectivity.\n\n| Parameter | Required? | Description | Type | Default Value |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_API_KEY` | **Yes for OpenAI. Optional for AOAI** | The API key. (Note: \\`OPENAI\\_API\\_KEY is also used as a fallback). If not defined when using AOAI, managed identity will be used. | `str` | `None` |\n| `GRAPHRAG_API_BASE` | **For AOAI** | The API Base URL | `str` | `None` |\n| `GRAPHRAG_API_VERSION` | **For AOAI** | The AOAI API version. | `str` | `None` |\n| `GRAPHRAG_API_ORGANIZATION` |     | The AOAI organization. | `str` | `None` |\n| `GRAPHRAG_API_PROXY` |     | The AOAI proxy. | `str` | `None` |\n\nText Generation Settings\n------------------------\n\nThese settings control the text generation model used by the pipeline. Any settings with a fallback will use the base LLM settings, if available.\n\n| Parameter | Required? | Description | Type | Default Value |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_LLM_TYPE` | **For AOAI** | The LLM operation type. Either `openai_chat` or `azure_openai_chat` | `str` | `openai_chat` |\n| `GRAPHRAG_LLM_DEPLOYMENT_NAME` | **For AOAI** | The AOAI model deployment name. | `str` | `None` |\n| `GRAPHRAG_LLM_API_KEY` | Yes (uses fallback) | The API key. If not defined when using AOAI, managed identity will be used. | `str` | `None` |\n| `GRAPHRAG_LLM_API_BASE` | For AOAI (uses fallback) | The API Base URL | `str` | `None` |\n| `GRAPHRAG_LLM_API_VERSION` | For AOAI (uses fallback) | The AOAI API version. | `str` | `None` |\n| `GRAPHRAG_LLM_API_ORGANIZATION` | For AOAI (uses fallback) | The AOAI organization. | `str` | `None` |\n| `GRAPHRAG_LLM_API_PROXY` |     | The AOAI proxy. | `str` | `None` |\n| `GRAPHRAG_LLM_MODEL` |     | The LLM model. | `str` | `gpt-4-turbo-preview` |\n| `GRAPHRAG_LLM_MAX_TOKENS` |     | The maximum number of tokens. | `int` | `4000` |\n| `GRAPHRAG_LLM_REQUEST_TIMEOUT` |     | The maximum number of seconds to wait for a response from the chat client. | `int` | `180` |\n| `GRAPHRAG_LLM_MODEL_SUPPORTS_JSON` |     | Indicates whether the given model supports JSON output mode. `True` to enable. | `str` | `None` |\n| `GRAPHRAG_LLM_THREAD_COUNT` |     | The number of threads to use for LLM parallelization. | `int` | 50  |\n| `GRAPHRAG_LLM_THREAD_STAGGER` |     | The time to wait (in seconds) between starting each thread. | `float` | 0.3 |\n| `GRAPHRAG_LLM_CONCURRENT_REQUESTS` |     | The number of concurrent requests to allow for the embedding client. | `int` | 25  |\n| `GRAPHRAG_LLM_TOKENS_PER_MINUTE` |     | The number of tokens per minute to allow for the LLM client. 0 = Bypass | `int` | 0   |\n| `GRAPHRAG_LLM_REQUESTS_PER_MINUTE` |     | The number of requests per minute to allow for the LLM client. 0 = Bypass | `int` | 0   |\n| `GRAPHRAG_LLM_MAX_RETRIES` |     | The maximum number of retries to attempt when a request fails. | `int` | 10  |\n| `GRAPHRAG_LLM_MAX_RETRY_WAIT` |     | The maximum number of seconds to wait between retries. | `int` | 10  |\n| `GRAPHRAG_LLM_SLEEP_ON_RATE_LIMIT_RECOMMENDATION` |     | Whether to sleep on rate limit recommendation. (Azure Only) | `bool` | `True` |\n| `GRAPHRAG_LLM_TEMPERATURE` |     | The temperature to use generation. | `float` | 0   |\n| `GRAPHRAG_LLM_TOP_P` |     | The top\\_p to use for sampling. | `float` | 1   |\n| `GRAPHRAG_LLM_N` |     | The number of responses to generate. | `int` | 1   |\n\nText Embedding Settings\n-----------------------\n\nThese settings control the text embedding model used by the pipeline. Any settings with a fallback will use the base LLM settings, if available.\n\n| Parameter | Required ? | Description | Type | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_EMBEDDING_TYPE` | **For AOAI** | The embedding client to use. Either `openai_embedding` or `azure_openai_embedding` | `str` | `openai_embedding` |\n| `GRAPHRAG_EMBEDDING_DEPLOYMENT_NAME` | **For AOAI** | The AOAI deployment name. | `str` | `None` |\n| `GRAPHRAG_EMBEDDING_API_KEY` | Yes (uses fallback) | The API key to use for the embedding client. If not defined when using AOAI, managed identity will be used. | `str` | `None` |\n| `GRAPHRAG_EMBEDDING_API_BASE` | For AOAI (uses fallback) | The API base URL. | `str` | `None` |\n| `GRAPHRAG_EMBEDDING_API_VERSION` | For AOAI (uses fallback) | The AOAI API version to use for the embedding client. | `str` | `None` |\n| `GRAPHRAG_EMBEDDING_API_ORGANIZATION` | For AOAI (uses fallback) | The AOAI organization to use for the embedding client. | `str` | `None` |\n| `GRAPHRAG_EMBEDDING_API_PROXY` |     | The AOAI proxy to use for the embedding client. | `str` | `None` |\n| `GRAPHRAG_EMBEDDING_MODEL` |     | The model to use for the embedding client. | `str` | `text-embedding-3-small` |\n| `GRAPHRAG_EMBEDDING_BATCH_SIZE` |     | The number of texts to embed at once. [(Azure limit is 16)](https://learn.microsoft.com/en-us/azure/ai-ce) | `int` | 16  |\n| `GRAPHRAG_EMBEDDING_BATCH_MAX_TOKENS` |     | The maximum tokens per batch [(Azure limit is 8191)](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference) | `int` | 8191 |\n| `GRAPHRAG_EMBEDDING_TARGET` |     | The target fields to embed. Either `required` or `all`. | `str` | `required` |\n| `GRAPHRAG_EMBEDDING_SKIP` |     | A comma-separated list of fields to skip embeddings for . (e.g. 'relationship.description') | `str` | `None` |\n| `GRAPHRAG_EMBEDDING_THREAD_COUNT` |     | The number of threads to use for parallelization for embeddings. | `int` |     |\n| `GRAPHRAG_EMBEDDING_THREAD_STAGGER` |     | The time to wait (in seconds) between starting each thread for embeddings. | `float` | 50  |\n| `GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS` |     | The number of concurrent requests to allow for the embedding client. | `int` | 25  |\n| `GRAPHRAG_EMBEDDING_TOKENS_PER_MINUTE` |     | The number of tokens per minute to allow for the embedding client. 0 = Bypass | `int` | 0   |\n| `GRAPHRAG_EMBEDDING_REQUESTS_PER_MINUTE` |     | The number of requests per minute to allow for the embedding client. 0 = Bypass | `int` | 0   |\n| `GRAPHRAG_EMBEDDING_MAX_RETRIES` |     | The maximum number of retries to attempt when a request fails. | `int` | 10  |\n| `GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT` |     | The maximum number of seconds to wait between retries. | `int` | 10  |\n| `GRAPHRAG_EMBEDDING_TARGET` |     | The target fields to embed. Either `required` or `all`. | `str` | `required` |\n| `GRAPHRAG_EMBEDDING_SLEEP_ON_RATE_LIMIT_RECOMMENDATION` |     | Whether to sleep on rate limit recommendation. (Azure Only) | `bool` | `True` |\n\nInput Settings\n--------------\n\nThese settings control the data input used by the pipeline. Any settings with a fallback will use the base LLM settings, if available.\n\n### Plaintext Input Data (`GRAPHRAG_INPUT_FILE_TYPE`\\=text)\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_INPUT_FILE_PATTERN` | The file pattern regexp to use when reading input files from the input directory. | `str` | optional | `.*\\.txt$` |\n\n### CSV Input Data (`GRAPHRAG_INPUT_FILE_TYPE`\\=csv)\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_INPUT_TYPE` | The input storage type to use when reading files. (`file` or `blob`) | `str` | optional | `file` |\n| `GRAPHRAG_INPUT_FILE_PATTERN` | The file pattern regexp to use when reading input files from the input directory. | `str` | optional | `.*\\.txt$` |\n| `GRAPHRAG_INPUT_SOURCE_COLUMN` | The 'source' column to use when reading CSV input files. | `str` | optional | `source` |\n| `GRAPHRAG_INPUT_TIMESTAMP_COLUMN` | The 'timestamp' column to use when reading CSV input files. | `str` | optional | `None` |\n| `GRAPHRAG_INPUT_TIMESTAMP_FORMAT` | The timestamp format to use when parsing timestamps in the timestamp column. | `str` | optional | `None` |\n| `GRAPHRAG_INPUT_TEXT_COLUMN` | The 'text' column to use when reading CSV input files. | `str` | optional | `text` |\n| `GRAPHRAG_INPUT_DOCUMENT_ATTRIBUTE_COLUMNS` | A list of CSV columns, comma-separated, to incorporate as document fields. | `str` | optional | `id` |\n| `GRAPHRAG_INPUT_TITLE_COLUMN` | The 'title' column to use when reading CSV input files. | `str` | optional | `title` |\n| `GRAPHRAG_INPUT_STORAGE_ACCOUNT_BLOB_URL` | The Azure Storage blob endpoint to use when in `blob` mode and using managed identity. Will have the format `https://<storage_account_name>.blob.core.windows.net` | `str` | optional | `None` |\n| `GRAPHRAG_INPUT_CONNECTION_STRING` | The connection string to use when reading CSV input files from Azure Blob Storage. | `str` | optional | `None` |\n| `GRAPHRAG_INPUT_CONTAINER_NAME` | The container name to use when reading CSV input files from Azure Blob Storage. | `str` | optional | `None` |\n| `GRAPHRAG_INPUT_BASE_DIR` | The base directory to read input files from. | `str` | optional | `None` |\n\nData Mapping Settings\n---------------------\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_INPUT_FILE_TYPE` | The type of input data, `csv` or `text` | `str` | optional | `text` |\n| `GRAPHRAG_INPUT_ENCODING` | The encoding to apply when reading CSV/text input files. | `str` | optional | `utf-8` |\n\nData Chunking\n-------------\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_CHUNK_SIZE` | The chunk size in tokens for text-chunk analysis windows. | `str` | optional | 1200 |\n| `GRAPHRAG_CHUNK_OVERLAP` | The chunk overlap in tokens for text-chunk analysis windows. | `str` | optional | 100 |\n| `GRAPHRAG_CHUNK_BY_COLUMNS` | A comma-separated list of document attributes to groupby when performing TextUnit chunking. | `str` | optional | `id` |\n| `GRAPHRAG_CHUNK_ENCODING_MODEL` | The encoding model to use for chunking. | `str` | optional | The top-level encoding model. |\n\nPrompting Overrides\n-------------------\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE` | The path (relative to the root) of an entity extraction prompt template text file. | `str` | optional | `None` |\n| `GRAPHRAG_ENTITY_EXTRACTION_MAX_GLEANINGS` | The maximum number of redrives (gleanings) to invoke when extracting entities in a loop. | `int` | optional | 1   |\n| `GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES` | A comma-separated list of entity types to extract. | `str` | optional | `organization,person,event,geo` |\n| `GRAPHRAG_ENTITY_EXTRACTION_ENCODING_MODEL` | The encoding model to use for entity extraction. | `str` | optional | The top-level encoding model. |\n| `GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE` | The path (relative to the root) of an description summarization prompt template text file. | `str` | optional | `None` |\n| `GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH` | The maximum number of tokens to generate per description summarization. | `int` | optional | 500 |\n| `GRAPHRAG_CLAIM_EXTRACTION_ENABLED` | Whether claim extraction is enabled for this pipeline. | `bool` | optional | `False` |\n| `GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION` | The claim\\_description prompting argument to utilize. | `string` | optional | \"Any claims or facts that could be relevant to threat analysis.\" |\n| `GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE` | The claim extraction prompt to utilize. | `string` | optional | `None` |\n| `GRAPHRAG_CLAIM_EXTRACTION_MAX_GLEANINGS` | The maximum number of redrives (gleanings) to invoke when extracting claims in a loop. | `int` | optional | 1   |\n| `GRAPHRAG_CLAIM_EXTRACTION_ENCODING_MODEL` | The encoding model to use for claim extraction. | `str` | optional | The top-level encoding model |\n| `GRAPHRAG_COMMUNITY_REPORTS_PROMPT_FILE` | The community reports extraction prompt to utilize. | `string` | optional | `None` |\n| `GRAPHRAG_COMMUNITY_REPORTS_MAX_LENGTH` | The maximum number of tokens to generate per community reports. | `int` | optional | 1500 |\n\nStorage\n-------\n\nThis section controls the storage mechanism used by the pipeline used for emitting output tables.\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_STORAGE_TYPE` | The type of reporter to use. Options are `file`, `memory`, or `blob` | `str` | optional | `file` |\n| `GRAPHRAG_STORAGE_STORAGE_ACCOUNT_BLOB_URL` | The Azure Storage blob endpoint to use when in `blob` mode and using managed identity. Will have the format `https://<storage_account_name>.blob.core.windows.net` | `str` | optional | None |\n| `GRAPHRAG_STORAGE_CONNECTION_STRING` | The Azure Storage connection string to use when in `blob` mode. | `str` | optional | None |\n| `GRAPHRAG_STORAGE_CONTAINER_NAME` | The Azure Storage container name to use when in `blob` mode. | `str` | optional | None |\n| `GRAPHRAG_STORAGE_BASE_DIR` | The base path to data outputs outputs. | `str` | optional | None |\n\nCache\n-----\n\nThis section controls the cache mechanism used by the pipeline. This is used to cache LLM invocation results.\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_CACHE_TYPE` | The type of cache to use. Options are `file`, `memory`, `none` or `blob` | `str` | optional | `file` |\n| `GRAPHRAG_CACHE_STORAGE_ACCOUNT_BLOB_URL` | The Azure Storage blob endpoint to use when in `blob` mode and using managed identity. Will have the format `https://<storage_account_name>.blob.core.windows.net` | `str` | optional | None |\n| `GRAPHRAG_CACHE_CONNECTION_STRING` | The Azure Storage connection string to use when in `blob` mode. | `str` | optional | None |\n| `GRAPHRAG_CACHE_CONTAINER_NAME` | The Azure Storage container name to use when in `blob` mode. | `str` | optional | None |\n| `GRAPHRAG_CACHE_BASE_DIR` | The base path to the reporting outputs. | `str` | optional | None |\n\nReporting\n---------\n\nThis section controls the reporting mechanism used by the pipeline, for common events and error messages. The default is to write reports to a file in the output directory. However, you can also choose to write reports to the console or to an Azure Blob Storage container.\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_REPORTING_TYPE` | The type of reporter to use. Options are `file`, `console`, or `blob` | `str` | optional | `file` |\n| `GRAPHRAG_REPORTING_STORAGE_ACCOUNT_BLOB_URL` | The Azure Storage blob endpoint to use when in `blob` mode and using managed identity. Will have the format `https://<storage_account_name>.blob.core.windows.net` | `str` | optional | None |\n| `GRAPHRAG_REPORTING_CONNECTION_STRING` | The Azure Storage connection string to use when in `blob` mode. | `str` | optional | None |\n| `GRAPHRAG_REPORTING_CONTAINER_NAME` | The Azure Storage container name to use when in `blob` mode. | `str` | optional | None |\n| `GRAPHRAG_REPORTING_BASE_DIR` | The base path to the reporting outputs. | `str` | optional | None |\n\nNode2Vec Parameters\n-------------------\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_NODE2VEC_ENABLED` | Whether to enable Node2Vec | `bool` | optional | False |\n| `GRAPHRAG_NODE2VEC_NUM_WALKS` | The Node2Vec number of walks to perform | `int` | optional | 10  |\n| `GRAPHRAG_NODE2VEC_WALK_LENGTH` | The Node2Vec walk length | `int` | optional | 40  |\n| `GRAPHRAG_NODE2VEC_WINDOW_SIZE` | The Node2Vec window size | `int` | optional | 2   |\n| `GRAPHRAG_NODE2VEC_ITERATIONS` | The number of iterations to run node2vec | `int` | optional | 3   |\n| `GRAPHRAG_NODE2VEC_RANDOM_SEED` | The random seed to use for node2vec | `int` | optional | 597832 |\n\nData Snapshotting\n-----------------\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_SNAPSHOT_GRAPHML` | Whether to enable GraphML snapshots. | `bool` | optional | False |\n| `GRAPHRAG_SNAPSHOT_RAW_ENTITIES` | Whether to enable raw entity snapshots. | `bool` | optional | False |\n| `GRAPHRAG_SNAPSHOT_TOP_LEVEL_NODES` | Whether to enable top-level node snapshots. | `bool` | optional | False |\n\nMiscellaneous Settings\n======================\n\n| Parameter | Description | Type | Required or Optional | Default |\n| --- | --- | --- | --- | --- |\n| `GRAPHRAG_ASYNC_MODE` | Which async mode to use. Either `asyncio` or `threaded`. | `str` | optional | `asyncio` |\n| `GRAPHRAG_ENCODING_MODEL` | The text encoding model, used in tiktoken, to encode text. | `str` | optional | `cl100k_base` |\n| `GRAPHRAG_MAX_CLUSTER_SIZE` | The maximum number of entities to include in a single Leiden cluster. | `int` | optional | 10  |\n| `GRAPHRAG_SKIP_WORKFLOWS` | A comma-separated list of workflow names to skip. | `str` | optional | `None` |\n| `GRAPHRAG_UMAP_ENABLED` | Whether to enable UMAP layouts | `bool` | optional | False |",
    "metadata": {
      "title": "Default Configuration Mode (using Env Vars)",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "https://learn.microsoft.com/en-us/azure/ai-ce",
      "https://learn.microsoft.com/en-us/azure/ai-services/openai/reference",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag",
      "https://github.com/Azure-Samples/graphrag-accelerator"
    ]
  },
  {
    "content": "Default Configuration Mode (using JSON/YAML)\n============================================\n\nThe default configuration mode may be configured by using a `config.json` or `config.yml` file in the data project root. If a `.env` file is present along with this config file, then it will be loaded, and the environment variables defined therein will be available for token replacements in your configuration document using `${ENV_VAR}` syntax.\n\nFor example:\n\n    # .env\n    API_KEY=some_api_key\n    \n    # config.json\n    {\n        \"llm\": {\n            \"api_key\": \"${API_KEY}\"\n        }\n    }\n    \n\nConfig Sections\n===============\n\ninput\n-----\n\n### Fields\n\n*   `type` **file|blob** - The input type to use. Default=`file`\n*   `file_type` **text|csv** - The type of input data to load. Either `text` or `csv`. Default is `text`\n*   `file_encoding` **str** - The encoding of the input file. Default is `utf-8`\n*   `file_pattern` **str** - A regex to match input files. Default is `.*\\.csv$` if in csv mode and `.*\\.txt$` if in text mode.\n*   `source_column` **str** - (CSV Mode Only) The source column name.\n*   `timestamp_column` **str** - (CSV Mode Only) The timestamp column name.\n*   `timestamp_format` **str** - (CSV Mode Only) The source format.\n*   `text_column` **str** - (CSV Mode Only) The text column name.\n*   `title_column` **str** - (CSV Mode Only) The title column name.\n*   `document_attribute_columns` **list\\[str\\]** - (CSV Mode Only) The additional document attributes to include.\n*   `connection_string` **str** - (blob only) The Azure Storage connection string.\n*   `container_name` **str** - (blob only) The Azure Storage container name.\n*   `base_dir` **str** - The base directory to read input from, relative to the root.\n*   `storage_account_blob_url` **str** - The storage account blob URL to use.\n\nllm\n---\n\nThis is the base LLM configuration section. Other steps may override this configuration with their own LLM configuration.\n\n### Fields\n\n*   `api_key` **str** - The OpenAI API key to use.\n*   `type` **openai\\_chat|azure\\_openai\\_chat|openai\\_embedding|azure\\_openai\\_embedding** - The type of LLM to use.\n*   `model` **str** - The model name.\n*   `max_tokens` **int** - The maximum number of output tokens.\n*   `request_timeout` **float** - The per-request timeout.\n*   `api_base` **str** - The API base url to use.\n*   `api_version` **str** - The API version\n*   `organization` **str** - The client organization.\n*   `proxy` **str** - The proxy URL to use.\n*   `cognitive_services_endpoint` **str** - The url endpoint for cognitive services.\n*   `deployment_name` **str** - The deployment name to use (Azure).\n*   `model_supports_json` **bool** - Whether the model supports JSON-mode output.\n*   `tokens_per_minute` **int** - Set a leaky-bucket throttle on tokens-per-minute.\n*   `requests_per_minute` **int** - Set a leaky-bucket throttle on requests-per-minute.\n*   `max_retries` **int** - The maximum number of retries to use.\n*   `max_retry_wait` **float** - The maximum backoff time.\n*   `sleep_on_rate_limit_recommendation` **bool** - Whether to adhere to sleep recommendations (Azure).\n*   `concurrent_requests` **int** The number of open requests to allow at once.\n*   `temperature` **float** - The temperature to use.\n*   `top_p` **float** - The top-p value to use.\n*   `n` **int** - The number of completions to generate.\n\nparallelization\n---------------\n\n### Fields\n\n*   `stagger` **float** - The threading stagger value.\n*   `num_threads` **int** - The maximum number of work threads.\n\nasync\\_mode\n-----------\n\n**asyncio|threaded** The async mode to use. Either `asyncio` or \\`threaded.\n\nembeddings\n----------\n\n### Fields\n\n*   `llm` (see LLM top-level config)\n*   `parallelization` (see Parallelization top-level config)\n*   `async_mode` (see Async Mode top-level config)\n*   `batch_size` **int** - The maximum batch size to use.\n*   `batch_max_tokens` **int** - The maximum batch #-tokens.\n*   `target` **required|all** - Determines which set of embeddings to emit.\n*   `skip` **list\\[str\\]** - Which embeddings to skip.\n*   `strategy` **dict** - Fully override the text-embedding strategy.\n\nchunks\n------\n\n### Fields\n\n*   `size` **int** - The max chunk size in tokens.\n*   `overlap` **int** - The chunk overlap in tokens.\n*   `group_by_columns` **list\\[str\\]** - group documents by fields before chunking.\n*   `encoding_model` **str** - The text encoding model to use. Default is to use the top-level encoding model.\n*   `strategy` **dict** - Fully override the chunking strategy.\n\ncache\n-----\n\n### Fields\n\n*   `type` **file|memory|none|blob** - The cache type to use. Default=`file`\n*   `connection_string` **str** - (blob only) The Azure Storage connection string.\n*   `container_name` **str** - (blob only) The Azure Storage container name.\n*   `base_dir` **str** - The base directory to write cache to, relative to the root.\n*   `storage_account_blob_url` **str** - The storage account blob URL to use.\n\nstorage\n-------\n\n### Fields\n\n*   `type` **file|memory|blob** - The storage type to use. Default=`file`\n*   `connection_string` **str** - (blob only) The Azure Storage connection string.\n*   `container_name` **str** - (blob only) The Azure Storage container name.\n*   `base_dir` **str** - The base directory to write reports to, relative to the root.\n*   `storage_account_blob_url` **str** - The storage account blob URL to use.\n\nreporting\n---------\n\n### Fields\n\n*   `type` **file|console|blob** - The reporting type to use. Default=`file`\n*   `connection_string` **str** - (blob only) The Azure Storage connection string.\n*   `container_name` **str** - (blob only) The Azure Storage container name.\n*   `base_dir` **str** - The base directory to write reports to, relative to the root.\n*   `storage_account_blob_url` **str** - The storage account blob URL to use.\n\nentity\\_extraction\n------------------\n\n### Fields\n\n*   `llm` (see LLM top-level config)\n*   `parallelization` (see Parallelization top-level config)\n*   `async_mode` (see Async Mode top-level config)\n*   `prompt` **str** - The prompt file to use.\n*   `entity_types` **list\\[str\\]** - The entity types to identify.\n*   `max_gleanings` **int** - The maximum number of gleaning cycles to use.\n*   `encoding_model` **str** - The text encoding model to use. By default, this will use the top-level encoding model.\n*   `strategy` **dict** - Fully override the entity extraction strategy.\n\nsummarize\\_descriptions\n-----------------------\n\n### Fields\n\n*   `llm` (see LLM top-level config)\n*   `parallelization` (see Parallelization top-level config)\n*   `async_mode` (see Async Mode top-level config)\n*   `prompt` **str** - The prompt file to use.\n*   `max_length` **int** - The maximum number of output tokens per summarization.\n*   `strategy` **dict** - Fully override the summarize description strategy.\n\nclaim\\_extraction\n-----------------\n\n### Fields\n\n*   `enabled` **bool** - Whether to enable claim extraction. default=False\n*   `llm` (see LLM top-level config)\n*   `parallelization` (see Parallelization top-level config)\n*   `async_mode` (see Async Mode top-level config)\n*   `prompt` **str** - The prompt file to use.\n*   `description` **str** - Describes the types of claims we want to extract.\n*   `max_gleanings` **int** - The maximum number of gleaning cycles to use.\n*   `encoding_model` **str** - The text encoding model to use. By default, this will use the top-level encoding model.\n*   `strategy` **dict** - Fully override the claim extraction strategy.\n\ncommunity\\_reports\n------------------\n\n### Fields\n\n*   `llm` (see LLM top-level config)\n*   `parallelization` (see Parallelization top-level config)\n*   `async_mode` (see Async Mode top-level config)\n*   `prompt` **str** - The prompt file to use.\n*   `max_length` **int** - The maximum number of output tokens per report.\n*   `max_input_length` **int** - The maximum number of input tokens to use when generating reports.\n*   `strategy` **dict** - Fully override the community reports strategy.\n\ncluster\\_graph\n--------------\n\n### Fields\n\n*   `max_cluster_size` **int** - The maximum cluster size to emit.\n*   `strategy` **dict** - Fully override the cluster\\_graph strategy.\n\nembed\\_graph\n------------\n\n### Fields\n\n*   `enabled` **bool** - Whether to enable graph embeddings.\n*   `num_walks` **int** - The node2vec number of walks.\n*   `walk_length` **int** - The node2vec walk length.\n*   `window_size` **int** - The node2vec window size.\n*   `iterations` **int** - The node2vec number of iterations.\n*   `random_seed` **int** - The node2vec random seed.\n*   `strategy` **dict** - Fully override the embed graph strategy.\n\numap\n----\n\n### Fields\n\n*   `enabled` **bool** - Whether to enable UMAP layouts.\n\nsnapshots\n---------\n\n### Fields\n\n*   `graphml` **bool** - Emit graphml snapshots.\n*   `raw_entities` **bool** - Emit raw entity snapshots.\n*   `top_level_nodes` **bool** - Emit top-level-node snapshots.\n\nencoding\\_model\n---------------\n\n**str** - The text encoding model to use. Default is `cl100k_base`.\n\nskip\\_workflows\n---------------\n\n**list\\[str\\]** - Which workflow names to skip.",
    "markdown": "Default Configuration Mode (using JSON/YAML)\n============================================\n\nThe default configuration mode may be configured by using a `config.json` or `config.yml` file in the data project root. If a `.env` file is present along with this config file, then it will be loaded, and the environment variables defined therein will be available for token replacements in your configuration document using `${ENV_VAR}` syntax.\n\nFor example:\n\n    # .env\n    API_KEY=some_api_key\n    \n    # config.json\n    {\n        \"llm\": {\n            \"api_key\": \"${API_KEY}\"\n        }\n    }\n    \n\nConfig Sections\n===============\n\ninput\n-----\n\n### Fields\n\n*   `type` **file|blob** - The input type to use. Default=`file`\n*   `file_type` **text|csv** - The type of input data to load. Either `text` or `csv`. Default is `text`\n*   `file_encoding` **str** - The encoding of the input file. Default is `utf-8`\n*   `file_pattern` **str** - A regex to match input files. Default is `.*\\.csv$` if in csv mode and `.*\\.txt$` if in text mode.\n*   `source_column` **str** - (CSV Mode Only) The source column name.\n*   `timestamp_column` **str** - (CSV Mode Only) The timestamp column name.\n*   `timestamp_format` **str** - (CSV Mode Only) The source format.\n*   `text_column` **str** - (CSV Mode Only) The text column name.\n*   `title_column` **str** - (CSV Mode Only) The title column name.\n*   `document_attribute_columns` **list\\[str\\]** - (CSV Mode Only) The additional document attributes to include.\n*   `connection_string` **str** - (blob only) The Azure Storage connection string.\n*   `container_name` **str** - (blob only) The Azure Storage container name.\n*   `base_dir` **str** - The base directory to read input from, relative to the root.\n*   `storage_account_blob_url` **str** - The storage account blob URL to use.\n\nllm\n---\n\nThis is the base LLM configuration section. Other steps may override this configuration with their own LLM configuration.\n\n### Fields\n\n*   `api_key` **str** - The OpenAI API key to use.\n*   `type` **openai\\_chat|azure\\_openai\\_chat|openai\\_embedding|azure\\_openai\\_embedding** - The type of LLM to use.\n*   `model` **str** - The model name.\n*   `max_tokens` **int** - The maximum number of output tokens.\n*   `request_timeout` **float** - The per-request timeout.\n*   `api_base` **str** - The API base url to use.\n*   `api_version` **str** - The API version\n*   `organization` **str** - The client organization.\n*   `proxy` **str** - The proxy URL to use.\n*   `cognitive_services_endpoint` **str** - The url endpoint for cognitive services.\n*   `deployment_name` **str** - The deployment name to use (Azure).\n*   `model_supports_json` **bool** - Whether the model supports JSON-mode output.\n*   `tokens_per_minute` **int** - Set a leaky-bucket throttle on tokens-per-minute.\n*   `requests_per_minute` **int** - Set a leaky-bucket throttle on requests-per-minute.\n*   `max_retries` **int** - The maximum number of retries to use.\n*   `max_retry_wait` **float** - The maximum backoff time.\n*   `sleep_on_rate_limit_recommendation` **bool** - Whether to adhere to sleep recommendations (Azure).\n*   `concurrent_requests` **int** The number of open requests to allow at once.\n*   `temperature` **float** - The temperature to use.\n*   `top_p` **float** - The top-p value to use.\n*   `n` **int** - The number of completions to generate.\n\nparallelization\n---------------\n\n### Fields\n\n*   `stagger` **float** - The threading stagger value.\n*   `num_threads` **int** - The maximum number of work threads.\n\nasync\\_mode\n-----------\n\n**asyncio|threaded** The async mode to use. Either `asyncio` or \\`threaded.\n\nembeddings\n----------\n\n### Fields\n\n*   `llm` (see LLM top-level config)\n*   `parallelization` (see Parallelization top-level config)\n*   `async_mode` (see Async Mode top-level config)\n*   `batch_size` **int** - The maximum batch size to use.\n*   `batch_max_tokens` **int** - The maximum batch #-tokens.\n*   `target` **required|all** - Determines which set of embeddings to emit.\n*   `skip` **list\\[str\\]** - Which embeddings to skip.\n*   `strategy` **dict** - Fully override the text-embedding strategy.\n\nchunks\n------\n\n### Fields\n\n*   `size` **int** - The max chunk size in tokens.\n*   `overlap` **int** - The chunk overlap in tokens.\n*   `group_by_columns` **list\\[str\\]** - group documents by fields before chunking.\n*   `encoding_model` **str** - The text encoding model to use. Default is to use the top-level encoding model.\n*   `strategy` **dict** - Fully override the chunking strategy.\n\ncache\n-----\n\n### Fields\n\n*   `type` **file|memory|none|blob** - The cache type to use. Default=`file`\n*   `connection_string` **str** - (blob only) The Azure Storage connection string.\n*   `container_name` **str** - (blob only) The Azure Storage container name.\n*   `base_dir` **str** - The base directory to write cache to, relative to the root.\n*   `storage_account_blob_url` **str** - The storage account blob URL to use.\n\nstorage\n-------\n\n### Fields\n\n*   `type` **file|memory|blob** - The storage type to use. Default=`file`\n*   `connection_string` **str** - (blob only) The Azure Storage connection string.\n*   `container_name` **str** - (blob only) The Azure Storage container name.\n*   `base_dir` **str** - The base directory to write reports to, relative to the root.\n*   `storage_account_blob_url` **str** - The storage account blob URL to use.\n\nreporting\n---------\n\n### Fields\n\n*   `type` **file|console|blob** - The reporting type to use. Default=`file`\n*   `connection_string` **str** - (blob only) The Azure Storage connection string.\n*   `container_name` **str** - (blob only) The Azure Storage container name.\n*   `base_dir` **str** - The base directory to write reports to, relative to the root.\n*   `storage_account_blob_url` **str** - The storage account blob URL to use.\n\nentity\\_extraction\n------------------\n\n### Fields\n\n*   `llm` (see LLM top-level config)\n*   `parallelization` (see Parallelization top-level config)\n*   `async_mode` (see Async Mode top-level config)\n*   `prompt` **str** - The prompt file to use.\n*   `entity_types` **list\\[str\\]** - The entity types to identify.\n*   `max_gleanings` **int** - The maximum number of gleaning cycles to use.\n*   `encoding_model` **str** - The text encoding model to use. By default, this will use the top-level encoding model.\n*   `strategy` **dict** - Fully override the entity extraction strategy.\n\nsummarize\\_descriptions\n-----------------------\n\n### Fields\n\n*   `llm` (see LLM top-level config)\n*   `parallelization` (see Parallelization top-level config)\n*   `async_mode` (see Async Mode top-level config)\n*   `prompt` **str** - The prompt file to use.\n*   `max_length` **int** - The maximum number of output tokens per summarization.\n*   `strategy` **dict** - Fully override the summarize description strategy.\n\nclaim\\_extraction\n-----------------\n\n### Fields\n\n*   `enabled` **bool** - Whether to enable claim extraction. default=False\n*   `llm` (see LLM top-level config)\n*   `parallelization` (see Parallelization top-level config)\n*   `async_mode` (see Async Mode top-level config)\n*   `prompt` **str** - The prompt file to use.\n*   `description` **str** - Describes the types of claims we want to extract.\n*   `max_gleanings` **int** - The maximum number of gleaning cycles to use.\n*   `encoding_model` **str** - The text encoding model to use. By default, this will use the top-level encoding model.\n*   `strategy` **dict** - Fully override the claim extraction strategy.\n\ncommunity\\_reports\n------------------\n\n### Fields\n\n*   `llm` (see LLM top-level config)\n*   `parallelization` (see Parallelization top-level config)\n*   `async_mode` (see Async Mode top-level config)\n*   `prompt` **str** - The prompt file to use.\n*   `max_length` **int** - The maximum number of output tokens per report.\n*   `max_input_length` **int** - The maximum number of input tokens to use when generating reports.\n*   `strategy` **dict** - Fully override the community reports strategy.\n\ncluster\\_graph\n--------------\n\n### Fields\n\n*   `max_cluster_size` **int** - The maximum cluster size to emit.\n*   `strategy` **dict** - Fully override the cluster\\_graph strategy.\n\nembed\\_graph\n------------\n\n### Fields\n\n*   `enabled` **bool** - Whether to enable graph embeddings.\n*   `num_walks` **int** - The node2vec number of walks.\n*   `walk_length` **int** - The node2vec walk length.\n*   `window_size` **int** - The node2vec window size.\n*   `iterations` **int** - The node2vec number of iterations.\n*   `random_seed` **int** - The node2vec random seed.\n*   `strategy` **dict** - Fully override the embed graph strategy.\n\numap\n----\n\n### Fields\n\n*   `enabled` **bool** - Whether to enable UMAP layouts.\n\nsnapshots\n---------\n\n### Fields\n\n*   `graphml` **bool** - Emit graphml snapshots.\n*   `raw_entities` **bool** - Emit raw entity snapshots.\n*   `top_level_nodes` **bool** - Emit top-level-node snapshots.\n\nencoding\\_model\n---------------\n\n**str** - The text encoding model to use. Default is `cl100k_base`.\n\nskip\\_workflows\n---------------\n\n**list\\[str\\]** - Which workflow names to skip.",
    "metadata": {
      "title": "Default Configuration Mode (using JSON/YAML)",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag",
      "https://github.com/Azure-Samples/graphrag-accelerator"
    ]
  },
  {
    "content": "Custom Configuration Mode\n=========================\n\nThe primary configuration sections for Indexing Engine pipelines are described below. Each configuration section can be expressed in Python (for use in Python API mode) as well as YAML, but YAML is show here for brevity.\n\nUsing custom configuration is an advanced use-case. Most users will want to use the [Default Configuration](/graphrag/posts/config/overview)\n instead.\n\nIndexing Engine Examples\n------------------------\n\nThe [examples](https://github.com/microsoft/graphrag/blob/main/examples/)\n directory contains several examples of how to use the indexing engine with _custom configuration_.\n\nMost examples include two different forms of running the pipeline, both are contained in the examples `run.py`\n\n1.  Using mostly the Python API\n2.  Using mostly the a pipeline configuration file\n\nTo run an example:\n\n*   Run `poetry shell` to activate a virtual environment with the required dependencies.\n*   Run `PYTHONPATH=\"$(pwd)\" python examples/path_to_example/run.py` from the `root` directory.\n\nFor example to run the single\\_verb example, you would run the following commands:\n\n    poetry shell\n\n    PYTHONPATH=\"$(pwd)\" python examples/single_verb/run.py\n\nConfiguration Sections\n======================\n\n\\> extends\n==========\n\nThis configuration allows you to extend a base configuration file or files.\n\n    # single base\n    extends: ../base_config.yml\n\n    # multiple bases\n    extends:\n      - ../base_config.yml\n      - ../base_config2.yml\n\n\\> root\\_dir\n============\n\nThis configuration allows you to set the root directory for the pipeline. All data inputs and outputs are assumed to be relative to this path.\n\n    root_dir: /workspace/data_project\n\n\\> storage\n==========\n\nThis configuration allows you define the output strategy for the pipeline.\n\n*   `type`: The type of storage to use. Options are `file`, `memory`, and `blob`\n*   `base_dir` (`type: file` only): The base directory to store the data in. This is relative to the config root.\n*   `connection_string` (`type: blob` only): The connection string to use for blob storage.\n*   `container_name` (`type: blob` only): The container to use for blob storage.\n\n\\> cache\n========\n\nThis configuration allows you define the cache strategy for the pipeline.\n\n*   `type`: The type of cache to use. Options are `file` and `memory`, and `blob`.\n*   `base_dir` (`type: file` only): The base directory to store the cache in. This is relative to the config root.\n*   `connection_string` (`type: blob` only): The connection string to use for blob storage.\n*   `container_name` (`type: blob` only): The container to use for blob storage.\n\n\\> reporting\n============\n\nThis configuration allows you define the reporting strategy for the pipeline. Report files are generated artifacts that summarize the performance metrics of the pipeline and emit any error messages.\n\n*   `type`: The type of reporting to use. Options are `file`, `memory`, and `blob`\n*   `base_dir` (`type: file` only): The base directory to store the reports in. This is relative to the config root.\n*   `connection_string` (`type: blob` only): The connection string to use for blob storage.\n*   `container_name` (`type: blob` only): The container to use for blob storage.\n\n\\> workflows\n============\n\nThis configuration section defines the workflow DAG for the pipeline. Here we define an array of workflows and express their inter-dependencies in steps:\n\n*   `name`: The name of the workflow. This is used to reference the workflow in other parts of the config.\n*   `steps`: The DataShaper steps that this workflow comprises. If a step defines an input in the form of `workflow:<workflow_name>`, then it is assumed to have a dependency on the output of that workflow.\n\n    workflows:\n      - name: workflow1\n        steps:\n          - verb: derive\n            args:\n              column1: \"col1\"\n              column2: \"col2\"\n      - name: workflow2\n        steps:\n          - verb: derive\n            args:\n              column1: \"col1\"\n              column2: \"col2\"\n            input:\n              # dependency established here\n              source: workflow:workflow1\n\n\\> input\n========\n\n*   `type`: The type of input to use. Options are `file` or `blob`.\n*   `file_type`: The file type field discriminates between the different input types. Options are `csv` and `text`.\n*   `base_dir`: The base directory to read the input files from. This is relative to the config file.\n*   `file_pattern`: A regex to match the input files. The regex must have named groups for each of the fields in the file\\_filter.\n*   `post_process`: A DataShaper workflow definition to apply to the input before executing the primary workflow.\n*   `source_column` (`type: csv` only): The column containing the source/author of the data\n*   `text_column` (`type: csv` only): The column containing the text of the data\n*   `timestamp_column` (`type: csv` only): The column containing the timestamp of the data\n*   `timestamp_format` (`type: csv` only): The format of the timestamp\n\n    input:\n      type: file\n      file_type: csv\n      base_dir: ../data/csv # the directory containing the CSV files, this is relative to the config file\n      file_pattern: '.*[\\/](?P<source>[^\\/]+)[\\/](?P<year>\\d{4})-(?P<month>\\d{2})-(?P<day>\\d{2})_(?P<author>[^_]+)_\\d+\\.csv$' # a regex to match the CSV files\n      # An additional file filter which uses the named groups from the file_pattern to further filter the files\n      # file_filter:\n      #   # source: (source_filter)\n      #   year: (2023)\n      #   month: (06)\n      #   # day: (22)\n      source_column: \"author\" # the column containing the source/author of the data\n      text_column: \"message\" # the column containing the text of the data\n      timestamp_column: \"date(yyyyMMddHHmmss)\" # optional, the column containing the timestamp of the data\n      timestamp_format: \"%Y%m%d%H%M%S\" # optional,  the format of the timestamp\n      post_process: # Optional, set of steps to process the data before going into the workflow\n        - verb: filter\n          args:\n            column: \"title\",\n            value: \"My document\"\n\n    input:\n      type: file\n      file_type: csv\n      base_dir: ../data/csv # the directory containing the CSV files, this is relative to the config file\n      file_pattern: '.*[\\/](?P<source>[^\\/]+)[\\/](?P<year>\\d{4})-(?P<month>\\d{2})-(?P<day>\\d{2})_(?P<author>[^_]+)_\\d+\\.csv$' # a regex to match the CSV files\n      # An additional file filter which uses the named groups from the file_pattern to further filter the files\n      # file_filter:\n      #   # source: (source_filter)\n      #   year: (2023)\n      #   month: (06)\n      #   # day: (22)\n      post_process: # Optional, set of steps to process the data before going into the workflow\n        - verb: filter\n          args:\n            column: \"title\",\n            value: \"My document\"",
    "markdown": "Custom Configuration Mode\n=========================\n\nThe primary configuration sections for Indexing Engine pipelines are described below. Each configuration section can be expressed in Python (for use in Python API mode) as well as YAML, but YAML is show here for brevity.\n\nUsing custom configuration is an advanced use-case. Most users will want to use the [Default Configuration](/graphrag/posts/config/overview)\n instead.\n\nIndexing Engine Examples\n------------------------\n\nThe [examples](https://github.com/microsoft/graphrag/blob/main/examples/)\n directory contains several examples of how to use the indexing engine with _custom configuration_.\n\nMost examples include two different forms of running the pipeline, both are contained in the examples `run.py`\n\n1.  Using mostly the Python API\n2.  Using mostly the a pipeline configuration file\n\nTo run an example:\n\n*   Run `poetry shell` to activate a virtual environment with the required dependencies.\n*   Run `PYTHONPATH=\"$(pwd)\" python examples/path_to_example/run.py` from the `root` directory.\n\nFor example to run the single\\_verb example, you would run the following commands:\n\n    poetry shell\n\n    PYTHONPATH=\"$(pwd)\" python examples/single_verb/run.py\n\nConfiguration Sections\n======================\n\n\\> extends\n==========\n\nThis configuration allows you to extend a base configuration file or files.\n\n    # single base\n    extends: ../base_config.yml\n\n    # multiple bases\n    extends:\n      - ../base_config.yml\n      - ../base_config2.yml\n\n\\> root\\_dir\n============\n\nThis configuration allows you to set the root directory for the pipeline. All data inputs and outputs are assumed to be relative to this path.\n\n    root_dir: /workspace/data_project\n\n\\> storage\n==========\n\nThis configuration allows you define the output strategy for the pipeline.\n\n*   `type`: The type of storage to use. Options are `file`, `memory`, and `blob`\n*   `base_dir` (`type: file` only): The base directory to store the data in. This is relative to the config root.\n*   `connection_string` (`type: blob` only): The connection string to use for blob storage.\n*   `container_name` (`type: blob` only): The container to use for blob storage.\n\n\\> cache\n========\n\nThis configuration allows you define the cache strategy for the pipeline.\n\n*   `type`: The type of cache to use. Options are `file` and `memory`, and `blob`.\n*   `base_dir` (`type: file` only): The base directory to store the cache in. This is relative to the config root.\n*   `connection_string` (`type: blob` only): The connection string to use for blob storage.\n*   `container_name` (`type: blob` only): The container to use for blob storage.\n\n\\> reporting\n============\n\nThis configuration allows you define the reporting strategy for the pipeline. Report files are generated artifacts that summarize the performance metrics of the pipeline and emit any error messages.\n\n*   `type`: The type of reporting to use. Options are `file`, `memory`, and `blob`\n*   `base_dir` (`type: file` only): The base directory to store the reports in. This is relative to the config root.\n*   `connection_string` (`type: blob` only): The connection string to use for blob storage.\n*   `container_name` (`type: blob` only): The container to use for blob storage.\n\n\\> workflows\n============\n\nThis configuration section defines the workflow DAG for the pipeline. Here we define an array of workflows and express their inter-dependencies in steps:\n\n*   `name`: The name of the workflow. This is used to reference the workflow in other parts of the config.\n*   `steps`: The DataShaper steps that this workflow comprises. If a step defines an input in the form of `workflow:<workflow_name>`, then it is assumed to have a dependency on the output of that workflow.\n\n    workflows:\n      - name: workflow1\n        steps:\n          - verb: derive\n            args:\n              column1: \"col1\"\n              column2: \"col2\"\n      - name: workflow2\n        steps:\n          - verb: derive\n            args:\n              column1: \"col1\"\n              column2: \"col2\"\n            input:\n              # dependency established here\n              source: workflow:workflow1\n\n\\> input\n========\n\n*   `type`: The type of input to use. Options are `file` or `blob`.\n*   `file_type`: The file type field discriminates between the different input types. Options are `csv` and `text`.\n*   `base_dir`: The base directory to read the input files from. This is relative to the config file.\n*   `file_pattern`: A regex to match the input files. The regex must have named groups for each of the fields in the file\\_filter.\n*   `post_process`: A DataShaper workflow definition to apply to the input before executing the primary workflow.\n*   `source_column` (`type: csv` only): The column containing the source/author of the data\n*   `text_column` (`type: csv` only): The column containing the text of the data\n*   `timestamp_column` (`type: csv` only): The column containing the timestamp of the data\n*   `timestamp_format` (`type: csv` only): The format of the timestamp\n\n    input:\n      type: file\n      file_type: csv\n      base_dir: ../data/csv # the directory containing the CSV files, this is relative to the config file\n      file_pattern: '.*[\\/](?P<source>[^\\/]+)[\\/](?P<year>\\d{4})-(?P<month>\\d{2})-(?P<day>\\d{2})_(?P<author>[^_]+)_\\d+\\.csv$' # a regex to match the CSV files\n      # An additional file filter which uses the named groups from the file_pattern to further filter the files\n      # file_filter:\n      #   # source: (source_filter)\n      #   year: (2023)\n      #   month: (06)\n      #   # day: (22)\n      source_column: \"author\" # the column containing the source/author of the data\n      text_column: \"message\" # the column containing the text of the data\n      timestamp_column: \"date(yyyyMMddHHmmss)\" # optional, the column containing the timestamp of the data\n      timestamp_format: \"%Y%m%d%H%M%S\" # optional,  the format of the timestamp\n      post_process: # Optional, set of steps to process the data before going into the workflow\n        - verb: filter\n          args:\n            column: \"title\",\n            value: \"My document\"\n\n    input:\n      type: file\n      file_type: csv\n      base_dir: ../data/csv # the directory containing the CSV files, this is relative to the config file\n      file_pattern: '.*[\\/](?P<source>[^\\/]+)[\\/](?P<year>\\d{4})-(?P<month>\\d{2})-(?P<day>\\d{2})_(?P<author>[^_]+)_\\d+\\.csv$' # a regex to match the CSV files\n      # An additional file filter which uses the named groups from the file_pattern to further filter the files\n      # file_filter:\n      #   # source: (source_filter)\n      #   year: (2023)\n      #   month: (06)\n      #   # day: (22)\n      post_process: # Optional, set of steps to process the data before going into the workflow\n        - verb: filter\n          args:\n            column: \"title\",\n            value: \"My document\"",
    "metadata": {
      "title": "Custom Configuration Mode",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/config/custom",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "https://microsoft.github.io/graphrag/posts/config/overview",
      "https://github.com/microsoft/graphrag/blob/main/examples/",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag",
      "https://github.com/Azure-Samples/graphrag-accelerator"
    ]
  },
  {
    "content": "Configuration Template\n======================\n\nThe following template can be used and stored as a `.env` in the the directory where you're are pointing the `--root` parameter on your Indexing Pipeline execution.\n\nFor details about how to run the Indexing Pipeline, refer to the [Index CLI](../../index/2-cli)\n documentation.\n\n.env File Template\n------------------\n\nRequired variables are uncommented. All the optional configuration can be turned on or off as needed.\n\n### Minimal Configuration\n\n    # Base LLM Settings\n    GRAPHRAG_API_KEY=\"your_api_key\"\n    GRAPHRAG_API_BASE=\"http://<domain>.openai.azure.com\" # For Azure OpenAI Users\n    GRAPHRAG_API_VERSION=\"api_version\" # For Azure OpenAI Users\n    \n    # Text Generation Settings\n    GRAPHRAG_LLM_TYPE=\"azure_openai_chat\" # or openai_chat\n    GRAPHRAG_LLM_DEPLOYMENT_NAME=\"gpt-4-turbo-preview\"\n    GRAPHRAG_LLM_MODEL_SUPPORTS_JSON=True\n    \n    # Text Embedding Settings\n    GRAPHRAG_EMBEDDING_TYPE=\"azure_openai_embedding\" # or openai_embedding\n    GRAPHRAG_LLM_DEPLOYMENT_NAME=\"text-embedding-3-small\"\n    \n    # Data Mapping Settings\n    GRAPHRAG_INPUT_TYPE=\"text\"\n    \n\n### Full Configuration\n\n    \n    # Required LLM Config\n    \n    # Input Data Configuration\n    GRAPHRAG_INPUT_TYPE=\"file\"\n    \n    # Plaintext Input Data Configuration\n    # GRAPHRAG_INPUT_FILE_PATTERN=.*\\.txt\n    \n    # Text Input Data Configuration\n    GRAPHRAG_INPUT_FILE_TYPE=\"text\"\n    GRAPHRAG_INPUT_FILE_PATTERN=\".*\\.txt$\"\n    GRAPHRAG_INPUT_SOURCE_COLUMN=source\n    # GRAPHRAG_INPUT_TIMESTAMP_COLUMN=None\n    # GRAPHRAG_INPUT_TIMESTAMP_FORMAT=None\n    # GRAPHRAG_INPUT_TEXT_COLUMN=\"text\"\n    # GRAPHRAG_INPUT_ATTRIBUTE_COLUMNS=id\n    # GRAPHRAG_INPUT_TITLE_COLUMN=\"title\"\n    # GRAPHRAG_INPUT_TYPE=\"file\"\n    # GRAPHRAG_INPUT_CONNECTION_STRING=None\n    # GRAPHRAG_INPUT_CONTAINER_NAME=None\n    # GRAPHRAG_INPUT_BASE_DIR=None\n    \n    # Base LLM Settings\n    GRAPHRAG_API_KEY=\"your_api_key\"\n    GRAPHRAG_API_BASE=\"http://<domain>.openai.azure.com\" # For Azure OpenAI Users\n    GRAPHRAG_API_VERSION=\"api_version\" # For Azure OpenAI Users\n    # GRAPHRAG_API_ORGANIZATION=None\n    # GRAPHRAG_API_PROXY=None\n    \n    # Text Generation Settings\n    # GRAPHRAG_LLM_TYPE=openai_chat\n    GRAPHRAG_LLM_API_KEY=\"your_api_key\" # If GRAPHRAG_API_KEY is not set\n    GRAPHRAG_LLM_API_BASE=\"http://<domain>.openai.azure.com\" # For Azure OpenAI Users and if GRAPHRAG_API_BASE is not set\n    GRAPHRAG_LLM_API_VERSION=\"api_version\" # For Azure OpenAI Users and if GRAPHRAG_API_VERSION is not set\n    GRAPHRAG_LLM_MODEL_SUPPORTS_JSON=True # Suggested by default\n    # GRAPHRAG_LLM_API_ORGANIZATION=None\n    # GRAPHRAG_LLM_API_PROXY=None\n    # GRAPHRAG_LLM_DEPLOYMENT_NAME=None\n    # GRAPHRAG_LLM_MODEL=gpt-4-turbo-preview\n    # GRAPHRAG_LLM_MAX_TOKENS=4000\n    # GRAPHRAG_LLM_REQUEST_TIMEOUT=180\n    # GRAPHRAG_LLM_THREAD_COUNT=50\n    # GRAPHRAG_LLM_THREAD_STAGGER=0.3\n    # GRAPHRAG_LLM_CONCURRENT_REQUESTS=25\n    # GRAPHRAG_LLM_TPM=0\n    # GRAPHRAG_LLM_RPM=0\n    # GRAPHRAG_LLM_MAX_RETRIES=10\n    # GRAPHRAG_LLM_MAX_RETRY_WAIT=10\n    # GRAPHRAG_LLM_SLEEP_ON_RATE_LIMIT_RECOMMENDATION=True\n    \n    # Text Embedding Settings\n    # GRAPHRAG_EMBEDDING_TYPE=openai_embedding\n    GRAPHRAG_EMBEDDING_API_KEY=\"your_api_key\" # If GRAPHRAG_API_KEY is not set\n    GRAPHRAG_EMBEDDING_API_BASE=\"http://<domain>.openai.azure.com\"  # For Azure OpenAI Users and if GRAPHRAG_API_BASE is not set\n    GRAPHRAG_EMBEDDING_API_VERSION=\"api_version\" # For Azure OpenAI Users and if GRAPHRAG_API_VERSION is not set\n    # GRAPHRAG_EMBEDDING_API_ORGANIZATION=None\n    # GRAPHRAG_EMBEDDING_API_PROXY=None\n    # GRAPHRAG_EMBEDDING_DEPLOYMENT_NAME=None\n    # GRAPHRAG_EMBEDDING_MODEL=text-embedding-3-small\n    # GRAPHRAG_EMBEDDING_BATCH_SIZE=16\n    # GRAPHRAG_EMBEDDING_BATCH_MAX_TOKENS=8191\n    # GRAPHRAG_EMBEDDING_TARGET=required\n    # GRAPHRAG_EMBEDDING_SKIP=None\n    # GRAPHRAG_EMBEDDING_THREAD_COUNT=None\n    # GRAPHRAG_EMBEDDING_THREAD_STAGGER=50\n    # GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS=25\n    # GRAPHRAG_EMBEDDING_TPM=0\n    # GRAPHRAG_EMBEDDING_RPM=0\n    # GRAPHRAG_EMBEDDING_MAX_RETRIES=10\n    # GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT=10\n    # GRAPHRAG_EMBEDDING_SLEEP_ON_RATE_LIMIT_RECOMMENDATION=True\n    \n    # Data Mapping Settings\n    # GRAPHRAG_INPUT_ENCODING=utf-8\n    \n    # Data Chunking\n    # GRAPHRAG_CHUNK_SIZE=1200\n    # GRAPHRAG_CHUNK_OVERLAP=100\n    # GRAPHRAG_CHUNK_BY_COLUMNS=id\n    \n    # Prompting Overrides\n    # GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE=None\n    # GRAPHRAG_ENTITY_EXTRACTION_MAX_GLEANINGS=1\n    # GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES=organization,person,event,geo\n    # GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE=None\n    # GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH=500\n    # GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION=\"Any claims or facts that could be relevant to threat analysis.\"\n    # GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE=None\n    # GRAPHRAG_CLAIM_EXTRACTION_MAX_GLEANINGS=1\n    # GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE=None\n    # GRAPHRAG_COMMUNITY_REPORT_MAX_LENGTH=1500\n    \n    # Storage\n    # GRAPHRAG_STORAGE_TYPE=file\n    # GRAPHRAG_STORAGE_CONNECTION_STRING=None\n    # GRAPHRAG_STORAGE_CONTAINER_NAME=None\n    # GRAPHRAG_STORAGE_BASE_DIR=None\n    \n    # Cache\n    # GRAPHRAG_CACHE_TYPE=file\n    # GRAPHRAG_CACHE_CONNECTION_STRING=None\n    # GRAPHRAG_CACHE_CONTAINER_NAME=None\n    # GRAPHRAG_CACHE_BASE_DIR=None\n    \n    # Reporting\n    # GRAPHRAG_REPORTING_TYPE=file\n    # GRAPHRAG_REPORTING_CONNECTION_STRING=None\n    # GRAPHRAG_REPORTING_CONTAINER_NAME=None\n    # GRAPHRAG_REPORTING_BASE_DIR=None\n    \n    # Node2Vec Parameters\n    # GRAPHRAG_NODE2VEC_ENABLED=False\n    # GRAPHRAG_NODE2VEC_NUM_WALKS=10\n    # GRAPHRAG_NODE2VEC_WALK_LENGTH=40\n    # GRAPHRAG_NODE2VEC_WINDOW_SIZE=2\n    # GRAPHRAG_NODE2VEC_ITERATIONS=3\n    # GRAPHRAG_NODE2VEC_RANDOM_SEED=597832\n    \n    # Data Snapshotting\n    # GRAPHRAG_SNAPSHOT_GRAPHML=False\n    # GRAPHRAG_SNAPSHOT_RAW_ENTITIES=False\n    # GRAPHRAG_SNAPSHOT_TOP_LEVEL_NODES=False\n    \n    # Miscellaneous Settings\n    # GRAPHRAG_ASYNC_MODE=asyncio\n    # GRAPHRAG_ENCODING_MODEL=cl100k_base\n    # GRAPHRAG_MAX_CLUSTER_SIZE=10\n    # GRAPHRAG_ENTITY_RESOLUTION_ENABLED=False\n    # GRAPHRAG_SKIP_WORKFLOWS=None\n    # GRAPHRAG_UMAP_ENABLED=False",
    "markdown": "Configuration Template\n======================\n\nThe following template can be used and stored as a `.env` in the the directory where you're are pointing the `--root` parameter on your Indexing Pipeline execution.\n\nFor details about how to run the Indexing Pipeline, refer to the [Index CLI](../../index/2-cli)\n documentation.\n\n.env File Template\n------------------\n\nRequired variables are uncommented. All the optional configuration can be turned on or off as needed.\n\n### Minimal Configuration\n\n    # Base LLM Settings\n    GRAPHRAG_API_KEY=\"your_api_key\"\n    GRAPHRAG_API_BASE=\"http://<domain>.openai.azure.com\" # For Azure OpenAI Users\n    GRAPHRAG_API_VERSION=\"api_version\" # For Azure OpenAI Users\n    \n    # Text Generation Settings\n    GRAPHRAG_LLM_TYPE=\"azure_openai_chat\" # or openai_chat\n    GRAPHRAG_LLM_DEPLOYMENT_NAME=\"gpt-4-turbo-preview\"\n    GRAPHRAG_LLM_MODEL_SUPPORTS_JSON=True\n    \n    # Text Embedding Settings\n    GRAPHRAG_EMBEDDING_TYPE=\"azure_openai_embedding\" # or openai_embedding\n    GRAPHRAG_LLM_DEPLOYMENT_NAME=\"text-embedding-3-small\"\n    \n    # Data Mapping Settings\n    GRAPHRAG_INPUT_TYPE=\"text\"\n    \n\n### Full Configuration\n\n    \n    # Required LLM Config\n    \n    # Input Data Configuration\n    GRAPHRAG_INPUT_TYPE=\"file\"\n    \n    # Plaintext Input Data Configuration\n    # GRAPHRAG_INPUT_FILE_PATTERN=.*\\.txt\n    \n    # Text Input Data Configuration\n    GRAPHRAG_INPUT_FILE_TYPE=\"text\"\n    GRAPHRAG_INPUT_FILE_PATTERN=\".*\\.txt$\"\n    GRAPHRAG_INPUT_SOURCE_COLUMN=source\n    # GRAPHRAG_INPUT_TIMESTAMP_COLUMN=None\n    # GRAPHRAG_INPUT_TIMESTAMP_FORMAT=None\n    # GRAPHRAG_INPUT_TEXT_COLUMN=\"text\"\n    # GRAPHRAG_INPUT_ATTRIBUTE_COLUMNS=id\n    # GRAPHRAG_INPUT_TITLE_COLUMN=\"title\"\n    # GRAPHRAG_INPUT_TYPE=\"file\"\n    # GRAPHRAG_INPUT_CONNECTION_STRING=None\n    # GRAPHRAG_INPUT_CONTAINER_NAME=None\n    # GRAPHRAG_INPUT_BASE_DIR=None\n    \n    # Base LLM Settings\n    GRAPHRAG_API_KEY=\"your_api_key\"\n    GRAPHRAG_API_BASE=\"http://<domain>.openai.azure.com\" # For Azure OpenAI Users\n    GRAPHRAG_API_VERSION=\"api_version\" # For Azure OpenAI Users\n    # GRAPHRAG_API_ORGANIZATION=None\n    # GRAPHRAG_API_PROXY=None\n    \n    # Text Generation Settings\n    # GRAPHRAG_LLM_TYPE=openai_chat\n    GRAPHRAG_LLM_API_KEY=\"your_api_key\" # If GRAPHRAG_API_KEY is not set\n    GRAPHRAG_LLM_API_BASE=\"http://<domain>.openai.azure.com\" # For Azure OpenAI Users and if GRAPHRAG_API_BASE is not set\n    GRAPHRAG_LLM_API_VERSION=\"api_version\" # For Azure OpenAI Users and if GRAPHRAG_API_VERSION is not set\n    GRAPHRAG_LLM_MODEL_SUPPORTS_JSON=True # Suggested by default\n    # GRAPHRAG_LLM_API_ORGANIZATION=None\n    # GRAPHRAG_LLM_API_PROXY=None\n    # GRAPHRAG_LLM_DEPLOYMENT_NAME=None\n    # GRAPHRAG_LLM_MODEL=gpt-4-turbo-preview\n    # GRAPHRAG_LLM_MAX_TOKENS=4000\n    # GRAPHRAG_LLM_REQUEST_TIMEOUT=180\n    # GRAPHRAG_LLM_THREAD_COUNT=50\n    # GRAPHRAG_LLM_THREAD_STAGGER=0.3\n    # GRAPHRAG_LLM_CONCURRENT_REQUESTS=25\n    # GRAPHRAG_LLM_TPM=0\n    # GRAPHRAG_LLM_RPM=0\n    # GRAPHRAG_LLM_MAX_RETRIES=10\n    # GRAPHRAG_LLM_MAX_RETRY_WAIT=10\n    # GRAPHRAG_LLM_SLEEP_ON_RATE_LIMIT_RECOMMENDATION=True\n    \n    # Text Embedding Settings\n    # GRAPHRAG_EMBEDDING_TYPE=openai_embedding\n    GRAPHRAG_EMBEDDING_API_KEY=\"your_api_key\" # If GRAPHRAG_API_KEY is not set\n    GRAPHRAG_EMBEDDING_API_BASE=\"http://<domain>.openai.azure.com\"  # For Azure OpenAI Users and if GRAPHRAG_API_BASE is not set\n    GRAPHRAG_EMBEDDING_API_VERSION=\"api_version\" # For Azure OpenAI Users and if GRAPHRAG_API_VERSION is not set\n    # GRAPHRAG_EMBEDDING_API_ORGANIZATION=None\n    # GRAPHRAG_EMBEDDING_API_PROXY=None\n    # GRAPHRAG_EMBEDDING_DEPLOYMENT_NAME=None\n    # GRAPHRAG_EMBEDDING_MODEL=text-embedding-3-small\n    # GRAPHRAG_EMBEDDING_BATCH_SIZE=16\n    # GRAPHRAG_EMBEDDING_BATCH_MAX_TOKENS=8191\n    # GRAPHRAG_EMBEDDING_TARGET=required\n    # GRAPHRAG_EMBEDDING_SKIP=None\n    # GRAPHRAG_EMBEDDING_THREAD_COUNT=None\n    # GRAPHRAG_EMBEDDING_THREAD_STAGGER=50\n    # GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS=25\n    # GRAPHRAG_EMBEDDING_TPM=0\n    # GRAPHRAG_EMBEDDING_RPM=0\n    # GRAPHRAG_EMBEDDING_MAX_RETRIES=10\n    # GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT=10\n    # GRAPHRAG_EMBEDDING_SLEEP_ON_RATE_LIMIT_RECOMMENDATION=True\n    \n    # Data Mapping Settings\n    # GRAPHRAG_INPUT_ENCODING=utf-8\n    \n    # Data Chunking\n    # GRAPHRAG_CHUNK_SIZE=1200\n    # GRAPHRAG_CHUNK_OVERLAP=100\n    # GRAPHRAG_CHUNK_BY_COLUMNS=id\n    \n    # Prompting Overrides\n    # GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE=None\n    # GRAPHRAG_ENTITY_EXTRACTION_MAX_GLEANINGS=1\n    # GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES=organization,person,event,geo\n    # GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE=None\n    # GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH=500\n    # GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION=\"Any claims or facts that could be relevant to threat analysis.\"\n    # GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE=None\n    # GRAPHRAG_CLAIM_EXTRACTION_MAX_GLEANINGS=1\n    # GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE=None\n    # GRAPHRAG_COMMUNITY_REPORT_MAX_LENGTH=1500\n    \n    # Storage\n    # GRAPHRAG_STORAGE_TYPE=file\n    # GRAPHRAG_STORAGE_CONNECTION_STRING=None\n    # GRAPHRAG_STORAGE_CONTAINER_NAME=None\n    # GRAPHRAG_STORAGE_BASE_DIR=None\n    \n    # Cache\n    # GRAPHRAG_CACHE_TYPE=file\n    # GRAPHRAG_CACHE_CONNECTION_STRING=None\n    # GRAPHRAG_CACHE_CONTAINER_NAME=None\n    # GRAPHRAG_CACHE_BASE_DIR=None\n    \n    # Reporting\n    # GRAPHRAG_REPORTING_TYPE=file\n    # GRAPHRAG_REPORTING_CONNECTION_STRING=None\n    # GRAPHRAG_REPORTING_CONTAINER_NAME=None\n    # GRAPHRAG_REPORTING_BASE_DIR=None\n    \n    # Node2Vec Parameters\n    # GRAPHRAG_NODE2VEC_ENABLED=False\n    # GRAPHRAG_NODE2VEC_NUM_WALKS=10\n    # GRAPHRAG_NODE2VEC_WALK_LENGTH=40\n    # GRAPHRAG_NODE2VEC_WINDOW_SIZE=2\n    # GRAPHRAG_NODE2VEC_ITERATIONS=3\n    # GRAPHRAG_NODE2VEC_RANDOM_SEED=597832\n    \n    # Data Snapshotting\n    # GRAPHRAG_SNAPSHOT_GRAPHML=False\n    # GRAPHRAG_SNAPSHOT_RAW_ENTITIES=False\n    # GRAPHRAG_SNAPSHOT_TOP_LEVEL_NODES=False\n    \n    # Miscellaneous Settings\n    # GRAPHRAG_ASYNC_MODE=asyncio\n    # GRAPHRAG_ENCODING_MODEL=cl100k_base\n    # GRAPHRAG_MAX_CLUSTER_SIZE=10\n    # GRAPHRAG_ENTITY_RESOLUTION_ENABLED=False\n    # GRAPHRAG_SKIP_WORKFLOWS=None\n    # GRAPHRAG_UMAP_ENABLED=False",
    "metadata": {
      "title": "Configuration Template",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/config/template",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "https://microsoft.github.io/graphrag/posts/config/template/../../index/2-cli",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag",
      "https://github.com/Azure-Samples/graphrag-accelerator"
    ]
  },
  {
    "content": "Prompt Tuning âš™ï¸\n================\n\nThis page provides an overview of the prompt tuning options available for the GraphRAG indexing engine.\n\nDefault Prompts\n---------------\n\nThe default prompts are the simplest way to get started with the GraphRAG system. It is designed to work out-of-the-box with minimal configuration. You can find more detail about these prompts in the following links:\n\n*   [Entity/Relationship Extraction](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/graph/prompts.py)\n    \n*   [Entity/Relationship Description Summarization](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/summarize/prompts.py)\n    \n*   [Claim Extraction](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/claims/prompts.py)\n    \n*   [Community Reports](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/community_reports/prompts.py)\n    \n\nAuto Templating\n---------------\n\nAuto Templating leverages your input data and LLM interactions to create domain adaptive templates for the generation of the knowledge graph. It is highly encouraged to run it as it will yield better results when executing an Index Run. For more details about how to use it, please refer to the [Auto Templating](/graphrag/posts/prompt_tuning/auto_prompt_tuning)\n documentation.\n\nManual Configuration\n--------------------\n\nManual configuration is an advanced use-case. Most users will want to use the Auto Templating feature instead. Details about how to use manual configuration are available in the [Manual Prompt Configuration](/graphrag/posts/prompt_tuning/manual_prompt_tuning)\n documentation.",
    "markdown": "Prompt Tuning âš™ï¸\n================\n\nThis page provides an overview of the prompt tuning options available for the GraphRAG indexing engine.\n\nDefault Prompts\n---------------\n\nThe default prompts are the simplest way to get started with the GraphRAG system. It is designed to work out-of-the-box with minimal configuration. You can find more detail about these prompts in the following links:\n\n*   [Entity/Relationship Extraction](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/graph/prompts.py)\n    \n*   [Entity/Relationship Description Summarization](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/summarize/prompts.py)\n    \n*   [Claim Extraction](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/claims/prompts.py)\n    \n*   [Community Reports](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/community_reports/prompts.py)\n    \n\nAuto Templating\n---------------\n\nAuto Templating leverages your input data and LLM interactions to create domain adaptive templates for the generation of the knowledge graph. It is highly encouraged to run it as it will yield better results when executing an Index Run. For more details about how to use it, please refer to the [Auto Templating](/graphrag/posts/prompt_tuning/auto_prompt_tuning)\n documentation.\n\nManual Configuration\n--------------------\n\nManual configuration is an advanced use-case. Most users will want to use the Auto Templating feature instead. Details about how to use manual configuration are available in the [Manual Prompt Configuration](/graphrag/posts/prompt_tuning/manual_prompt_tuning)\n documentation.",
    "metadata": {
      "title": "Prompt Tuning âš™ï¸",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/graph/prompts.py",
      "http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/summarize/prompts.py",
      "http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/claims/prompts.py",
      "http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/community_reports/prompts.py",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag",
      "https://github.com/Azure-Samples/graphrag-accelerator"
    ]
  },
  {
    "content": "Prompt Tuning âš™ï¸\n================\n\nGraphRAG provides the ability to create domain adaptive templates for the generation of the knowledge graph. This step is optional, though it is highly encouraged to run it as it will yield better results when executing an Index Run.\n\nThe templates are generated by loading the inputs, splitting them into chunks (text units) and then running a series of LLM invocations and template substitutions to generate the final prompts. We suggest using the default values provided by the script, but in this page you'll find the detail of each in case you want to further explore and tweak the template generation algorithm.\n\nPrerequisites\n-------------\n\nBefore running the automatic template generation make sure you have already initialized your workspace with the `graphrag.index --init` command. This will create the necessary configuration files and the default prompts. Refer to the [Init Documentation](/graphrag/posts/config/init)\n for more information about the initialization process.\n\nUsage\n-----\n\nYou can run the main script from the command line with various options:\n\n    python -m graphrag.prompt_tune [--root ROOT] [--domain DOMAIN]  [--method METHOD] [--limit LIMIT] [--language LANGUAGE] [--max-tokens MAX_TOKENS] [--chunk-size CHUNK_SIZE] [--no-entity-types] [--output OUTPUT]\n\nCommand-Line Options\n--------------------\n\n*   `--root` (optional): The data project root directory, including the config files (YML, JSON, or .env). Defaults to the current directory.\n    \n*   `--domain` (optional): The domain related to your input data, such as 'space science', 'microbiology', or 'environmental news'. If left empty, the domain will be inferred from the input data.\n    \n*   `--method` (optional): The method to select documents. Options are all, random, or top. Default is random.\n    \n*   `--limit` (optional): The limit of text units to load when using random or top selection. Default is 15.\n    \n*   `--language` (optional): The language to use for input processing. If it is different from the inputs' language, the LLM will translate. Default is \"\" meaning it will be automatically detected from the inputs.\n    \n*   `--max-tokens` (optional): Maximum token count for prompt generation. Default is 2000.\n    \n*   `--chunk-size` (optional): The size in tokens to use for generating text units from input documents. Default is 200.\n    \n*   `--no-entity-types` (optional): Use untyped entity extraction generation. We recommend using this when your data covers a lot of topics or it is highly randomized.\n    \n*   `--output` (optional): The folder to save the generated prompts. Default is \"prompts\".\n    \n\nExample Usage\n-------------\n\n    python -m graphrag.prompt_tune --root /path/to/project --domain \"environmental news\" --method random --limit 10 --language English --max-tokens 2048 --chunk-size 256 --no-entity-types --output /path/to/output\n\nor, with minimal configuration (suggested):\n\n    python -m graphrag.prompt_tune --root /path/to/project --no-entity-types\n\nDocument Selection Methods\n--------------------------\n\nThe auto template feature ingests the input data and then divides it into text units the size of the chunk size parameter. After that, it uses one of the following selection methods to pick a sample to work with for template generation:\n\n*   `random`: Select text units randomly. This is the default and recommended option.\n*   `top`: Select the head n text units.\n*   `all`: Use all text units for the generation. Use only with small datasets; this option is not usually recommended.\n\nModify Env Vars\n---------------\n\nAfter running auto-templating, you should modify the following environment variables (or config variables) to pick up the new prompts on your index run. Note: Please make sure to update the correct path to the generated prompts, in this example we are using the default \"prompts\" path.\n\n*   `GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE` = \"prompts/entity\\_extraction.txt\"\n    \n*   `GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE` = \"prompts/community\\_report.txt\"\n    \n*   `GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE` = \"prompts/summarize\\_descriptions.txt\"",
    "markdown": "Prompt Tuning âš™ï¸\n================\n\nGraphRAG provides the ability to create domain adaptive templates for the generation of the knowledge graph. This step is optional, though it is highly encouraged to run it as it will yield better results when executing an Index Run.\n\nThe templates are generated by loading the inputs, splitting them into chunks (text units) and then running a series of LLM invocations and template substitutions to generate the final prompts. We suggest using the default values provided by the script, but in this page you'll find the detail of each in case you want to further explore and tweak the template generation algorithm.\n\nPrerequisites\n-------------\n\nBefore running the automatic template generation make sure you have already initialized your workspace with the `graphrag.index --init` command. This will create the necessary configuration files and the default prompts. Refer to the [Init Documentation](/graphrag/posts/config/init)\n for more information about the initialization process.\n\nUsage\n-----\n\nYou can run the main script from the command line with various options:\n\n    python -m graphrag.prompt_tune [--root ROOT] [--domain DOMAIN]  [--method METHOD] [--limit LIMIT] [--language LANGUAGE] [--max-tokens MAX_TOKENS] [--chunk-size CHUNK_SIZE] [--no-entity-types] [--output OUTPUT]\n\nCommand-Line Options\n--------------------\n\n*   `--root` (optional): The data project root directory, including the config files (YML, JSON, or .env). Defaults to the current directory.\n    \n*   `--domain` (optional): The domain related to your input data, such as 'space science', 'microbiology', or 'environmental news'. If left empty, the domain will be inferred from the input data.\n    \n*   `--method` (optional): The method to select documents. Options are all, random, or top. Default is random.\n    \n*   `--limit` (optional): The limit of text units to load when using random or top selection. Default is 15.\n    \n*   `--language` (optional): The language to use for input processing. If it is different from the inputs' language, the LLM will translate. Default is \"\" meaning it will be automatically detected from the inputs.\n    \n*   `--max-tokens` (optional): Maximum token count for prompt generation. Default is 2000.\n    \n*   `--chunk-size` (optional): The size in tokens to use for generating text units from input documents. Default is 200.\n    \n*   `--no-entity-types` (optional): Use untyped entity extraction generation. We recommend using this when your data covers a lot of topics or it is highly randomized.\n    \n*   `--output` (optional): The folder to save the generated prompts. Default is \"prompts\".\n    \n\nExample Usage\n-------------\n\n    python -m graphrag.prompt_tune --root /path/to/project --domain \"environmental news\" --method random --limit 10 --language English --max-tokens 2048 --chunk-size 256 --no-entity-types --output /path/to/output\n\nor, with minimal configuration (suggested):\n\n    python -m graphrag.prompt_tune --root /path/to/project --no-entity-types\n\nDocument Selection Methods\n--------------------------\n\nThe auto template feature ingests the input data and then divides it into text units the size of the chunk size parameter. After that, it uses one of the following selection methods to pick a sample to work with for template generation:\n\n*   `random`: Select text units randomly. This is the default and recommended option.\n*   `top`: Select the head n text units.\n*   `all`: Use all text units for the generation. Use only with small datasets; this option is not usually recommended.\n\nModify Env Vars\n---------------\n\nAfter running auto-templating, you should modify the following environment variables (or config variables) to pick up the new prompts on your index run. Note: Please make sure to update the correct path to the generated prompts, in this example we are using the default \"prompts\" path.\n\n*   `GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE` = \"prompts/entity\\_extraction.txt\"\n    \n*   `GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE` = \"prompts/community\\_report.txt\"\n    \n*   `GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE` = \"prompts/summarize\\_descriptions.txt\"",
    "metadata": {
      "title": "Prompt Tuning âš™ï¸",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag",
      "https://github.com/Azure-Samples/graphrag-accelerator"
    ]
  },
  {
    "content": "Prompt Tuningâš™ï¸\n===============\n\nThe GraphRAG indexer, by default, will run with a handful of prompts that are designed to work well in the broad context of knowledge discovery. However, it is quite common to want to tune the prompts to better suit your specific use case. We provide a means for you to do this by allowing you to specify a custom prompt file, which will each use a series of token-replacements internally.\n\nEach of these prompts may be overridden by writing a custom prompt file in plaintext. We use token-replacements in the form of `{token_name}`, and the descriptions for the available tokens can be found below.\n\nEntity/Relationship Extraction\n------------------------------\n\n[Prompt Source](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/graph/prompts.py)\n\n### Tokens (values provided by extractor)\n\n*   **{input\\_text}** - The input text to be processed.\n*   **{entity\\_types}** - A list of entity types\n*   **{tuple\\_delimiter}** - A delimiter for separating values within a tuple. A single tuple is used to represent an individual entity or relationship.\n*   **{record\\_delimiter}** - A delimiter for separating tuple instances.\n*   **{completion\\_delimiter}** - An indicator for when generation is complete.\n\nSummarize Entity/Relationship Descriptions\n------------------------------------------\n\n[Prompt Source](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/summarize/prompts.py)\n\n### Tokens (values provided by extractor)\n\n*   **{entity\\_name}** - The name of the entity or the source/target pair of the relationship.\n*   **{description\\_list}** - A list of descriptions for the entity or relationship.\n\nClaim Extraction\n----------------\n\n[Prompt Source](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/claims/prompts.py)\n\n### Tokens (values provided by extractor)\n\n*   **{input\\_text}** - The input text to be processed.\n*   **{tuple\\_delimiter}** - A delimiter for separating values within a tuple. A single tuple is used to represent an individual entity or relationship.\n*   **{record\\_delimiter}** - A delimiter for separating tuple instances.\n*   **{completion\\_delimiter}** - An indicator for when generation is complete.\n\nNote: there is additional paramater for the `Claim Description` that is used in claim extraction. The default value is\n\n`\"Any claims or facts that could be relevant to information discovery.\"`\n\nSee the [configuration documentation](/graphrag/posts/config/overview/)\n for details on how to change this.\n\nGenerate Community Reports\n--------------------------\n\n[Prompt Source](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/community_reports/prompts.py)\n\n### Tokens (values provided by extractor)\n\n*   **{input\\_text}** - The input text to generate the report with. This will contain tables of entities and relationships.",
    "markdown": "Prompt Tuningâš™ï¸\n===============\n\nThe GraphRAG indexer, by default, will run with a handful of prompts that are designed to work well in the broad context of knowledge discovery. However, it is quite common to want to tune the prompts to better suit your specific use case. We provide a means for you to do this by allowing you to specify a custom prompt file, which will each use a series of token-replacements internally.\n\nEach of these prompts may be overridden by writing a custom prompt file in plaintext. We use token-replacements in the form of `{token_name}`, and the descriptions for the available tokens can be found below.\n\nEntity/Relationship Extraction\n------------------------------\n\n[Prompt Source](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/graph/prompts.py)\n\n### Tokens (values provided by extractor)\n\n*   **{input\\_text}** - The input text to be processed.\n*   **{entity\\_types}** - A list of entity types\n*   **{tuple\\_delimiter}** - A delimiter for separating values within a tuple. A single tuple is used to represent an individual entity or relationship.\n*   **{record\\_delimiter}** - A delimiter for separating tuple instances.\n*   **{completion\\_delimiter}** - An indicator for when generation is complete.\n\nSummarize Entity/Relationship Descriptions\n------------------------------------------\n\n[Prompt Source](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/summarize/prompts.py)\n\n### Tokens (values provided by extractor)\n\n*   **{entity\\_name}** - The name of the entity or the source/target pair of the relationship.\n*   **{description\\_list}** - A list of descriptions for the entity or relationship.\n\nClaim Extraction\n----------------\n\n[Prompt Source](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/claims/prompts.py)\n\n### Tokens (values provided by extractor)\n\n*   **{input\\_text}** - The input text to be processed.\n*   **{tuple\\_delimiter}** - A delimiter for separating values within a tuple. A single tuple is used to represent an individual entity or relationship.\n*   **{record\\_delimiter}** - A delimiter for separating tuple instances.\n*   **{completion\\_delimiter}** - An indicator for when generation is complete.\n\nNote: there is additional paramater for the `Claim Description` that is used in claim extraction. The default value is\n\n`\"Any claims or facts that could be relevant to information discovery.\"`\n\nSee the [configuration documentation](/graphrag/posts/config/overview/)\n for details on how to change this.\n\nGenerate Community Reports\n--------------------------\n\n[Prompt Source](http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/community_reports/prompts.py)\n\n### Tokens (values provided by extractor)\n\n*   **{input\\_text}** - The input text to generate the report with. This will contain tables of entities and relationships.",
    "metadata": {
      "title": "Prompt Tuningâš™ï¸",
      "sourceURL": "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "pageStatusCode": 200,
      "ogLocaleAlternate": []
    },
    "linksOnPage": [
      "https://microsoft.github.io/graphrag/",
      "https://microsoft.github.io/graphrag/posts/get_started/",
      "https://microsoft.github.io/graphrag/posts/developing/",
      "https://microsoft.github.io/graphrag/posts/index/overview/",
      "https://microsoft.github.io/graphrag/posts/index/0-architecture/",
      "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/",
      "https://microsoft.github.io/graphrag/posts/index/2-cli/",
      "https://microsoft.github.io/graphrag/posts/config/overview/",
      "https://microsoft.github.io/graphrag/posts/config/init",
      "https://microsoft.github.io/graphrag/posts/config/env_vars",
      "https://microsoft.github.io/graphrag/posts/config/json_yaml",
      "https://microsoft.github.io/graphrag/posts/config/custom",
      "https://microsoft.github.io/graphrag/posts/config/template",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/",
      "https://microsoft.github.io/graphrag/posts/query/overview/",
      "https://microsoft.github.io/graphrag/posts/query/1-local_search/",
      "https://microsoft.github.io/graphrag/posts/query/2-question_generation/",
      "https://microsoft.github.io/graphrag/posts/query/0-global_search/",
      "https://microsoft.github.io/graphrag/posts/query/3-cli/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb",
      "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb",
      "http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/graph/prompts.py",
      "http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/summarize/prompts.py",
      "http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/claims/prompts.py",
      "http://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/community_reports/prompts.py",
      "https://go.microsoft.com/fwlink/?LinkId=521839",
      "https://go.microsoft.com/fwlink/?LinkId=2259814",
      "https://go.microsoft.com/fwlink/?LinkID=206977",
      "https://www.microsoft.com/trademarks",
      "https://www.microsoft.com",
      "https://github.com/microsoft/graphrag",
      "https://github.com/Azure-Samples/graphrag-accelerator"
    ]
  }
]