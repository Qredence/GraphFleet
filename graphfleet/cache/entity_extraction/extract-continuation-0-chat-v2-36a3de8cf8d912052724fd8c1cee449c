{"result": "## \n(\"entity\"<|>DATASET SIZES<|>METRIC, FACTOR<|>Dataset sizes refer to the amount of data used in the analysis and validation of sensemaking questions and target metrics)\n##\n(\"entity\"<|>SENSEMAKING QUESTIONS<|>CONCEPT, QUESTION<|>Sensemaking questions are queries designed to help understand and interpret data)\n##\n(\"entity\"<|>TARGET METRICS<|>METRIC, FACTOR<|>Target metrics are specific measurements used to evaluate the performance of a system or approach)\n##\n(\"entity\"<|>FABRICATION RATES<|>METRIC, FACTOR<|>Fabrication rates refer to the frequency of generating false or misleading information)\n##\n(\"entity\"<|>TRADE-OFFS<|>CONCEPT, PHENOMENON<|>Trade-offs refer to the balancing of different factors when making a decision, such as building a graph index)\n##\n(\"entity\"<|>HEAD-TO-HEAD RESULTS<|>METRIC, FACTOR<|>Head-to-head results are direct comparisons between different methods or approaches)\n##\n(\"entity\"<|>SOURCE TEXTS<|>DATA, COLLECTION<|>Source texts are the original texts used for summarization and analysis)\n##\n(\"entity\"<|>REAL-WORLD DECISION<|>CONCEPT, PHENOMENON<|>Real-world decision refers to practical choices made based on multiple factors)\n##\n(\"entity\"<|>FUTURE WORK<|>CONCEPT, PHENOMENON<|>Future work refers to potential areas of research and development for improving the current approach)\n##\n(\"entity\"<|>HYBRID RAG SCHEMES<|>TECHNOLOGY, METHOD<|>Hybrid RAG schemes combine different approaches, such as embedding-based matching and map-reduce summarization)\n##\n(\"entity\"<|>ROLL-UP OPERATION<|>TECHNIQUE, METHOD<|>Roll-up operation is a method for summarizing information across multiple levels of a hierarchy)\n##\n(\"entity\"<|>DRILL DOWN MECHANISM<|>TECHNIQUE, METHOD<|>Drill down mechanism is a method for exploring detailed information by following higher-level summaries)\n##\n(\"entity\"<|>CONCLUSION<|>SECTION, PART<|>The conclusion summarizes the findings and implications of the study)\n##\n(\"entity\"<|>KNOWLEDGE GRAPH GENERATION<|>TECHNIQUE, METHOD<|>Knowledge graph generation is a process used in Graph RAG to create structured representations of information)\n##\n(\"entity\"<|>RETRIEVAL-AUGMENTED GENERATION (RAG)<|>TECHNIQUE, METHOD<|>Retrieval-augmented generation (RAG) is a technique that combines information retrieval with text generation)\n##\n(\"entity\"<|>INITIAL EVALUATIONS<|>PROCESS, METHOD<|>Initial evaluations refer to the preliminary assessments of the performance of a method or approach)\n##\n(\"entity\"<|>COMPREHENSIVENESS<|>METRIC, FACTOR<|>Comprehensiveness refers to the extent to which a method covers all relevant aspects of the data)\n##\n(\"entity\"<|>DIVERSITY OF ANSWERS<|>METRIC, FACTOR<|>Diversity of answers refers to the variety of responses generated by a method)\n##\n(\"entity\"<|>OPEN-SOURCE IMPLEMENTATION<|>TECHNOLOGY, TOOL<|>An open-source implementation is a publicly available version of a software tool or method)\n##\n(\"entity\"<|>PYTHON-BASED IMPLEMENTATION<|>TECHNOLOGY, TOOL<|>A Python-based implementation is a version of a software tool or method developed using the Python programming language)\n##\n(\"entity\"<|>ACKNOWLEDGEMENTS<|>SECTION, PART<|>The acknowledgements section thanks individuals who contributed to the work)\n##\n(\"relationship\"<|>DATASET SIZES<|>SENSEMAKING QUESTIONS<|>Dataset sizes are used to validate sensemaking questions<|>7)\n##\n(\"relationship\"<|>DATASET SIZES<|>TARGET METRICS<|>Dataset sizes are used to validate target metrics<|>7)\n##\n(\"relationship\"<|>SENSEMAKING QUESTIONS<|>TARGET METRICS<|>Sensemaking questions and target metrics are validated with end users<|>8)\n##\n(\"relationship\"<|>FABRICATION RATES<|>SELFHECKGPT<|>SelfCheckGPT is mentioned as an approach for comparing fabrication rates<|>8)\n##\n(\"relationship\"<|>TRADE-OFFS<|>GRAPH INDEX<|>Trade-offs are considered when deciding whether to invest in building a graph index<|>7)\n##\n(\"relationship\"<|>HEAD-TO-HEAD RESULTS<|>GRAPHRAG<|>Graph RAG achieves the best head-to-head results against other methods<|>8)\n##\n(\"relationship\"<|>SOURCE TEXTS<|>GLOBAL SUMMARIZATION<|>Global summarization involves summarizing source texts without using a graph-based approach<|>7)\n##\n(\"relationship\"<|>REAL-WORLD DECISION<|>GRAPH INDEX<|>The real-world decision to invest in a graph index depends on multiple factors<|>8)\n##\n(\"relationship\"<|>FUTURE WORK<|>GRAPHRAG<|>Future work includes possibilities for refining and adapting the Graph RAG approach<|>8)\n##\n(\"relationship\"<|>HYBRID RAG SCHEMES<|>GRAPHRAG<|>Future work includes the possibility of hybrid RAG schemes that combine different approaches<|>7)\n##\n(\"relationship\"<|>ROLL-UP OPERATION<|>GRAPHRAG<|>The roll-up operation could be extended across more levels of the community hierarchy in Graph RAG<|>7)\n##\n(\"relationship\"<|>DRILL DOWN MECHANISM<|>GRAPHRAG<|>The drill down mechanism could be implemented to follow information scent in higher-level community summaries in Graph RAG<|>7)\n##\n(\"relationship\"<|>CONCLUSION<|>GRAPHRAG<|>The conclusion summarizes the findings and implications of the Graph RAG study<|>8)\n##\n(\"relationship\"<|>KNOWLEDGE GRAPH GENERATION<|>GRAPHRAG<|>Graph RAG combines knowledge graph generation to support human sensemaking<|>9)\n##\n(\"relationship\"<|>RETRIEVAL-AUGMENTED GENERATION (RAG)<|>GRAPHRAG<|>Graph RAG combines retrieval-augmented generation (RAG) to support human sensemaking<|>9)\n##\n(\"relationship\"<|>INITIAL EVALUATIONS<|>GRAPHRAG<|>Initial evaluations show substantial improvements of Graph RAG over a naïve RAG baseline<|>8)\n##\n(\"relationship\"<|>COMPREHENSIVENESS<|>GRAPHRAG<|>Graph RAG shows substantial improvements in the comprehensiveness of answers<|>8)\n##\n(\"relationship\"<|>DIVERSITY OF ANSWERS<|>GRAPHRAG<|>Graph RAG shows substantial improvements in the diversity of answers<|>8)\n##\n(\"relationship\"<|>OPEN-SOURCE IMPLEMENTATION<|>GRAPHRAG<|>An open-source implementation of Graph RAG is forthcoming<|>7)\n##\n(\"relationship\"<|>PYTHON-BASED IMPLEMENTATION<|>GRAPHRAG<|>A Python-based implementation of Graph RAG is forthcoming<|>7)\n##\n(\"relationship\"<|>ACKNOWLEDGEMENTS<|>GRAPHRAG<|>The acknowledgements section thanks individuals who contributed to the work on Graph RAG<|>6)\n<|COMPLETE|>", "input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n", "parameters": {"model": "gpt-4o", "temperature": 0.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "top_p": 1.0, "max_tokens": 4000, "n": 1}, "history": [{"role": "user", "content": "\n-Goal-\nGiven a text document that is potentially relevant to this activity, first identify all entities needed from the text in order to capture the information and ideas in the text.\nNext, report all relationships among the identified entities.\n\n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: Suggest several labels or categories for the entity. The categories should not be specific, but should be as general as possible.\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>)\n\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\nFormat each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n\n3. Return output in The primary language of the provided text is \"English.\" as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n\n4. If you have to translate into The primary language of the provided text is \"English.\", just translate the descriptions, nothing else!\n\n5. When finished, output <|COMPLETE|>.\n\n-Examples-\n######################\n\nExample 1:\n\ntext:\n he tasks studied in the lab thus far have tended to \nbe those for which researchers hypothesized generative AI would \nperform well . This was, in fact, the focus of most of the studies \npresented in the first AI and Productivity report we published  \n(Cambon et al. 2023) . Actual information work , however, often \nincludes a huge variety of tasks  and much of the unstructured and \ninformal work in people’s jobs is not yet directly supported by the \nfirst-generation of generative AI tools.  Software developer \nworkflows , for example,  involve far more than the hands-on coding supported by GitHub Copilot (Meyer et al. 2017). The ability to \nshed light on generative AI's productivity dynamics in the natural \ncomplexity of entire workflows is a key advantage of field studies \nof generative AI’s productivity impacts , and a major reason we \nhope to see many more field studies emerging in the literature\n------------------------\noutput:\n(\"entity\"<|>LAB<|>LOCATION, RESEARCH ENVIRONMENT<|>The lab is where tasks are studied to hypothesize the performance of generative AI)\n##\n(\"entity\"<|>GENERATIVE AI<|>TECHNOLOGY, TOOL<|>Generative AI refers to artificial intelligence systems that can generate content, such as text, based on input data)\n##\n(\"entity\"<|>AI AND PRODUCTIVITY REPORT<|>DOCUMENT, PUBLICATION<|>A report published by Cambon et al. in 2023 focusing on the performance of generative AI in various tasks)\n##\n(\"entity\"<|>INFORMATION WORK<|>ACTIVITY, TASK<|>Information work includes a variety of tasks, often unstructured and informal, that are part of people's jobs)\n##\n(\"entity\"<|>FIRST-GENERATION GENERATIVE AI TOOLS<|>TECHNOLOGY, TOOL<|>The initial versions of generative AI tools that support specific tasks but not the full range of unstructured work)\n##\n(\"entity\"<|>SOFTWARE DEVELOPER WORKFLOWS<|>ACTIVITY, TASK<|>The comprehensive set of activities involved in software development, beyond just coding)\n##\n(\"entity\"<|>GITHUB COPILOT<|>TECHNOLOGY, TOOL<|>A generative AI tool that assists with hands-on coding tasks for software developers)\n##\n(\"entity\"<|>FIELD STUDIES<|>RESEARCH METHOD, STUDY TYPE<|>Studies conducted in natural settings to observe the real-world impacts of generative AI on productivity)\n##\n(\"entity\"<|>PRODUCTIVITY DYNAMICS<|>CONCEPT, PHENOMENON<|>The various factors and interactions that affect productivity when using generative AI in real-world workflows)\n##\n(\"entity\"<|>LITERATURE<|>BODY OF WORK, RESEARCH<|>The collection of academic and professional studies and publications on a given topic)\n##\n(\"relationship\"<|>LAB<|>GENERATIVE AI<|>The lab is where tasks are studied to hypothesize the performance of generative AI<|>7)\n##\n(\"relationship\"<|>AI AND PRODUCTIVITY REPORT<|>GENERATIVE AI<|>The report focuses on the performance of generative AI in various tasks<|>8)\n##\n(\"relationship\"<|>INFORMATION WORK<|>FIRST-GENERATION GENERATIVE AI TOOLS<|>First-generation generative AI tools do not yet directly support much of the unstructured and informal information work<|>6)\n##\n(\"relationship\"<|>SOFTWARE DEVELOPER WORKFLOWS<|>GITHUB COPILOT<|>GitHub Copilot supports hands-on coding, which is a part of software developer workflows<|>9)\n##\n(\"relationship\"<|>FIELD STUDIES<|>PRODUCTIVITY DYNAMICS<|>Field studies help shed light on the productivity dynamics of generative AI in real-world settings<|>8)\n##\n(\"relationship\"<|>FIELD STUDIES<|>LITERATURE<|>The hope is to see more field studies emerging in the literature to understand generative AI's productivity impacts<|>7)\n<|COMPLETE|>\n#############################\n\n\nExample 2:\n\ntext:\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. (2023). Lost\nin the middle: How language models use long contexts. arXiv:2307.03172.\nLiu, Y . and Lapata, M. (2019). Hierarchical transformers for multi-document summarization. arXiv\npreprint arXiv:1905.13164 .\nLlamaIndex (2024). LlamaIndex Knowledge Graph Index. https://docs .llamaindex .ai/en/stable/\nexamples/index structs/knowledge graph/KnowledgeGraphDemo .html.\nManakul, P., Liusie, A., and Gales, M. J. (2023). Selfcheckgpt: Zero-resource black-box hallucina-\ntion detection for generative large language models. arXiv preprint ar\n------------------------\noutput:\n(\"entity\"<|>LIU, N. F.<|>PERSON<|>Liu, N. F. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>LIN, K.<|>PERSON<|>Lin, K. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>HEWITT, J.<|>PERSON<|>Hewitt, J. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>PARANJAPE, A.<|>PERSON<|>Paranjape, A. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>BEVILACQUA, M.<|>PERSON<|>Bevilacqua, M. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>PETRONI, F.<|>PERSON<|>Petroni, F. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>LIANG, P.<|>PERSON<|>Liang, P. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>LIU, Y.<|>PERSON<|>Liu, Y. is an author of the paper \"Hierarchical transformers for multi-document summarization\" published in 2019)\n##\n(\"entity\"<|>LAPATA, M.<|>PERSON<|>Lapata, M. is an author of the paper \"Hierarchical transformers for multi-document summarization\" published in 2019)\n##\n(\"entity\"<|>LLAMAINDEX<|>ORGANIZATION<|>LlamaIndex is an organization that developed the LlamaIndex Knowledge Graph Index, with documentation available online)\n##\n(\"entity\"<|>MANAKUL, P.<|>PERSON<|>Manakul, P. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published in 2023)\n##\n(\"entity\"<|>LIUSIE, A.<|>PERSON<|>Liusie, A. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published in 2023)\n##\n(\"entity\"<|>GALES, M. J.<|>PERSON<|>Gales, M. J. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published in 2023)\n##\n(\"entity\"<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>DOCUMENT<|>A paper published in 2023 by Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P.)\n##\n(\"entity\"<|>HIERARCHICAL TRANSFORMERS FOR MULTI-DOCUMENT SUMMARIZATION<|>DOCUMENT<|>A paper published in 2019 by Liu, Y. and Lapata, M.)\n##\n(\"entity\"<|>SELFCHECKGPT: ZERO-RESOURCE BLACK-BOX HALLUCINATION DETECTION FOR GENERATIVE LARGE LANGUAGE MODELS<|>DOCUMENT<|>A paper published in 2023 by Manakul, P., Liusie, A., and Gales, M. J.)\n##\n(\"relationship\"<|>LIU, N. F.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Liu, N. F. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>LIN, K.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Lin, K. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>HEWITT, J.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Hewitt, J. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>PARANJAPE, A.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Paranjape, A. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>BEVILACQUA, M.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Bevilacqua, M. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>PETRONI, F.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Petroni, F. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>LIANG, P.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Liang, P. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>LIU, Y.<|>HIERARCHICAL TRANSFORMERS FOR MULTI-DOCUMENT SUMMARIZATION<|>Liu, Y. is an author of the paper \"Hierarchical transformers for multi-document summarization\"<|>9)\n##\n(\"relationship\"<|>LAPATA, M.<|>HIERARCHICAL TRANSFORMERS FOR MULTI-DOCUMENT SUMMARIZATION<|>Lapata, M. is an author of the paper \"Hierarchical transformers for multi-document summarization\"<|>9)\n##\n(\"relationship\"<|>MANAKUL, P.<|>SELFCHECKGPT: ZERO-RESOURCE BLACK-BOX HALLUCINATION DETECTION FOR GENERATIVE LARGE LANGUAGE MODELS<|>Manakul, P. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\"<|>9)\n##\n(\"relationship\"<|>LIUSIE, A.<|>SELFCHECKGPT: ZERO-RESOURCE BLACK-BOX HALLUCINATION DETECTION FOR GENERATIVE LARGE LANGUAGE MODELS<|>Liusie, A. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\"<|>9)\n##\n(\"relationship\"<|>GALES, M. J.<|>SELFCHECKGPT: ZERO-RESOURCE BLACK-BOX HALLUCINATION DETECTION FOR GENERATIVE LARGE LANGUAGE MODELS<|>Gales, M. J. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\"<|>9)\n<|COMPLETE|>\n#############################\n\n\n\n-Real Data-\n######################\ntext: dataset sizes, as well as to validate our sensemaking questions and target metrics with end users.\nComparison of fabrication rates, e.g., using approaches like SelfCheckGPT (Manakul et al., 2023),\nwould also improve on the current analysis.\nTrade-offs of building a graph index . We consistently observed Graph RAG achieve the best head-\nto-head results against other methods, but in many cases the graph-free approach to global summa-\nrization of source texts performed competitively. The real-world decision about whether to invest in\nbuilding a graph index depends on multiple factors, including the compute budget, expected number\nof lifetime queries per dataset, and value obtained from other aspects of the graph index (including\nthe generic community summaries and the use of other graph-related RAG approaches).\nFuture work . The graph index, rich text annotations, and hierarchical community structure support-\ning the current Graph RAG approach offer many possibilities for refinement and adaptation. This\nincludes RAG approaches that operate in a more local manner, via embedding-based matching of\nuser queries and graph annotations, as well as the possibility of hybrid RAG schemes that combine\nembedding-based matching against community reports before employing our map-reduce summa-\nrization mechanisms. This “roll-up” operation could also be extended across more levels of the\ncommunity hierarchy, as well as implemented as a more exploratory “drill down” mechanism that\nfollows the information scent contained in higher-level community summaries.\n6 Conclusion\nWe have presented a global approach to Graph RAG, combining knowledge graph generation,\nretrieval-augmented generation (RAG), and query-focused summarization (QFS) to support human\nsensemaking over entire text corpora. Initial evaluations show substantial improvements over a\nna¨ıve RAG baseline for both the comprehensiveness and diversity of answers, as well as favorable\ncomparisons to a global but graph-free approach using map-reduce source text summarization. For\nsituations requiring many global queries over the same dataset, summaries of root-level communi-\nties in the entity-based graph index provide a data index that is both superior to na ¨ıve RAG and\nachieves competitive performance to other global methods at a fraction of the token cost.\nAn open-source, Python-based implementation of both global and local Graph RAG approaches is\nforthcoming at https://aka .ms/graphrag .\n11Acknowledgements\nWe would also like to thank the following people who contributed to the work: Alonso\n######################\noutput:\n"}, {"role": "assistant", "content": "## \n(\"entity\"<|>SELFHECKGPT<|>TECHNOLOGY, TOOL<|>SelfCheckGPT is an approach mentioned for comparing fabrication rates, developed by Manakul et al. in 2023)\n##\n(\"entity\"<|>GRAPHRAG<|>TECHNOLOGY, TOOL<|>Graph RAG is a method that combines knowledge graph generation, retrieval-augmented generation (RAG), and query-focused summarization (QFS) to support human sensemaking over text corpora)\n##\n(\"entity\"<|>GRAPH INDEX<|>TECHNOLOGY, TOOL<|>A graph index is a structure used in Graph RAG to achieve better summarization results and support various graph-related RAG approaches)\n##\n(\"entity\"<|>GLOBAL SUMMARIZATION<|>PROCESS, METHOD<|>Global summarization refers to summarizing source texts without using a graph-based approach)\n##\n(\"entity\"<|>COMPUTE BUDGET<|>RESOURCE, FACTOR<|>Compute budget refers to the computational resources allocated for building and using a graph index)\n##\n(\"entity\"<|>LIFETIME QUERIES<|>METRIC, FACTOR<|>Lifetime queries refer to the expected number of queries made over the lifetime of a dataset)\n##\n(\"entity\"<|>COMMUNITY SUMMARIES<|>OUTPUT, RESULT<|>Community summaries are generic summaries generated from the graph index, useful for various graph-related RAG approaches)\n##\n(\"entity\"<|>RICH TEXT ANNOTATIONS<|>DATA, ATTRIBUTE<|>Rich text annotations are detailed annotations that support the current Graph RAG approach)\n##\n(\"entity\"<|>HIERARCHICAL COMMUNITY STRUCTURE<|>STRUCTURE, FRAMEWORK<|>Hierarchical community structure supports the Graph RAG approach by organizing information in a layered manner)\n##\n(\"entity\"<|>EMBEDDING-BASED MATCHING<|>TECHNIQUE, METHOD<|>Embedding-based matching is a technique for matching user queries with graph annotations in a more local manner)\n##\n(\"entity\"<|>MAP-REDUCE SUMMARIZATION<|>TECHNIQUE, METHOD<|>Map-reduce summarization is a method used for summarizing source texts in a global but graph-free approach)\n##\n(\"entity\"<|>QUERY-FOCUSED SUMMARIZATION (QFS)<|>TECHNIQUE, METHOD<|>Query-focused summarization (QFS) is a technique used in Graph RAG to generate summaries based on specific queries)\n##\n(\"entity\"<|>NAÏVE RAG<|>BASELINE, METHOD<|>Naïve RAG is a baseline method for retrieval-augmented generation that does not use advanced techniques like graph indexing)\n##\n(\"entity\"<|>TEXT CORPORA<|>DATA, COLLECTION<|>Text corpora refer to large collections of texts used for analysis and summarization in Graph RAG)\n##\n(\"entity\"<|>TOKEN COST<|>METRIC, FACTOR<|>Token cost refers to the computational cost associated with processing tokens in summarization methods)\n##\n(\"entity\"<|>ALONSO<|>PERSON<|>Alonso is a person acknowledged for contributing to the work on Graph RAG)\n##\n(\"relationship\"<|>SELFHECKGPT<|>GRAPHRAG<|>SelfCheckGPT is mentioned as an approach for comparing fabrication rates, which could improve the current analysis involving Graph RAG<|>6)\n##\n(\"relationship\"<|>GRAPHRAG<|>GRAPH INDEX<|>Graph RAG uses a graph index to achieve better summarization results and support various graph-related RAG approaches<|>9)\n##\n(\"relationship\"<|>GRAPHRAG<|>GLOBAL SUMMARIZATION<|>Graph RAG is compared to global summarization methods that do not use a graph-based approach<|>8)\n##\n(\"relationship\"<|>GRAPH INDEX<|>COMPUTE BUDGET<|>The decision to invest in building a graph index depends on factors like the compute budget<|>7)\n##\n(\"relationship\"<|>GRAPH INDEX<|>LIFETIME QUERIES<|>The decision to invest in building a graph index depends on factors like the expected number of lifetime queries per dataset<|>7)\n##\n(\"relationship\"<|>GRAPH INDEX<|>COMMUNITY SUMMARIES<|>The value obtained from a graph index includes the generation of community summaries<|>8)\n##\n(\"relationship\"<|>GRAPHRAG<|>RICH TEXT ANNOTATIONS<|>Rich text annotations support the current Graph RAG approach<|>8)\n##\n(\"relationship\"<|>GRAPHRAG<|>HIERARCHICAL COMMUNITY STRUCTURE<|>Hierarchical community structure supports the current Graph RAG approach<|>8)\n##\n(\"relationship\"<|>GRAPHRAG<|>EMBEDDING-BASED MATCHING<|>Future work on Graph RAG includes embedding-based matching of user queries and graph annotations<|>7)\n##\n(\"relationship\"<|>GRAPHRAG<|>MAP-REDUCE SUMMARIZATION<|>Graph RAG is compared to a global but graph-free approach using map-reduce source text summarization<|>8)\n##\n(\"relationship\"<|>GRAPHRAG<|>QUERY-FOCUSED SUMMARIZATION (QFS)<|>Graph RAG combines query-focused summarization (QFS) to support human sensemaking over text corpora<|>9)\n##\n(\"relationship\"<|>GRAPHRAG<|>NAÏVE RAG<|>Initial evaluations show Graph RAG has substantial improvements over a naïve RAG baseline<|>9)\n##\n(\"relationship\"<|>GRAPHRAG<|>TEXT CORPORA<|>Graph RAG supports human sensemaking over entire text corpora<|>9)\n##\n(\"relationship\"<|>GRAPHRAG<|>TOKEN COST<|>Graph RAG achieves competitive performance to other global methods at a fraction of the token cost<|>8)\n##\n(\"relationship\"<|>GRAPHRAG<|>ALONSO<|>Alonso is acknowledged for contributing to the work on Graph RAG<|>6)\n<|COMPLETE|>"}]}