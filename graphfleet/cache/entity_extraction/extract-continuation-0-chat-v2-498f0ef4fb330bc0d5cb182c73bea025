{"result": "## \n(\"entity\"<|>GLOBAL NATURE QUESTIONS<|>CONCEPT, PHENOMENON<|>Questions of a global nature that are asked to understand broader contexts or datasets)\n##\n(\"entity\"<|>ENTIRE DATASETS<|>DATA, COLLECTION<|>Complete sets of data that are used for analysis or answering user questions)\n##\n(\"entity\"<|>LOCAL TEXT REGIONS<|>DATA, TEXT<|>Specific regions of text that contain answers or relevant information for a given task)\n##\n(\"entity\"<|>NATURAL LANGUAGE SUMMARIES<|>OUTPUT, TEXT<|>Summaries generated in natural language, as opposed to concatenated excerpts)\n##\n(\"entity\"<|>CONCATENATED EXCERPTS<|>OUTPUT, TEXT<|>Text outputs that are simply concatenations of relevant excerpts, rather than fully generated summaries)\n##\n(\"entity\"<|>SUMMARIZATION TASKS<|>ACTIVITY, TASK<|>Tasks that involve creating summaries of text, which can be abstractive or extractive, generic or query-focused, and single-document or multi-document)\n##\n(\"entity\"<|>STATE-OF-THE-ART<|>CONCEPT, PHENOMENON<|>The highest level of development or achievement in a particular field at a given time)\n##\n(\"entity\"<|>IN-CONTEXT LEARNING<|>TECHNOLOGY, METHOD<|>A method used by modern LLMs to learn and summarize content based on the context provided within their context window)\n##\n(\"entity\"<|>LONGER CONTEXTS<|>DATA, TEXT<|>Extended regions of text that can exceed the processing limits of LLM context windows)\n##\n(\"entity\"<|>TEXT CHUNKS<|>DATA, TEXT<|>Smaller segments of text that can be retrieved and processed individually)\n##\n(\"entity\"<|>ALTERNATIVE FORM OF PRE-INDEXING<|>TECHNOLOGY, METHOD<|>A different method of indexing that could enhance retrieval-augmented generation for global summarization tasks)\n##\n(\"entity\"<|>STRUCTURED RETRIEVAL AND TRAVERSAL<|>TECHNOLOGY, METHOD<|>Methods that utilize the structured nature of graph indexes for efficient information retrieval and traversal)\n##\n(\"entity\"<|>GRAPH INDEXES<|>TECHNOLOGY, DATA STRUCTURE<|>Structured data representations that facilitate efficient retrieval and traversal of information)\n##\n(\"relationship\"<|>GLOBAL NATURE QUESTIONS<|>ENTIRE DATASETS<|>Global nature questions are asked to understand broader contexts or entire datasets<|>7)\n##\n(\"relationship\"<|>RETRIEVAL-AUGMENTED GENERATION (RAG)<|>LOCAL TEXT REGIONS<|>RAG retrieves specific regions of text to provide grounding for the generation task<|>8)\n##\n(\"relationship\"<|>QUERY-FOCUSED SUMMARIZATION (QFS)<|>NATURAL LANGUAGE SUMMARIES<|>QFS generates natural language summaries based on specific queries<|>9)\n##\n(\"relationship\"<|>QUERY-FOCUSED SUMMARIZATION (QFS)<|>CONCATENATED EXCERPTS<|>QFS focuses on generating natural language summaries, not just concatenated excerpts<|>8)\n##\n(\"relationship\"<|>SUMMARIZATION TASKS<|>TRANSFORMER ARCHITECTURE<|>The transformer architecture showed substantial improvements in various summarization tasks<|>8)\n##\n(\"relationship\"<|>STATE-OF-THE-ART<|>TRANSFORMER ARCHITECTURE<|>Early applications of the transformer architecture improved the state-of-the-art in summarization tasks<|>8)\n##\n(\"relationship\"<|>IN-CONTEXT LEARNING<|>MODERN LLMS<|>Modern LLMs use in-context learning to summarize content<|>9)\n##\n(\"relationship\"<|>LONGER CONTEXTS<|>LLM CONTEXT WINDOWS<|>Longer contexts can exceed the limits of LLM context windows<|>8)\n##\n(\"relationship\"<|>TEXT CHUNKS<|>NAÏVE RAG<|>Naïve RAG retrieves text chunks, which may be inadequate for QFS tasks<|>7)\n##\n(\"relationship\"<|>ALTERNATIVE FORM OF PRE-INDEXING<|>PRE-INDEXING<|>An alternative form of pre-indexing could support a new RAG approach<|>7)\n##\n(\"relationship\"<|>STRUCTURED RETRIEVAL AND TRAVERSAL<|>GRAPH RAG<|>Graph RAG exploits structured retrieval and traversal affordances of graph indexes<|>8)\n##\n(\"relationship\"<|>GRAPH INDEXES<|>STRUCTURED RETRIEVAL AND TRAVERSAL<|>Graph indexes facilitate structured retrieval and traversal of information<|>8)\n<|COMPLETE|>", "input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n", "parameters": {"model": "gpt-4o", "temperature": 0.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "top_p": 1.0, "max_tokens": 4000, "n": 1}, "history": [{"role": "user", "content": "\n-Goal-\nGiven a text document that is potentially relevant to this activity, first identify all entities needed from the text in order to capture the information and ideas in the text.\nNext, report all relationships among the identified entities.\n\n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: Suggest several labels or categories for the entity. The categories should not be specific, but should be as general as possible.\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>)\n\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\nFormat each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n\n3. Return output in The primary language of the provided text is \"English.\" as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n\n4. If you have to translate into The primary language of the provided text is \"English.\", just translate the descriptions, nothing else!\n\n5. When finished, output <|COMPLETE|>.\n\n-Examples-\n######################\n\nExample 1:\n\ntext:\n he tasks studied in the lab thus far have tended to \nbe those for which researchers hypothesized generative AI would \nperform well . This was, in fact, the focus of most of the studies \npresented in the first AI and Productivity report we published  \n(Cambon et al. 2023) . Actual information work , however, often \nincludes a huge variety of tasks  and much of the unstructured and \ninformal work in people’s jobs is not yet directly supported by the \nfirst-generation of generative AI tools.  Software developer \nworkflows , for example,  involve far more than the hands-on coding supported by GitHub Copilot (Meyer et al. 2017). The ability to \nshed light on generative AI's productivity dynamics in the natural \ncomplexity of entire workflows is a key advantage of field studies \nof generative AI’s productivity impacts , and a major reason we \nhope to see many more field studies emerging in the literature\n------------------------\noutput:\n(\"entity\"<|>LAB<|>LOCATION, RESEARCH ENVIRONMENT<|>The lab is where tasks are studied to hypothesize the performance of generative AI)\n##\n(\"entity\"<|>GENERATIVE AI<|>TECHNOLOGY, TOOL<|>Generative AI refers to artificial intelligence systems that can generate content, such as text, based on input data)\n##\n(\"entity\"<|>AI AND PRODUCTIVITY REPORT<|>DOCUMENT, PUBLICATION<|>A report published by Cambon et al. in 2023 focusing on the performance of generative AI in various tasks)\n##\n(\"entity\"<|>INFORMATION WORK<|>ACTIVITY, TASK<|>Information work includes a variety of tasks, often unstructured and informal, that are part of people's jobs)\n##\n(\"entity\"<|>FIRST-GENERATION GENERATIVE AI TOOLS<|>TECHNOLOGY, TOOL<|>The initial versions of generative AI tools that support specific tasks but not the full range of unstructured work)\n##\n(\"entity\"<|>SOFTWARE DEVELOPER WORKFLOWS<|>ACTIVITY, TASK<|>The comprehensive set of activities involved in software development, beyond just coding)\n##\n(\"entity\"<|>GITHUB COPILOT<|>TECHNOLOGY, TOOL<|>A generative AI tool that assists with hands-on coding tasks for software developers)\n##\n(\"entity\"<|>FIELD STUDIES<|>RESEARCH METHOD, STUDY TYPE<|>Studies conducted in natural settings to observe the real-world impacts of generative AI on productivity)\n##\n(\"entity\"<|>PRODUCTIVITY DYNAMICS<|>CONCEPT, PHENOMENON<|>The various factors and interactions that affect productivity when using generative AI in real-world workflows)\n##\n(\"entity\"<|>LITERATURE<|>BODY OF WORK, RESEARCH<|>The collection of academic and professional studies and publications on a given topic)\n##\n(\"relationship\"<|>LAB<|>GENERATIVE AI<|>The lab is where tasks are studied to hypothesize the performance of generative AI<|>7)\n##\n(\"relationship\"<|>AI AND PRODUCTIVITY REPORT<|>GENERATIVE AI<|>The report focuses on the performance of generative AI in various tasks<|>8)\n##\n(\"relationship\"<|>INFORMATION WORK<|>FIRST-GENERATION GENERATIVE AI TOOLS<|>First-generation generative AI tools do not yet directly support much of the unstructured and informal information work<|>6)\n##\n(\"relationship\"<|>SOFTWARE DEVELOPER WORKFLOWS<|>GITHUB COPILOT<|>GitHub Copilot supports hands-on coding, which is a part of software developer workflows<|>9)\n##\n(\"relationship\"<|>FIELD STUDIES<|>PRODUCTIVITY DYNAMICS<|>Field studies help shed light on the productivity dynamics of generative AI in real-world settings<|>8)\n##\n(\"relationship\"<|>FIELD STUDIES<|>LITERATURE<|>The hope is to see more field studies emerging in the literature to understand generative AI's productivity impacts<|>7)\n<|COMPLETE|>\n#############################\n\n\nExample 2:\n\ntext:\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. (2023). Lost\nin the middle: How language models use long contexts. arXiv:2307.03172.\nLiu, Y . and Lapata, M. (2019). Hierarchical transformers for multi-document summarization. arXiv\npreprint arXiv:1905.13164 .\nLlamaIndex (2024). LlamaIndex Knowledge Graph Index. https://docs .llamaindex .ai/en/stable/\nexamples/index structs/knowledge graph/KnowledgeGraphDemo .html.\nManakul, P., Liusie, A., and Gales, M. J. (2023). Selfcheckgpt: Zero-resource black-box hallucina-\ntion detection for generative large language models. arXiv preprint ar\n------------------------\noutput:\n(\"entity\"<|>LIU, N. F.<|>PERSON<|>Liu, N. F. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>LIN, K.<|>PERSON<|>Lin, K. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>HEWITT, J.<|>PERSON<|>Hewitt, J. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>PARANJAPE, A.<|>PERSON<|>Paranjape, A. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>BEVILACQUA, M.<|>PERSON<|>Bevilacqua, M. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>PETRONI, F.<|>PERSON<|>Petroni, F. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>LIANG, P.<|>PERSON<|>Liang, P. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>LIU, Y.<|>PERSON<|>Liu, Y. is an author of the paper \"Hierarchical transformers for multi-document summarization\" published in 2019)\n##\n(\"entity\"<|>LAPATA, M.<|>PERSON<|>Lapata, M. is an author of the paper \"Hierarchical transformers for multi-document summarization\" published in 2019)\n##\n(\"entity\"<|>LLAMAINDEX<|>ORGANIZATION<|>LlamaIndex is an organization that developed the LlamaIndex Knowledge Graph Index, with documentation available online)\n##\n(\"entity\"<|>MANAKUL, P.<|>PERSON<|>Manakul, P. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published in 2023)\n##\n(\"entity\"<|>LIUSIE, A.<|>PERSON<|>Liusie, A. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published in 2023)\n##\n(\"entity\"<|>GALES, M. J.<|>PERSON<|>Gales, M. J. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published in 2023)\n##\n(\"entity\"<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>DOCUMENT<|>A paper published in 2023 by Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P.)\n##\n(\"entity\"<|>HIERARCHICAL TRANSFORMERS FOR MULTI-DOCUMENT SUMMARIZATION<|>DOCUMENT<|>A paper published in 2019 by Liu, Y. and Lapata, M.)\n##\n(\"entity\"<|>SELFCHECKGPT: ZERO-RESOURCE BLACK-BOX HALLUCINATION DETECTION FOR GENERATIVE LARGE LANGUAGE MODELS<|>DOCUMENT<|>A paper published in 2023 by Manakul, P., Liusie, A., and Gales, M. J.)\n##\n(\"relationship\"<|>LIU, N. F.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Liu, N. F. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>LIN, K.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Lin, K. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>HEWITT, J.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Hewitt, J. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>PARANJAPE, A.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Paranjape, A. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>BEVILACQUA, M.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Bevilacqua, M. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>PETRONI, F.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Petroni, F. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>LIANG, P.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Liang, P. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>LIU, Y.<|>HIERARCHICAL TRANSFORMERS FOR MULTI-DOCUMENT SUMMARIZATION<|>Liu, Y. is an author of the paper \"Hierarchical transformers for multi-document summarization\"<|>9)\n##\n(\"relationship\"<|>LAPATA, M.<|>HIERARCHICAL TRANSFORMERS FOR MULTI-DOCUMENT SUMMARIZATION<|>Lapata, M. is an author of the paper \"Hierarchical transformers for multi-document summarization\"<|>9)\n##\n(\"relationship\"<|>MANAKUL, P.<|>SELFCHECKGPT: ZERO-RESOURCE BLACK-BOX HALLUCINATION DETECTION FOR GENERATIVE LARGE LANGUAGE MODELS<|>Manakul, P. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\"<|>9)\n##\n(\"relationship\"<|>LIUSIE, A.<|>SELFCHECKGPT: ZERO-RESOURCE BLACK-BOX HALLUCINATION DETECTION FOR GENERATIVE LARGE LANGUAGE MODELS<|>Liusie, A. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\"<|>9)\n##\n(\"relationship\"<|>GALES, M. J.<|>SELFCHECKGPT: ZERO-RESOURCE BLACK-BOX HALLUCINATION DETECTION FOR GENERATIVE LARGE LANGUAGE MODELS<|>Gales, M. J. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\"<|>9)\n<|COMPLETE|>\n#############################\n\n\n\n-Real Data-\n######################\ntext: et al., 2006b) by asking questions of a global nature.\nRetrieval-augmented generation (RAG, Lewis et al., 2020) is an established approach to answering\nuser questions over entire datasets, but it is designed for situations where these answers are contained\nlocally within regions of text whose retrieval provides sufficient grounding for the generation task.\nInstead, a more appropriate task framing is query-focused summarization (QFS, Dang, 2006), and in\nparticular, query-focused abstractive summarization that generates natural language summaries and\nnot just concatenated excerpts (Baumel et al., 2018; Laskar et al., 2020; Yao et al., 2017) . In recent\nyears, however, such distinctions between summarization tasks that are abstractive versus extractive,\ngeneric versus query-focused, and single-document versus multi-document, have become less rele-\nvant. While early applications of the transformer architecture showed substantial improvements on\nthe state-of-the-art for all such summarization tasks (Goodwin et al., 2020; Laskar et al., 2022; Liu\nand Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam\net al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) series,\nall of which can use in-context learning to summarize any content provided in their context window.\nThe challenge remains, however, for query-focused abstractive summarization over an entire corpus.\nSuch volumes of text can greatly exceed the limits of LLM context windows, and the expansion of\nsuch windows may not be enough given that information can be “lost in the middle” of longer\ncontexts (Kuratov et al., 2024; Liu et al., 2023). In addition, although the direct retrieval of text\nchunks in na ¨ıve RAG is likely inadequate for QFS tasks, it is possible that an alternative form of\npre-indexing could support a new RAG approach specifically targeting global summarization.\nIn this paper, we present a Graph RAG approach based on global summarization of an LLM-derived\nknowledge graph (Figure 1). In contrast with related work that exploits the structured retrieval\nand traversal affordances of graph indexes (subsection 4.2\n######################\noutput:\n"}, {"role": "assistant", "content": "## \n(\"entity\"<|>RETRIEVAL-AUGMENTED GENERATION (RAG)<|>TECHNOLOGY, APPROACH<|>An established approach to answering user questions over entire datasets by retrieving relevant text regions to provide grounding for the generation task)\n##\n(\"entity\"<|>QUERY-FOCUSED SUMMARIZATION (QFS)<|>TECHNOLOGY, APPROACH<|>A task framing that focuses on generating natural language summaries based on specific queries, rather than just concatenating excerpts)\n##\n(\"entity\"<|>TRANSFORMER ARCHITECTURE<|>TECHNOLOGY, FRAMEWORK<|>An architecture that has shown substantial improvements in various summarization tasks, including abstractive and extractive, generic and query-focused, and single-document and multi-document summarization)\n##\n(\"entity\"<|>MODERN LLMS<|>TECHNOLOGY, TOOL<|>Modern large language models, such as GPT, Llama, and Gemini, which can use in-context learning to summarize content provided in their context window)\n##\n(\"entity\"<|>GPT<|>TECHNOLOGY, TOOL<|>A series of large language models capable of in-context learning to summarize content)\n##\n(\"entity\"<|>LLAMA<|>TECHNOLOGY, TOOL<|>A series of large language models capable of in-context learning to summarize content)\n##\n(\"entity\"<|>GEMINI<|>TECHNOLOGY, TOOL<|>A series of large language models capable of in-context learning to summarize content)\n##\n(\"entity\"<|>QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION<|>TECHNOLOGY, APPROACH<|>A summarization task that generates natural language summaries based on specific queries, particularly challenging over an entire corpus)\n##\n(\"entity\"<|>LLM CONTEXT WINDOWS<|>TECHNOLOGY, LIMITATION<|>The context windows of large language models, which can limit the amount of text they can process at once)\n##\n(\"entity\"<|>GRAPH RAG<|>TECHNOLOGY, APPROACH<|>A new approach based on global summarization of an LLM-derived knowledge graph, contrasting with traditional RAG methods)\n##\n(\"entity\"<|>KNOWLEDGE GRAPH<|>TECHNOLOGY, DATA STRUCTURE<|>A structured representation of knowledge used in the Graph RAG approach for global summarization)\n##\n(\"entity\"<|>GLOBAL SUMMARIZATION<|>TECHNOLOGY, APPROACH<|>A summarization approach that targets summarizing entire datasets or corpora, rather than localized text regions)\n##\n(\"entity\"<|>NAÏVE RAG<|>TECHNOLOGY, APPROACH<|>A basic form of retrieval-augmented generation that may be inadequate for query-focused summarization tasks)\n##\n(\"entity\"<|>PRE-INDEXING<|>TECHNOLOGY, APPROACH<|>An alternative form of indexing that could support a new RAG approach specifically targeting global summarization)\n##\n(\"entity\"<|>FIGURE 1<|>DOCUMENT ELEMENT<|>A figure in the paper that illustrates the Graph RAG approach based on global summarization of an LLM-derived knowledge graph)\n##\n(\"entity\"<|>STRUCTURED RETRIEVAL<|>TECHNOLOGY, APPROACH<|>A method that exploits the structured retrieval and traversal affordances of graph indexes)\n##\n(\"entity\"<|>TRAVERSAL AFFORDANCES<|>TECHNOLOGY, FEATURE<|>Features of graph indexes that allow for efficient traversal and retrieval of information)\n##\n(\"entity\"<|>KURATOV ET AL., 2024<|>REFERENCE, PUBLICATION<|>A reference to a publication discussing the limitations of LLM context windows and the issue of information being \"lost in the middle\" of longer contexts)\n##\n(\"entity\"<|>LIU ET AL., 2023<|>REFERENCE, PUBLICATION<|>A reference to a publication discussing the limitations of LLM context windows and the issue of information being \"lost in the middle\" of longer contexts)\n##\n(\"entity\"<|>LEWIS ET AL., 2020<|>REFERENCE, PUBLICATION<|>A reference to a publication on retrieval-augmented generation (RAG))\n##\n(\"entity\"<|>DANG, 2006<|>REFERENCE, PUBLICATION<|>A reference to a publication on query-focused summarization (QFS))\n##\n(\"entity\"<|>BAUMEL ET AL., 2018<|>REFERENCE, PUBLICATION<|>A reference to a publication on query-focused abstractive summarization)\n##\n(\"entity\"<|>LASKAR ET AL., 2020<|>REFERENCE, PUBLICATION<|>A reference to a publication on query-focused abstractive summarization)\n##\n(\"entity\"<|>YAO ET AL., 2017<|>REFERENCE, PUBLICATION<|>A reference to a publication on query-focused abstractive summarization)\n##\n(\"entity\"<|>GOODWIN ET AL., 2020<|>REFERENCE, PUBLICATION<|>A reference to a publication on the early applications of the transformer architecture in summarization tasks)\n##\n(\"entity\"<|>LASKAR ET AL., 2022<|>REFERENCE, PUBLICATION<|>A reference to a publication on the early applications of the transformer architecture in summarization tasks)\n##\n(\"entity\"<|>LIU AND LAPATA, 2019<|>REFERENCE, PUBLICATION<|>A reference to a publication on the early applications of the transformer architecture in summarization tasks)\n##\n(\"entity\"<|>ACHIAM ET AL., 2023<|>REFERENCE, PUBLICATION<|>A reference to a publication on the GPT series of large language models)\n##\n(\"entity\"<|>BROWN ET AL., 2020<|>REFERENCE, PUBLICATION<|>A reference to a publication on the GPT series of large language models)\n##\n(\"entity\"<|>TOUVRON ET AL., 2023<|>REFERENCE, PUBLICATION<|>A reference to a publication on the Llama series of large language models)\n##\n(\"entity\"<|>ANIL ET AL., 2023<|>REFERENCE, PUBLICATION<|>A reference to a publication on the Gemini series of large language models)\n##\n(\"relationship\"<|>RETRIEVAL-AUGMENTED GENERATION (RAG)<|>QUERY-FOCUSED SUMMARIZATION (QFS)<|>RAG is designed for situations where answers are contained locally within text regions, while QFS focuses on generating summaries based on specific queries<|>7)\n##\n(\"relationship\"<|>TRANSFORMER ARCHITECTURE<|>MODERN LLMS<|>Early applications of the transformer architecture showed substantial improvements in summarization tasks, which are now trivialized by modern LLMs<|>8)\n##\n(\"relationship\"<|>GPT<|>MODERN LLMS<|>GPT is a series of modern large language models capable of in-context learning to summarize content<|>9)\n##\n(\"relationship\"<|>LLAMA<|>MODERN LLMS<|>Llama is a series of modern large language models capable of in-context learning to summarize content<|>9)\n##\n(\"relationship\"<|>GEMINI<|>MODERN LLMS<|>Gemini is a series of modern large language models capable of in-context learning to summarize content<|>9)\n##\n(\"relationship\"<|>QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION<|>LLM CONTEXT WINDOWS<|>Query-focused abstractive summarization over an entire corpus is challenging due to the limits of LLM context windows<|>8)\n##\n(\"relationship\"<|>GRAPH RAG<|>KNOWLEDGE GRAPH<|>Graph RAG is based on global summarization of an LLM-derived knowledge graph<|>9)\n##\n(\"relationship\"<|>GLOBAL SUMMARIZATION<|>GRAPH RAG<|>Graph RAG targets global summarization, contrasting with traditional RAG methods<|>8)\n##\n(\"relationship\"<|>NAÏVE RAG<|>QUERY-FOCUSED SUMMARIZATION (QFS)<|>Naïve RAG is likely inadequate for QFS tasks<|>7)\n##\n(\"relationship\"<|>PRE-INDEXING<|>NAÏVE RAG<|>An alternative form of pre-indexing could support a new RAG approach specifically targeting global summarization<|>7)\n##\n(\"relationship\"<|>STRUCTURED RETRIEVAL<|>GRAPH RAG<|>Graph RAG exploits the structured retrieval affordances of graph indexes<|>8)\n##\n(\"relationship\"<|>TRAVERSAL AFFORDANCES<|>GRAPH RAG<|>Graph RAG exploits the traversal affordances of graph indexes<|>8)\n##\n(\"relationship\"<|>KURATOV ET AL., 2024<|>LLM CONTEXT WINDOWS<|>Kuratov et al. discuss the limitations of LLM context windows and the issue of information being \"lost in the middle\" of longer contexts<|>8)\n##\n(\"relationship\"<|>LIU ET AL., 2023<|>LLM CONTEXT WINDOWS<|>Liu et al. discuss the limitations of LLM context windows and the issue of information being \"lost in the middle\" of longer contexts<|>8)\n##\n(\"relationship\"<|>LEWIS ET AL., 2020<|>RETRIEVAL-AUGMENTED GENERATION (RAG)<|>Lewis et al. published a paper on retrieval-augmented generation (RAG)<|>9)\n##\n(\"relationship\"<|>DANG, 2006<|>QUERY-FOCUSED SUMMARIZATION (QFS)<|>Dang published a paper on query-focused summarization (QFS)<|>9)\n##\n(\"relationship\"<|>BAUMEL ET AL., 2018<|>QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION<|>Baumel et al. published a paper on query-focused abstractive summarization<|>9)\n##\n(\"relationship\"<|>LASKAR ET AL., 2020<|>QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION<|>Laskar et al. published a paper on query-focused abstractive summarization<|>9)\n##\n(\"relationship\"<|>YAO ET AL., 2017<|>QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION<|>Yao et al. published a paper on query-focused abstractive summarization<|>9)\n##\n(\"relationship\"<|>GOODWIN ET AL., 2020<|>TRANSFORMER ARCHITECTURE<|>Goodwin et al. published a paper on the early applications of the transformer architecture in summarization tasks<|>9)\n##\n(\"relationship\"<|>LASKAR ET AL., 2022<|>TRANSFORMER ARCHITECTURE<|>Laskar et al. published a paper on the early applications of the transformer architecture in summarization tasks<|>9)\n##\n(\"relationship\"<|>LIU AND LAPATA, 2019<|>TRANSFORMER ARCHITECTURE<|>Liu and Lapata published a paper on the early applications of the transformer architecture in summarization tasks<|>9)\n##\n(\"relationship\"<|>ACHIAM ET AL., 2023<|>GPT<|>Achiam et al. published a paper on the GPT series of large language models<|>9)\n##\n(\"relationship\"<|>BROWN ET AL., 2020<|>GPT<|>Brown et al. published a paper on the GPT series of large language models<|>9)\n##\n(\"relationship\"<|>TOUVRON ET AL., 2023<|>LLAMA<|>Touvron et al. published a paper on the Llama series of large language models<|>9)\n##\n(\"relationship\"<|>ANIL ET AL., 2023<|>GEMINI<|>Anil et al. published a paper on the Gemini series of large language models<|>9)\n<|COMPLETE|>"}]}