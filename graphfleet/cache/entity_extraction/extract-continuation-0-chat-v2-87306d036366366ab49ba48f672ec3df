{"result": "## \n(\"entity\"<|>EXTERNAL KNOWLEDGE SOURCE<|>TECHNOLOGY, TOOL<|>A source of information outside the primary document collection, used in retrieval-augmented generation (RAG) to provide relevant information)\n##\n(\"entity\"<|>PRIVATE TEXT CORPORA<|>DATA, DOCUMENT COLLECTION<|>Collections of documents that are not publicly accessible, used in the Graph RAG approach for question answering)\n##\n(\"entity\"<|>MAIN THEMES<|>CONCEPT, TOPIC<|>The primary subjects or ideas present in a dataset, which are often the focus of global questions)\n##\n(\"entity\"<|>TEXT CORPUS<|>DATA, DOCUMENT COLLECTION<|>A large collection of texts or documents, often used in language model training and information retrieval tasks)\n##\n(\"entity\"<|>USER QUESTIONS<|>CONCEPT, INPUT<|>Queries posed by users that the Graph RAG approach aims to answer comprehensively and accurately)\n##\n(\"entity\"<|>SOURCE DOCUMENTS<|>DATA, DOCUMENT COLLECTION<|>The original texts from which information is retrieved and summarized in the Graph RAG approach)\n##\n(\"entity\"<|>PARTIAL RESPONSE<|>CONCEPT, OUTPUT<|>An intermediate answer generated from community summaries, which is later combined into a final response)\n##\n(\"entity\"<|>FINAL RESPONSE<|>CONCEPT, OUTPUT<|>The comprehensive answer provided to the user after summarizing all partial responses)\n##\n(\"entity\"<|>COMPREHENSIVENESS<|>CONCEPT, QUALITY<|>The extent to which an answer covers all relevant aspects of a question, improved by the Graph RAG approach)\n##\n(\"entity\"<|>DIVERSITY<|>CONCEPT, QUALITY<|>The variety of perspectives and information included in an answer, enhanced by the Graph RAG approach)\n##\n(\"entity\"<|>NAÏVE RAG BASELINE<|>TECHNOLOGY, METHOD<|>A basic implementation of retrieval-augmented generation (RAG) used as a comparison point in the study)\n##\n(\"entity\"<|>PYTHON<|>TECHNOLOGY, PROGRAMMING LANGUAGE<|>The programming language used to implement the open-source version of the Graph RAG approach)\n##\n(\"relationship\"<|>EXTERNAL KNOWLEDGE SOURCE<|>RETRIEVAL-AUGMENTED GENERATION (RAG)<|>External knowledge sources are used in RAG to retrieve relevant information<|>9)\n##\n(\"relationship\"<|>PRIVATE TEXT CORPORA<|>GRAPH RAG<|>Graph RAG is used to answer questions over private text corpora<|>9)\n##\n(\"relationship\"<|>MAIN THEMES<|>GLOBAL SENSEMAKING QUESTIONS<|>Global sensemaking questions often ask about the main themes in a dataset<|>8)\n##\n(\"relationship\"<|>TEXT CORPUS<|>RETRIEVAL-AUGMENTED GENERATION (RAG)<|>RAG retrieves information from a text corpus to answer questions<|>8)\n##\n(\"relationship\"<|>USER QUESTIONS<|>GRAPH RAG<|>Graph RAG aims to answer user questions comprehensively and accurately<|>9)\n##\n(\"relationship\"<|>SOURCE DOCUMENTS<|>ENTITY KNOWLEDGE GRAPH<|>An entity knowledge graph is derived from source documents in the Graph RAG approach<|>9)\n##\n(\"relationship\"<|>PARTIAL RESPONSE<|>COMMUNITY SUMMARIES<|>Community summaries are used to generate partial responses<|>9)\n##\n(\"relationship\"<|>FINAL RESPONSE<|>PARTIAL RESPONSE<|>Partial responses are summarized to form the final response<|>9)\n##\n(\"relationship\"<|>COMPREHENSIVENESS<|>GRAPH RAG<|>Graph RAG improves the comprehensiveness of generated answers<|>9)\n##\n(\"relationship\"<|>DIVERSITY<|>GRAPH RAG<|>Graph RAG enhances the diversity of generated answers<|>9)\n##\n(\"relationship\"<|>NAÏVE RAG BASELINE<|>GRAPH RAG<|>Graph RAG shows substantial improvements over a naïve RAG baseline<|>8)\n##\n(\"relationship\"<|>PYTHON<|>OPEN-SOURCE IMPLEMENTATION<|>The open-source implementation of Graph RAG is developed in Python<|>8)\n<|COMPLETE|>", "input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n", "parameters": {"model": "gpt-4o", "temperature": 0.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "top_p": 1.0, "max_tokens": 4000, "n": 1}, "history": [{"role": "user", "content": "\n-Goal-\nGiven a text document that is potentially relevant to this activity, first identify all entities needed from the text in order to capture the information and ideas in the text.\nNext, report all relationships among the identified entities.\n\n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: Suggest several labels or categories for the entity. The categories should not be specific, but should be as general as possible.\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>)\n\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\nFormat each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n\n3. Return output in The primary language of the provided text is \"English.\" as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n\n4. If you have to translate into The primary language of the provided text is \"English.\", just translate the descriptions, nothing else!\n\n5. When finished, output <|COMPLETE|>.\n\n-Examples-\n######################\n\nExample 1:\n\ntext:\n he tasks studied in the lab thus far have tended to \nbe those for which researchers hypothesized generative AI would \nperform well . This was, in fact, the focus of most of the studies \npresented in the first AI and Productivity report we published  \n(Cambon et al. 2023) . Actual information work , however, often \nincludes a huge variety of tasks  and much of the unstructured and \ninformal work in people’s jobs is not yet directly supported by the \nfirst-generation of generative AI tools.  Software developer \nworkflows , for example,  involve far more than the hands-on coding supported by GitHub Copilot (Meyer et al. 2017). The ability to \nshed light on generative AI's productivity dynamics in the natural \ncomplexity of entire workflows is a key advantage of field studies \nof generative AI’s productivity impacts , and a major reason we \nhope to see many more field studies emerging in the literature\n------------------------\noutput:\n(\"entity\"<|>LAB<|>LOCATION, RESEARCH ENVIRONMENT<|>The lab is where tasks are studied to hypothesize the performance of generative AI)\n##\n(\"entity\"<|>GENERATIVE AI<|>TECHNOLOGY, TOOL<|>Generative AI refers to artificial intelligence systems that can generate content, such as text, based on input data)\n##\n(\"entity\"<|>AI AND PRODUCTIVITY REPORT<|>DOCUMENT, PUBLICATION<|>A report published by Cambon et al. in 2023 focusing on the performance of generative AI in various tasks)\n##\n(\"entity\"<|>INFORMATION WORK<|>ACTIVITY, TASK<|>Information work includes a variety of tasks, often unstructured and informal, that are part of people's jobs)\n##\n(\"entity\"<|>FIRST-GENERATION GENERATIVE AI TOOLS<|>TECHNOLOGY, TOOL<|>The initial versions of generative AI tools that support specific tasks but not the full range of unstructured work)\n##\n(\"entity\"<|>SOFTWARE DEVELOPER WORKFLOWS<|>ACTIVITY, TASK<|>The comprehensive set of activities involved in software development, beyond just coding)\n##\n(\"entity\"<|>GITHUB COPILOT<|>TECHNOLOGY, TOOL<|>A generative AI tool that assists with hands-on coding tasks for software developers)\n##\n(\"entity\"<|>FIELD STUDIES<|>RESEARCH METHOD, STUDY TYPE<|>Studies conducted in natural settings to observe the real-world impacts of generative AI on productivity)\n##\n(\"entity\"<|>PRODUCTIVITY DYNAMICS<|>CONCEPT, PHENOMENON<|>The various factors and interactions that affect productivity when using generative AI in real-world workflows)\n##\n(\"entity\"<|>LITERATURE<|>BODY OF WORK, RESEARCH<|>The collection of academic and professional studies and publications on a given topic)\n##\n(\"relationship\"<|>LAB<|>GENERATIVE AI<|>The lab is where tasks are studied to hypothesize the performance of generative AI<|>7)\n##\n(\"relationship\"<|>AI AND PRODUCTIVITY REPORT<|>GENERATIVE AI<|>The report focuses on the performance of generative AI in various tasks<|>8)\n##\n(\"relationship\"<|>INFORMATION WORK<|>FIRST-GENERATION GENERATIVE AI TOOLS<|>First-generation generative AI tools do not yet directly support much of the unstructured and informal information work<|>6)\n##\n(\"relationship\"<|>SOFTWARE DEVELOPER WORKFLOWS<|>GITHUB COPILOT<|>GitHub Copilot supports hands-on coding, which is a part of software developer workflows<|>9)\n##\n(\"relationship\"<|>FIELD STUDIES<|>PRODUCTIVITY DYNAMICS<|>Field studies help shed light on the productivity dynamics of generative AI in real-world settings<|>8)\n##\n(\"relationship\"<|>FIELD STUDIES<|>LITERATURE<|>The hope is to see more field studies emerging in the literature to understand generative AI's productivity impacts<|>7)\n<|COMPLETE|>\n#############################\n\n\nExample 2:\n\ntext:\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. (2023). Lost\nin the middle: How language models use long contexts. arXiv:2307.03172.\nLiu, Y . and Lapata, M. (2019). Hierarchical transformers for multi-document summarization. arXiv\npreprint arXiv:1905.13164 .\nLlamaIndex (2024). LlamaIndex Knowledge Graph Index. https://docs .llamaindex .ai/en/stable/\nexamples/index structs/knowledge graph/KnowledgeGraphDemo .html.\nManakul, P., Liusie, A., and Gales, M. J. (2023). Selfcheckgpt: Zero-resource black-box hallucina-\ntion detection for generative large language models. arXiv preprint ar\n------------------------\noutput:\n(\"entity\"<|>LIU, N. F.<|>PERSON<|>Liu, N. F. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>LIN, K.<|>PERSON<|>Lin, K. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>HEWITT, J.<|>PERSON<|>Hewitt, J. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>PARANJAPE, A.<|>PERSON<|>Paranjape, A. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>BEVILACQUA, M.<|>PERSON<|>Bevilacqua, M. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>PETRONI, F.<|>PERSON<|>Petroni, F. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>LIANG, P.<|>PERSON<|>Liang, P. is an author of the paper \"Lost in the middle: How language models use long contexts\" published in 2023)\n##\n(\"entity\"<|>LIU, Y.<|>PERSON<|>Liu, Y. is an author of the paper \"Hierarchical transformers for multi-document summarization\" published in 2019)\n##\n(\"entity\"<|>LAPATA, M.<|>PERSON<|>Lapata, M. is an author of the paper \"Hierarchical transformers for multi-document summarization\" published in 2019)\n##\n(\"entity\"<|>LLAMAINDEX<|>ORGANIZATION<|>LlamaIndex is an organization that developed the LlamaIndex Knowledge Graph Index, with documentation available online)\n##\n(\"entity\"<|>MANAKUL, P.<|>PERSON<|>Manakul, P. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published in 2023)\n##\n(\"entity\"<|>LIUSIE, A.<|>PERSON<|>Liusie, A. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published in 2023)\n##\n(\"entity\"<|>GALES, M. J.<|>PERSON<|>Gales, M. J. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published in 2023)\n##\n(\"entity\"<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>DOCUMENT<|>A paper published in 2023 by Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P.)\n##\n(\"entity\"<|>HIERARCHICAL TRANSFORMERS FOR MULTI-DOCUMENT SUMMARIZATION<|>DOCUMENT<|>A paper published in 2019 by Liu, Y. and Lapata, M.)\n##\n(\"entity\"<|>SELFCHECKGPT: ZERO-RESOURCE BLACK-BOX HALLUCINATION DETECTION FOR GENERATIVE LARGE LANGUAGE MODELS<|>DOCUMENT<|>A paper published in 2023 by Manakul, P., Liusie, A., and Gales, M. J.)\n##\n(\"relationship\"<|>LIU, N. F.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Liu, N. F. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>LIN, K.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Lin, K. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>HEWITT, J.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Hewitt, J. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>PARANJAPE, A.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Paranjape, A. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>BEVILACQUA, M.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Bevilacqua, M. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>PETRONI, F.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Petroni, F. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>LIANG, P.<|>LOST IN THE MIDDLE: HOW LANGUAGE MODELS USE LONG CONTEXTS<|>Liang, P. is an author of the paper \"Lost in the middle: How language models use long contexts\"<|>9)\n##\n(\"relationship\"<|>LIU, Y.<|>HIERARCHICAL TRANSFORMERS FOR MULTI-DOCUMENT SUMMARIZATION<|>Liu, Y. is an author of the paper \"Hierarchical transformers for multi-document summarization\"<|>9)\n##\n(\"relationship\"<|>LAPATA, M.<|>HIERARCHICAL TRANSFORMERS FOR MULTI-DOCUMENT SUMMARIZATION<|>Lapata, M. is an author of the paper \"Hierarchical transformers for multi-document summarization\"<|>9)\n##\n(\"relationship\"<|>MANAKUL, P.<|>SELFCHECKGPT: ZERO-RESOURCE BLACK-BOX HALLUCINATION DETECTION FOR GENERATIVE LARGE LANGUAGE MODELS<|>Manakul, P. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\"<|>9)\n##\n(\"relationship\"<|>LIUSIE, A.<|>SELFCHECKGPT: ZERO-RESOURCE BLACK-BOX HALLUCINATION DETECTION FOR GENERATIVE LARGE LANGUAGE MODELS<|>Liusie, A. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\"<|>9)\n##\n(\"relationship\"<|>GALES, M. J.<|>SELFCHECKGPT: ZERO-RESOURCE BLACK-BOX HALLUCINATION DETECTION FOR GENERATIVE LARGE LANGUAGE MODELS<|>Gales, M. J. is an author of the paper \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\"<|>9)\n<|COMPLETE|>\n#############################\n\n\n\n-Real Data-\n######################\ntext: From Local to Global: A Graph RAG Approach to\nQuery-Focused Summarization\nDarren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3\nApurva Mody3Steven Truitt2\nJonathan Larson1\n1Microsoft Research\n2Microsoft Strategic Missions and Technologies\n3Microsoft Office of the CTO\n{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso }\n@microsoft.com\n†These authors contributed equally to this work\nAbstract\nThe use of retrieval-augmented generation (RAG) to retrieve relevant informa-\ntion from an external knowledge source enables large language models (LLMs)\nto answer questions over private and/or previously unseen document collections.\nHowever, RAG fails on global questions directed at an entire text corpus, such\nas “What are the main themes in the dataset?”, since this is inherently a query-\nfocused summarization (QFS) task, rather than an explicit retrieval task. Prior\nQFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\nRAG systems. To combine the strengths of these contrasting methods, we propose\na Graph RAG approach to question answering over private text corpora that scales\nwith both the generality of user questions and the quantity of source text to be in-\ndexed. Our approach uses an LLM to build a graph-based text index in two stages:\nfirst to derive an entity knowledge graph from the source documents, then to pre-\ngenerate community summaries for all groups of closely-related entities. Given a\nquestion, each community summary is used to generate a partial response, before\nall partial responses are again summarized in a final response to the user. For a\nclass of global sensemaking questions over datasets in the 1 million token range,\nwe show that Graph RAG leads to substantial improvements over a na ¨ıve RAG\nbaseline for both the comprehensiveness and diversity of generated answers. An\nopen-source, Python-based implementation of both global and local Graph RAG\napproaches is forthcoming at https://aka .ms/graphrag .\n1 Introduction\nHuman endeavors across a range of domains rely on our ability to read and reason about large\ncollections of documents, often reaching conclusions that go beyond anything stated in the source\ntexts themselves. With the emergence of large language models (LLMs), we are already witnessing\nattempts to automate human-like sense\n######################\noutput:\n"}, {"role": "assistant", "content": "## \n(\"entity\"<|>FROM LOCAL TO GLOBAL: A GRAPH RAG APPROACH TO QUERY-FOCUSED SUMMARIZATION<|>DOCUMENT, PUBLICATION<|>A research paper discussing the use of a Graph RAG approach for query-focused summarization, authored by Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson)\n##\n(\"entity\"<|>DARREN EDGE<|>PERSON<|>Darren Edge is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\" and is affiliated with Microsoft Research)\n##\n(\"entity\"<|>HA TRINH<|>PERSON<|>Ha Trinh is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\" and is affiliated with Microsoft Research)\n##\n(\"entity\"<|>NEWMAN CHENG<|>PERSON<|>Newman Cheng is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\" and is affiliated with Microsoft Strategic Missions and Technologies)\n##\n(\"entity\"<|>JOSHUA BRADLEY<|>PERSON<|>Joshua Bradley is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\" and is affiliated with Microsoft Strategic Missions and Technologies)\n##\n(\"entity\"<|>ALEX CHAO<|>PERSON<|>Alex Chao is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\" and is affiliated with Microsoft Office of the CTO)\n##\n(\"entity\"<|>APURVA MODY<|>PERSON<|>Apurva Mody is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\" and is affiliated with Microsoft Office of the CTO)\n##\n(\"entity\"<|>STEVEN TRUITT<|>PERSON<|>Steven Truitt is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\" and is affiliated with Microsoft Strategic Missions and Technologies)\n##\n(\"entity\"<|>JONATHAN LARSON<|>PERSON<|>Jonathan Larson is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\" and is affiliated with Microsoft Research)\n##\n(\"entity\"<|>MICROSOFT RESEARCH<|>ORGANIZATION<|>A division of Microsoft involved in research and development, affiliated with authors Darren Edge, Ha Trinh, and Jonathan Larson)\n##\n(\"entity\"<|>MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES<|>ORGANIZATION<|>A division of Microsoft involved in strategic missions and technologies, affiliated with authors Newman Cheng, Joshua Bradley, and Steven Truitt)\n##\n(\"entity\"<|>MICROSOFT OFFICE OF THE CTO<|>ORGANIZATION<|>A division of Microsoft led by the Chief Technology Officer, affiliated with authors Alex Chao and Apurva Mody)\n##\n(\"entity\"<|>RETRIEVAL-AUGMENTED GENERATION (RAG)<|>TECHNOLOGY, METHOD<|>A method that uses retrieval from external knowledge sources to enable large language models to answer questions over private and/or previously unseen document collections)\n##\n(\"entity\"<|>LARGE LANGUAGE MODELS (LLMS)<|>TECHNOLOGY, TOOL<|>Advanced AI models capable of understanding and generating human-like text, used in the Graph RAG approach)\n##\n(\"entity\"<|>QUERY-FOCUSED SUMMARIZATION (QFS)<|>TECHNOLOGY, METHOD<|>A task that involves summarizing information based on specific queries, rather than explicit retrieval tasks)\n##\n(\"entity\"<|>GRAPH RAG<|>TECHNOLOGY, METHOD<|>An approach that combines retrieval-augmented generation with graph-based text indexing to answer questions over private text corpora)\n##\n(\"entity\"<|>ENTITY KNOWLEDGE GRAPH<|>TECHNOLOGY, METHOD<|>A graph-based representation of entities derived from source documents, used in the Graph RAG approach)\n##\n(\"entity\"<|>COMMUNITY SUMMARIES<|>TECHNOLOGY, METHOD<|>Pre-generated summaries for groups of closely-related entities, used to generate partial responses in the Graph RAG approach)\n##\n(\"entity\"<|>GLOBAL SENSEMAKING QUESTIONS<|>CONCEPT, TASK<|>Questions that require understanding and summarizing large datasets to provide comprehensive and diverse answers)\n##\n(\"entity\"<|>OPEN-SOURCE IMPLEMENTATION<|>TECHNOLOGY, TOOL<|>A forthcoming Python-based implementation of both global and local Graph RAG approaches, available at https://aka.ms/graphrag)\n##\n(\"relationship\"<|>DARREN EDGE<|>FROM LOCAL TO GLOBAL: A GRAPH RAG APPROACH TO QUERY-FOCUSED SUMMARIZATION<|>Darren Edge is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<|>9)\n##\n(\"relationship\"<|>HA TRINH<|>FROM LOCAL TO GLOBAL: A GRAPH RAG APPROACH TO QUERY-FOCUSED SUMMARIZATION<|>Ha Trinh is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<|>9)\n##\n(\"relationship\"<|>NEWMAN CHENG<|>FROM LOCAL TO GLOBAL: A GRAPH RAG APPROACH TO QUERY-FOCUSED SUMMARIZATION<|>Newman Cheng is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<|>9)\n##\n(\"relationship\"<|>JOSHUA BRADLEY<|>FROM LOCAL TO GLOBAL: A GRAPH RAG APPROACH TO QUERY-FOCUSED SUMMARIZATION<|>Joshua Bradley is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<|>9)\n##\n(\"relationship\"<|>ALEX CHAO<|>FROM LOCAL TO GLOBAL: A GRAPH RAG APPROACH TO QUERY-FOCUSED SUMMARIZATION<|>Alex Chao is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<|>9)\n##\n(\"relationship\"<|>APURVA MODY<|>FROM LOCAL TO GLOBAL: A GRAPH RAG APPROACH TO QUERY-FOCUSED SUMMARIZATION<|>Apurva Mody is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<|>9)\n##\n(\"relationship\"<|>STEVEN TRUITT<|>FROM LOCAL TO GLOBAL: A GRAPH RAG APPROACH TO QUERY-FOCUSED SUMMARIZATION<|>Steven Truitt is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<|>9)\n##\n(\"relationship\"<|>JONATHAN LARSON<|>FROM LOCAL TO GLOBAL: A GRAPH RAG APPROACH TO QUERY-FOCUSED SUMMARIZATION<|>Jonathan Larson is one of the authors of the paper \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<|>9)\n##\n(\"relationship\"<|>MICROSOFT RESEARCH<|>FROM LOCAL TO GLOBAL: A GRAPH RAG APPROACH TO QUERY-FOCUSED SUMMARIZATION<|>Microsoft Research is the organization affiliated with authors Darren Edge, Ha Trinh, and Jonathan Larson<|>8)\n##\n(\"relationship\"<|>MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES<|>FROM LOCAL TO GLOBAL: A GRAPH RAG APPROACH TO QUERY-FOCUSED SUMMARIZATION<|>Microsoft Strategic Missions and Technologies is the organization affiliated with authors Newman Cheng, Joshua Bradley, and Steven Truitt<|>8)\n##\n(\"relationship\"<|>MICROSOFT OFFICE OF THE CTO<|>FROM LOCAL TO GLOBAL: A GRAPH RAG APPROACH TO QUERY-FOCUSED SUMMARIZATION<|>Microsoft Office of the CTO is the organization affiliated with authors Alex Chao and Apurva Mody<|>8)\n##\n(\"relationship\"<|>RETRIEVAL-AUGMENTED GENERATION (RAG)<|>GRAPH RAG<|>Graph RAG combines the strengths of retrieval-augmented generation (RAG) and graph-based text indexing<|>9)\n##\n(\"relationship\"<|>LARGE LANGUAGE MODELS (LLMS)<|>GRAPH RAG<|>Large language models (LLMs) are used in the Graph RAG approach to build a graph-based text index<|>9)\n##\n(\"relationship\"<|>QUERY-FOCUSED SUMMARIZATION (QFS)<|>GRAPH RAG<|>Graph RAG addresses the limitations of traditional query-focused summarization (QFS) methods<|>8)\n##\n(\"relationship\"<|>ENTITY KNOWLEDGE GRAPH<|>GRAPH RAG<|>Graph RAG uses an entity knowledge graph derived from source documents<|>9)\n##\n(\"relationship\"<|>COMMUNITY SUMMARIES<|>GRAPH RAG<|>Graph RAG uses community summaries to generate partial responses<|>9)\n##\n(\"relationship\"<|>GLOBAL SENSEMAKING QUESTIONS<|>GRAPH RAG<|>Graph RAG improves the comprehensiveness and diversity of answers to global sensemaking questions<|>8)\n##\n(\"relationship\"<|>OPEN-SOURCE IMPLEMENTATION<|>GRAPH RAG<|>An open-source implementation of Graph RAG is forthcoming<|>7)\n<|COMPLETE|>"}]}