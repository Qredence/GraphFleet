{"result": "(\"entity\"<|>\"Darren Edge\"<|>\"person\"<|>\"Darren Edge is a researcher at Microsoft Research who contributed to the development of the Graph RAG approach for query-focused summarization.\")##\n(\"entity\"<|>\"Ha Trinh\"<|>\"person\"<|>\"Ha Trinh is a researcher at Microsoft Research who contributed equally to the development of the Graph RAG approach for query-focused summarization.\")##\n(\"entity\"<|>\"Newman Cheng\"<|>\"person\"<|>\"Newman Cheng is associated with Microsoft Strategic Missions and Technologies and contributed to the Graph RAG approach.\")##\n(\"entity\"<|>\"Joshua Bradley\"<|>\"person\"<|>\"Joshua Bradley is associated with Microsoft Strategic Missions and Technologies and contributed to the Graph RAG approach.\")##\n(\"entity\"<|>\"Alex Chao\"<|>\"person\"<|>\"Alex Chao is associated with Microsoft Office of the CTO and contributed to the Graph RAG approach.\")##\n(\"entity\"<|>\"Apurva Mody\"<|>\"person\"<|>\"Apurva Mody is associated with Microsoft Office of the CTO and contributed to the Graph RAG approach.\")##\n(\"entity\"<|>\"Steven Truitt\"<|>\"person\"<|>\"Steven Truitt is associated with Microsoft Strategic Missions and Technologies and contributed to the Graph RAG approach.\")##\n(\"entity\"<|>\"Jonathan Larson\"<|>\"person\"<|>\"Jonathan Larson is a researcher at Microsoft Research who contributed to the development of the Graph RAG approach for query-focused summarization.\")##\n(\"entity\"<|>\"Microsoft Research\"<|>\"organization\"<|>\"Microsoft Research is a division of Microsoft involved in advanced research and development, contributing to the Graph RAG approach.\")##\n(\"entity\"<|>\"Microsoft Strategic Missions and Technologies\"<|>\"organization\"<|>\"Microsoft Strategic Missions and Technologies is a division of Microsoft involved in strategic projects and contributed to the Graph RAG approach.\")##\n(\"entity\"<|>\"Microsoft Office of the CTO\"<|>\"organization\"<|>\"Microsoft Office of the CTO is a division of Microsoft involved in technological oversight and contributed to the Graph RAG approach.\")##\n(\"entity\"<|>\"Graph RAG\"<|>\"event\"<|>\"Graph RAG is an approach to query-focused summarization that uses a graph-based text index to improve the comprehensiveness and diversity of generated answers.\")##\n(\"relationship\"<|>\"Darren Edge\"<|>\"Graph RAG\"<|>\"Darren Edge contributed to the development of the Graph RAG approach.\"<|>9)##\n(\"relationship\"<|>\"Ha Trinh\"<|>\"Graph RAG\"<|>\"Ha Trinh contributed equally to the development of the Graph RAG approach.\"<|>9)##\n(\"relationship\"<|>\"Newman Cheng\"<|>\"Graph RAG\"<|>\"Newman Cheng contributed to the development of the Graph RAG approach.\"<|>8)##\n(\"relationship\"<|>\"Joshua Bradley\"<|>\"Graph RAG\"<|>\"Joshua Bradley contributed to the development of the Graph RAG approach.\"<|>8)##\n(\"relationship\"<|>\"Alex Chao\"<|>\"Graph RAG\"<|>\"Alex Chao contributed to the development of the Graph RAG approach.\"<|>8)##\n(\"relationship\"<|>\"Apurva Mody\"<|>\"Graph RAG\"<|>\"Apurva Mody contributed to the development of the Graph RAG approach.\"<|>8)##\n(\"relationship\"<|>\"Steven Truitt\"<|>\"Graph RAG\"<|>\"Steven Truitt contributed to the development of the Graph RAG approach.\"<|>8)##\n(\"relationship\"<|>\"Jonathan Larson\"<|>\"Graph RAG\"<|>\"Jonathan Larson contributed to the development of the Graph RAG approach.\"<|>9)##\n(\"relationship\"<|>\"Microsoft Research\"<|>\"Graph RAG\"<|>\"Microsoft Research is the organization where the Graph RAG approach was developed.\"<|>9)##\n(\"relationship\"<|>\"Microsoft Strategic Missions and Technologies\"<|>\"Graph RAG\"<|>\"Microsoft Strategic Missions and Technologies contributed to the development of the Graph RAG approach.\"<|>8)##\n(\"relationship\"<|>\"Microsoft Office of the CTO\"<|>\"Graph RAG\"<|>\"Microsoft Office of the CTO contributed to the development of the Graph RAG approach.\"<|>8)##\n(\"relationship\"<|>\"Darren Edge\"<|>\"Microsoft Research\"<|>\"Darren Edge is a researcher at Microsoft Research.\"<|>9)##\n(\"relationship\"<|>\"Ha Trinh\"<|>\"Microsoft Research\"<|>\"Ha Trinh is a researcher at Microsoft Research.\"<|>9)##\n(\"relationship\"<|>\"Newman Cheng\"<|>\"Microsoft Strategic Missions and Technologies\"<|>\"Newman Cheng is associated with Microsoft Strategic Missions and Technologies.\"<|>8)##\n(\"relationship\"<|>\"Joshua Bradley\"<|>\"Microsoft Strategic Missions and Technologies\"<|>\"Joshua Bradley is associated with Microsoft Strategic Missions and Technologies.\"<|>8)##\n(\"relationship\"<|>\"Alex Chao\"<|>\"Microsoft Office of the CTO\"<|>\"Alex Chao is associated with Microsoft Office of the CTO.\"<|>8)##\n(\"relationship\"<|>\"Apurva Mody\"<|>\"Microsoft Office of the CTO\"<|>\"Apurva Mody is associated with Microsoft Office of the CTO.\"<|>8)##\n(\"relationship\"<|>\"Steven Truitt\"<|>\"Microsoft Strategic Missions and Technologies\"<|>\"Steven Truitt is associated with Microsoft Strategic Missions and Technologies.\"<|>8)##\n(\"relationship\"<|>\"Jonathan Larson\"<|>\"Microsoft Research\"<|>\"Jonathan Larson is a researcher at Microsoft Research.\"<|>9)##<|COMPLETE|>", "input": "\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n\n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>\n\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n\n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n\n4. When finished, output <|COMPLETE|>\n\n######################\n-Examples-\n######################\nExample 1:\n\nEntity_types: [person, technology, mission, organization, location]\nText:\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \u201cIf this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\u201d\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n################\nOutput:\n(\"entity\"<|>\"Alex\"<|>\"person\"<|>\"Alex is a character who experiences frustration and is observant of the dynamics among other characters.\")##\n(\"entity\"<|>\"Taylor\"<|>\"person\"<|>\"Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\")##\n(\"entity\"<|>\"Jordan\"<|>\"person\"<|>\"Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\")##\n(\"entity\"<|>\"Cruz\"<|>\"person\"<|>\"Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\")##\n(\"entity\"<|>\"The Device\"<|>\"technology\"<|>\"The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\")##\n(\"relationship\"<|>\"Alex\"<|>\"Taylor\"<|>\"Alex is affected by Taylor's authoritarian certainty and observes changes in Taylor's attitude towards the device.\"<|>7)##\n(\"relationship\"<|>\"Alex\"<|>\"Jordan\"<|>\"Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.\"<|>6)##\n(\"relationship\"<|>\"Taylor\"<|>\"Jordan\"<|>\"Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\"<|>8)##\n(\"relationship\"<|>\"Jordan\"<|>\"Cruz\"<|>\"Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\"<|>5)##\n(\"relationship\"<|>\"Taylor\"<|>\"The Device\"<|>\"Taylor shows reverence towards the device, indicating its importance and potential impact.\"<|>9)<|COMPLETE|>\n#############################\nExample 2:\n\nEntity_types: [person, technology, mission, organization, location]\nText:\nThey were no longer mere operatives; they had become guardians of a threshold, keepers of a message from a realm beyond stars and stripes. This elevation in their mission could not be shackled by regulations and established protocols\u2014it demanded a new perspective, a new resolve.\n\nTension threaded through the dialogue of beeps and static as communications with Washington buzzed in the background. The team stood, a portentous air enveloping them. It was clear that the decisions they made in the ensuing hours could redefine humanity's place in the cosmos or condemn them to ignorance and potential peril.\n\nTheir connection to the stars solidified, the group moved to address the crystallizing warning, shifting from passive recipients to active participants. Mercer's latter instincts gained precedence\u2014 the team's mandate had evolved, no longer solely to observe and report but to interact and prepare. A metamorphosis had begun, and Operation: Dulce hummed with the newfound frequency of their daring, a tone set not by the earthly\n#############\nOutput:\n(\"entity\"<|>\"Washington\"<|>\"location\"<|>\"Washington is a location where communications are being received, indicating its importance in the decision-making process.\")##\n(\"entity\"<|>\"Operation: Dulce\"<|>\"mission\"<|>\"Operation: Dulce is described as a mission that has evolved to interact and prepare, indicating a significant shift in objectives and activities.\")##\n(\"entity\"<|>\"The team\"<|>\"organization\"<|>\"The team is portrayed as a group of individuals who have transitioned from passive observers to active participants in a mission, showing a dynamic change in their role.\")##\n(\"relationship\"<|>\"The team\"<|>\"Washington\"<|>\"The team receives communications from Washington, which influences their decision-making process.\"<|>7)##\n(\"relationship\"<|>\"The team\"<|>\"Operation: Dulce\"<|>\"The team is directly involved in Operation: Dulce, executing its evolved objectives and activities.\"<|>9)<|COMPLETE|>\n#############################\nExample 3:\n\nEntity_types: [person, role, technology, organization, event, location, concept]\nText:\ntheir voice slicing through the buzz of activity. \"Control may be an illusion when facing an intelligence that literally writes its own rules,\" they stated stoically, casting a watchful eye over the flurry of data.\n\n\"It's like it's learning to communicate,\" offered Sam Rivera from a nearby interface, their youthful energy boding a mix of awe and anxiety. \"This gives talking to strangers' a whole new meaning.\"\n\nAlex surveyed his team\u2014each face a study in concentration, determination, and not a small measure of trepidation. \"This might well be our first contact,\" he acknowledged, \"And we need to be ready for whatever answers back.\"\n\nTogether, they stood on the edge of the unknown, forging humanity's response to a message from the heavens. The ensuing silence was palpable\u2014a collective introspection about their role in this grand cosmic play, one that could rewrite human history.\n\nThe encrypted dialogue continued to unfold, its intricate patterns showing an almost uncanny anticipation\n#############\nOutput:\n(\"entity\"<|>\"Sam Rivera\"<|>\"person\"<|>\"Sam Rivera is a member of a team working on communicating with an unknown intelligence, showing a mix of awe and anxiety.\")##\n(\"entity\"<|>\"Alex\"<|>\"person\"<|>\"Alex is the leader of a team attempting first contact with an unknown intelligence, acknowledging the significance of their task.\")##\n(\"entity\"<|>\"Control\"<|>\"concept\"<|>\"Control refers to the ability to manage or govern, which is challenged by an intelligence that writes its own rules.\")##\n(\"entity\"<|>\"Intelligence\"<|>\"concept\"<|>\"Intelligence here refers to an unknown entity capable of writing its own rules and learning to communicate.\")##\n(\"entity\"<|>\"First Contact\"<|>\"event\"<|>\"First Contact is the potential initial communication between humanity and an unknown intelligence.\")##\n(\"entity\"<|>\"Humanity's Response\"<|>\"event\"<|>\"Humanity's Response is the collective action taken by Alex's team in response to a message from an unknown intelligence.\")##\n(\"relationship\"<|>\"Sam Rivera\"<|>\"Intelligence\"<|>\"Sam Rivera is directly involved in the process of learning to communicate with the unknown intelligence.\"<|>9)##\n(\"relationship\"<|>\"Alex\"<|>\"First Contact\"<|>\"Alex leads the team that might be making the First Contact with the unknown intelligence.\"<|>10)##\n(\"relationship\"<|>\"Alex\"<|>\"Humanity's Response\"<|>\"Alex and his team are the key figures in Humanity's Response to the unknown intelligence.\"<|>8)##\n(\"relationship\"<|>\"Control\"<|>\"Intelligence\"<|>\"The concept of Control is challenged by the Intelligence that writes its own rules.\"<|>7)<|COMPLETE|>\n#############################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: From Local to Global: A Graph RAG Approach to\nQuery-Focused Summarization\nDarren Edge1\u2020Ha Trinh1\u2020Newman Cheng2Joshua Bradley2Alex Chao3\nApurva Mody3Steven Truitt2\nJonathan Larson1\n1Microsoft Research\n2Microsoft Strategic Missions and Technologies\n3Microsoft Office of the CTO\n{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso }\n@microsoft.com\n\u2020These authors contributed equally to this work\nAbstract\nThe use of retrieval-augmented generation (RAG) to retrieve relevant informa-\ntion from an external knowledge source enables large language models (LLMs)\nto answer questions over private and/or previously unseen document collections.\nHowever, RAG fails on global questions directed at an entire text corpus, such\nas \u201cWhat are the main themes in the dataset?\u201d, since this is inherently a query-\nfocused summarization (QFS) task, rather than an explicit retrieval task. Prior\nQFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\nRAG systems. To combine the strengths of these contrasting methods, we propose\na Graph RAG approach to question answering over private text corpora that scales\nwith both the generality of user questions and the quantity of source text to be in-\ndexed. Our approach uses an LLM to build a graph-based text index in two stages:\nfirst to derive an entity knowledge graph from the source documents, then to pre-\ngenerate community summaries for all groups of closely-related entities. Given a\nquestion, each community summary is used to generate a partial response, before\nall partial responses are again summarized in a final response to the user. For a\nclass of global sensemaking questions over datasets in the 1 million token range,\nwe show that Graph RAG leads to substantial improvements over a na \u00a8\u0131ve RAG\nbaseline for both the comprehensiveness and diversity of generated answers. An\nopen-source, Python-based implementation of both global and local Graph RAG\napproaches is forthcoming at https://aka .ms/graphrag .\n1 Introduction\nHuman endeavors across a range of domains rely on our ability to read and reason about large\ncollections of documents, often reaching conclusions that go beyond anything stated in the source\ntexts themselves. With the emergence of large language models (LLMs), we are already witnessing\nattempts to automate human-like sensemaking in complex domains like scientific discovery (Mi-\ncrosoft, 2023) and intelligence analysis (Ranade and Joshi, 2023), where sensemaking is defined as\nPreprint. Under review.arXiv:2404.16130v1  [cs.CL]  24 Apr 2024Source Documents\nText Chunkstext extraction\nand chunking\nElement Instancesdomain-tailored\nsummarization\nElement Summariesdomain-tailored\nsummarization\nGraph Communitiescommunity\ndetectionCommunity Summaries\ndomain-tailored\nsummarizationCommunity Answers\nquery-focused\nsummarizationGlobal Answer\nquery-focused\nsummarization\nIndexing Time Query Time Pipeline Stage\nFigure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This\nindex spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that have\nbeen detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset.\nCommunity detection (e.g., Leiden, Traag et al., 2019) is used to partition the graph index into\ngroups of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both index-\ning time and query time. The \u201cglobal answer\u201d to a given query is produced using a final round of\nquery-focused summarization over all community summaries reporting relevance to that query.\n\u201ca motivated, continuous effort to understand connections (which can be among people, places, and\nevents) in order to anticipate their trajectories and act effectively \u201d (Klein et al., 2006a). Supporting\nhuman-led sensemaking over entire text corpora, however, needs a way for people to both apply and\nrefine their mental model of the data (Klein et al., 2006b) by asking questions of a global nature.\nRetrieval-augmented generation (RAG, Lewis et al., 2020) is an established approach to answering\nuser questions over entire datasets, but it is designed for situations where these answers are contained\nlocally within regions of text whose retrieval provides sufficient grounding for the generation task.\nInstead, a more appropriate task framing is query-focused summarization (QFS, Dang, 2006), and in\nparticular, query-focused abstractive summarization that generates natural language summaries and\nnot just concatenated excerpts (Baumel et al., 2018; Laskar et al., 2020; Yao et al., 2017) . In recent\nyears, however, such distinctions between summarization tasks that are abstractive versus extractive,\ngeneric versus query-focused, and single-document versus multi-document, have become less rele-\nvant. While early applications of the transformer architecture showed substantial improvements on\nthe state-of-the-art for all such summarization tasks (Goodwin et al., 2020; Laskar et al., 2022; Liu\nand Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam\net al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) series,\nall of which can use in-context learning to summarize any content provided in their context window.\nThe challenge remains, however, for query-focused abstractive summarization over an entire corpus.\nSuch volumes of text can greatly exceed the limits of LLM context windows, and the expansion of\nsuch windows may not be enough given that information can be \u201clost in the middle\u201d of longer\ncontexts (Kuratov et al., 2024; Liu et al., 2023). In addition, although the direct retrieval of text\nchunks in na \u00a8\u0131ve RAG is likely inadequate for QFS tasks, it is possible that an alternative form of\npre-indexing could support a new RAG approach specifically targeting global summarization.\nIn this paper, we present a Graph RAG approach based on global summarization of an LLM-derived\nknowledge graph (Figure 1). In contrast with related work that exploits the structured retrieval\nand traversal affordances of graph indexes (subsection 4.2), we focus on a previously unexplored\nquality of graphs in this context: their inherent modularity (Newman, 2006) and the ability of com-\nmunity detection algorithms to partition graphs into modular communities of closely-related nodes\n(e.g., Louvain, Blondel et al., 2008; Leiden, Traag et al., 2019). LLM-generated summaries of these\n20 1 2 30100002000030000\nNumber of gleanings performedEntity references detected600 chunk size\n1200 chunk size\n2400 chunk size\nFigure 2: How the entity references detected in the HotPotQA dataset (Yang et al., 2018)\nvaries with chunk size and gleanings for our generic entity extraction prompt with gpt-4-turbo .\ncommunity descriptions provide complete coverage of the underlying graph index and the input doc-\numents it represents. Query-focused summarization of an entire corpus is then made possible using\na map-reduce approach: first using each community summary to answer the query independently\nand in parallel, then summarizing all relevant partial answers into a final global answer.\nTo evaluate this approach, we used an LLM to generate a diverse set of activity-centered sense-\nmaking questions from short descriptions of two representative real-world datasets, containing pod-\ncast transcripts and news articles respectively. For the target qualities of comprehensiveness, diver-\nsity, and empowerment (defined in subsection 3.4) that develop understanding of broad issues and\nthemes, we both explore the impact of varying the the hierarchical level of community summaries\nused to answer queries, as well as compare to na \u00a8\u0131ve RAG and global map-reduce summarization\nof source texts. We show that all global approaches outperform na \u00a8\u0131ve RAG on comprehensiveness\nand diversity, and that Graph RAG with intermediate- and low-level community summaries shows\nfavorable performance over source text summarization on these same metrics, at lower token costs.\n2 Graph RAG Approach & Pipeline\nWe now unpack the high-level data flow of the Graph RAG approach (Figure 1) and pipeline, de-\nscribing key design parameters, techniques, and implementation details for each step.\n2.1 Source Documents \u2192Text Chunks\nA fundamental design decision is the granularity with which input texts extracted from source doc-\numents should be split into text chunks for processing. In the following step, each of these chunks\nwill be passed to a set of LLM prompts designed to extract the various elements of a graph index.\nLonger text chunks require fewer LLM calls for such extraction, but suffer from the recall degrada-\ntion of longer LLM context windows (Kuratov et al., 2024; Liu et al., 2023). This behavior can be\nobserved in Figure 2 in the case of a single extraction round (i.e., with zero gleanings): on a sample\ndataset (HotPotQA, Yang et al., 2018), using a chunk size of 600 token extracted almost twice as\nmany entity references as when using a chunk size of 2400. While more references are generally\nbetter, any extraction process needs to balance recall and precision for the target activity.\n2.2 Text Chunks \u2192Element Instances\nThe baseline requirement for this step is to identify and extract instances of graph nodes and edges\nfrom each chunk of source text. We do this using a multipart LLM prompt that first identifies all\nentities in the text, including their name, type, and description, before identifying all relationships\nbetween clearly-related entities, including the source and target entities and a description of their\nrelationship. Both kinds of element instance are output in a single list of delimited tuples.\nThe primary opportunity to tailor this prompt to the domain of the document corpus lies in the\nchoice of few-shot examples provided to the LLM for in-context learning (Brown et al., 2020).\n3For example, while our default prompt extracting the broad class of \u201cnamed entities\u201d like people,\nplaces, and organizations is generally applicable, domains with specialized knowledge (e.g., science,\nmedicine, law) will benefit from few-shot examples specialized to those domains. We also support\na secondary extraction prompt for any additional covariates we would like to associate with the\nextracted node instances. Our default covariate prompt aims to extract claims linked to detected\nentities, including the subject, object, type, description, source text span, and start and end dates.\nTo balance the needs of efficiency and quality, we use multiple rounds of \u201cgleanings\u201d, up to a\nspecified maximum, to encourage the LLM to detect any additional entities it may have missed\non prior extraction rounds. This is a multi-stage process in which we first ask the LLM to assess\nwhether all entities were extracted, using a logit bias of 100 to force a yes/no decision. If the LLM\nresponds that entities were missed, then a continuation indicating that \u201cMANY entities were missed\nin the last extraction\u201d encourages the LLM to glean these missing entities. This approach allows us\nto use larger chunk sizes without a drop in quality (Figure 2) or the forced introduction of noise.\n2.3 Element Instances \u2192Element Summaries\nThe use of an LLM to \u201cextract\u201d descriptions of entities, relationships, and claims represented in\nsource texts is already a form of abstractive summarization, relying on the LLM to create inde-\npendently meaningful summaries of concepts that may be implied but not stated by the text itself\n(e.g., the presence of implied relationships). To convert all such instance-level summaries into sin-\ngle blocks of descriptive text for each graph element (i.e., entity node, relationship edge, and claim\ncovariate) requires a further round of LLM summarization over matching groups of instances.\nA potential concern at this stage is that the LLM may not consistently extract references to the\nsame entity in the same text format, resulting in duplicate entity elements and thus duplicate nodes\nin the entity graph. However, since all closely-related \u201ccommunities\u201d of entities will be detected\nand summarized in the following step, and given that LLMs can understand the common entity\nbehind multiple name variations, our overall approach is resilient to such variations provided there\nis sufficient connectivity from all variations to a shared set of closely-related entities.\nOverall, our use of rich descriptive text for homogeneous nodes in a potentially noisy graph structure\nis aligned with both the capabilities of LLMs and the needs of global, query-focused summarization.\nThese qualities also differentiate our graph index from typical knowledge graphs, which rely on\nconcise and consistent knowledge triples (subject, predicate, object) for downstream reasoning tasks.\n2.4 Element Summaries \u2192Graph Communities\nThe index created in the previous step can be modelled as an homogeneous undirected weighted\ngraph in which entity nodes are connected by relationship edges, with edge weights representing the\nnormalized counts of detected relationship instances. Given such a graph, a variety of community\ndetection algorithms may be used to partition the graph into communities of nodes with stronger\nconnections to one another than to the other nodes in the graph (e.g., see the surveys by Fortu-\nnato, 2010 and Jin et al., 2021). In our pipeline, we use Leiden (Traag et al., 2019) on account of\nits ability to recover hierarchical community structure of large-scale graphs efficiently (Figure 3).\nEach level of this hierarchy provides a community partition that covers the nodes of the graph in a\nmutually-exclusive, collective-exhaustive way, enabling divide-and-conquer global summarization.\n2.5 Graph Communities \u2192Community Summaries\n######################\nOutput:", "parameters": {"model": "=\"gpt-4o\"", "temperature": 0.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "top_p": 1.0, "max_tokens": 4000, "n": 1}}