{"id":"0c932f7def033fa2b1bf210fbb771e7d","chunk":"From Local to Global: A Graph RAG Approach to\nQuery-Focused Summarization\nDarren Edge1\u2020Ha Trinh1\u2020Newman Cheng2Joshua Bradley2Alex Chao3\nApurva Mody3Steven Truitt2\nJonathan Larson1\n1Microsoft Research\n2Microsoft Strategic Missions and Technologies\n3Microsoft Office of the CTO\n{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso }\n@microsoft.com\n\u2020These authors contributed equally to this work\nAbstract\nThe use of retrieval-augmented generation (RAG) to retrieve relevant informa-\ntion from an external knowledge source enables large language models (LLMs)\nto answer questions over private and\/or previously unseen document collections.\nHowever, RAG fails on global questions directed at an entire text corpus, such\nas \u201cWhat are the main themes in the dataset?\u201d, since this is inherently a query-\nfocused summarization (QFS) task, rather than an explicit retrieval task. Prior\nQFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\nRAG systems. To combine the strengths of these contrasting methods, we propose\na Graph RAG approach to question answering over private text corpora that scales\nwith both the generality of user questions and the quantity of source text to be in-\ndexed. Our approach uses an LLM to build a graph-based text index in two stages:\nfirst to derive an entity knowledge graph from the source documents, then to pre-\ngenerate community summaries for all groups of closely-related entities. Given a\nquestion, each community summary is used to generate a partial response, before\nall partial responses are again summarized in a final response to the user. For a\nclass of global sensemaking questions over datasets in the 1 million token range,\nwe show that Graph RAG leads to substantial improvements over a na \u00a8\u0131ve RAG\nbaseline for both the comprehensiveness and diversity of generated answers. An\nopen-source, Python-based implementation of both global and local Graph RAG\napproaches is forthcoming at https:\/\/aka .ms\/graphrag .\n1 Introduction\nHuman endeavors across a range of domains rely on our ability to read and reason about large\ncollections of documents, often reaching conclusions that go beyond anything stated in the source\ntexts themselves. With the emergence of large language models (LLMs), we are already witnessing\nattempts to automate human-like sensemaking in complex domains like scientific discovery (Mi-\ncrosoft, 2023) and intelligence analysis (Ranade and Joshi, 2023), where sensemaking is defined as\nPreprint. Under review.arXiv:2404.16130v1  [cs.CL]  24 Apr 2024Source Documents\nText Chunkstext extraction\nand chunking\nElement Instancesdomain-tailored\nsummarization\nElement Summariesdomain-tailored\nsummarization\nGraph Communitiescommunity\ndetectionCommunity Summaries\ndomain-tailored\nsummarizationCommunity Answers\nquery-focused\nsummarizationGlobal Answer\nquery-focused\nsummarization\nIndexing Time Query Time Pipeline Stage\nFigure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This\nindex spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that have\nbeen detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset.\nCommunity detection (e.g., Leiden, Traag et al., 2019) is used to partition the graph index into\ngroups of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both index-\ning time and query time. The \u201cglobal answer\u201d to a given query is produced using a final round of\nquery-focused summarization over all community summaries reporting relevance to that query.\n\u201ca motivated, continuous effort to understand connections (which can be among people, places, and\nevents) in order to anticipate their trajectories and act effectively \u201d (Klein et al., 2006a). Supporting\nhuman-led sensemaking over entire text corpora, however, needs a way for people to both apply and\nrefine their mental model of the data (Klein et al., 2006b) by asking questions of a global nature.\nRetrieval-augmented generation (RAG, Lewis et al., 2020) is an established approach to answering\nuser questions over entire datasets, but it is designed for situations where these answers are contained\nlocally within regions of text whose retrieval provides sufficient grounding for the generation task.\nInstead, a more appropriate task framing is query-focused summarization (QFS, Dang, 2006), and in\nparticular, query-focused abstractive summarization that generates natural language summaries and\nnot just concatenated excerpts (Baumel et al., 2018; Laskar et al., 2020; Yao et al., 2017) . In recent\nyears, however, such distinctions between summarization tasks that are abstractive versus extractive,\ngeneric versus query-focused, and single-document versus multi-document, have become less rele-\nvant. While early applications of the transformer architecture showed substantial improvements on\nthe state-of-the-art for all such summarization tasks (Goodwin et al., 2020; Laskar et al., 2022; Liu\nand Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam\net al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al.,","chunk_id":"0c932f7def033fa2b1bf210fbb771e7d","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"GRAPH RAG APPROACH","type":"TECHNIQUE","description":"The Graph RAG approach is a method for query-focused summarization that combines retrieval-augmented generation with a graph-based text index to improve the comprehensiveness and diversity of generated answers over large text corpora.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"QUERY-FOCUSED SUMMARIZATION (QFS)","type":"TECHNIQUE","description":"Query-focused summarization (QFS) is a task that aims to generate summaries based on specific user queries, rather than simply retrieving text excerpts.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"LARGE LANGUAGE MODELS (LLMS)","type":"TECHNOLOGY","description":"Large language models (LLMs) are advanced AI models designed to understand and generate human-like text, often used for tasks like summarization and question answering.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"SOURCE DOCUMENTS","type":"DATA FORMAT","description":"Source documents are the original texts from which information is extracted for processing in the Graph RAG approach.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"COMMUNITY SUMMARIES","type":"DATA FORMAT","description":"Community summaries are condensed representations of groups of closely-related entities, generated to provide partial responses to user queries.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"MICROSOFT RESEARCH","type":"ORGANIZATION","description":"Microsoft Research is a division of Microsoft focused on advancing the state of the art in computing through research and innovation.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES","type":"ORGANIZATION","description":"Microsoft Strategic Missions and Technologies is a division within Microsoft that focuses on strategic initiatives and technological advancements.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"MICROSOFT OFFICE OF THE CTO","type":"ORGANIZATION","description":"Microsoft Office of the CTO is a division that oversees technological strategy and innovation within Microsoft.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"DARREN EDGE","type":"PERSON","description":"Darren Edge is a researcher affiliated with Microsoft Research, contributing to advancements in AI and summarization techniques.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"HA TRINH","type":"PERSON","description":"Ha Trinh is a researcher affiliated with Microsoft Research, contributing to advancements in AI and summarization techniques.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"NEWMAN CHENG","type":"PERSON","description":"Newman Cheng is a researcher affiliated with Microsoft Strategic Missions and Technologies, contributing to advancements in AI and summarization techniques.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"JOSHUA BRADLEY","type":"PERSON","description":"Joshua Bradley is a researcher affiliated with Microsoft Strategic Missions and Technologies, contributing to advancements in AI and summarization techniques.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"ALEX CHAO","type":"PERSON","description":"Alex Chao is a researcher affiliated with Microsoft Office of the CTO, contributing to advancements in AI and summarization techniques.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"APURVA MODY","type":"PERSON","description":"Apurva Mody is a researcher affiliated with Microsoft Office of the CTO, contributing to advancements in AI and summarization techniques.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"STEVEN TRUITT","type":"PERSON","description":"Steven Truitt is a researcher affiliated with Microsoft Strategic Missions and Technologies, contributing to advancements in AI and summarization techniques.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"JONATHAN LARSON","type":"PERSON","description":"Jonathan Larson is a researcher affiliated with Microsoft Research, contributing to advancements in AI and summarization techniques.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"RETRIEVAL-AUGMENTED GENERATION (RAG)","type":"TECHNIQUE","description":"Retrieval-augmented generation (RAG) is an approach that combines retrieval of relevant information from external sources with generative models to answer user queries.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"GLOBAL SENSEMAKING","type":"CONCEPT","description":"Global sensemaking refers to the process of understanding and interpreting large datasets to derive insights that go beyond the explicit information contained within the texts.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"COMMUNITY DETECTION","type":"TECHNIQUE","description":"Community detection is a method used to identify groups of closely-related entities within a dataset, facilitating more efficient summarization and analysis.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"LEIDEN ALGORITHM","type":"TECHNIQUE","description":"The Leiden algorithm is a specific method used for community detection in networks, known for its efficiency and accuracy in identifying clusters.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"TOKEN RANGE","type":"DATA FORMAT","description":"Token range refers to the number of tokens (words or characters) in a dataset, which can impact the performance of summarization techniques.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GRAPH RAG APPROACH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Graph RAG approach is a method for query-focused summarization that combines retrieval-augmented generation with a graph-based text index to improve the comprehensiveness and diversity of generated answers over large text corpora.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"QUERY-FOCUSED SUMMARIZATION (QFS)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Query-focused summarization (QFS) is a task that aims to generate summaries based on specific user queries, rather than simply retrieving text excerpts.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"LARGE LANGUAGE MODELS (LLMS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Large language models (LLMs) are advanced AI models designed to understand and generate human-like text, often used for tasks like summarization and question answering.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"SOURCE DOCUMENTS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Source documents are the original texts from which information is extracted for processing in the Graph RAG approach.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"COMMUNITY SUMMARIES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Community summaries are condensed representations of groups of closely-related entities, generated to provide partial responses to user queries.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"MICROSOFT RESEARCH\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Microsoft Research is a division of Microsoft focused on advancing the state of the art in computing through research and innovation.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Microsoft Strategic Missions and Technologies is a division within Microsoft that focuses on strategic initiatives and technological advancements.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"MICROSOFT OFFICE OF THE CTO\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Microsoft Office of the CTO is a division that oversees technological strategy and innovation within Microsoft.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"DARREN EDGE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Darren Edge is a researcher affiliated with Microsoft Research, contributing to advancements in AI and summarization techniques.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"HA TRINH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ha Trinh is a researcher affiliated with Microsoft Research, contributing to advancements in AI and summarization techniques.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"NEWMAN CHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Newman Cheng is a researcher affiliated with Microsoft Strategic Missions and Technologies, contributing to advancements in AI and summarization techniques.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"JOSHUA BRADLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshua Bradley is a researcher affiliated with Microsoft Strategic Missions and Technologies, contributing to advancements in AI and summarization techniques.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"ALEX CHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex Chao is a researcher affiliated with Microsoft Office of the CTO, contributing to advancements in AI and summarization techniques.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"APURVA MODY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Apurva Mody is a researcher affiliated with Microsoft Office of the CTO, contributing to advancements in AI and summarization techniques.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"STEVEN TRUITT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Steven Truitt is a researcher affiliated with Microsoft Strategic Missions and Technologies, contributing to advancements in AI and summarization techniques.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"JONATHAN LARSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jonathan Larson is a researcher affiliated with Microsoft Research, contributing to advancements in AI and summarization techniques.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED GENERATION (RAG)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Retrieval-augmented generation (RAG) is an approach that combines retrieval of relevant information from external sources with generative models to answer user queries.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"GLOBAL SENSEMAKING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Global sensemaking refers to the process of understanding and interpreting large datasets to derive insights that go beyond the explicit information contained within the texts.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"COMMUNITY DETECTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Community detection is a method used to identify groups of closely-related entities within a dataset, facilitating more efficient summarization and analysis.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"LEIDEN ALGORITHM\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Leiden algorithm is a specific method used for community detection in networks, known for its efficiency and accuracy in identifying clusters.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"TOKEN RANGE\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Token range refers to the number of tokens (words or characters) in a dataset, which can impact the performance of summarization techniques.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <edge source=\"GRAPH RAG APPROACH\" target=\"QUERY-FOCUSED SUMMARIZATION (QFS)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach is designed to improve query-focused summarization by utilizing a graph-based text index for better responses.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"LARGE LANGUAGE MODELS (LLMS)\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Graph RAG approach employs large language models to build a graph-based text index and generate summaries.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"SOURCE DOCUMENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Source documents are the basis for the Graph RAG approach, providing the text from which information is extracted and summarized.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"COMMUNITY SUMMARIES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Community summaries are generated as part of the Graph RAG approach to provide partial answers to user queries.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"RETRIEVAL-AUGMENTED GENERATION (RAG)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach builds upon the principles of retrieval-augmented generation to enhance query-focused summarization capabilities.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"COMMUNITY DETECTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach utilizes community detection to partition the graph index into groups for more effective summarization.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"TOKEN RANGE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The token range of a dataset influences the performance of the Graph RAG approach, particularly in terms of comprehensiveness and diversity of answers.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"QUERY-FOCUSED SUMMARIZATION (QFS)\" target=\"GLOBAL SENSEMAKING\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Query-focused summarization is a key component of global sensemaking, as it helps derive insights from large datasets based on specific queries.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"MICROSOFT RESEARCH\" target=\"DARREN EDGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Darren Edge is a researcher at Microsoft Research, contributing to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"MICROSOFT RESEARCH\" target=\"HA TRINH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Ha Trinh is a researcher at Microsoft Research, contributing to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"MICROSOFT RESEARCH\" target=\"JONATHAN LARSON\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Jonathan Larson is a researcher at Microsoft Research, contributing to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES\" target=\"NEWMAN CHENG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Newman Cheng is a researcher at Microsoft Strategic Missions and Technologies, contributing to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES\" target=\"JOSHUA BRADLEY\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Joshua Bradley is a researcher at Microsoft Strategic Missions and Technologies, contributing to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES\" target=\"STEVEN TRUITT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Steven Truitt is a researcher at Microsoft Strategic Missions and Technologies, contributing to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"MICROSOFT OFFICE OF THE CTO\" target=\"ALEX CHAO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Alex Chao is a researcher at Microsoft Office of the CTO, contributing to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"MICROSOFT OFFICE OF THE CTO\" target=\"APURVA MODY\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Apurva Mody is a researcher at Microsoft Office of the CTO, contributing to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"COMMUNITY DETECTION\" target=\"LEIDEN ALGORITHM\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Community detection can be performed using the Leiden algorithm, which is known for its effectiveness in identifying clusters within data.<\/data>      <data key=\"d5\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"64476a39d7d8b87b399e3bd3cead79c7","chunk":" on\nthe state-of-the-art for all such summarization tasks (Goodwin et al., 2020; Laskar et al., 2022; Liu\nand Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam\net al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) series,\nall of which can use in-context learning to summarize any content provided in their context window.\nThe challenge remains, however, for query-focused abstractive summarization over an entire corpus.\nSuch volumes of text can greatly exceed the limits of LLM context windows, and the expansion of\nsuch windows may not be enough given that information can be \u201clost in the middle\u201d of longer\ncontexts (Kuratov et al., 2024; Liu et al., 2023). In addition, although the direct retrieval of text\nchunks in na \u00a8\u0131ve RAG is likely inadequate for QFS tasks, it is possible that an alternative form of\npre-indexing could support a new RAG approach specifically targeting global summarization.\nIn this paper, we present a Graph RAG approach based on global summarization of an LLM-derived\nknowledge graph (Figure 1). In contrast with related work that exploits the structured retrieval\nand traversal affordances of graph indexes (subsection 4.2), we focus on a previously unexplored\nquality of graphs in this context: their inherent modularity (Newman, 2006) and the ability of com-\nmunity detection algorithms to partition graphs into modular communities of closely-related nodes\n(e.g., Louvain, Blondel et al., 2008; Leiden, Traag et al., 2019). LLM-generated summaries of these\n20 1 2 30100002000030000\nNumber of gleanings performedEntity references detected600 chunk size\n1200 chunk size\n2400 chunk size\nFigure 2: How the entity references detected in the HotPotQA dataset (Yang et al., 2018)\nvaries with chunk size and gleanings for our generic entity extraction prompt with gpt-4-turbo .\ncommunity descriptions provide complete coverage of the underlying graph index and the input doc-\numents it represents. Query-focused summarization of an entire corpus is then made possible using\na map-reduce approach: first using each community summary to answer the query independently\nand in parallel, then summarizing all relevant partial answers into a final global answer.\nTo evaluate this approach, we used an LLM to generate a diverse set of activity-centered sense-\nmaking questions from short descriptions of two representative real-world datasets, containing pod-\ncast transcripts and news articles respectively. For the target qualities of comprehensiveness, diver-\nsity, and empowerment (defined in subsection 3.4) that develop understanding of broad issues and\nthemes, we both explore the impact of varying the the hierarchical level of community summaries\nused to answer queries, as well as compare to na \u00a8\u0131ve RAG and global map-reduce summarization\nof source texts. We show that all global approaches outperform na \u00a8\u0131ve RAG on comprehensiveness\nand diversity, and that Graph RAG with intermediate- and low-level community summaries shows\nfavorable performance over source text summarization on these same metrics, at lower token costs.\n2 Graph RAG Approach & Pipeline\nWe now unpack the high-level data flow of the Graph RAG approach (Figure 1) and pipeline, de-\nscribing key design parameters, techniques, and implementation details for each step.\n2.1 Source Documents \u2192Text Chunks\nA fundamental design decision is the granularity with which input texts extracted from source doc-\numents should be split into text chunks for processing. In the following step, each of these chunks\nwill be passed to a set of LLM prompts designed to extract the various elements of a graph index.\nLonger text chunks require fewer LLM calls for such extraction, but suffer from the recall degrada-\ntion of longer LLM context windows (Kuratov et al., 2024; Liu et al., 2023). This behavior can be\nobserved in Figure 2 in the case of a single extraction round (i.e., with zero gleanings): on a sample\ndataset (HotPotQA, Yang et al., 2018), using a chunk size of 600 token extracted almost twice as\nmany entity references as when using a chunk size of 2400. While more references are generally\nbetter, any extraction process needs to balance recall and precision for the target activity.\n2.2 Text Chunks \u2192Element Instances\nThe baseline requirement for this step is to identify and extract instances of graph nodes and edges\nfrom each chunk of source text. We do this using a multipart LLM prompt that first identifies all\nentities in the text, including their name, type, and description, before identifying all relationships\nbetween clearly-related entities, including the source and target entities and a description of their\nrelationship. Both kinds of element instance are output in a single list of delimited tuples.\nThe primary opportunity to tailor this prompt to the domain of the document corpus lies in the\nchoice of few-shot examples provided to the LLM for in-context learning (Brown et al., 2020).\n3For example, while our default prompt extracting the broad class of \u201cnamed entities\u201d like people,\nplaces, and organizations is generally applicable, domains with specialized knowledge (e.g., science,\nmedicine, law) will benefit from few-shot examples specialized to those domains. We also support\na secondary extraction prompt for any additional covariates we would like to associate with the\nextracted node instances. Our","chunk_id":"64476a39d7d8b87b399e3bd3cead79c7","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"GRAPH RAG APPROACH","type":"TECHNIQUE","description":"The Graph RAG approach is a method for processing text data that focuses on global summarization using a knowledge graph derived from LLM outputs.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION","type":"TECHNIQUE","description":"Query-focused abstractive summarization is a task that aims to summarize content based on specific queries, often requiring handling large volumes of text.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"LLM","type":"TECHNOLOGY","description":"LLM (Large Language Model) refers to advanced AI models capable of understanding and generating human-like text, used in various applications including summarization.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"SOURCE DOCUMENTS","type":"DATA FORMAT","description":"Source documents are the original texts from which input texts are extracted for processing in the Graph RAG approach.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"TEXT CHUNKS","type":"DATA FORMAT","description":"Text chunks are segments of input texts extracted from source documents, which are processed for further analysis.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"KURATOV ET AL. (2024)","type":"PERSON","description":"Kuratov et al. (2024) is a reference to a study discussing the limitations of LLM context windows in processing longer text chunks.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"LIU ET AL. (2023)","type":"PERSON","description":"Liu et al. (2023) is a reference to a study discussing the limitations of LLM context windows in processing longer text chunks.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a dataset used for evaluating question answering systems, particularly in the context of entity reference detection.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"COMMUNITY DETECTION ALGORITHMS","type":"TECHNIQUE","description":"Community detection algorithms are methods used to identify and partition graphs into modular communities of closely-related nodes.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"MODULARITY","type":"CONCEPT","description":"Modularity is a property of a graph that measures the strength of division of a network into modules or communities.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"LLAMA","type":"TECHNOLOGY","description":"Llama is a series of large language models designed for efficient text processing and generation, capable of handling various natural language tasks.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"GEMINI","type":"TECHNOLOGY","description":"Gemini is a series of advanced language models that utilize in-context learning to summarize and process text effectively.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"ACHIAM ET AL. (2023)","type":"PERSON","description":"Achiam et al. (2023) is a reference to a study discussing the capabilities of the GPT series in summarization tasks.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"BROWN ET AL. (2020)","type":"PERSON","description":"Brown et al. (2020) is a reference to a study that discusses the foundational aspects of large language models, including their training and applications.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"TOUVRON ET AL. (2023)","type":"PERSON","description":"Touvron et al. (2023) is a reference to a study that discusses the Llama series of models and their performance in various tasks.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"ANIL ET AL. (2023)","type":"PERSON","description":"Anil et al. (2023) is a reference to a study that discusses the Gemini series of models and their capabilities in text processing.","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"GPT","type":"","description":"","source_id":"64476a39d7d8b87b399e3bd3cead79c7"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GRAPH RAG APPROACH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Graph RAG approach is a method for processing text data that focuses on global summarization using a knowledge graph derived from LLM outputs.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Query-focused abstractive summarization is a task that aims to summarize content based on specific queries, often requiring handling large volumes of text.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM (Large Language Model) refers to advanced AI models capable of understanding and generating human-like text, used in various applications including summarization.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"SOURCE DOCUMENTS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Source documents are the original texts from which input texts are extracted for processing in the Graph RAG approach.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"TEXT CHUNKS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Text chunks are segments of input texts extracted from source documents, which are processed for further analysis.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"KURATOV ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kuratov et al. (2024) is a reference to a study discussing the limitations of LLM context windows in processing longer text chunks.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"LIU ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu et al. (2023) is a reference to a study discussing the limitations of LLM context windows in processing longer text chunks.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a dataset used for evaluating question answering systems, particularly in the context of entity reference detection.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"COMMUNITY DETECTION ALGORITHMS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Community detection algorithms are methods used to identify and partition graphs into modular communities of closely-related nodes.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"MODULARITY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Modularity is a property of a graph that measures the strength of division of a network into modules or communities.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"LLAMA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Llama is a series of large language models designed for efficient text processing and generation, capable of handling various natural language tasks.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"GEMINI\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Gemini is a series of advanced language models that utilize in-context learning to summarize and process text effectively.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"ACHIAM ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Achiam et al. (2023) is a reference to a study discussing the capabilities of the GPT series in summarization tasks.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"BROWN ET AL. (2020)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brown et al. (2020) is a reference to a study that discusses the foundational aspects of large language models, including their training and applications.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"TOUVRON ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Touvron et al. (2023) is a reference to a study that discusses the Llama series of models and their performance in various tasks.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"ANIL ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anil et al. (2023) is a reference to a study that discusses the Gemini series of models and their capabilities in text processing.<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"GPT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <edge source=\"GRAPH RAG APPROACH\" target=\"QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach aims to enhance query-focused abstractive summarization by utilizing a knowledge graph for better context handling.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"LLM\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach employs LLMs to generate summaries and extract information from text chunks.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"KURATOV ET AL. (2024)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Kuratov et al. (2024) provides insights relevant to the performance and limitations of the Graph RAG approach in handling longer text chunks.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"LIU ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Liu et al. (2023) provides insights relevant to the performance and limitations of the Graph RAG approach in handling longer text chunks.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"COMMUNITY DETECTION ALGORITHMS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Graph RAG approach utilizes community detection algorithms to partition graphs into modular communities for summarization tasks.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"GPT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach leverages the capabilities of GPT models for summarization tasks, enhancing performance through advanced language processing.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"LLAMA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach utilizes Llama models to improve the efficiency and effectiveness of text summarization.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"GEMINI\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach incorporates Gemini models to enhance its summarization capabilities through in-context learning.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"ACHIAM ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Achiam et al. (2023) provides insights into the performance of GPT models, relevant to the Graph RAG approach.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"BROWN ET AL. (2020)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Brown et al. (2020) discusses foundational aspects of LLMs that inform the design of the Graph RAG approach.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"TOUVRON ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Touvron et al. (2023) provides insights into the Llama models that are relevant to the Graph RAG approach.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"ANIL ET AL. (2023)\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Anil et al. (2023) discusses the capabilities of Gemini models, which are relevant to the Graph RAG approach.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"SOURCE DOCUMENTS\" target=\"TEXT CHUNKS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Text chunks are derived from source documents for further processing in the Graph RAG approach.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"TEXT CHUNKS\" target=\"HOTPOTQA\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Text chunks are evaluated using the HotPotQA dataset to measure the effectiveness of entity reference detection.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"COMMUNITY DETECTION ALGORITHMS\" target=\"MODULARITY\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Modularity is a key concept in community detection algorithms, influencing how graphs are partitioned into communities.<\/data>      <data key=\"d5\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"e66ed885a08f92cc69f4895302c33047","chunk":" examples provided to the LLM for in-context learning (Brown et al., 2020).\n3For example, while our default prompt extracting the broad class of \u201cnamed entities\u201d like people,\nplaces, and organizations is generally applicable, domains with specialized knowledge (e.g., science,\nmedicine, law) will benefit from few-shot examples specialized to those domains. We also support\na secondary extraction prompt for any additional covariates we would like to associate with the\nextracted node instances. Our default covariate prompt aims to extract claims linked to detected\nentities, including the subject, object, type, description, source text span, and start and end dates.\nTo balance the needs of efficiency and quality, we use multiple rounds of \u201cgleanings\u201d, up to a\nspecified maximum, to encourage the LLM to detect any additional entities it may have missed\non prior extraction rounds. This is a multi-stage process in which we first ask the LLM to assess\nwhether all entities were extracted, using a logit bias of 100 to force a yes\/no decision. If the LLM\nresponds that entities were missed, then a continuation indicating that \u201cMANY entities were missed\nin the last extraction\u201d encourages the LLM to glean these missing entities. This approach allows us\nto use larger chunk sizes without a drop in quality (Figure 2) or the forced introduction of noise.\n2.3 Element Instances \u2192Element Summaries\nThe use of an LLM to \u201cextract\u201d descriptions of entities, relationships, and claims represented in\nsource texts is already a form of abstractive summarization, relying on the LLM to create inde-\npendently meaningful summaries of concepts that may be implied but not stated by the text itself\n(e.g., the presence of implied relationships). To convert all such instance-level summaries into sin-\ngle blocks of descriptive text for each graph element (i.e., entity node, relationship edge, and claim\ncovariate) requires a further round of LLM summarization over matching groups of instances.\nA potential concern at this stage is that the LLM may not consistently extract references to the\nsame entity in the same text format, resulting in duplicate entity elements and thus duplicate nodes\nin the entity graph. However, since all closely-related \u201ccommunities\u201d of entities will be detected\nand summarized in the following step, and given that LLMs can understand the common entity\nbehind multiple name variations, our overall approach is resilient to such variations provided there\nis sufficient connectivity from all variations to a shared set of closely-related entities.\nOverall, our use of rich descriptive text for homogeneous nodes in a potentially noisy graph structure\nis aligned with both the capabilities of LLMs and the needs of global, query-focused summarization.\nThese qualities also differentiate our graph index from typical knowledge graphs, which rely on\nconcise and consistent knowledge triples (subject, predicate, object) for downstream reasoning tasks.\n2.4 Element Summaries \u2192Graph Communities\nThe index created in the previous step can be modelled as an homogeneous undirected weighted\ngraph in which entity nodes are connected by relationship edges, with edge weights representing the\nnormalized counts of detected relationship instances. Given such a graph, a variety of community\ndetection algorithms may be used to partition the graph into communities of nodes with stronger\nconnections to one another than to the other nodes in the graph (e.g., see the surveys by Fortu-\nnato, 2010 and Jin et al., 2021). In our pipeline, we use Leiden (Traag et al., 2019) on account of\nits ability to recover hierarchical community structure of large-scale graphs efficiently (Figure 3).\nEach level of this hierarchy provides a community partition that covers the nodes of the graph in a\nmutually-exclusive, collective-exhaustive way, enabling divide-and-conquer global summarization.\n2.5 Graph Communities \u2192Community Summaries\nThe next step is to create report-like summaries of each community in the Leiden hierarchy, using\na method designed to scale to very large datasets. These summaries are independently useful in\ntheir own right as a way to understand the global structure and semantics of the dataset, and may\nthemselves be used to make sense of a corpus in the absence of a question. For example, a user\nmay scan through community summaries at one level looking for general themes of interest, then\nfollow links to the reports at the lower level that provide more details for each of the subtopics. Here,\nhowever, we focus on their utility as part of a graph-based index used for answering global queries.\nCommunity summaries are generated in the following way:\n4(a) Root communities at level 0 (b) Sub-communities at level 1\nFigure 3: Graph communities detected using the Leiden algorithm (Traag et al., 2019) over the\nMultiHop-RAG (Tang and Yang, 2024) dataset as indexed. Circles represent entity nodes with size\nproportional to their degree. Node layout was performed via OpenORD (Martin et al., 2011) and\nForce Atlas 2 (Jacomy et al., 2014). Node colors represent entity communities, shown at two levels\nof hierarchical clustering: (a) Level 0, corresponding to the hierarchical partition with maximum\nmodularity, and (b) Level 1, which reveals internal structure within these root-level communities.\n\u2022Leaf-level communities . The element summaries of a leaf-level community (nodes, edges,\ncovariates) are prioritized and then iteratively added to the LLM context window until\nthe token limit is reached. The prioritization is as follows: for each community edge in\ndecreasing order of combined source and target node degree (i.e., overall prominance), add\ndescriptions of the source node, target node, linked covari","chunk_id":"e66ed885a08f92cc69f4895302c33047","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"LLM","type":"TECHNOLOGY","description":"LLM (Large Language Model) refers to advanced AI models designed to understand and generate human-like text based on input prompts.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"NAMED ENTITIES","type":"DATA TYPE","description":"Named entities are specific categories of information such as people, places, and organizations that are extracted from text.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"FEW-SHOT EXAMPLES","type":"TECHNIQUE","description":"Few-shot examples are specific instances provided to the LLM to improve its performance in specialized domains by offering context and guidance.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"COVARIATES","type":"DATA TYPE","description":"Covariates are additional variables or attributes associated with extracted entities, such as claims linked to those entities.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"EXTRACTION PROMPT","type":"TECHNIQUE","description":"An extraction prompt is a specific instruction given to the LLM to guide it in identifying and extracting relevant information from text.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"GLEANINGS","type":"PROCESS","description":"Gleanings refer to multiple rounds of extraction attempts aimed at identifying any missed entities in the text.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"ABSTRACTIVE SUMMARIZATION","type":"TECHNIQUE","description":"Abstractive summarization is a method where the LLM generates concise summaries that capture the essence of the original text, including implied relationships.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"ENTITY GRAPH","type":"DATA STRUCTURE","description":"An entity graph is a representation of entities and their relationships, structured as nodes and edges to illustrate connections.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"COMMUNITY DETECTION ALGORITHMS","type":"TECHNIQUE","description":"Community detection algorithms are methods used to identify groups of closely related nodes within a graph.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"LEIDEN ALGORITHM","type":"TECHNIQUE","description":"The Leiden algorithm is a community detection method known for efficiently recovering hierarchical structures in large-scale graphs.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"MULTIHOP-RAG DATASET","type":"DATASET","description":"The MultiHop-RAG dataset is a collection of data used for testing and evaluating the performance of the Graph RAG approach.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"HIERARCHICAL CLUSTERING","type":"TECHNIQUE","description":"Hierarchical clustering is a method of organizing data into a hierarchy of clusters based on similarity.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"GLOBAL SUMMARIZATION","type":"PROCESS","description":"Global summarization refers to the process of creating comprehensive summaries that capture the overall structure and semantics of a dataset.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"NODE","type":"DATA STRUCTURE","description":"A node is an individual entity within a graph, representing a specific item or concept connected to other nodes through relationships.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"EDGE","type":"DATA STRUCTURE","description":"An edge is a connection between two nodes in a graph, representing the relationship or interaction between those entities.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"CLAIMS","type":"DATA TYPE","description":"Claims are assertions or statements linked to entities, providing additional context or information about them.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"GRAPH INDEX","type":"","description":"","source_id":"e66ed885a08f92cc69f4895302c33047"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM (Large Language Model) refers to advanced AI models designed to understand and generate human-like text based on input prompts.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"NAMED ENTITIES\">      <data key=\"d0\">DATA TYPE<\/data>      <data key=\"d1\">Named entities are specific categories of information such as people, places, and organizations that are extracted from text.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"FEW-SHOT EXAMPLES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Few-shot examples are specific instances provided to the LLM to improve its performance in specialized domains by offering context and guidance.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"COVARIATES\">      <data key=\"d0\">DATA TYPE<\/data>      <data key=\"d1\">Covariates are additional variables or attributes associated with extracted entities, such as claims linked to those entities.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"EXTRACTION PROMPT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">An extraction prompt is a specific instruction given to the LLM to guide it in identifying and extracting relevant information from text.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"GLEANINGS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Gleanings refer to multiple rounds of extraction attempts aimed at identifying any missed entities in the text.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"ABSTRACTIVE SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Abstractive summarization is a method where the LLM generates concise summaries that capture the essence of the original text, including implied relationships.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"ENTITY GRAPH\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">An entity graph is a representation of entities and their relationships, structured as nodes and edges to illustrate connections.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"COMMUNITY DETECTION ALGORITHMS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Community detection algorithms are methods used to identify groups of closely related nodes within a graph.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"LEIDEN ALGORITHM\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Leiden algorithm is a community detection method known for efficiently recovering hierarchical structures in large-scale graphs.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"MULTIHOP-RAG DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">The MultiHop-RAG dataset is a collection of data used for testing and evaluating the performance of the Graph RAG approach.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"HIERARCHICAL CLUSTERING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Hierarchical clustering is a method of organizing data into a hierarchy of clusters based on similarity.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"GLOBAL SUMMARIZATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Global summarization refers to the process of creating comprehensive summaries that capture the overall structure and semantics of a dataset.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"NODE\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">A node is an individual entity within a graph, representing a specific item or concept connected to other nodes through relationships.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"EDGE\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">An edge is a connection between two nodes in a graph, representing the relationship or interaction between those entities.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"CLAIMS\">      <data key=\"d0\">DATA TYPE<\/data>      <data key=\"d1\">Claims are assertions or statements linked to entities, providing additional context or information about them.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"GRAPH INDEX\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <edge source=\"LLM\" target=\"NAMED ENTITIES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The LLM is used to extract named entities from text, identifying specific categories of information like people and organizations.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"FEW-SHOT EXAMPLES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Few-shot examples are provided to the LLM to enhance its ability to extract relevant information in specialized domains.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"EXTRACTION PROMPT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The extraction prompt guides the LLM in identifying and extracting relevant information from the text.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"GLEANINGS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The LLM performs gleanings to ensure that all relevant entities are extracted, even those missed in previous attempts.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"ABSTRACTIVE SUMMARIZATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The LLM utilizes abstractive summarization to create meaningful summaries of entities and relationships from the text.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"ENTITY GRAPH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The LLM contributes to the creation of an entity graph by extracting and summarizing relationships between entities.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"MULTIHOP-RAG DATASET\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The MultiHop-RAG dataset is utilized by the LLM for testing and evaluating the performance of the Graph RAG approach.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"GRAPH INDEX\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The LLM contributes to the creation of a graph index by extracting and summarizing relationships between entities.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"NAMED ENTITIES\" target=\"COVARIATES\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Covariates are linked to named entities, providing additional context and attributes related to those entities.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"NAMED ENTITIES\" target=\"CLAIMS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Claims are associated with named entities, providing context and additional information relevant to those entities.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"ENTITY GRAPH\" target=\"COMMUNITY DETECTION ALGORITHMS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Community detection algorithms analyze the entity graph to identify groups of closely related nodes.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"COMMUNITY DETECTION ALGORITHMS\" target=\"LEIDEN ALGORITHM\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Leiden algorithm is a specific type of community detection algorithm used for efficiently partitioning graphs.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"COMMUNITY DETECTION ALGORITHMS\" target=\"HIERARCHICAL CLUSTERING\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Hierarchical clustering is a technique used within community detection algorithms to organize nodes into a hierarchy.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"GLOBAL SUMMARIZATION\" target=\"GRAPH INDEX\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Global summarization utilizes the graph index to provide comprehensive insights into the dataset's structure and semantics.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"NODE\" target=\"EDGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Nodes are connected by edges in a graph, illustrating the relationships between different entities.<\/data>      <data key=\"d5\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"4930fce6da868f894757a9da465807ba","chunk":" which reveals internal structure within these root-level communities.\n\u2022Leaf-level communities . The element summaries of a leaf-level community (nodes, edges,\ncovariates) are prioritized and then iteratively added to the LLM context window until\nthe token limit is reached. The prioritization is as follows: for each community edge in\ndecreasing order of combined source and target node degree (i.e., overall prominance), add\ndescriptions of the source node, target node, linked covariates, and the edge itself.\n\u2022Higher-level communities . If all element summaries fit within the token limit of the con-\ntext window, proceed as for leaf-level communities and summarize all element summaries\nwithin the community. Otherwise, rank sub-communities in decreasing order of element\nsummary tokens and iteratively substitute sub-community summaries (shorter) for their\nassociated element summaries (longer) until fit within the context window is achieved.\n2.6 Community Summaries \u2192Community Answers \u2192Global Answer\nGiven a user query, the community summaries generated in the previous step can be used to generate\na final answer in a multi-stage process. The hierarchical nature of the community structure also\nmeans that questions can be answered using the community summaries from different levels, raising\nthe question of whether a particular level in the hierarchical community structure offers the best\nbalance of summary detail and scope for general sensemaking questions (evaluated in section 3).\nFor a given community level, the global answer to any user query is generated as follows:\n\u2022Prepare community summaries . Community summaries are randomly shuffled and divided\ninto chunks of pre-specified token size. This ensures relevant information is distributed\nacross chunks, rather than concentrated (and potentially lost) in a single context window.\n\u2022Map community answers . Generate intermediate answers in parallel, one for each chunk.\nThe LLM is also asked to generate a score between 0-100 indicating how helpful the gen-\nerated answer is in answering the target question. Answers with score 0 are filtered out.\n\u2022Reduce to global answer . Intermediate community answers are sorted in descending order\nof helpfulness score and iteratively added into a new context window until the token limit\nis reached. This final context is used to generate the global answer returned to the user.\n5Dataset Example activity framing and generation of global sensemaking questions\nPodcast\ntranscriptsUser : A tech journalist looking for insights and trends in the tech industry\nTask: Understanding how tech leaders view the role of policy and regulation\nQuestions :\n1. Which episodes deal primarily with tech policy and government regulation?\n2. How do guests perceive the impact of privacy laws on technology development?\n3. Do any guests discuss the balance between innovation and ethical considerations?\n4. What are the suggested changes to current policies mentioned by the guests?\n5. Are collaborations between tech companies and governments discussed and how?\nNews\narticlesUser : Educator incorporating current affairs into curricula\nTask: Teaching about health and wellness\nQuestions :\n1. What current topics in health can be integrated into health education curricula?\n2. How do news articles address the concepts of preventive medicine and wellness?\n3. Are there examples of health articles that contradict each other, and if so, why?\n4. What insights can be gleaned about public health priorities based on news coverage?\n5. How can educators use the dataset to highlight the importance of health literacy?\nTable 1: Examples of potential users, tasks, and questions generated by the LLM based on short\ndescriptions of the target datasets. Questions target global understanding rather than specific details.\n3 Evaluation\n3.1 Datasets\nWe selected two datasets in the one million token range, each equivalent to about 10 novels of text\nand representative of the kind of corpora that users may encounter in their real world activities:\n\u2022Podcast transcripts . Compiled transcripts of podcast conversations between Kevin Scott,\nMicrosoft CTO, and other technology leaders (Behind the Tech, Scott, 2024). Size: 1669\n\u00d7600-token text chunks, with 100-token overlaps between chunks ( \u223c1 million tokens).\n\u2022News articles . Benchmark dataset comprising news articles published from September\n2013 to December 2023 in a range of categories, including entertainment, business, sports,\ntechnology, health, and science (MultiHop-RAG; Tang and Yang, 2024). Size: 3197 \u00d7\n600-token text chunks, with 100-token overlaps between chunks ( \u223c1.7 million tokens).\n3.2 Queries\nMany benchmark datasets for open-domain question answering exist, including HotPotQA (Yang\net al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However,\nthe associated question sets target explicit fact retrieval rather than summarization for the purpose\nof data sensemaking, i.e., the process though which people inspect, engage with, and contextualize\ndata within the broader scope of real-world activities (Koesten et al., 2021). Similarly, methods for\nextracting latent summarization queries from source texts also exist (Xu and Lapata, 2021), but such\nextracted questions can target details that betray prior knowledge of the texts.\nTo evaluate the effectiveness of RAG systems for more global sensemaking tasks, we need questions\nthat convey only a high-level understanding of dataset contents, and not the details of specific texts.\nWe used an activity-centered approach to automate the generation of such questions: given a short\ndescription of a dataset, we asked the LLM to identify Npotential users and Ntasks per user,\nthen for each (user, task) combination, we asked the LLM to generate Nquestions that require\nunderstanding of the entire corpus. For our evaluation, a value of N= 5 resulted in 125 test questions\nper","chunk_id":"4930fce6da868f894757a9da465807ba","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"LEAF-LEVEL COMMUNITIES","type":"COMMUNITY STRUCTURE","description":"Leaf-level communities are the lowest hierarchical level in a community structure, where element summaries are prioritized and added to the LLM context window for processing.","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"HIGHER-LEVEL COMMUNITIES","type":"COMMUNITY STRUCTURE","description":"Higher-level communities are the upper hierarchical levels in a community structure, which summarize element summaries and manage sub-communities based on token limits.","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"LLM CONTEXT WINDOW","type":"DATA STRUCTURE","description":"The LLM context window is a limit on the amount of information that can be processed at one time, affecting how summaries and answers are generated.","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"COMMUNITY SUMMARIES","type":"DATA FORMAT","description":"Community summaries are aggregated descriptions of elements within a community, used to generate answers to user queries.","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"GLOBAL ANSWER","type":"OUTPUT","description":"The global answer is the final response generated for a user query, synthesized from community summaries and evaluated for helpfulness.","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"USER QUERY","type":"REQUEST","description":"A user query is a question posed by a user seeking information or insights from the community summaries.","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"DATASET EXAMPLES","type":"DATA FORMAT","description":"Dataset examples are specific instances of data used to illustrate potential user tasks and questions in the context of information retrieval.","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"PODCAST TRANSCRIPTS","type":"DATASET","description":"Podcast transcripts are written records of conversations between technology leaders, used as a dataset for analysis and question generation.","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"NEWS ARTICLES","type":"DATASET","description":"News articles are written reports on current events, used as a dataset for analysis and question generation in various categories.","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"EVALUATION","type":"PROCESS","description":"Evaluation is the process of assessing the effectiveness of RAG systems in generating high-level understanding questions from datasets.","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"NODES","type":"DATA ELEMENT","description":"Nodes are the individual elements or entities within a community structure that represent distinct points of interest or data.","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"EDGES","type":"DATA ELEMENT","description":"Edges are the connections between nodes in a community structure, indicating relationships or interactions between the entities.","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"SUB-COMMUNITIES","type":"COMMUNITY STRUCTURE","description":"Sub-communities are smaller groups within higher-level communities that can be summarized and substituted to fit within the LLM context window.","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"COVARIATES","type":"","description":"","source_id":"4930fce6da868f894757a9da465807ba"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LEAF-LEVEL COMMUNITIES\">      <data key=\"d0\">COMMUNITY STRUCTURE<\/data>      <data key=\"d1\">Leaf-level communities are the lowest hierarchical level in a community structure, where element summaries are prioritized and added to the LLM context window for processing.<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"HIGHER-LEVEL COMMUNITIES\">      <data key=\"d0\">COMMUNITY STRUCTURE<\/data>      <data key=\"d1\">Higher-level communities are the upper hierarchical levels in a community structure, which summarize element summaries and manage sub-communities based on token limits.<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"LLM CONTEXT WINDOW\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">The LLM context window is a limit on the amount of information that can be processed at one time, affecting how summaries and answers are generated.<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"COMMUNITY SUMMARIES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Community summaries are aggregated descriptions of elements within a community, used to generate answers to user queries.<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"GLOBAL ANSWER\">      <data key=\"d0\">OUTPUT<\/data>      <data key=\"d1\">The global answer is the final response generated for a user query, synthesized from community summaries and evaluated for helpfulness.<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"USER QUERY\">      <data key=\"d0\">REQUEST<\/data>      <data key=\"d1\">A user query is a question posed by a user seeking information or insights from the community summaries.<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"DATASET EXAMPLES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Dataset examples are specific instances of data used to illustrate potential user tasks and questions in the context of information retrieval.<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"PODCAST TRANSCRIPTS\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Podcast transcripts are written records of conversations between technology leaders, used as a dataset for analysis and question generation.<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"NEWS ARTICLES\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">News articles are written reports on current events, used as a dataset for analysis and question generation in various categories.<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"EVALUATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Evaluation is the process of assessing the effectiveness of RAG systems in generating high-level understanding questions from datasets.<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"NODES\">      <data key=\"d0\">DATA ELEMENT<\/data>      <data key=\"d1\">Nodes are the individual elements or entities within a community structure that represent distinct points of interest or data.<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"EDGES\">      <data key=\"d0\">DATA ELEMENT<\/data>      <data key=\"d1\">Edges are the connections between nodes in a community structure, indicating relationships or interactions between the entities.<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"SUB-COMMUNITIES\">      <data key=\"d0\">COMMUNITY STRUCTURE<\/data>      <data key=\"d1\">Sub-communities are smaller groups within higher-level communities that can be summarized and substituted to fit within the LLM context window.<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"COVARIATES\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <edge source=\"LEAF-LEVEL COMMUNITIES\" target=\"HIGHER-LEVEL COMMUNITIES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Leaf-level communities serve as the foundational elements that contribute to the summaries of higher-level communities.<\/data>      <data key=\"d5\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"LEAF-LEVEL COMMUNITIES\" target=\"LLM CONTEXT WINDOW\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The LLM context window limits the amount of information that can be processed from leaf-level communities at one time.<\/data>      <data key=\"d5\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"HIGHER-LEVEL COMMUNITIES\" target=\"SUB-COMMUNITIES\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Sub-communities are components of higher-level communities, contributing to the overall structure and summaries generated from them.<\/data>      <data key=\"d5\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"GLOBAL ANSWER\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Community summaries are used to generate the global answer for user queries, synthesizing information from various levels of the community structure.<\/data>      <data key=\"d5\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"GLOBAL ANSWER\" target=\"USER QUERY\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">A user query initiates the process of generating a global answer based on community summaries.<\/data>      <data key=\"d5\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"DATASET EXAMPLES\" target=\"PODCAST TRANSCRIPTS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Dataset examples illustrate potential user tasks and questions derived from podcast transcripts.<\/data>      <data key=\"d5\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"DATASET EXAMPLES\" target=\"NEWS ARTICLES\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Dataset examples illustrate potential user tasks and questions derived from news articles.<\/data>      <data key=\"d5\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"DATASET EXAMPLES\" target=\"EVALUATION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Evaluation assesses the effectiveness of the generated questions based on dataset examples.<\/data>      <data key=\"d5\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"NODES\" target=\"EDGES\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Nodes are connected by edges, forming the fundamental structure of a community and representing relationships between entities.<\/data>      <data key=\"d5\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"NODES\" target=\"COVARIATES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Covariates provide additional context to nodes, enhancing the understanding of their significance within the community structure.<\/data>      <data key=\"d5\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"26b2dad01a219bc034ac7d6a32d07582","chunk":" understanding of dataset contents, and not the details of specific texts.\nWe used an activity-centered approach to automate the generation of such questions: given a short\ndescription of a dataset, we asked the LLM to identify Npotential users and Ntasks per user,\nthen for each (user, task) combination, we asked the LLM to generate Nquestions that require\nunderstanding of the entire corpus. For our evaluation, a value of N= 5 resulted in 125 test questions\nper dataset. Table 1 shows example questions for each of the two evaluation datasets.\n63.3 Conditions\nWe compare six different conditions in our analysis, including Graph RAG using four levels of graph\ncommunities ( C0,C1,C2,C3), a text summarization method applying our map-reduce approach\ndirectly to source texts ( TS), and a na \u00a8\u0131ve \u201csemantic search\u201d RAG approach ( SS):\n\u2022CO. Uses root-level community summaries (fewest in number) to answer user queries.\n\u2022C1. Uses high-level community summaries to answer queries. These are sub-communities\nof C0, if present, otherwise C0 communities projected down.\n\u2022C2. Uses intermediate-level community summaries to answer queries. These are sub-\ncommunities of C1, if present, otherwise C1 communities projected down.\n\u2022C3. Uses low-level community summaries (greatest in number) to answer queries. These\nare sub-communities of C2, if present, otherwise C2 communities projected down.\n\u2022TS. The same method as in subsection 2.6, except source texts (rather than community\nsummaries) are shuffled and chunked for the map-reduce summarization stages.\n\u2022SS. An implementation of na \u00a8\u0131ve RAG in which text chunks are retrieved and added to the\navailable context window until the specified token limit is reached.\nThe size of the context window and the prompts used for answer generation are the same across\nall six conditions (except for minor modifications to reference styles to match the types of context\ninformation used). Conditions only differ in how the contents of the context window are created.\nThe graph index supporting conditions C0-C3was created using our generic prompts for entity and\nrelationship extraction only, with entity types and few-shot examples tailored to the domain of the\ndata. The graph indexing process used a context window size of 600 tokens with 1 gleaning for the\nPodcast dataset and 0 gleanings for the News dataset.\n3.4 Metrics\nLLMs have been shown to be good evaluators of natural language generation, achieving state-of-\nthe-art or competitive results compared against human judgements (Wang et al., 2023a; Zheng et al.,\n2024). While this approach can generate reference-based metrics when gold standard answers are\nknown, it is also capable of measuring the qualities of generated texts (e.g., fluency) in a reference-\nfree style (Wang et al., 2023a) as well as in head-to-head comparison of competing outputs (LLM-\nas-a-judge, Zheng et al., 2024). LLMs have also shown promise at evaluating the performance of\nconventional RAG systems, automatically evaluating qualities like context relevance, faithfulness,\nand answer relevance (RAGAS, Es et al., 2023).\nGiven the multi-stage nature of our Graph RAG mechanism, the multiple conditions we wanted to\ncompare, and the lack of gold standard answers to our activity-based sensemaking questions, we\ndecided to adopt a head-to-head comparison approach using an LLM evaluator. We selected three\ntarget metrics capturing qualities that are desirable for sensemaking activities, as well as a control\nmetric (directness) used as a indicator of validity. Since directness is effectively in opposition to\ncomprehensiveness and diversity, we would not expect any method to win across all four metrics.\nOur head-to-head measures computed using an LLM evaluator are as follows:\n\u2022Comprehensiveness . How much detail does the answer provide to cover all aspects and\ndetails of the question?\n\u2022Diversity . How varied and rich is the answer in providing different perspectives and insights\non the question?\n\u2022Empowerment . How well does the answer help the reader understand and make informed\njudgements about the topic?\n\u2022Directness . How specifically and clearly does the answer address the question?\nFor our evaluation, the LLM is provided with the question, target metric, and a pair of answers, and\nasked to assess which answer is better according to the metric, as well as why. It returns the winner\nif one exists, otherwise a tie if they are fundamentally similar and the differences are negligible.\nTo account for the stochasticity of LLMs, we run each comparison five times and use mean scores.\nTable 2 shows an example of LLM-generated assessment.\n7Question Which public figures are repeatedly mentioned across various entertainment articles?\nGraph\nRAGOverview of Prominent Public Figures in Entertainment\nThe entertainment industry is vast and diverse, encompassing film, television, music, sports, and\ndigital media. Certain public figures stand out due to their significant contributions and influence\nacross these sectors. The following summary highlights key individuals who are repeatedly\nmentioned in various entertainment articles, reflecting their impact and presence within the industry.\nActors and Directors [...]Public Figures in Controversy [...]Musicians and Executives [...]\nAthletes and Coaches [...]Influencers and Entrepreneurs [...]\nThe repeated mention of these figures in entertainment articles signifies their ongoing relevance and\nthe public\u2019s interest in their work. Their influence spans across various aspects of entertainment, from\nshaping cultural narratives in film and television to driving trends in music and digital media. These\nindividuals not only contribute to their respective fields but also influence the broader cultural","chunk_id":"26b2dad01a219bc034ac7d6a32d07582","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"GRAPH RAG APPROACH","type":"TECHNIQUE","description":"The Graph RAG approach is a method for automating the generation of questions based on dataset descriptions, utilizing a structured pipeline for processing and analysis.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"DATASET","type":"DATA FORMAT","description":"A dataset is a collection of related data that can be analyzed to generate insights or questions, often used in machine learning and data analysis.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"LLM","type":"TECHNOLOGY","description":"LLM stands for Large Language Model, which is an AI model designed to understand and generate human-like text based on input data.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"USER","type":"PERSON","description":"A user is an individual or entity that interacts with the dataset and LLM to generate questions or perform tasks related to data analysis.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"TASK","type":"ACTIVITY","description":"A task is a specific activity or job that a user aims to accomplish using the dataset and LLM, often involving question generation or data interpretation.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"QUESTION","type":"DATA FORMAT","description":"A question is an inquiry generated by the LLM that requires understanding of the dataset and its contents for a comprehensive answer.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"EVALUATION METRICS","type":"CRITERIA","description":"Evaluation metrics are standards or criteria used to assess the quality and effectiveness of the generated answers or questions.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"COMPREHENSIVENESS","type":"CRITERIA","description":"Comprehensiveness is a metric that measures how thoroughly an answer covers all aspects of a question.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"DIVERSITY","type":"CRITERIA","description":"Diversity is a metric that evaluates the variety and richness of perspectives provided in an answer.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"EMPOWERMENT","type":"CRITERIA","description":"Empowerment is a metric that assesses how well an answer helps the reader understand and make informed judgments about a topic.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"DIRECTNESS","type":"CRITERIA","description":"Directness is a metric that measures how clearly and specifically an answer addresses the question posed.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"C0","type":"GRAPH COMMUNITY","description":"C0 is the root-level community summary used to answer user queries, representing the highest level of abstraction in the Graph RAG approach.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"C1","type":"GRAPH COMMUNITY","description":"C1 is a high-level community summary that provides answers to queries, serving as sub-communities of C0 when available.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"C2","type":"GRAPH COMMUNITY","description":"C2 is an intermediate-level community summary that answers queries, serving as sub-communities of C1 when available.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"C3","type":"GRAPH COMMUNITY","description":"C3 is a low-level community summary that provides the greatest number of answers to queries, serving as sub-communities of C2 when available.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"TS","type":"METHOD","description":"TS refers to a text summarization method that applies a map-reduce approach directly to source texts for generating summaries.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"SS","type":"METHOD","description":"SS is a na \u00efve semantic search RAG approach that retrieves text chunks and adds them to the context window until a specified token limit is reached.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"PODCAST DATASET","type":"DATASET","description":"The Podcast dataset is a specific dataset used in the evaluation of the Graph RAG approach, characterized by its unique content and structure.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"NEWS DATASET","type":"DATASET","description":"The News dataset is another specific dataset used in the evaluation of the Graph RAG approach, characterized by its unique content and structure.","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"GRAPH COMMUNITIES","type":"","description":"","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"SOURCE TEXTS","type":"","description":"","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"TEXT CHUNKS","type":"","description":"","source_id":"26b2dad01a219bc034ac7d6a32d07582"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GRAPH RAG APPROACH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Graph RAG approach is a method for automating the generation of questions based on dataset descriptions, utilizing a structured pipeline for processing and analysis.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"DATASET\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">A dataset is a collection of related data that can be analyzed to generate insights or questions, often used in machine learning and data analysis.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM stands for Large Language Model, which is an AI model designed to understand and generate human-like text based on input data.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"USER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A user is an individual or entity that interacts with the dataset and LLM to generate questions or perform tasks related to data analysis.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"TASK\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">A task is a specific activity or job that a user aims to accomplish using the dataset and LLM, often involving question generation or data interpretation.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"QUESTION\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">A question is an inquiry generated by the LLM that requires understanding of the dataset and its contents for a comprehensive answer.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"EVALUATION METRICS\">      <data key=\"d0\">CRITERIA<\/data>      <data key=\"d1\">Evaluation metrics are standards or criteria used to assess the quality and effectiveness of the generated answers or questions.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"COMPREHENSIVENESS\">      <data key=\"d0\">CRITERIA<\/data>      <data key=\"d1\">Comprehensiveness is a metric that measures how thoroughly an answer covers all aspects of a question.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"DIVERSITY\">      <data key=\"d0\">CRITERIA<\/data>      <data key=\"d1\">Diversity is a metric that evaluates the variety and richness of perspectives provided in an answer.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"EMPOWERMENT\">      <data key=\"d0\">CRITERIA<\/data>      <data key=\"d1\">Empowerment is a metric that assesses how well an answer helps the reader understand and make informed judgments about a topic.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"DIRECTNESS\">      <data key=\"d0\">CRITERIA<\/data>      <data key=\"d1\">Directness is a metric that measures how clearly and specifically an answer addresses the question posed.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"C0\">      <data key=\"d0\">GRAPH COMMUNITY<\/data>      <data key=\"d1\">C0 is the root-level community summary used to answer user queries, representing the highest level of abstraction in the Graph RAG approach.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"C1\">      <data key=\"d0\">GRAPH COMMUNITY<\/data>      <data key=\"d1\">C1 is a high-level community summary that provides answers to queries, serving as sub-communities of C0 when available.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"C2\">      <data key=\"d0\">GRAPH COMMUNITY<\/data>      <data key=\"d1\">C2 is an intermediate-level community summary that answers queries, serving as sub-communities of C1 when available.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"C3\">      <data key=\"d0\">GRAPH COMMUNITY<\/data>      <data key=\"d1\">C3 is a low-level community summary that provides the greatest number of answers to queries, serving as sub-communities of C2 when available.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"TS\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">TS refers to a text summarization method that applies a map-reduce approach directly to source texts for generating summaries.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"SS\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">SS is a na &#239;ve semantic search RAG approach that retrieves text chunks and adds them to the context window until a specified token limit is reached.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"PODCAST DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">The Podcast dataset is a specific dataset used in the evaluation of the Graph RAG approach, characterized by its unique content and structure.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"NEWS DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">The News dataset is another specific dataset used in the evaluation of the Graph RAG approach, characterized by its unique content and structure.<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"GRAPH COMMUNITIES\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"SOURCE TEXTS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"TEXT CHUNKS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <edge source=\"GRAPH RAG APPROACH\" target=\"DATASET\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach utilizes datasets to generate questions and tasks for users, facilitating data analysis.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"GRAPH COMMUNITIES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach utilizes different graph communities (C0, C1, C2, C3) to structure and summarize data for answering user queries.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"PODCAST DATASET\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Podcast dataset is used in the evaluation of the Graph RAG approach, contributing to the analysis of its effectiveness.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"NEWS DATASET\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The News dataset is used in the evaluation of the Graph RAG approach, contributing to the analysis of its effectiveness.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"LLM\" target=\"USER\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The LLM interacts with users to generate questions based on dataset descriptions, aiding in their tasks.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"USER\" target=\"TASK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Users perform tasks that involve generating questions and analyzing datasets using the LLM.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"TASK\" target=\"QUESTION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Tasks often result in the generation of questions that require understanding of the dataset for effective answers.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"QUESTION\" target=\"EVALUATION METRICS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Evaluation metrics are used to assess the quality of the questions generated by the LLM and the answers provided.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"EVALUATION METRICS\" target=\"COMPREHENSIVENESS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Comprehensiveness is one of the evaluation metrics used to measure the quality of answers generated by the LLM.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"EVALUATION METRICS\" target=\"DIVERSITY\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Diversity is another evaluation metric that assesses the variety of answers generated by the LLM.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"EVALUATION METRICS\" target=\"EMPOWERMENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Empowerment is an evaluation metric that measures how well answers help users understand topics.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"EVALUATION METRICS\" target=\"DIRECTNESS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Directness is an evaluation metric that assesses how clearly answers address the questions posed.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"C0\" target=\"C1\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">C1 serves as a sub-community of C0, providing more detailed summaries for answering user queries.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"C1\" target=\"C2\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">C2 serves as a sub-community of C1, providing further granularity in summarization for answering queries.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"C2\" target=\"C3\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">C3 serves as a sub-community of C2, offering the most detailed summaries for answering user queries.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"TS\" target=\"SOURCE TEXTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The TS method applies a map-reduce approach to source texts, shuffling and chunking them for summarization stages.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"SS\" target=\"TEXT CHUNKS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The SS method retrieves text chunks to build context for answering queries, adding them until a token limit is reached.<\/data>      <data key=\"d5\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"c8e8019de153e439d6a79dcf209b943b","chunk":" Directors [...]Public Figures in Controversy [...]Musicians and Executives [...]\nAthletes and Coaches [...]Influencers and Entrepreneurs [...]\nThe repeated mention of these figures in entertainment articles signifies their ongoing relevance and\nthe public\u2019s interest in their work. Their influence spans across various aspects of entertainment, from\nshaping cultural narratives in film and television to driving trends in music and digital media. These\nindividuals not only contribute to their respective fields but also influence the broader cultural\nlandscape, often becoming central figures in social discussions and public discourse.\nNa\u00a8\u0131ve\nRAGPublic figures who are repeatedly mentioned across various entertainment articles include Taylor\nSwift, Travis Kelce, Britney Spears, and Justin Timberlake. These individuals have been highlighted\nfor various reasons, ranging from their professional achievements to their personal lives.\nTaylor Swift [...]Travis Kelce [...]Britney Spears [...]Justin Timberlake [...]\nThese figures are frequently covered due to their high-profile status and the public\u2019s interest in their\ncareers and personal lives. Their activities, whether in music, sports, or personal relationships, have\nsignificant cultural and economic impacts, as evidenced by the media coverage and public reactions.\nLLM\nDecisionComprehensiveness: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a more comprehensive and detailed list of public figures from a\nwider range of entertainment sectors, including film, television, music, sports, gaming, and digital\nmedia. It also includes specific examples of their contributions and the impact they have on their\nrespective fields, as well as mentions of controversies and their implications. Answer 2, while\ndetailed in its coverage of a few individuals, is limited to a smaller number of public figures and\nfocuses primarily on their personal lives and relationships rather than a broad spectrum of their\nprofessional influence across the entertainment industry.\nDiversity: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a more varied and rich response by covering a wide range of\npublic figures from different sectors of the entertainment industry, including film, television, music,\nsports, gaming, and digital media. It offers insights into the contributions and influence of these\nfigures, as well as controversies and their impact on public discourse. The answer also cites specific\ndata sources for each mentioned figure, indicating a diverse range of evidence to support the claims.\nIn contrast, Answer 2 focuses on a smaller group of public figures, primarily from the music industry\nand sports, and relies heavily on a single source for data, which makes it less diverse in perspectives\nand insights.\nEmpowerment: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a comprehensive and structured overview of public figures\nacross various sectors of the entertainment industry, including film, television, music, sports, and\ndigital media. It lists multiple individuals, providing specific examples of their contributions and the\ncontext in which they are mentioned in entertainment articles, along with references to data reports\nfor each claim. This approach helps the reader understand the breadth of the topic and make informed\njudgments without being misled. In contrast, Answer 2 focuses on a smaller group of public figures\nand primarily discusses their personal lives and relationships, which may not provide as broad an\nunderstanding of the topic. While Answer 2 also cites sources, it does not match the depth and variety\nof Answer 1.\nDirectness: Winner=2 (Na \u00a8\u0131ve RAG)\nAnswer 2 is better because it directly lists specific public figures who are repeatedly mentioned\nacross various entertainment articles, such as Taylor Swift, Travis Kelce, Britney Spears, and Justin\nTimberlake, and provides concise explanations for their frequent mentions. Answer 1, while\ncomprehensive, includes a lot of detailed information about various figures in different sectors of\nentertainment, which, while informative, does not directly answer the question with the same level of\nconciseness and specificity as Answer 2.\nTable 2: Example question for the News article dataset, with generated answers from Graph RAG\n(C2) and Na \u00a8\u0131ve RAG, as well as LLM-generated assessments.\n8Podcast transcripts\n501728252221\n835050484344\n725050535049\n755247505250\n785750485052\n795651504850SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nComprehensiveness501823251919\n825050504346\n775050504644\n755050504445\n815754565048\n815456555250SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nDiversity504257524951\n585059555251\n434150494748\n484551504950\n514853515051\n494952504950SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nEmpowerment505665606060\n445055525152\n354550474848\n404853505050\n404952505050\n404852505050SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nDirectness\nNews articles\n502028252121\n805044413836\n725650525452\n755948505855\n796246425059\n796448454150SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nComprehensiveness503338352931\n675053454440\n624750404141\n655560505050\n715659505051\n696059504950SS\nTS\nC0\nC1\nC","chunk_id":"c8e8019de153e439d6a79dcf209b943b","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"TAYLOR SWIFT","type":"PERSON","description":"Taylor Swift is a prominent musician known for her significant contributions to the music industry and her influence on popular culture.","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"TRAVIS KELCE","type":"PERSON","description":"Travis Kelce is a well-known athlete, recognized for his achievements in sports and his impact on popular culture.","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"BRITNEY SPEARS","type":"PERSON","description":"Britney Spears is a famous musician and public figure, noted for her influence in the music industry and her personal life controversies.","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"JUSTIN TIMBERLAKE","type":"PERSON","description":"Justin Timberlake is a renowned musician and entertainer, recognized for his contributions to music and his public persona.","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"PUBLIC FIGURES","type":"CATEGORY","description":"Public figures are individuals who have gained significant attention and influence in various sectors, including entertainment, sports, and media.","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"ENTERTAINMENT ARTICLES","type":"MEDIA FORMAT","description":"Entertainment articles are written pieces that cover various aspects of the entertainment industry, including news about public figures and cultural trends.","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"COACHES","type":"CATEGORY","description":"Coaches are individuals who train and guide athletes, playing a significant role in sports and team dynamics.","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"INFLUENCERS","type":"CATEGORY","description":"Influencers are individuals who have the power to affect the purchasing decisions of others due to their authority, knowledge, position, or relationship with their audience.","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"ENTREPRENEURS","type":"CATEGORY","description":"Entrepreneurs are individuals who create and manage businesses, often influencing trends and economic landscapes.","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"CULTURAL NARRATIVES","type":"CONCEPT","description":"Cultural narratives are the stories and themes that shape societal values and beliefs, often influenced by public figures in entertainment.","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"SOCIAL DISCUSSIONS","type":"CONCEPT","description":"Social discussions are conversations and debates that occur in public discourse, often involving topics related to public figures and their influence.","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"ATHLETES","type":"","description":"","source_id":"c8e8019de153e439d6a79dcf209b943b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"TAYLOR SWIFT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Taylor Swift is a prominent musician known for her significant contributions to the music industry and her influence on popular culture.<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"TRAVIS KELCE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Travis Kelce is a well-known athlete, recognized for his achievements in sports and his impact on popular culture.<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"BRITNEY SPEARS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Britney Spears is a famous musician and public figure, noted for her influence in the music industry and her personal life controversies.<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"JUSTIN TIMBERLAKE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Justin Timberlake is a renowned musician and entertainer, recognized for his contributions to music and his public persona.<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"PUBLIC FIGURES\">      <data key=\"d0\">CATEGORY<\/data>      <data key=\"d1\">Public figures are individuals who have gained significant attention and influence in various sectors, including entertainment, sports, and media.<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"ENTERTAINMENT ARTICLES\">      <data key=\"d0\">MEDIA FORMAT<\/data>      <data key=\"d1\">Entertainment articles are written pieces that cover various aspects of the entertainment industry, including news about public figures and cultural trends.<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"COACHES\">      <data key=\"d0\">CATEGORY<\/data>      <data key=\"d1\">Coaches are individuals who train and guide athletes, playing a significant role in sports and team dynamics.<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"INFLUENCERS\">      <data key=\"d0\">CATEGORY<\/data>      <data key=\"d1\">Influencers are individuals who have the power to affect the purchasing decisions of others due to their authority, knowledge, position, or relationship with their audience.<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"ENTREPRENEURS\">      <data key=\"d0\">CATEGORY<\/data>      <data key=\"d1\">Entrepreneurs are individuals who create and manage businesses, often influencing trends and economic landscapes.<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"CULTURAL NARRATIVES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Cultural narratives are the stories and themes that shape societal values and beliefs, often influenced by public figures in entertainment.<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"SOCIAL DISCUSSIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Social discussions are conversations and debates that occur in public discourse, often involving topics related to public figures and their influence.<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"ATHLETES\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <edge source=\"TAYLOR SWIFT\" target=\"PUBLIC FIGURES\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Taylor Swift is a public figure frequently mentioned in entertainment articles due to her influence and popularity in the music industry.<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"TRAVIS KELCE\" target=\"PUBLIC FIGURES\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Travis Kelce is a public figure frequently mentioned in entertainment articles due to his achievements in sports and cultural relevance.<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"BRITNEY SPEARS\" target=\"PUBLIC FIGURES\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Britney Spears is a public figure frequently mentioned in entertainment articles, often due to her music career and personal life controversies.<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"JUSTIN TIMBERLAKE\" target=\"PUBLIC FIGURES\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Justin Timberlake is a public figure frequently mentioned in entertainment articles for his contributions to music and entertainment.<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"ENTERTAINMENT ARTICLES\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Public figures are often the subjects of entertainment articles, highlighting their influence and relevance in popular culture.<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"ATHLETES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Athletes are often considered public figures due to their visibility and influence in sports and entertainment.<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"COACHES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Coaches can also be public figures, especially when they gain recognition for their leadership and impact on athletes and teams.<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"INFLUENCERS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Influencers are public figures who shape trends and opinions through their platforms and reach.<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"ENTREPRENEURS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Entrepreneurs often become public figures as they influence markets and cultural trends through their businesses.<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"CULTURAL NARRATIVES\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Public figures play a significant role in shaping cultural narratives through their actions and public personas.<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"SOCIAL DISCUSSIONS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Public figures often become central to social discussions, influencing public opinion and discourse.<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"ede7063998065122cf7a7152979c1909","chunk":"\n502028252121\n805044413836\n725650525452\n755948505855\n796246425059\n796448454150SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nComprehensiveness503338352931\n675053454440\n624750404141\n655560505050\n715659505051\n696059504950SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nDiversity504757495050\n535058505048\n434250424544\n515058505251\n505055485050\n505256495050SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nEmpowerment505459555554\n465055535252\n414550484847\n454752504949\n454852515049\n464853515150SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nDirectness\nFigure 4: Head-to-head win rate percentages of (row condition) over (column condition) across two\ndatasets, four metrics, and 125 questions per comparison (each repeated five times and averaged).\nThe overall winner per dataset and metric is shown in bold. Self-win rates were not computed but\nare shown as the expected 50% for reference. All Graph RAG conditions outperformed na \u00a8\u0131ve RAG\non comprehensiveness and diversity. Conditions C1-C3 also showed slight improvements in answer\ncomprehensiveness and diversity over TS (global text summarization without a graph index).\n3.5 Configuration\nThe effect of context window size on any particular task is unclear, especially for models like\ngpt-4-turbo with a large context size of 128k tokens. Given the potential for information to\nbe \u201clost in the middle\u201d of longer contexts (Kuratov et al., 2024; Liu et al., 2023), we wanted to ex-\nplore the effects of varying the context window size for our combinations of datasets, questions, and\nmetrics. In particular, our goal was to determine the optimum context size for our baseline condition\n(SS) and then use this uniformly for all query-time LLM use. To that end, we tested four context\nwindow sizes: 8k, 16k, 32k and 64k. Surprisingly, the smallest context window size tested (8k)\nwas universally better for all comparisons on comprehensiveness (average win rate of 58.1%), while\nperforming comparably with larger context sizes on diversity (average win rate = 52.4%), and em-\npowerment (average win rate = 51.3%). Given our preference for more comprehensive and diverse\nanswers, we therefore used a fixed context window size of 8k tokens for the final evaluation.\n3.6 Results\nThe indexing process resulted in a graph consisting of 8564 nodes and 20691 edges for the Podcast\ndataset, and a larger graph of 15754 nodes and 19520 edges for the News dataset. Table 3 shows the\nnumber of community summaries at different levels of each graph community hierarchy.\nGlobal approaches vs. na \u00a8\u0131ve RAG . As shown in Figure 4, global approaches consistently out-\nperformed the na \u00a8\u0131ve RAG ( SS) approach in both comprehensiveness and diversity metrics across\ndatasets. Specifically, global approaches achieved comprehensiveness win rates between 72-83%\nfor Podcast transcripts and 72-80% for News articles, while diversity win rates ranged from 75-82%\nand 62-71% respectively. Our use of directness as a validity test also achieved the expected results,\ni.e., that na \u00a8\u0131ve RAG produces the most direct responses across all comparisons.\n9Podcast Transcripts News Articles\nC0 C1 C2 C3 TS C0 C1 C2 C3 TS\nUnits 34 367 969 1310 1669 55 555 1797 2142 3197\nTokens 26657 225756 565720 746100 1014611 39770 352641 980898 1140266 1707694\n% Max 2.6 22.2 55.8 73.5 100 2.3 20.7 57.4 66.8 100\nTable 3: Number of context units (community summaries for C0-C3 and text chunks for TS), corre-\nsponding token counts, and percentage of the maximum token count. Map-reduce summarization of\nsource texts is the most resource-intensive approach requiring the highest number of context tokens.\nRoot-level community summaries ( C0) require dramatically fewer tokens per query (9x-43x).\nCommunity summaries vs. source texts. When comparing community summaries to source texts\nusing Graph RAG, community summaries generally provided a small but consistent improvement\nin answer comprehensiveness and diversity, except for root-level summaries. Intermediate-level\nsummaries in the Podcast dataset and low-level community summaries in the News dataset achieved\ncomprehensiveness win rates of 57% and 64%, respectively. Diversity win rates were 57% for\nPodcast intermediate-level summaries and 60% for News low-level community summaries. Table 3\nalso illustrates the scalability advantages of Graph RAG compared to source text summarization: for\nlow-level community summaries ( C3), Graph RAG required 26-33% fewer context tokens, while\nfor root-level community summaries ( C0), it required over 97% fewer tokens.","chunk_id":"ede7063998065122cf7a7152979c1909","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"GRAPH RAG","type":"TECHNIQUE","description":"Graph RAG is a method for processing and summarizing text data using a graph-based approach to improve comprehensiveness and diversity in responses.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"PODCAST DATASET","type":"DATASET","description":"The Podcast dataset consists of transcripts used for evaluating the performance of the Graph RAG approach in terms of summarization and response quality.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"NEWS DATASET","type":"DATASET","description":"The News dataset consists of articles used for evaluating the performance of the Graph RAG approach in terms of summarization and response quality.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"COMPREHENSIVENESS","type":"METRIC","description":"Comprehensiveness is a metric used to evaluate the quality of responses, indicating how well the responses cover the necessary information.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"DIVERSITY","type":"METRIC","description":"Diversity is a metric used to evaluate the quality of responses, indicating the variety of responses generated from the same input.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"EMPOWERMENT","type":"METRIC","description":"Empowerment is a metric used to evaluate the quality of responses, indicating how well the responses enable user understanding or action.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"DIRECTNESS","type":"METRIC","description":"Directness is a metric used to evaluate the quality of responses, indicating how straightforward and clear the responses are.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"CONTEXT WINDOW SIZE","type":"PARAMETER","description":"Context window size refers to the number of tokens that can be processed at once by the model, affecting the performance of the Graph RAG approach.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"COMMUNITY SUMMARIES","type":"DATA FORMAT","description":"Community summaries are condensed representations of information derived from the original texts, aimed at improving response quality in the Graph RAG approach.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"C1","type":"CONDITION","description":"C1 is a condition used in the Graph RAG approach to evaluate performance metrics across datasets and comparisons.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"C2","type":"CONDITION","description":"C2 is a condition used in the Graph RAG approach to evaluate performance metrics across datasets and comparisons.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"C3","type":"CONDITION","description":"C3 is a condition used in the Graph RAG approach to evaluate performance metrics across datasets and comparisons.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"TS","type":"CONDITION","description":"TS refers to the global text summarization condition used for comparison against the Graph RAG conditions.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"WIN RATE","type":"METRIC","description":"Win rate is a performance metric indicating the percentage of successful outcomes in comparisons between different conditions.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"TOKEN COUNT","type":"METRIC","description":"Token count refers to the number of tokens processed or generated in the context of the Graph RAG approach.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"COMMUNITY SUMMARY LEVEL","type":"DATA FORMAT","description":"Community summary levels refer to the hierarchical structure of summaries generated from the original texts in the Graph RAG approach.","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"C0","type":"","description":"","source_id":"ede7063998065122cf7a7152979c1909"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GRAPH RAG\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Graph RAG is a method for processing and summarizing text data using a graph-based approach to improve comprehensiveness and diversity in responses.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"PODCAST DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">The Podcast dataset consists of transcripts used for evaluating the performance of the Graph RAG approach in terms of summarization and response quality.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"NEWS DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">The News dataset consists of articles used for evaluating the performance of the Graph RAG approach in terms of summarization and response quality.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"COMPREHENSIVENESS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Comprehensiveness is a metric used to evaluate the quality of responses, indicating how well the responses cover the necessary information.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"DIVERSITY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Diversity is a metric used to evaluate the quality of responses, indicating the variety of responses generated from the same input.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"EMPOWERMENT\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Empowerment is a metric used to evaluate the quality of responses, indicating how well the responses enable user understanding or action.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"DIRECTNESS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Directness is a metric used to evaluate the quality of responses, indicating how straightforward and clear the responses are.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"CONTEXT WINDOW SIZE\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Context window size refers to the number of tokens that can be processed at once by the model, affecting the performance of the Graph RAG approach.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"COMMUNITY SUMMARIES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Community summaries are condensed representations of information derived from the original texts, aimed at improving response quality in the Graph RAG approach.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"C1\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">C1 is a condition used in the Graph RAG approach to evaluate performance metrics across datasets and comparisons.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"C2\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">C2 is a condition used in the Graph RAG approach to evaluate performance metrics across datasets and comparisons.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"C3\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">C3 is a condition used in the Graph RAG approach to evaluate performance metrics across datasets and comparisons.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"TS\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">TS refers to the global text summarization condition used for comparison against the Graph RAG conditions.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"WIN RATE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Win rate is a performance metric indicating the percentage of successful outcomes in comparisons between different conditions.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"TOKEN COUNT\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Token count refers to the number of tokens processed or generated in the context of the Graph RAG approach.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"COMMUNITY SUMMARY LEVEL\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Community summary levels refer to the hierarchical structure of summaries generated from the original texts in the Graph RAG approach.<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"C0\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <edge source=\"GRAPH RAG\" target=\"PODCAST DATASET\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach is applied to the Podcast dataset to evaluate its performance in summarization and response quality.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"NEWS DATASET\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach is applied to the News dataset to evaluate its performance in summarization and response quality.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"COMPREHENSIVENESS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Graph RAG approach aims to improve comprehensiveness in responses as one of its key performance metrics.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"DIVERSITY\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Graph RAG approach aims to improve diversity in responses as one of its key performance metrics.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"EMPOWERMENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach aims to enhance empowerment in responses as one of its key performance metrics.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"DIRECTNESS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach aims to improve directness in responses as one of its key performance metrics.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"CONTEXT WINDOW SIZE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The context window size is a critical parameter that affects the performance of the Graph RAG approach in processing text data.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"COMMUNITY SUMMARIES\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Community summaries are generated as part of the Graph RAG approach to enhance the quality of responses.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"C0\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">C0 is one of the conditions evaluated in the Graph RAG approach to assess performance metrics.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"C1\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">C1 is one of the conditions evaluated in the Graph RAG approach to assess performance metrics.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"C2\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">C2 is one of the conditions evaluated in the Graph RAG approach to assess performance metrics.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"C3\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">C3 is one of the conditions evaluated in the Graph RAG approach to assess performance metrics.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"TS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">TS is used as a baseline for comparison against the Graph RAG conditions in performance evaluations.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"WIN RATE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Win rate is a key performance metric used to evaluate the effectiveness of the Graph RAG approach across different conditions.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"TOKEN COUNT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Token count is a metric that influences the performance and efficiency of the Graph RAG approach in processing data.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"COMMUNITY SUMMARY LEVEL\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Community summary levels are utilized in the Graph RAG approach to organize and evaluate the generated summaries.<\/data>      <data key=\"d5\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"edab4014b8f55e5b25bd7f396314be1f","chunk":" win rates of 57% and 64%, respectively. Diversity win rates were 57% for\nPodcast intermediate-level summaries and 60% for News low-level community summaries. Table 3\nalso illustrates the scalability advantages of Graph RAG compared to source text summarization: for\nlow-level community summaries ( C3), Graph RAG required 26-33% fewer context tokens, while\nfor root-level community summaries ( C0), it required over 97% fewer tokens. For a modest drop in\nperformance compared with other global methods, root-level Graph RAG offers a highly efficient\nmethod for the iterative question answering that characterizes sensemaking activity, while retaining\nadvantages in comprehensiveness (72% win rate) and diversity (62% win rate) over na \u00a8\u0131ve RAG.\nEmpowerment . Empowerment comparisons showed mixed results for both global approaches versus\nna\u00a8\u0131ve RAG ( SS) and Graph RAG approaches versus source text summarization ( TS). Ad-hoc LLM\nuse to analyze LLM reasoning for this measure indicated that the ability to provide specific exam-\nples, quotes, and citations was judged to be key to helping users reach an informed understanding.\nTuning element extraction prompts may help to retain more of these details in the Graph RAG index.\n4 Related Work\n4.1 RAG Approaches and Systems\nWhen using LLMs, RAG involves first retrieving relevant information from external data sources,\nthen adding this information to the context window of the LLM along with the original query (Ram\net al., 2023). Na \u00a8\u0131ve RAG approaches (Gao et al., 2023) do this by converting documents to text,\nsplitting text into chunks, and embedding these chunks into a vector space in which similar positions\nrepresent similar semantics. Queries are then embedded into the same vector space, with the text\nchunks of the nearest kvectors used as context. More advanced variations exist, but all solve the\nproblem of what to do when an external dataset of interest exceeds the LLM\u2019s context window.\nAdvanced RAG systems include pre-retrieval, retrieval, post-retrieval strategies designed to over-\ncome the drawbacks of Na \u00a8\u0131ve RAG, while Modular RAG systems include patterns for iterative and\ndynamic cycles of interleaved retrieval and generation (Gao et al., 2023). Our implementation of\nGraph RAG incorporates multiple concepts related to other systems. For example, our community\nsummaries are a kind of self-memory (Selfmem, Cheng et al., 2024) for generation-augmented re-\ntrieval (GAR, Mao et al., 2020) that facilitates future generation cycles, while our parallel generation\nof community answers from these summaries is a kind of iterative (Iter-RetGen, Shao et al., 2023)\nor federated (FeB4RAG, Wang et al., 2024) retrieval-generation strategy. Other systems have also\ncombined these concepts for multi-document summarization (CAiRE-COVID, Su et al., 2020) and\nmulti-hop question answering (ITRG, Feng et al., 2023; IR-CoT, Trivedi et al., 2022; DSP, Khattab\net al., 2022). Our use of a hierarchical index and summarization also bears resemblance to further\napproaches, such as generating a hierarchical index of text chunks by clustering the vectors of text\nembeddings (RAPTOR, Sarthi et al., 2024) or generating a \u201ctree of clarifications\u201d to answer mul-\ntiple interpretations of ambiguous questions (Kim et al., 2023). However, none of these iterative or\nhierarchical approaches use the kind of self-generated graph index that enables Graph RAG.\n104.2 Graphs and LLMs\nUse of graphs in connection with LLMs and RAG is a developing research area, with multiple\ndirections already established. These include using LLMs for knowledge graph creation (Tra-\njanoska et al., 2023) and completion (Yao et al., 2023), as well as for the extraction of causal\ngraphs (Ban et al., 2023; Zhang et al., 2024) from source texts. They also include forms of ad-\nvanced RAG (Gao et al., 2023) where the index is a knowledge graph (KAPING, Baek et al., 2023),\nwhere subsets of the graph structure (G-Retriever, He et al., 2024) or derived graph metrics (Graph-\nToolFormer, Zhang, 2023) are the objects of enquiry, where narrative outputs are strongly grounded\nin the facts of retrieved subgraphs (SURGE, Kang et al., 2023), where retrieved event-plot sub-\ngraphs are serialized using narrative templates (FABULA, Ranade and Joshi, 2023), and where the\nsystem supports both creation and traversal of text-relationship graphs for multi-hop question an-\nswering (Wang et al., 2023b). In terms of open-source software, a variety a graph databases are\nsupported by both the LangChain (LangChain, 2024) and LlamaIndex (LlamaIndex, 2024) libraries,\nwhile a more general class of graph-based RAG applications is also emerging, including systems that\ncan create and reason over knowledge graphs in both Neo4J (NaLLM, Neo4J, 2024) and Nebula-\nGraph (GraphRAG, NebulaGraph, 2024) formats. Unlike our Graph RAG approach, however, none\nof these systems use the natural modularity of graphs to partition data for global summarization","chunk_id":"edab4014b8f55e5b25bd7f396314be1f","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"GRAPH RAG","type":"TECHNIQUE","description":"Graph RAG is a method for processing and summarizing text data, emphasizing efficiency and scalability in comparison to traditional source text summarization methods.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"SOURCE TEXT SUMMARIZATION","type":"TECHNIQUE","description":"Source text summarization refers to the process of condensing original texts into shorter summaries while retaining essential information.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"PODCAST INTERMEDIATE-LEVEL SUMMARIES","type":"DATA FORMAT","description":"Podcast intermediate-level summaries are summaries derived from podcast content, aimed at providing a balanced overview of the material.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"NEWS LOW-LEVEL COMMUNITY SUMMARIES","type":"DATA FORMAT","description":"News low-level community summaries are concise summaries of news articles, focusing on community-related content.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"LLM","type":"TECHNOLOGY","description":"LLM (Large Language Model) refers to advanced AI models designed to understand and generate human-like text based on input data.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"EMPOWERMENT COMPARISONS","type":"ANALYSIS","description":"Empowerment comparisons analyze the effectiveness of different summarization approaches in helping users achieve informed understanding.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"AD-HOC LLM USE","type":"TECHNIQUE","description":"Ad-hoc LLM use involves employing large language models for specific tasks, such as analyzing reasoning and providing examples or citations.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"SELF-MEMORY","type":"CONCEPT","description":"Self-memory refers to a mechanism in which generated summaries serve as a memory aid for future retrieval and generation tasks.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"GENERATING HIERARCHICAL INDEX","type":"TECHNIQUE","description":"Generating a hierarchical index involves organizing text chunks into a structured format to facilitate efficient retrieval and summarization.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"KNOWLEDGE GRAPH","type":"DATA STRUCTURE","description":"A knowledge graph is a structured representation of information that captures relationships between entities, often used for enhancing AI understanding.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"GRAPH DATABASES","type":"DATA STRUCTURE","description":"Graph databases are specialized databases designed to store and manage graph structures, enabling efficient querying and analysis of relationships.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"LANGCHAIN","type":"SOFTWARE","description":"LangChain is a software library that supports the development of applications utilizing large language models and graph databases.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"LLAMAINDEX","type":"SOFTWARE","description":"LlamaIndex is a software library that facilitates the integration of large language models with graph-based data structures.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"NA\u00cfVE RAG","type":"TECHNIQUE","description":"Na\u00efve RAG refers to basic retrieval-augmented generation methods that convert documents to text and split them into chunks for embedding in a vector space.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"PRE-RETRIEVAL","type":"TECHNIQUE","description":"Pre-retrieval is a strategy in advanced RAG systems that involves preparing data before the retrieval process to improve efficiency and accuracy.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"RETRIEVAL","type":"TECHNIQUE","description":"Retrieval is the process of fetching relevant information from a dataset or external source to assist in generating responses.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"POST-RETRIEVAL","type":"TECHNIQUE","description":"Post-retrieval refers to strategies applied after the retrieval process to refine or enhance the generated output.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"MODULAR RAG","type":"TECHNIQUE","description":"Modular RAG systems incorporate patterns for iterative and dynamic cycles of retrieval and generation, improving the overall process of information synthesis.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"MULTI-DOCUMENT SUMMARIZATION","type":"TECHNIQUE","description":"Multi-document summarization involves condensing information from multiple sources into a coherent summary, often using advanced techniques like RAG.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"MULTI-HOP QUESTION ANSWERING","type":"TECHNIQUE","description":"Multi-hop question answering refers to the ability of a system to answer questions that require information from multiple sources or steps.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"HIERARCHICAL INDEX","type":"DATA STRUCTURE","description":"A hierarchical index organizes data in a tree-like structure, facilitating efficient retrieval and summarization of information.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"TEXT EMBEDDINGS","type":"DATA STRUCTURE","description":"Text embeddings are numerical representations of text data that capture semantic meaning, often used in machine learning and natural language processing.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"VECTOR SPACE","type":"DATA STRUCTURE","description":"Vector space is a mathematical representation where text data is transformed into vectors for similarity comparisons and retrieval tasks.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"KAPING","type":"PERSON","description":"KAPING refers to a study or work that discusses advanced RAG systems and their applications in knowledge graph contexts.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"TRAJANOSKA ET AL. (2023)","type":"PERSON","description":"Trajanoska et al. (2023) is a reference to a study that explores the use of LLMs for knowledge graph creation and completion.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"YAO ET AL. (2023)","type":"PERSON","description":"Yao et al. (2023) is a reference to a study that discusses the extraction of causal graphs from source texts using LLMs.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"BAN ET AL. (2023)","type":"PERSON","description":"Ban et al. (2023) is a reference to a study that focuses on the extraction of causal graphs from textual data.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"ZHANG ET AL. (2024)","type":"PERSON","description":"Zhang et al. (2024) is a reference to a study that discusses advancements in the extraction of causal graphs from source texts.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"G-RETRIEVER","type":"TECHNIQUE","description":"G-Retriever is a system that focuses on retrieving subsets of graph structures for enhanced information retrieval tasks.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"GRAPH-TOOLFORMER","type":"TECHNIQUE","description":"Graph-ToolFormer is a method that utilizes derived graph metrics for various analytical tasks in the context of RAG.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"SURGE","type":"TECHNIQUE","description":"SURGE is a system that emphasizes narrative outputs grounded in the facts of retrieved subgraphs for enhanced storytelling.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"FABULA","type":"TECHNIQUE","description":"FABULA is a method that serializes event-plot sub-graphs using narrative templates to create coherent stories.","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"WANG ET AL. (2023)","type":"PERSON","description":"Wang et al. (2023) is a reference to a study that discusses systems supporting the creation and traversal of text-relationship graphs for multi-hop question answering.)<|COMPLETE|>","source_id":"edab4014b8f55e5b25bd7f396314be1f"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GRAPH RAG\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Graph RAG is a method for processing and summarizing text data, emphasizing efficiency and scalability in comparison to traditional source text summarization methods.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"SOURCE TEXT SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Source text summarization refers to the process of condensing original texts into shorter summaries while retaining essential information.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"PODCAST INTERMEDIATE-LEVEL SUMMARIES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Podcast intermediate-level summaries are summaries derived from podcast content, aimed at providing a balanced overview of the material.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"NEWS LOW-LEVEL COMMUNITY SUMMARIES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">News low-level community summaries are concise summaries of news articles, focusing on community-related content.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM (Large Language Model) refers to advanced AI models designed to understand and generate human-like text based on input data.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"EMPOWERMENT COMPARISONS\">      <data key=\"d0\">ANALYSIS<\/data>      <data key=\"d1\">Empowerment comparisons analyze the effectiveness of different summarization approaches in helping users achieve informed understanding.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"AD-HOC LLM USE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Ad-hoc LLM use involves employing large language models for specific tasks, such as analyzing reasoning and providing examples or citations.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"SELF-MEMORY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Self-memory refers to a mechanism in which generated summaries serve as a memory aid for future retrieval and generation tasks.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"GENERATING HIERARCHICAL INDEX\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Generating a hierarchical index involves organizing text chunks into a structured format to facilitate efficient retrieval and summarization.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"KNOWLEDGE GRAPH\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">A knowledge graph is a structured representation of information that captures relationships between entities, often used for enhancing AI understanding.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"GRAPH DATABASES\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Graph databases are specialized databases designed to store and manage graph structures, enabling efficient querying and analysis of relationships.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"LANGCHAIN\">      <data key=\"d0\">SOFTWARE<\/data>      <data key=\"d1\">LangChain is a software library that supports the development of applications utilizing large language models and graph databases.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"LLAMAINDEX\">      <data key=\"d0\">SOFTWARE<\/data>      <data key=\"d1\">LlamaIndex is a software library that facilitates the integration of large language models with graph-based data structures.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"NA&#207;VE RAG\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Na&#239;ve RAG refers to basic retrieval-augmented generation methods that convert documents to text and split them into chunks for embedding in a vector space.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"PRE-RETRIEVAL\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Pre-retrieval is a strategy in advanced RAG systems that involves preparing data before the retrieval process to improve efficiency and accuracy.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"RETRIEVAL\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Retrieval is the process of fetching relevant information from a dataset or external source to assist in generating responses.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"POST-RETRIEVAL\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Post-retrieval refers to strategies applied after the retrieval process to refine or enhance the generated output.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"MODULAR RAG\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Modular RAG systems incorporate patterns for iterative and dynamic cycles of retrieval and generation, improving the overall process of information synthesis.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"MULTI-DOCUMENT SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Multi-document summarization involves condensing information from multiple sources into a coherent summary, often using advanced techniques like RAG.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"MULTI-HOP QUESTION ANSWERING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Multi-hop question answering refers to the ability of a system to answer questions that require information from multiple sources or steps.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"HIERARCHICAL INDEX\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">A hierarchical index organizes data in a tree-like structure, facilitating efficient retrieval and summarization of information.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"TEXT EMBEDDINGS\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Text embeddings are numerical representations of text data that capture semantic meaning, often used in machine learning and natural language processing.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"VECTOR SPACE\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Vector space is a mathematical representation where text data is transformed into vectors for similarity comparisons and retrieval tasks.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"KAPING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">KAPING refers to a study or work that discusses advanced RAG systems and their applications in knowledge graph contexts.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"TRAJANOSKA ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Trajanoska et al. (2023) is a reference to a study that explores the use of LLMs for knowledge graph creation and completion.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"YAO ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. (2023) is a reference to a study that discusses the extraction of causal graphs from source texts using LLMs.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"BAN ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ban et al. (2023) is a reference to a study that focuses on the extraction of causal graphs from textual data.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"ZHANG ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang et al. (2024) is a reference to a study that discusses advancements in the extraction of causal graphs from source texts.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"G-RETRIEVER\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">G-Retriever is a system that focuses on retrieving subsets of graph structures for enhanced information retrieval tasks.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"GRAPH-TOOLFORMER\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Graph-ToolFormer is a method that utilizes derived graph metrics for various analytical tasks in the context of RAG.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"SURGE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">SURGE is a system that emphasizes narrative outputs grounded in the facts of retrieved subgraphs for enhanced storytelling.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"FABULA\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">FABULA is a method that serializes event-plot sub-graphs using narrative templates to create coherent stories.<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"WANG ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang et al. (2023) is a reference to a study that discusses systems supporting the creation and traversal of text-relationship graphs for multi-hop question answering.)&lt;|COMPLETE|&gt;<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <edge source=\"GRAPH RAG\" target=\"SOURCE TEXT SUMMARIZATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Graph RAG is a more efficient alternative to source text summarization, requiring fewer tokens while maintaining performance.<\/data>      <data key=\"d5\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"PODCAST INTERMEDIATE-LEVEL SUMMARIES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Podcast intermediate-level summaries are generated using the Graph RAG approach, which enhances their efficiency.<\/data>      <data key=\"d5\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"NEWS LOW-LEVEL COMMUNITY SUMMARIES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">News low-level community summaries are produced through the Graph RAG method, showcasing its application in diverse contexts.<\/data>      <data key=\"d5\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"LLM\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Graph RAG approach utilizes LLMs to process and generate summaries from text data effectively.<\/data>      <data key=\"d5\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"EMPOWERMENT COMPARISONS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Empowerment comparisons evaluate the effectiveness of the Graph RAG method in aiding user understanding compared to other approaches.<\/data>      <data key=\"d5\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"SELF-MEMORY\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Self-memory is a concept integrated into the Graph RAG approach to enhance the retrieval and generation process.<\/data>      <data key=\"d5\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"GENERATING HIERARCHICAL INDEX\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Generating a hierarchical index is a technique employed within the Graph RAG framework to improve summarization efficiency.<\/data>      <data key=\"d5\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"KNOWLEDGE GRAPH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Knowledge graphs are utilized in the Graph RAG approach to enhance the organization and retrieval of information.<\/data>      <data key=\"d5\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"EMPOWERMENT COMPARISONS\" target=\"AD-HOC LLM USE\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Ad-hoc LLM use is analyzed in the context of empowerment comparisons to assess its impact on user comprehension.<\/data>      <data key=\"d5\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH DATABASES\" target=\"LANGCHAIN\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LangChain supports the use of graph databases, facilitating the integration of graph structures in applications utilizing LLMs.<\/data>      <data key=\"d5\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH DATABASES\" target=\"LLAMAINDEX\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">LlamaIndex provides support for graph databases, enabling efficient management of graph-based data in LLM applications.<\/data>      <data key=\"d5\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"ac21ebe9a9d70d691c717f961d3f10c8","chunk":"Index, 2024) libraries,\nwhile a more general class of graph-based RAG applications is also emerging, including systems that\ncan create and reason over knowledge graphs in both Neo4J (NaLLM, Neo4J, 2024) and Nebula-\nGraph (GraphRAG, NebulaGraph, 2024) formats. Unlike our Graph RAG approach, however, none\nof these systems use the natural modularity of graphs to partition data for global summarization.\n5 Discussion\nLimitations of evaluation approach . Our evaluation to date has only examined a certain class of\nsensemaking questions for two corpora in the region of 1 million tokens. More work is needed\nto understand how performance varies across different ranges of question types, data types, and\ndataset sizes, as well as to validate our sensemaking questions and target metrics with end users.\nComparison of fabrication rates, e.g., using approaches like SelfCheckGPT (Manakul et al., 2023),\nwould also improve on the current analysis.\nTrade-offs of building a graph index . We consistently observed Graph RAG achieve the best head-\nto-head results against other methods, but in many cases the graph-free approach to global summa-\nrization of source texts performed competitively. The real-world decision about whether to invest in\nbuilding a graph index depends on multiple factors, including the compute budget, expected number\nof lifetime queries per dataset, and value obtained from other aspects of the graph index (including\nthe generic community summaries and the use of other graph-related RAG approaches).\nFuture work . The graph index, rich text annotations, and hierarchical community structure support-\ning the current Graph RAG approach offer many possibilities for refinement and adaptation. This\nincludes RAG approaches that operate in a more local manner, via embedding-based matching of\nuser queries and graph annotations, as well as the possibility of hybrid RAG schemes that combine\nembedding-based matching against community reports before employing our map-reduce summa-\nrization mechanisms. This \u201croll-up\u201d operation could also be extended across more levels of the\ncommunity hierarchy, as well as implemented as a more exploratory \u201cdrill down\u201d mechanism that\nfollows the information scent contained in higher-level community summaries.\n6 Conclusion\nWe have presented a global approach to Graph RAG, combining knowledge graph generation,\nretrieval-augmented generation (RAG), and query-focused summarization (QFS) to support human\nsensemaking over entire text corpora. Initial evaluations show substantial improvements over a\nna\u00a8\u0131ve RAG baseline for both the comprehensiveness and diversity of answers, as well as favorable\ncomparisons to a global but graph-free approach using map-reduce source text summarization. For\nsituations requiring many global queries over the same dataset, summaries of root-level communi-\nties in the entity-based graph index provide a data index that is both superior to na \u00a8\u0131ve RAG and\nachieves competitive performance to other global methods at a fraction of the token cost.\nAn open-source, Python-based implementation of both global and local Graph RAG approaches is\nforthcoming at https:\/\/aka .ms\/graphrag .\n11Acknowledgements\nWe would also like to thank the following people who contributed to the work: Alonso Guevara\nFern \u00b4andez, Amber Hoak, Andr \u00b4es Morales Esquivel, Ben Cutler, Billie Rinaldi, Chris Sanchez,\nChris Trevino, Christine Caggiano, David Tittsworth, Dayenne de Souza, Douglas Orbaker, Ed\nClark, Gabriel Nieves-Ponce, Gaudy Blanco Meneses, Kate Lytvynets, Katy Smith, M \u00b4onica Carva-\njal, Nathan Evans, Richard Ortega, Rodrigo Racanicci, Sarah Smith, and Shane Solomon.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Al-\ntenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv preprint\narXiv:2303.08774 .\nAnil, R., Borgeaud, S., Wu, Y ., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M.,\nHauth, A., et al. (2023). Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805 .\nBaek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for\nzero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136 .\nBan, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing\nlarge language models for advanced causal discovery from data.\nBaumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorpo-\nrating query relevance, multi-document coverage, and summary length constraints into seq2seq\nmodels. arXiv preprint arXiv:1801.07704 .\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\n2008(10):P10008.\nBrown, T., Mann, B., Ryder, N., Subbia","chunk_id":"ac21ebe9a9d70d691c717f961d3f10c8","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"GRAPH RAG APPROACH","type":"TECHNIQUE","description":"The Graph RAG approach is a method for processing text data that combines knowledge graph generation, retrieval-augmented generation (RAG), and query-focused summarization (QFS) to support human sensemaking over entire text corpora.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"KNOWLEDGE GRAPHS","type":"DATA STRUCTURE","description":"Knowledge graphs are structured representations of information that allow for reasoning and querying over interconnected data points.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"NATURAL MODULARITY","type":"CONCEPT","description":"Natural modularity refers to the inherent structure within graphs that allows for effective partitioning of data for summarization.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"SOURCE TEXTS","type":"DATA FORMAT","description":"Source texts are the original documents or data from which information is extracted for processing in the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"SENSEMAKING QUESTIONS","type":"CONCEPT","description":"Sensemaking questions are inquiries designed to extract meaningful insights from data, guiding the analysis process.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"GRAPH-FREE APPROACH","type":"TECHNIQUE","description":"The graph-free approach is a method of summarizing source texts without utilizing graph structures, often employing map-reduce techniques.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"OPEN-SOURCE IMPLEMENTATION","type":"SOFTWARE","description":"An open-source implementation refers to publicly available software that allows users to access and modify the Graph RAG approaches.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"COMMUNITY SUMMARIES","type":"DATA FORMAT","description":"Community summaries are aggregated insights derived from groups within a knowledge graph, providing a high-level overview of information.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"ALONSO GUEVARA FERN\u00c1NDEZ","type":"PERSON","description":"Alonso Guevara Fern\u00e1ndez is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"AMBER HOAK","type":"PERSON","description":"Amber Hoak is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"ANDR\u00c9S MORALES ESQUIVEL","type":"PERSON","description":"Andr\u00e9s Morales Esquivel is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"BEN CUTLER","type":"PERSON","description":"Ben Cutler is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"BILLIE RINALDI","type":"PERSON","description":"Billie Rinaldi is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"CHRIS SANCHEZ","type":"PERSON","description":"Chris Sanchez is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"CHRIS TREVI\u00d1O","type":"PERSON","description":"Chris Trevi\u00f1o is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"CHRISTINE CAGGIANO","type":"PERSON","description":"Christine Caggiano is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"DAVID TITTSWORTH","type":"PERSON","description":"David Tittsworth is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"DAYENNE DE SOUZA","type":"PERSON","description":"Dayenne de Souza is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"DOUGLAS ORBAKER","type":"PERSON","description":"Douglas Orbaker is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"ED CLARK","type":"PERSON","description":"Ed Clark is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"GABRIEL NIEVES-PONCE","type":"PERSON","description":"Gabriel Nieves-Ponce is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"GAUDY BLANCO MENES","type":"PERSON","description":"Gaudy Blanco Meneses is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"KATE LYTVYNETS","type":"PERSON","description":"Kate Lytvynets is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"KATY SMITH","type":"PERSON","description":"Katy Smith is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"M\u00d3NICA CARVAJAL","type":"PERSON","description":"M\u00f3nica Carvajal is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"NATHAN EVANS","type":"PERSON","description":"Nathan Evans is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"RICHARD ORTEGA","type":"PERSON","description":"Richard Ortega is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"RODRIGO RACANICCI","type":"PERSON","description":"Rodrigo Racanicci is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"SARAH SMITH","type":"PERSON","description":"Sarah Smith is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"SHANE SOLOMON","type":"PERSON","description":"Shane Solomon is one of the contributors to the work on the Graph RAG approach.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"FABRICATION RATES","type":"CONCEPT","description":"Fabrication rates refer to the frequency of inaccuracies or false information generated by language models, which is crucial for assessing their performance.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"QUERY-FOCUSED SUMMARIZATION","type":"TECHNIQUE","description":"Query-focused summarization is a method that tailors summaries based on specific queries, enhancing the relevance of the information presented.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"HYBRID RAG SCHEMES","type":"TECHNIQUE","description":"Hybrid RAG schemes combine different retrieval-augmented generation techniques to improve the efficiency and effectiveness of information retrieval.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"EMBEDDING-BASED MATCHING","type":"TECHNIQUE","description":"Embedding-based matching is a technique that uses vector representations of data to find relevant information based on user queries.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"MAP-REDUCE SUMMARIZATION","type":"TECHNIQUE","description":"Map-reduce summarization is a computational approach that processes large datasets by dividing them into smaller chunks for efficient summarization.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"HIERARCHICAL COMMUNITY STRUCTURE","type":"DATA STRUCTURE","description":"Hierarchical community structure refers to the organization of data into nested groups, facilitating better analysis and understanding of relationships within the data.","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"TOKEN COST","type":"CONCEPT","description":"Token cost refers to the computational expense associated with processing text data in language models, impacting the efficiency of various approaches.)<|COMPLETE|>","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GRAPH RAG APPROACH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Graph RAG approach is a method for processing text data that combines knowledge graph generation, retrieval-augmented generation (RAG), and query-focused summarization (QFS) to support human sensemaking over entire text corpora.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"KNOWLEDGE GRAPHS\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Knowledge graphs are structured representations of information that allow for reasoning and querying over interconnected data points.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"NATURAL MODULARITY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Natural modularity refers to the inherent structure within graphs that allows for effective partitioning of data for summarization.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"SOURCE TEXTS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Source texts are the original documents or data from which information is extracted for processing in the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"SENSEMAKING QUESTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Sensemaking questions are inquiries designed to extract meaningful insights from data, guiding the analysis process.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"GRAPH-FREE APPROACH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The graph-free approach is a method of summarizing source texts without utilizing graph structures, often employing map-reduce techniques.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"OPEN-SOURCE IMPLEMENTATION\">      <data key=\"d0\">SOFTWARE<\/data>      <data key=\"d1\">An open-source implementation refers to publicly available software that allows users to access and modify the Graph RAG approaches.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"COMMUNITY SUMMARIES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Community summaries are aggregated insights derived from groups within a knowledge graph, providing a high-level overview of information.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"ALONSO GUEVARA FERN&#193;NDEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alonso Guevara Fern&#225;ndez is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"AMBER HOAK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amber Hoak is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"ANDR&#201;S MORALES ESQUIVEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andr&#233;s Morales Esquivel is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"BEN CUTLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ben Cutler is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"BILLIE RINALDI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Billie Rinaldi is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"CHRIS SANCHEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chris Sanchez is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"CHRIS TREVI&#209;O\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chris Trevi&#241;o is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"CHRISTINE CAGGIANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Christine Caggiano is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"DAVID TITTSWORTH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David Tittsworth is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"DAYENNE DE SOUZA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dayenne de Souza is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"DOUGLAS ORBAKER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Douglas Orbaker is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"ED CLARK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ed Clark is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"GABRIEL NIEVES-PONCE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gabriel Nieves-Ponce is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"GAUDY BLANCO MENES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gaudy Blanco Meneses is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"KATE LYTVYNETS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kate Lytvynets is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"KATY SMITH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Katy Smith is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"M&#211;NICA CARVAJAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M&#243;nica Carvajal is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"NATHAN EVANS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nathan Evans is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"RICHARD ORTEGA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Richard Ortega is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"RODRIGO RACANICCI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rodrigo Racanicci is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"SARAH SMITH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sarah Smith is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"SHANE SOLOMON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shane Solomon is one of the contributors to the work on the Graph RAG approach.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"FABRICATION RATES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Fabrication rates refer to the frequency of inaccuracies or false information generated by language models, which is crucial for assessing their performance.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"QUERY-FOCUSED SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Query-focused summarization is a method that tailors summaries based on specific queries, enhancing the relevance of the information presented.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"HYBRID RAG SCHEMES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Hybrid RAG schemes combine different retrieval-augmented generation techniques to improve the efficiency and effectiveness of information retrieval.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"EMBEDDING-BASED MATCHING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Embedding-based matching is a technique that uses vector representations of data to find relevant information based on user queries.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"MAP-REDUCE SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Map-reduce summarization is a computational approach that processes large datasets by dividing them into smaller chunks for efficient summarization.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"HIERARCHICAL COMMUNITY STRUCTURE\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Hierarchical community structure refers to the organization of data into nested groups, facilitating better analysis and understanding of relationships within the data.<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"TOKEN COST\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Token cost refers to the computational expense associated with processing text data in language models, impacting the efficiency of various approaches.)&lt;|COMPLETE|&gt;<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <edge source=\"GRAPH RAG APPROACH\" target=\"KNOWLEDGE GRAPHS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Graph RAG approach utilizes knowledge graphs to enhance data processing and reasoning capabilities.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"NATURAL MODULARITY\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Graph RAG approach leverages natural modularity to effectively partition data for summarization.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"SOURCE TEXTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Source texts are the foundational data that the Graph RAG approach processes for analysis.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"SENSEMAKING QUESTIONS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Sensemaking questions guide the analysis process within the Graph RAG approach, helping to extract insights from data.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"GRAPH-FREE APPROACH\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The Graph RAG approach is compared to the graph-free approach, which summarizes source texts without graph structures.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"OPEN-SOURCE IMPLEMENTATION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">An open-source implementation of the Graph RAG approach allows users to access and modify the techniques used.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"COMMUNITY SUMMARIES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Community summaries provide aggregated insights that support the Graph RAG approach's analysis capabilities.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"ALONSO GUEVARA FERN&#193;NDEZ\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"AMBER HOAK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Amber Hoak contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"ANDR&#201;S MORALES ESQUIVEL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Andr&#233;s Morales Esquivel contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"BEN CUTLER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Ben Cutler contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"BILLIE RINALDI\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Billie Rinaldi contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"CHRIS SANCHEZ\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chris Sanchez contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"CHRIS TREVI&#209;O\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chris Trevi&#241;o contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"CHRISTINE CAGGIANO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Christine Caggiano contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"DAVID TITTSWORTH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">David Tittsworth contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"DAYENNE DE SOUZA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Dayenne de Souza contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"DOUGLAS ORBAKER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Douglas Orbaker contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"ED CLARK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Ed Clark contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"GABRIEL NIEVES-PONCE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Gabriel Nieves-Ponce contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"GAUDY BLANCO MENES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Gaudy Blanco Meneses contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"KATE LYTVYNETS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Kate Lytvynets contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"KATY SMITH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Katy Smith contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"M&#211;NICA CARVAJAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">M&#243;nica Carvajal contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"NATHAN EVANS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Nathan Evans contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"RICHARD ORTEGA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Richard Ortega contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"RODRIGO RACANICCI\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Rodrigo Racanicci contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"SARAH SMITH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Sarah Smith contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"SHANE SOLOMON\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Shane Solomon contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"aa79049289e6532592eec17b9e76adfb","chunk":" summary length constraints into seq2seq\nmodels. arXiv preprint arXiv:1801.07704 .\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\n2008(10):P10008.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in\nneural information processing systems , 33:1877\u20131901.\nCheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., and Yan, R. (2024). Lift yourself up: Retrieval-\naugmented text generation with self-memory. Advances in Neural Information Processing Sys-\ntems, 36.\nDang, H. T. (2006). Duc 2005: Evaluation of question-focused summarization systems. In Proceed-\nings of the Workshop on Task-Focused Summarization and Question Answering , pages 48\u201355.\nEs, S., James, J., Espinosa-Anke, L., and Schockaert, S. (2023). Ragas: Automated evaluation of\nretrieval augmented generation. arXiv preprint arXiv:2309.15217 .\nFeng, Z., Feng, X., Zhao, D., Yang, M., and Qin, B. (2023). Retrieval-generation synergy augmented\nlarge language models. arXiv preprint arXiv:2310.05149 .\nFortunato, S. (2010). Community detection in graphs. Physics reports , 486(3-5):75\u2013174.\nGao, Y ., Xiong, Y ., Gao, X., Jia, K., Pan, J., Bi, Y ., Dai, Y ., Sun, J., and Wang, H. (2023). Retrieval-\naugmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 .\nGoodwin, T. R., Savery, M. E., and Demner-Fushman, D. (2020). Flight of the pegasus? comparing\ntransformers on few-shot and zero-shot multi-document abstractive summarization. In Proceed-\nings of COLING. International Conference on Computational Linguistics , volume 2020, page\n5640. NIH Public Access.\nHe, X., Tian, Y ., Sun, Y ., Chawla, N. V ., Laurent, T., LeCun, Y ., Bresson, X., and Hooi, B. (2024).\nG-retriever: Retrieval-augmented generation for textual graph understanding and question an-\nswering. arXiv preprint arXiv:2402.07630 .\n12Jacomy, M., Venturini, T., Heymann, S., and Bastian, M. (2014). Forceatlas2, a continuous graph\nlayout algorithm for handy network visualization designed for the gephi software. PLoS ONE\n9(6): e98679. https:\/\/doi.org\/10.1371\/journal.pone.0098679 .\nJin, D., Yu, Z., Jiao, P., Pan, S., He, D., Wu, J., Philip, S. Y ., and Zhang, W. (2021). A survey of\ncommunity detection approaches: From statistical modeling to deep learning. IEEE Transactions\non Knowledge and Data Engineering , 35(2):1149\u20131170.\nKang, M., Kwak, J. M., Baek, J., and Hwang, S. J. (2023). Knowledge graph-augmented language\nmodels for knowledge-grounded dialogue generation. arXiv preprint arXiv:2305.18846 .\nKhattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. (2022).\nDemonstrate-search-predict: Composing retrieval and language models for knowledge-intensive\nnlp. arXiv preprint arXiv:2212.14024 .\nKim, G., Kim, S., Jeon, B., Park, J., and Kang, J. (2023). Tree of clarifications: Answering ambigu-\nous questions with retrieval-augmented large language models. arXiv preprint arXiv:2310.14696 .\nKlein, G., Moon, B., and Hoffman, R. R. (2006a). Making sense of sensemaking 1: Alternative\nperspectives. IEEE intelligent systems , 21(4):70\u201373.\nKlein, G., Moon, B., and Hoffman, R. R. (2006b). Making sense of sensemaking 2: A macrocogni-\ntive model. IEEE Intelligent systems , 21(5):88\u201392.\nKoesten, L., Gregory, K., Groth, P., and Simperl, E. (2021). Talking datasets\u2013understanding data\nsensemaking behaviours. International journal of human-computer studies , 146:102562.\nKuratov, Y ., Bulatov, A., Anokhin, P., Sorokin, D., Sorokin, A., and Burtsev, M. (202","chunk_id":"aa79049289e6532592eec17b9e76adfb","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"BROWN ET AL. (2020)","type":"PERSON","description":"Brown et al. (2020) is a reference to a study discussing the capabilities of language models as few-shot learners.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"CHENG ET AL. (2024)","type":"PERSON","description":"Cheng et al. (2024) is a reference to a study on retrieval-augmented text generation with self-memory.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DANG H. T. (2006)","type":"PERSON","description":"Dang H. T. (2006) is a reference to a study evaluating question-focused summarization systems.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ES ET AL. (2023)","type":"PERSON","description":"Es et al. (2023) is a reference to a study on automated evaluation of retrieval-augmented generation.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FENG ET AL. (2023)","type":"PERSON","description":"Feng et al. (2023) is a reference to a study on retrieval-generation synergy augmented large language models.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FORTUNATO S. (2010)","type":"PERSON","description":"Fortunato S. (2010) is a reference to a study on community detection in graphs.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"GAO ET AL. (2023)","type":"PERSON","description":"Gao et al. (2023) is a reference to a survey on retrieval-augmented generation for large language models.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"GOODWIN ET AL. (2020)","type":"PERSON","description":"Goodwin et al. (2020) is a reference to a study comparing transformers on multi-document abstractive summarization.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"HE ET AL. (2024)","type":"PERSON","description":"He et al. (2024) is a reference to a study on retrieval-augmented generation for textual graph understanding.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JACOMY ET AL. (2014)","type":"PERSON","description":"Jacomy et al. (2014) is a reference to a study on a graph layout algorithm for network visualization.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JIN ET AL. (2021)","type":"PERSON","description":"Jin et al. (2021) is a reference to a survey of community detection approaches.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KANG ET AL. (2023)","type":"PERSON","description":"Kang et al. (2023) is a reference to a study on knowledge graph-augmented language models.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KHATTAB ET AL. (2022)","type":"PERSON","description":"Khattab et al. (2022) is a reference to a study on composing retrieval and language models for knowledge-intensive NLP.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KIM ET AL. (2023)","type":"PERSON","description":"Kim et al. (2023) is a reference to a study on answering ambiguous questions with retrieval-augmented large language models.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KLEIN ET AL. (2006)","type":"PERSON","description":"Klein et al. (2006) is a reference to a study on sensemaking perspectives.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KOESTEN ET AL. (2021)","type":"PERSON","description":"Koesten et al. (2021) is a reference to a study on understanding data sensemaking behaviors.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LANGUAGE MODELS","type":"","description":"\nLanguage models are statistical models that predict the next word in a sequence, playing a crucial role in natural language processing tasks.","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"TECHNOLOGY"},{"name":"RETRIEVAL-AUGMENTED TEXT GENERATION","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"QUESTION-FOCUSED SUMMARIZATION","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"AUTOMATED EVALUATION","type":"","description":"\nAutomated evaluation refers to the use of algorithms and metrics to assess the quality of generated text without human intervention.","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"CONCEPT"},{"name":"RETRIEVAL-GENERATION SYNERGY","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"COMMUNITY DETECTION","type":"","description":"\nCommunity detection is the process of identifying groups of nodes in a network that are more densely connected to each other than to the rest of the network.","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"CONCEPT"},{"name":"SURVEY ON RETRIEVAL-AUGMENTED GENERATION","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"TRANSFORMERS","type":"","description":"\nTransformers are a type of neural network architecture that has become the foundation for many state-of-the-art models in natural language processing.","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"TECHNOLOGY"},{"name":"TEXTUAL GRAPH UNDERSTANDING","type":"","description":"\nTextual graph understanding involves interpreting and analyzing information represented in graph structures derived from text.","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"TASK"},{"name":"GRAPH LAYOUT ALGORITHM","type":"","description":"\nGraph layout algorithms are methods used to visualize graph structures in a way that highlights relationships and patterns among nodes.","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"TECHNIQUE"},{"name":"COMMUNITY DETECTION APPROACHES","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KNOWLEDGE-GROUNDED DIALOGUE","type":"","description":"\nKnowledge-grounded dialogue refers to conversational systems that utilize external knowledge sources to provide contextually relevant responses.","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"TASK"},{"name":"RETRIEVAL AND LANGUAGE MODELS","type":"","description":"\nThe integration of retrieval and language models involves combining information retrieval techniques with language generation capabilities for improved performance.","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"CONCEPT"},{"name":"AMBIGUOUS QUESTIONS","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SENSEMAKING","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DATA SENSEMAKING","type":"","description":"\nData sensemaking refers to the process of interpreting and understanding data to derive insights and inform decision-making.","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"CONCEPT"},{"name":"SEQ2SEQ MODELS","type":"TECHNOLOGY","description":"Sequence-to-sequence (seq2seq) models are a type of neural network architecture used for tasks such as translation and summarization, where input sequences are transformed into output sequences.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FAST UNFOLDING OF COMMUNITIES","type":"TECHNIQUE","description":"Fast unfolding of communities is a method for detecting community structures in large networks, as discussed by Blondel et al. (2008).","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RETRIEVAL-AUGMENTED GENERATION","type":"TECHNIQUE","description":"Retrieval-augmented generation is a technique that combines retrieval of relevant information with generation of text, enhancing the quality of generated outputs.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"MULTI-DOCUMENT ABSTRACTIVE SUMMARIZATION","type":"TASK","description":"Multi-document abstractive summarization is the task of generating concise summaries from multiple documents, capturing the main ideas and information.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"EVALUATION OF QUESTION-FOCUSED SUMMARIZATION","type":"TASK","description":"Evaluation of question-focused summarization involves assessing the effectiveness of summarization systems that target specific questions.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SELF-MEMORY","type":"CONCEPT","description":"Self-memory refers to a mechanism in retrieval-augmented generation that allows models to retain and utilize past information for generating relevant responses.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"NEURAL INFORMATION PROCESSING SYSTEMS","type":"CONFERENCE","description":"Neural Information Processing Systems (NeurIPS) is a prominent conference focused on machine learning and computational neuroscience.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"COLING","type":"CONFERENCE","description":"COLING is the International Conference on Computational Linguistics, which focuses on research in natural language processing and computational linguistics.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING","type":"JOURNAL","description":"IEEE Transactions on Knowledge and Data Engineering is a scholarly journal that publishes research on knowledge and data engineering topics.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JOURNAL OF STATISTICAL MECHANICS","type":"JOURNAL","description":"Journal of Statistical Mechanics is a scientific journal that publishes research in statistical mechanics and related fields.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PLOS ONE","type":"JOURNAL","description":"PLoS ONE is a multidisciplinary open-access journal that publishes research across all areas of science and medicine.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS","type":"JOURNAL","description":"Advances in Neural Information Processing Systems is a conference proceedings publication that features research in machine learning and artificial intelligence.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"INTERNATIONAL JOURNAL OF HUMAN-COMPUTER STUDIES","type":"JOURNAL","description":"International Journal of Human-Computer Studies is a journal that publishes research on the interaction between humans and computers.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"NIH PUBLIC ACCESS","type":"CONCEPT","description":"NIH Public Access refers to the policy that ensures public access to research funded by the National Institutes of Health.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ARXIV","type":"REPOSITORY","description":"arXiv is a free distribution service and an open-access archive for scholarly articles in various fields, including physics, mathematics, and computer science.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"EVALUATION METRICS","type":"CONCEPT","description":"Evaluation metrics are quantitative measures used to assess the performance of models and systems in various tasks, including summarization and generation.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"TASK-FOCUSED SUMMARIZATION","type":"TASK","description":"Task-focused summarization is the process of generating summaries that are specifically tailored to answer particular questions or fulfill specific tasks.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ZERO-SHOT LEARNING","type":"CONCEPT","description":"Zero-shot learning is a machine learning paradigm where a model is able to recognize objects or perform tasks it has not been explicitly trained on.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FEW-SHOT LEARNING","type":"CONCEPT","description":"Few-shot learning is a machine learning approach that enables models to learn from a small number of training examples.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LARGE LANGUAGE MODELS","type":"TECHNOLOGY","description":"Large language models are advanced neural networks trained on vast amounts of text data to perform a variety of language tasks.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"GRAPH RETRIEVER","type":"TECHNOLOGY","description":"Graph retrievers are systems designed to extract relevant information from graph structures to support various applications, including question answering.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"TEXT GENERATION","type":"TASK","description":"Text generation is the process of creating coherent and contextually relevant text based on input data or prompts.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"NEURAL NETWORK ARCHITECTURE","type":"CONCEPT","description":"Neural network architecture refers to the design and structure of neural networks, which determine how they process information.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"CROSS-DOMAIN LEARNING","type":"CONCEPT","description":"Cross-domain learning involves applying knowledge or models from one domain to improve performance in another domain.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"MULTIMODAL LEARNING","type":"CONCEPT","description":"Multimodal learning refers to the integration of multiple types of data (e.g., text, images) to enhance learning and understanding.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DATA VISUALIZATION","type":"CONCEPT","description":"Data visualization is the graphical representation of information and data, making complex data more accessible and understandable.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"NLP","type":"FIELD","description":"Natural Language Processing (NLP) is a field of artificial intelligence focused on the interaction between computers and human language.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DEEP LEARNING","type":"FIELD","description":"Deep learning is a subset of machine learning that uses neural networks with many layers to analyze various forms of data.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"MACHINE LEARNING","type":"FIELD","description":"Machine learning is a field of artificial intelligence that focuses on the development of algorithms that allow computers to learn from and make predictions based on data.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"COMPUTATIONAL LINGUISTICS","type":"FIELD","description":"Computational linguistics is an interdisciplinary field that combines linguistics and computer science to process and analyze human language.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"STATISTICAL MODELING","type":"CONCEPT","description":"Statistical modeling involves using statistical methods to represent and analyze data, often to make predictions or infer relationships.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DEEP LEARNING MODELS","type":"TECHNOLOGY","description":"Deep learning models are advanced algorithms that use multiple layers of processing to learn representations of data for various tasks.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RETRIEVAL SYSTEMS","type":"TECHNOLOGY","description":"Retrieval systems are technologies designed to search and retrieve relevant information from large datasets or databases.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KNOWLEDGE BASES","type":"CONCEPT","description":"Knowledge bases are organized collections of information that can be used to support reasoning and decision-making in various applications.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"INFORMATION RETRIEVAL","type":"CONCEPT","description":"Information retrieval is the process of obtaining information system resources that are relevant to an information need from a collection of those resources.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DATA ANALYSIS","type":"CONCEPT","description":"Data analysis involves inspecting, cleaning, and modeling data to discover useful information and support decision-making.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"EXPERIMENTAL STUDIES","type":"CONCEPT","description":"Experimental studies are research designs that involve manipulating one or more variables to determine their effect on a dependent variable.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RESEARCH PAPERS","type":"CONCEPT","description":"Research papers are scholarly articles that present original findings or reviews of existing research in a specific field.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"TECHNICAL REPORTS","type":"CONCEPT","description":"Technical reports are documents that describe the process, progress, or results of technical or scientific research.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SURVEYS","type":"CONCEPT","description":"Surveys are research methods used to collect data from a predefined group of respondents to gain insights into specific topics.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"WORKSHOPS","type":"EVENT","description":"Workshops are interactive training sessions focused on specific topics, often involving hands-on activities and discussions.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"CONFERENCES","type":"EVENT","description":"Conferences are formal meetings where researchers and professionals gather to discuss and share findings in a specific field.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SYMPOSIA","type":"EVENT","description":"Symposia are formal gatherings of experts to discuss a particular topic, often resulting in published proceedings.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PRESENTATIONS","type":"EVENT","description":"Presentations are formal displays of information or research findings, often delivered at conferences or workshops.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"POSTERS","type":"EVENT","description":"Posters are visual presentations of research findings, typically displayed at conferences for attendees to review and discuss.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DEMONSTRATIONS","type":"EVENT","description":"Demonstrations are practical displays of a product or process, often used to showcase new technologies or methods.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"TUTORIALS","type":"EVENT","description":"Tutorials are instructional sessions designed to teach specific skills or knowledge, often in a hands-on format.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"COLLABORATIONS","type":"CONCEPT","description":"Collaborations involve partnerships between researchers or organizations to work together on projects or studies.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"NETWORKS","type":"CONCEPT","description":"Networks refer to interconnected systems or structures, often used to represent relationships among entities in various fields.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SYSTEMS","type":"CONCEPT","description":"Systems refer to organized collections of components that work together to achieve a specific goal or function.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"APPLICATIONS","type":"CONCEPT","description":"Applications refer to practical uses of theories, models, or technologies in real-world scenarios.","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"CONCEPT"},{"name":"PLATFORMS","type":"CONCEPT","description":"Platforms refer to foundational technologies or services that support the development and deployment of applications.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"TOOLS","type":"CONCEPT","description":"Tools refer to software or hardware resources used to perform specific tasks or functions in various fields.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"TECHNOLOGIES","type":"CONCEPT","description":"Technologies refer to the application of scientific knowledge for practical purposes, often resulting in new tools or methods.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"METHODS","type":"CONCEPT","description":"Methods refer to systematic approaches or techniques used to conduct research or analysis in various fields.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PROCESSES","type":"CONCEPT","description":"Processes refer to series of actions or steps taken to achieve a particular end in various contexts.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FRAMEWORKS","type":"CONCEPT","description":"Frameworks refer to structured approaches or systems that provide guidance for developing or analyzing specific topics.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"MODELS","type":"CONCEPT","description":"Models refer to simplified representations of complex systems or phenomena used for analysis or prediction.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"STRATEGIES","type":"CONCEPT","description":"Strategies refer to plans of action designed to achieve specific goals or objectives in various contexts.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"APPROACHES","type":"CONCEPT","description":"Approaches refer to methods or ways of dealing with a particular situation or problem.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PRACTICES","type":"CONCEPT","description":"Practices refer to established methods or techniques used in a particular field or profession.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"STANDARDS","type":"CONCEPT","description":"Standards refer to established norms or criteria used to measure quality or performance in various fields.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"GUIDELINES","type":"CONCEPT","description":"Guidelines refer to recommended practices or procedures designed to assist in decision-making or actions.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"POLICIES","type":"CONCEPT","description":"Policies refer to formal rules or guidelines that govern actions and decisions within organizations or systems.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"REGULATIONS","type":"CONCEPT","description":"Regulations refer to rules or directives made and maintained by an authority to regulate conduct within a specific area.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LAWS","type":"CONCEPT","description":"Laws refer to system of rules that are created and enforced through social or governmental institutions to regulate behavior.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ETHICS","type":"CONCEPT","description":"Ethics refer to moral principles that govern a person's behavior or the conducting of an activity.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"STUDIES","type":"CONCEPT","description":"Studies refer to systematic investigations or analyses conducted to discover or interpret facts or principles.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RESEARCH","type":"CONCEPT","description":"Research refers to the systematic investigation into and study of materials and sources to establish facts and reach new conclusions.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FINDINGS","type":"CONCEPT","description":"Findings refer to the results or conclusions drawn from research or analysis.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"CONCLUSIONS","type":"CONCEPT","description":"Conclusions refer to judgments or decisions reached after consideration of the relevant facts and evidence.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"IMPLICATIONS","type":"CONCEPT","description":"Implications refer to the possible effects or outcomes that may result from a particular action or decision.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RECOMMENDATIONS","type":"CONCEPT","description":"Recommendations refer to suggestions or proposals for actions based on findings or conclusions.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FUTURE WORK","type":"CONCEPT","description":"Future work refers to proposed directions for further research or development based on current findings.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LIMITATIONS","type":"CONCEPT","description":"Limitations refer to constraints or restrictions that may affect the validity or applicability of research findings.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"CONTRIBUTIONS","type":"CONCEPT","description":"Contributions refer to the input or additions made to a field or area of study through research or practice.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PERSPECTIVES","type":"CONCEPT","description":"Perspectives refer to particular attitudes or ways of considering a situation or topic.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"INSIGHTS","type":"CONCEPT","description":"Insights refer to deep understandings or perceptions gained from analysis or experience.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"EXAMPLES","type":"CONCEPT","description":"Examples refer to specific instances or cases that illustrate a concept or principle.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"CASE STUDIES","type":"CONCEPT","description":"Case studies refer to in-depth examinations of specific instances or examples within a real-world context.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FUTURE DIRECTIONS","type":"CONCEPT","description":"Future directions refer to potential paths for research or development based on current trends and findings.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RESEARCH AGENDA","type":"CONCEPT","description":"Research agenda refers to a plan or outline of topics and questions to be explored in future research.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RESEARCH QUESTIONS","type":"CONCEPT","description":"Research questions refer to specific queries that guide the focus and direction of a research study.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"HYPOTHESES","type":"CONCEPT","description":"Hypotheses refer to proposed explanations or predictions that can be tested through research.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"VARIABLES","type":"CONCEPT","description":"Variables refer to elements or factors that can change and affect the outcome of a study or experiment.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DATA COLLECTION","type":"CONCEPT","description":"Data collection refers to the systematic gathering of information for analysis and interpretation.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ANALYSIS","type":"CONCEPT","description":"Analysis refers to the process of examining data to draw conclusions or insights.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RESULTS","type":"CONCEPT","description":"Results refer to the outcomes or findings derived from research or analysis.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"VALIDATION","type":"CONCEPT","description":"Validation refers to the process of confirming that a method or model is accurate and reliable.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"REPRODUCIBILITY","type":"CONCEPT","description":"Reproducibility refers to the ability to achieve consistent results using the same methods or procedures in research.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"TRANSPARENCY","type":"CONCEPT","description":"Transparency refers to the openness and clarity with which research processes and findings are communicated.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ACCOUNTABILITY","type":"CONCEPT","description":"Accountability refers to the obligation to explain, justify, and take responsibility for actions and decisions in research.","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"COLLABORATIVE RESEARCH","type":"CONCEPT","description":"Collaborative research refers to joint efforts by multiple researchers or institutions to conduct studies and","source_id":"aa79049289e6532592eec17b9e76adfb"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"BROWN ET AL. (2020)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brown et al. (2020) is a reference to a study discussing the capabilities of language models as few-shot learners.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"CHENG ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cheng et al. (2024) is a reference to a study on retrieval-augmented text generation with self-memory.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DANG H. T. (2006)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dang H. T. (2006) is a reference to a study evaluating question-focused summarization systems.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ES ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Es et al. (2023) is a reference to a study on automated evaluation of retrieval-augmented generation.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FENG ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Feng et al. (2023) is a reference to a study on retrieval-generation synergy augmented large language models.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FORTUNATO S. (2010)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fortunato S. (2010) is a reference to a study on community detection in graphs.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"GAO ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gao et al. (2023) is a reference to a survey on retrieval-augmented generation for large language models.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"GOODWIN ET AL. (2020)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Goodwin et al. (2020) is a reference to a study comparing transformers on multi-document abstractive summarization.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"HE ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">He et al. (2024) is a reference to a study on retrieval-augmented generation for textual graph understanding.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JACOMY ET AL. (2014)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacomy et al. (2014) is a reference to a study on a graph layout algorithm for network visualization.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JIN ET AL. (2021)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jin et al. (2021) is a reference to a survey of community detection approaches.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KANG ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kang et al. (2023) is a reference to a study on knowledge graph-augmented language models.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KHATTAB ET AL. (2022)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Khattab et al. (2022) is a reference to a study on composing retrieval and language models for knowledge-intensive NLP.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KIM ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kim et al. (2023) is a reference to a study on answering ambiguous questions with retrieval-augmented large language models.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KLEIN ET AL. (2006)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Klein et al. (2006) is a reference to a study on sensemaking perspectives.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KOESTEN ET AL. (2021)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Koesten et al. (2021) is a reference to a study on understanding data sensemaking behaviors.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LANGUAGE MODELS\">      <data key=\"d0\" \/>      <data key=\"d1\">Language models are statistical models that predict the next word in a sequence, playing a crucial role in natural language processing tasks.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED TEXT GENERATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"QUESTION-FOCUSED SUMMARIZATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"AUTOMATED EVALUATION\">      <data key=\"d0\" \/>      <data key=\"d1\">Automated evaluation refers to the use of algorithms and metrics to assess the quality of generated text without human intervention.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"RETRIEVAL-GENERATION SYNERGY\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"COMMUNITY DETECTION\">      <data key=\"d0\" \/>      <data key=\"d1\">Community detection is the process of identifying groups of nodes in a network that are more densely connected to each other than to the rest of the network.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SURVEY ON RETRIEVAL-AUGMENTED GENERATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"TRANSFORMERS\">      <data key=\"d0\" \/>      <data key=\"d1\">Transformers are a type of neural network architecture that has become the foundation for many state-of-the-art models in natural language processing.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TEXTUAL GRAPH UNDERSTANDING\">      <data key=\"d0\" \/>      <data key=\"d1\">Textual graph understanding involves interpreting and analyzing information represented in graph structures derived from text.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"GRAPH LAYOUT ALGORITHM\">      <data key=\"d0\" \/>      <data key=\"d1\">Graph layout algorithms are methods used to visualize graph structures in a way that highlights relationships and patterns among nodes.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"COMMUNITY DETECTION APPROACHES\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KNOWLEDGE-GROUNDED DIALOGUE\">      <data key=\"d0\" \/>      <data key=\"d1\">Knowledge-grounded dialogue refers to conversational systems that utilize external knowledge sources to provide contextually relevant responses.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"RETRIEVAL AND LANGUAGE MODELS\">      <data key=\"d0\" \/>      <data key=\"d1\">The integration of retrieval and language models involves combining information retrieval techniques with language generation capabilities for improved performance.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AMBIGUOUS QUESTIONS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SENSEMAKING\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DATA SENSEMAKING\">      <data key=\"d0\" \/>      <data key=\"d1\">Data sensemaking refers to the process of interpreting and understanding data to derive insights and inform decision-making.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SEQ2SEQ MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Sequence-to-sequence (seq2seq) models are a type of neural network architecture used for tasks such as translation and summarization, where input sequences are transformed into output sequences.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FAST UNFOLDING OF COMMUNITIES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Fast unfolding of communities is a method for detecting community structures in large networks, as discussed by Blondel et al. (2008).<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED GENERATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Retrieval-augmented generation is a technique that combines retrieval of relevant information with generation of text, enhancing the quality of generated outputs.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"MULTI-DOCUMENT ABSTRACTIVE SUMMARIZATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Multi-document abstractive summarization is the task of generating concise summaries from multiple documents, capturing the main ideas and information.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"EVALUATION OF QUESTION-FOCUSED SUMMARIZATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Evaluation of question-focused summarization involves assessing the effectiveness of summarization systems that target specific questions.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SELF-MEMORY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Self-memory refers to a mechanism in retrieval-augmented generation that allows models to retain and utilize past information for generating relevant responses.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"NEURAL INFORMATION PROCESSING SYSTEMS\">      <data key=\"d0\">CONFERENCE<\/data>      <data key=\"d1\">Neural Information Processing Systems (NeurIPS) is a prominent conference focused on machine learning and computational neuroscience.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"COLING\">      <data key=\"d0\">CONFERENCE<\/data>      <data key=\"d1\">COLING is the International Conference on Computational Linguistics, which focuses on research in natural language processing and computational linguistics.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">IEEE Transactions on Knowledge and Data Engineering is a scholarly journal that publishes research on knowledge and data engineering topics.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JOURNAL OF STATISTICAL MECHANICS\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">Journal of Statistical Mechanics is a scientific journal that publishes research in statistical mechanics and related fields.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PLOS ONE\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">PLoS ONE is a multidisciplinary open-access journal that publishes research across all areas of science and medicine.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">Advances in Neural Information Processing Systems is a conference proceedings publication that features research in machine learning and artificial intelligence.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"INTERNATIONAL JOURNAL OF HUMAN-COMPUTER STUDIES\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">International Journal of Human-Computer Studies is a journal that publishes research on the interaction between humans and computers.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"NIH PUBLIC ACCESS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">NIH Public Access refers to the policy that ensures public access to research funded by the National Institutes of Health.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ARXIV\">      <data key=\"d0\">REPOSITORY<\/data>      <data key=\"d1\">arXiv is a free distribution service and an open-access archive for scholarly articles in various fields, including physics, mathematics, and computer science.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"EVALUATION METRICS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Evaluation metrics are quantitative measures used to assess the performance of models and systems in various tasks, including summarization and generation.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"TASK-FOCUSED SUMMARIZATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Task-focused summarization is the process of generating summaries that are specifically tailored to answer particular questions or fulfill specific tasks.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ZERO-SHOT LEARNING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Zero-shot learning is a machine learning paradigm where a model is able to recognize objects or perform tasks it has not been explicitly trained on.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FEW-SHOT LEARNING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Few-shot learning is a machine learning approach that enables models to learn from a small number of training examples.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LARGE LANGUAGE MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Large language models are advanced neural networks trained on vast amounts of text data to perform a variety of language tasks.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"GRAPH RETRIEVER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Graph retrievers are systems designed to extract relevant information from graph structures to support various applications, including question answering.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"TEXT GENERATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Text generation is the process of creating coherent and contextually relevant text based on input data or prompts.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"NEURAL NETWORK ARCHITECTURE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Neural network architecture refers to the design and structure of neural networks, which determine how they process information.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"CROSS-DOMAIN LEARNING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Cross-domain learning involves applying knowledge or models from one domain to improve performance in another domain.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"MULTIMODAL LEARNING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Multimodal learning refers to the integration of multiple types of data (e.g., text, images) to enhance learning and understanding.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DATA VISUALIZATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Data visualization is the graphical representation of information and data, making complex data more accessible and understandable.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"NLP\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Natural Language Processing (NLP) is a field of artificial intelligence focused on the interaction between computers and human language.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DEEP LEARNING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Deep learning is a subset of machine learning that uses neural networks with many layers to analyze various forms of data.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"MACHINE LEARNING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Machine learning is a field of artificial intelligence that focuses on the development of algorithms that allow computers to learn from and make predictions based on data.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"COMPUTATIONAL LINGUISTICS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Computational linguistics is an interdisciplinary field that combines linguistics and computer science to process and analyze human language.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"STATISTICAL MODELING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Statistical modeling involves using statistical methods to represent and analyze data, often to make predictions or infer relationships.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DEEP LEARNING MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Deep learning models are advanced algorithms that use multiple layers of processing to learn representations of data for various tasks.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RETRIEVAL SYSTEMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Retrieval systems are technologies designed to search and retrieve relevant information from large datasets or databases.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KNOWLEDGE BASES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Knowledge bases are organized collections of information that can be used to support reasoning and decision-making in various applications.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"INFORMATION RETRIEVAL\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Information retrieval is the process of obtaining information system resources that are relevant to an information need from a collection of those resources.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DATA ANALYSIS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Data analysis involves inspecting, cleaning, and modeling data to discover useful information and support decision-making.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"EXPERIMENTAL STUDIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Experimental studies are research designs that involve manipulating one or more variables to determine their effect on a dependent variable.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RESEARCH PAPERS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Research papers are scholarly articles that present original findings or reviews of existing research in a specific field.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"TECHNICAL REPORTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Technical reports are documents that describe the process, progress, or results of technical or scientific research.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SURVEYS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Surveys are research methods used to collect data from a predefined group of respondents to gain insights into specific topics.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"WORKSHOPS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Workshops are interactive training sessions focused on specific topics, often involving hands-on activities and discussions.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"CONFERENCES\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Conferences are formal meetings where researchers and professionals gather to discuss and share findings in a specific field.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SYMPOSIA\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Symposia are formal gatherings of experts to discuss a particular topic, often resulting in published proceedings.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PRESENTATIONS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Presentations are formal displays of information or research findings, often delivered at conferences or workshops.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"POSTERS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Posters are visual presentations of research findings, typically displayed at conferences for attendees to review and discuss.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DEMONSTRATIONS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Demonstrations are practical displays of a product or process, often used to showcase new technologies or methods.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"TUTORIALS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Tutorials are instructional sessions designed to teach specific skills or knowledge, often in a hands-on format.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"COLLABORATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Collaborations involve partnerships between researchers or organizations to work together on projects or studies.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"NETWORKS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Networks refer to interconnected systems or structures, often used to represent relationships among entities in various fields.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SYSTEMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Systems refer to organized collections of components that work together to achieve a specific goal or function.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"APPLICATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Applications refer to practical uses of theories, models, or technologies in real-world scenarios.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"PLATFORMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Platforms refer to foundational technologies or services that support the development and deployment of applications.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"TOOLS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Tools refer to software or hardware resources used to perform specific tasks or functions in various fields.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"TECHNOLOGIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Technologies refer to the application of scientific knowledge for practical purposes, often resulting in new tools or methods.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"METHODS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Methods refer to systematic approaches or techniques used to conduct research or analysis in various fields.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PROCESSES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Processes refer to series of actions or steps taken to achieve a particular end in various contexts.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FRAMEWORKS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Frameworks refer to structured approaches or systems that provide guidance for developing or analyzing specific topics.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"MODELS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Models refer to simplified representations of complex systems or phenomena used for analysis or prediction.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"STRATEGIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Strategies refer to plans of action designed to achieve specific goals or objectives in various contexts.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"APPROACHES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Approaches refer to methods or ways of dealing with a particular situation or problem.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PRACTICES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Practices refer to established methods or techniques used in a particular field or profession.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"STANDARDS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Standards refer to established norms or criteria used to measure quality or performance in various fields.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"GUIDELINES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Guidelines refer to recommended practices or procedures designed to assist in decision-making or actions.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"POLICIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Policies refer to formal rules or guidelines that govern actions and decisions within organizations or systems.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"REGULATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Regulations refer to rules or directives made and maintained by an authority to regulate conduct within a specific area.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LAWS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Laws refer to system of rules that are created and enforced through social or governmental institutions to regulate behavior.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ETHICS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Ethics refer to moral principles that govern a person's behavior or the conducting of an activity.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"STUDIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Studies refer to systematic investigations or analyses conducted to discover or interpret facts or principles.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RESEARCH\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Research refers to the systematic investigation into and study of materials and sources to establish facts and reach new conclusions.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FINDINGS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Findings refer to the results or conclusions drawn from research or analysis.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"CONCLUSIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Conclusions refer to judgments or decisions reached after consideration of the relevant facts and evidence.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"IMPLICATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Implications refer to the possible effects or outcomes that may result from a particular action or decision.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RECOMMENDATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Recommendations refer to suggestions or proposals for actions based on findings or conclusions.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FUTURE WORK\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Future work refers to proposed directions for further research or development based on current findings.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LIMITATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Limitations refer to constraints or restrictions that may affect the validity or applicability of research findings.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"CONTRIBUTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Contributions refer to the input or additions made to a field or area of study through research or practice.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PERSPECTIVES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Perspectives refer to particular attitudes or ways of considering a situation or topic.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"INSIGHTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Insights refer to deep understandings or perceptions gained from analysis or experience.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"EXAMPLES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Examples refer to specific instances or cases that illustrate a concept or principle.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"CASE STUDIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Case studies refer to in-depth examinations of specific instances or examples within a real-world context.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FUTURE DIRECTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Future directions refer to potential paths for research or development based on current trends and findings.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RESEARCH AGENDA\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Research agenda refers to a plan or outline of topics and questions to be explored in future research.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RESEARCH QUESTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Research questions refer to specific queries that guide the focus and direction of a research study.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"HYPOTHESES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Hypotheses refer to proposed explanations or predictions that can be tested through research.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"VARIABLES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Variables refer to elements or factors that can change and affect the outcome of a study or experiment.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DATA COLLECTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Data collection refers to the systematic gathering of information for analysis and interpretation.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ANALYSIS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Analysis refers to the process of examining data to draw conclusions or insights.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RESULTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Results refer to the outcomes or findings derived from research or analysis.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"VALIDATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Validation refers to the process of confirming that a method or model is accurate and reliable.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"REPRODUCIBILITY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Reproducibility refers to the ability to achieve consistent results using the same methods or procedures in research.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"TRANSPARENCY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Transparency refers to the openness and clarity with which research processes and findings are communicated.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ACCOUNTABILITY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Accountability refers to the obligation to explain, justify, and take responsibility for actions and decisions in research.<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"COLLABORATIVE RESEARCH\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Collaborative research refers to joint efforts by multiple researchers or institutions to conduct studies and<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <edge source=\"BROWN ET AL. (2020)\" target=\"LANGUAGE MODELS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Brown et al. (2020) discusses the capabilities of language models, establishing their role in few-shot learning.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"CHENG ET AL. (2024)\" target=\"RETRIEVAL-AUGMENTED TEXT GENERATION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Cheng et al. (2024) explores retrieval-augmented text generation, contributing to advancements in this area.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"DANG H. T. (2006)\" target=\"QUESTION-FOCUSED SUMMARIZATION\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Dang H. T. (2006) evaluates systems focused on summarization, relevant to the field of question-focused summarization.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"ES ET AL. (2023)\" target=\"AUTOMATED EVALUATION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Es et al. (2023) provides insights into automated evaluation methods for retrieval-augmented generation.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"FENG ET AL. (2023)\" target=\"RETRIEVAL-GENERATION SYNERGY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Feng et al. (2023) discusses the synergy between retrieval and generation in large language models.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"FORTUNATO S. (2010)\" target=\"COMMUNITY DETECTION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Fortunato S. (2010) provides foundational knowledge on community detection in graphs, relevant to network analysis.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"GAO ET AL. (2023)\" target=\"SURVEY ON RETRIEVAL-AUGMENTED GENERATION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Gao et al. (2023) surveys the landscape of retrieval-augmented generation for large language models.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"GOODWIN ET AL. (2020)\" target=\"TRANSFORMERS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Goodwin et al. (2020) compares transformers, contributing to the understanding of their performance in summarization tasks.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"HE ET AL. (2024)\" target=\"TEXTUAL GRAPH UNDERSTANDING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">He et al. (2024) focuses on retrieval-augmented generation for understanding textual graphs, linking retrieval and comprehension.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"JACOMY ET AL. (2014)\" target=\"GRAPH LAYOUT ALGORITHM\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Jacomy et al. (2014) discusses a graph layout algorithm, relevant for visualizing network data.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"JIN ET AL. (2021)\" target=\"COMMUNITY DETECTION APPROACHES\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Jin et al. (2021) surveys community detection methods, contributing to the understanding of this field.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"KANG ET AL. (2023)\" target=\"KNOWLEDGE-GROUNDED DIALOGUE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Kang et al. (2023) explores knowledge graph-augmented models for dialogue generation, linking knowledge and dialogue.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"KHATTAB ET AL. (2022)\" target=\"RETRIEVAL AND LANGUAGE MODELS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Khattab et al. (2022) discusses the integration of retrieval and language models, relevant for knowledge-intensive NLP tasks.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"KIM ET AL. (2023)\" target=\"AMBIGUOUS QUESTIONS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Kim et al. (2023) addresses the challenge of answering ambiguous questions using retrieval-augmented models.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"KLEIN ET AL. (2006)\" target=\"SENSEMAKING\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Klein et al. (2006) provides perspectives on sensemaking, contributing to cognitive models in understanding information.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"KOESTEN ET AL. (2021)\" target=\"DATA SENSEMAKING\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Koesten et al. (2021) investigates behaviors related to data sensemaking, relevant for human-computer interaction studies.<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"df50c95dff7da074cbb2f68e88686f88","chunk":" , 21(5):88\u201392.\nKoesten, L., Gregory, K., Groth, P., and Simperl, E. (2021). Talking datasets\u2013understanding data\nsensemaking behaviours. International journal of human-computer studies , 146:102562.\nKuratov, Y ., Bulatov, A., Anokhin, P., Sorokin, D., Sorokin, A., and Burtsev, M. (2024). In search\nof needles in a 11m haystack: Recurrent memory finds what llms miss.\nLangChain (2024). Langchain graphs. https:\/\/python .langchain .com\/docs\/use cases\/graph\/.\nLaskar, M. T. R., Hoque, E., and Huang, J. (2020). Query focused abstractive summarization via\nincorporating query relevance and transfer learning with transformer models. In Advances in\nArtificial Intelligence: 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020,\nOttawa, ON, Canada, May 13\u201315, 2020, Proceedings 33 , pages 342\u2013348. Springer.\nLaskar, M. T. R., Hoque, E., and Huang, J. X. (2022). Domain adaptation with pre-trained transform-\ners for query-focused abstractive text summarization. Computational Linguistics , 48(2):279\u2013320.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., K \u00a8uttler, H., Lewis, M., Yih,\nW.-t., Rockt \u00a8aschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp\ntasks. Advances in Neural Information Processing Systems , 33:9459\u20139474.\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. (2023). Lost\nin the middle: How language models use long contexts. arXiv:2307.03172.\nLiu, Y . and Lapata, M. (2019). Hierarchical transformers for multi-document summarization. arXiv\npreprint arXiv:1905.13164 .\nLlamaIndex (2024). LlamaIndex Knowledge Graph Index. https:\/\/docs .llamaindex .ai\/en\/stable\/\nexamples\/index structs\/knowledge graph\/KnowledgeGraphDemo .html.\nManakul, P., Liusie, A., and Gales, M. J. (2023). Selfcheckgpt: Zero-resource black-box hallucina-\ntion detection for generative large language models. arXiv preprint arXiv:2303.08896 .\nMao, Y ., He, P., Liu, X., Shen, Y ., Gao, J., Han, J., and Chen, W. (2020). Generation-augmented\nretrieval for open-domain question answering. arXiv preprint arXiv:2009.08553 .\nMartin, S., Brown, W. M., Klavans, R., and Boyack, K. (2011). Openord: An open-source toolbox\nfor large graph layout. SPIE Conference on Visualization and Data Analysis (VDA) .\nMicrosoft (2023). The impact of large language models on scientific discovery: a preliminary study\nusing gpt-4.\n13NebulaGraph (2024). Nebulagraph launches industry-first graph rag: Retrieval-augmented genera-\ntion with llm based on knowledge graphs. https:\/\/www .nebula-graph .io\/posts\/graph-RAG.\nNeo4J (2024). Project NaLLM. https:\/\/github .com\/neo4j\/NaLLM.\nNewman, M. E. (2006). Modularity and community structure in networks. Proceedings of the\nnational academy of sciences , 103(23):8577\u20138582.\nRam, O., Levine, Y ., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham,\nY . (2023). In-context retrieval-augmented language models. Transactions of the Association for\nComputational Linguistics , 11:1316\u20131331.\nRanade, P. and Joshi, A. (2023). Fabula: Intelligence report generation using retrieval-augmented\nnarrative construction. arXiv preprint arXiv:2310.13848 .\nSarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., and Manning, C. D. (2024). Raptor:\nRecursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059 .\nScott, K. (2024). Behind the Tech. https:\/\/www .microsoft .com\/en-us\/behind-the-tech.\nShao, Z., Gong, Y ., Shen, Y ., Huang, M., Duan, N., and Chen, W. (2023). Enhancing retrieval-\naugmented large language models with iterative retrieval-generation synergy. arXiv preprint\narXiv:2305.15294 .\nSu, D., Xu, Y ., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A ques-\ntion answering and query-focused multi-document summarization","chunk_id":"df50c95dff7da074cbb2f68e88686f88","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"KOESTEN, L.","type":"PERSON","description":"L. Koesten is a researcher who co-authored a study on data sensemaking behaviors in human-computer interaction.","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"GREGORY, K.","type":"PERSON","description":"K. Gregory is a researcher who co-authored a study on data sensemaking behaviors in human-computer interaction.","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"GROTH, P.","type":"PERSON","description":"P. Groth is a researcher who co-authored a study on data sensemaking behaviors in human-computer interaction.","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"SIMPURL, E.","type":"PERSON","description":"E. Simperl is a researcher who co-authored a study on data sensemaking behaviors in human-computer interaction.","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"KURATOV, Y.","type":"PERSON","description":"Y. Kuratov is a researcher who co-authored a study on the limitations of large language models in finding relevant information.","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"BULATOV, A.","type":"PERSON","description":"A. Bulatov is a researcher who co-authored a study on the limitations of large language models in finding relevant information.","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"ANOKHIN, P.","type":"PERSON","description":"P. Anokhin is a researcher who co-authored a study on the limitations of large language models in finding relevant information.","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"SOROKIN, D.","type":"PERSON","description":"D. Sorokin is a researcher who co-authored a study on the limitations of large language models in finding relevant information.","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"SOROKIN, A.","type":"PERSON","description":"A. Sorokin is a researcher who co-authored a study on the limitations of large language models in finding relevant information.","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"BURTSEV, M.","type":"PERSON","description":"M. Burtsev is a researcher who co-authored a study on the limitations of large language models in finding relevant information.","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"LANGCHAIN","type":"TECHNOLOGY","description":"LangChain is a framework for building applications with language models, including graph-based functionalities.","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"LASKAR, M. T. R.","type":"PERSON","description":"M. T. R. Laskar is a researcher who co-authored studies on query-focused abstractive summarization.","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"HOQUE, E.","type":"PERSON","description":"E. Hoque is a researcher who co-authored studies on query-focused abstractive summarization.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"HUANG, J.","type":"PERSON","description":"J. Huang is a researcher who co-authored studies on query-focused abstractive summarization.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"LEWIS, P.","type":"PERSON","description":"P. Lewis is a researcher who co-authored a study on retrieval-augmented generation for NLP tasks.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"PEREZ, E.","type":"PERSON","description":"E. Perez is a researcher who co-authored a study on retrieval-augmented generation for NLP tasks.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"PIKTUS, A.","type":"PERSON","description":"A. Piktus is a researcher who co-authored a study on retrieval-augmented generation for NLP tasks.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"PETRONI, F.","type":"PERSON","description":"F. Petroni is a researcher who co-authored a study on retrieval-augmented generation for NLP tasks.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"KARPUKHIN, V.","type":"PERSON","description":"V. Karpukhin is a researcher who co-authored a study on retrieval-augmented generation for NLP tasks.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"GOYAL, N.","type":"PERSON","description":"N. Goyal is a researcher who co-authored a study on retrieval-augmented generation for NLP tasks.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"K\u00dcTTLER, H.","type":"PERSON","description":"H. K\u00fcttler is a researcher who co-authored a study on retrieval-augmented generation for NLP tasks.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"MARTIN, S.","type":"PERSON","description":"S. Martin is a researcher who co-authored a study on graph layout tools.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"MICROSOFT","type":"ORGANIZATION","description":"Microsoft is a technology company that conducts research on the impact of large language models on scientific discovery.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"ORGANIZATION"},{"name":"NEBULAGRAPH","type":"ORGANIZATION","description":"NebulaGraph is a company that launched a graph-based retrieval-augmented generation system.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"ORGANIZATION"},{"name":"NEO4J","type":"ORGANIZATION","description":"Neo4J is a graph database company that is involved in projects related to language models.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"ORGANIZATION"},{"name":"NEWMAN, M. E.","type":"PERSON","description":"M. E. Newman is a researcher known for his work on modularity and community structure in networks.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"RAM, O.","type":"PERSON","description":"O. Ram is a researcher who co-authored a study on in-context retrieval-augmented language models.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"RANADE, P.","type":"PERSON","description":"P. Ranade is a researcher who co-authored a study on intelligence report generation using retrieval-augmented narrative construction.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"SARTHI, P.","type":"PERSON","description":"P. Sarthi is a researcher who co-authored a study on recursive abstractive processing for retrieval.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"SHAO, Z.","type":"PERSON","description":"Z. Shao is a researcher who co-authored a study on enhancing retrieval-augmented language models.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"SU, D.","type":"PERSON","description":"D. Su is a researcher who co-authored a study on question answering and multi-document summarization.","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"KOESTEN, L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">L. Koesten is a researcher who co-authored a study on data sensemaking behaviors in human-computer interaction.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"GREGORY, K.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">K. Gregory is a researcher who co-authored a study on data sensemaking behaviors in human-computer interaction.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"GROTH, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Groth is a researcher who co-authored a study on data sensemaking behaviors in human-computer interaction.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"SIMPURL, E.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">E. Simperl is a researcher who co-authored a study on data sensemaking behaviors in human-computer interaction.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"KURATOV, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Kuratov is a researcher who co-authored a study on the limitations of large language models in finding relevant information.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"BULATOV, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Bulatov is a researcher who co-authored a study on the limitations of large language models in finding relevant information.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"ANOKHIN, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Anokhin is a researcher who co-authored a study on the limitations of large language models in finding relevant information.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"SOROKIN, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">D. Sorokin is a researcher who co-authored a study on the limitations of large language models in finding relevant information.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"SOROKIN, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Sorokin is a researcher who co-authored a study on the limitations of large language models in finding relevant information.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"BURTSEV, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. Burtsev is a researcher who co-authored a study on the limitations of large language models in finding relevant information.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"LANGCHAIN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LangChain is a framework for building applications with language models, including graph-based functionalities.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"LASKAR, M. T. R.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. T. R. Laskar is a researcher who co-authored studies on query-focused abstractive summarization.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"HOQUE, E.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">E. Hoque is a researcher who co-authored studies on query-focused abstractive summarization.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HUANG, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Huang is a researcher who co-authored studies on query-focused abstractive summarization.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEWIS, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Lewis is a researcher who co-authored a study on retrieval-augmented generation for NLP tasks.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PEREZ, E.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">E. Perez is a researcher who co-authored a study on retrieval-augmented generation for NLP tasks.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PIKTUS, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Piktus is a researcher who co-authored a study on retrieval-augmented generation for NLP tasks.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PETRONI, F.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">F. Petroni is a researcher who co-authored a study on retrieval-augmented generation for NLP tasks.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KARPUKHIN, V.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">V. Karpukhin is a researcher who co-authored a study on retrieval-augmented generation for NLP tasks.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GOYAL, N.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">N. Goyal is a researcher who co-authored a study on retrieval-augmented generation for NLP tasks.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"K&#220;TTLER, H.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">H. K&#252;ttler is a researcher who co-authored a study on retrieval-augmented generation for NLP tasks.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MARTIN, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Martin is a researcher who co-authored a study on graph layout tools.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MICROSOFT\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Microsoft is a technology company that conducts research on the impact of large language models on scientific discovery.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"NEBULAGRAPH\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">NebulaGraph is a company that launched a graph-based retrieval-augmented generation system.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"NEO4J\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Neo4J is a graph database company that is involved in projects related to language models.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"NEWMAN, M. E.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. E. Newman is a researcher known for his work on modularity and community structure in networks.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RAM, O.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">O. Ram is a researcher who co-authored a study on in-context retrieval-augmented language models.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RANADE, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Ranade is a researcher who co-authored a study on intelligence report generation using retrieval-augmented narrative construction.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SARTHI, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Sarthi is a researcher who co-authored a study on recursive abstractive processing for retrieval.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHAO, Z.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Z. Shao is a researcher who co-authored a study on enhancing retrieval-augmented language models.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SU, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">D. Su is a researcher who co-authored a study on question answering and multi-document summarization.<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <edge source=\"KOESTEN, L.\" target=\"GREGORY, K.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">L. Koesten and K. Gregory co-authored a study on data sensemaking behaviors in human-computer interaction.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"KOESTEN, L.\" target=\"GROTH, P.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">L. Koesten and P. Groth co-authored a study on data sensemaking behaviors in human-computer interaction.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"KOESTEN, L.\" target=\"SIMPURL, E.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">L. Koesten and E. Simperl co-authored a study on data sensemaking behaviors in human-computer interaction.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"GREGORY, K.\" target=\"GROTH, P.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">K. Gregory and P. Groth co-authored a study on data sensemaking behaviors in human-computer interaction.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"GREGORY, K.\" target=\"SIMPURL, E.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">K. Gregory and E. Simperl co-authored a study on data sensemaking behaviors in human-computer interaction.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"GROTH, P.\" target=\"SIMPURL, E.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">P. Groth and E. Simperl co-authored a study on data sensemaking behaviors in human-computer interaction.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"KURATOV, Y.\" target=\"BULATOV, A.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Y. Kuratov and A. Bulatov co-authored a study on the limitations of large language models.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"KURATOV, Y.\" target=\"ANOKHIN, P.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Y. Kuratov and P. Anokhin co-authored a study on the limitations of large language models.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"KURATOV, Y.\" target=\"SOROKIN, D.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Y. Kuratov and D. Sorokin co-authored a study on the limitations of large language models.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"KURATOV, Y.\" target=\"SOROKIN, A.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Y. Kuratov and A. Sorokin co-authored a study on the limitations of large language models.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"KURATOV, Y.\" target=\"BURTSEV, M.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Y. Kuratov and M. Burtsev co-authored a study on the limitations of large language models.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LANGCHAIN\" target=\"NEO4J\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LangChain is involved in projects related to Neo4J, particularly in graph-based applications.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LANGCHAIN\" target=\"NEBULAGRAPH\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">NebulaGraph's graph-based retrieval-augmented generation system is relevant to LangChain's functionalities.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LASKAR, M. T. R.\" target=\"HOQUE, E.\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">M. T. R. Laskar and E. Hoque co-authored studies on query-focused abstractive summarization.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LASKAR, M. T. R.\" target=\"HUANG, J.\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">M. T. R. Laskar and J. Huang co-authored studies on query-focused abstractive summarization.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LASKAR, M. T. R.\" target=\"SU, D.\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">D. Su and M. T. R. Laskar co-authored a study on question answering and multi-document summarization.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"HOQUE, E.\" target=\"HUANG, J.\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">E. Hoque and J. Huang co-authored studies on query-focused abstractive summarization.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LEWIS, P.\" target=\"PEREZ, E.\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">P. Lewis and E. Perez co-authored a study on retrieval-augmented generation for NLP tasks.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LEWIS, P.\" target=\"PIKTUS, A.\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">P. Lewis and A. Piktus co-authored a study on retrieval-augmented generation for NLP tasks.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LEWIS, P.\" target=\"PETRONI, F.\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">P. Lewis and F. Petroni co-authored a study on retrieval-augmented generation for NLP tasks.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LEWIS, P.\" target=\"KARPUKHIN, V.\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">P. Lewis and V. Karpukhin co-authored a study on retrieval-augmented generation for NLP tasks.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LEWIS, P.\" target=\"GOYAL, N.\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">P. Lewis and N. Goyal co-authored a study on retrieval-augmented generation for NLP tasks.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LEWIS, P.\" target=\"K&#220;TTLER, H.\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">P. Lewis and H. K&#252;ttler co-authored a study on retrieval-augmented generation for NLP tasks.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LEWIS, P.\" target=\"RAM, O.\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">O. Ram and P. Lewis co-authored a study on in-context retrieval-augmented language models.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LEWIS, P.\" target=\"SHAO, Z.\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Z. Shao and P. Lewis co-authored a study on enhancing retrieval-augmented language models.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"RANADE, P.\" target=\"SARTHI, P.\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">P. Ranade and P. Sarthi co-authored a study on retrieval-augmented narrative construction.<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","chunk":"., Duan, N., and Chen, W. (2023). Enhancing retrieval-\naugmented large language models with iterative retrieval-generation synergy. arXiv preprint\narXiv:2305.15294 .\nSu, D., Xu, Y ., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A ques-\ntion answering and query-focused multi-document summarization system for covid-19 scholarly\ninformation management. arXiv preprint arXiv:2005.03975 .\nTang, Y . and Yang, Y . (2024). MultiHop-RAG: Benchmarking retrieval-augmented generation for\nmulti-hop queries. arXiv preprint arXiv:2401.15391 .\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,\nBhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288 .\nTraag, V . A., Waltman, L., and Van Eck, N. J. (2019). From Louvain to Leiden: guaranteeing\nwell-connected communities. Scientific Reports , 9(1).\nTrajanoska, M., Stojanov, R., and Trajanov, D. (2023). Enhancing knowledge graph construction\nusing large language models. ArXiv , abs\/2305.04676.\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. (2022). Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint\narXiv:2212.10509 .\nWang, J., Liang, Y ., Meng, F., Sun, Z., Shi, H., Li, Z., Xu, J., Qu, J., and Zhou, J. (2023a). Is chatgpt\na good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048 .\nWang, S., Khramtsova, E., Zhuang, S., and Zuccon, G. (2024). Feb4rag: Evaluating federated search\nin the context of retrieval augmented generation. arXiv preprint arXiv:2402.11891 .\nWang, Y ., Lipka, N., Rossi, R. A., Siu, A., Zhang, R., and Derr, T. (2023b). Knowledge graph\nprompting for multi-document question answering.\nXu, Y . and Lapata, M. (2021). Text summarization with latent queries. arXiv preprint\narXiv:2106.00104 .\nYang, Z., Qi, P., Zhang, S., Bengio, Y ., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. (2018).\nHotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) .\nYao, J.-g., Wan, X., and Xiao, J. (2017). Recent advances in document summarization. Knowledge\nand Information Systems , 53:297\u2013336.\n14Yao, L., Peng, J., Mao, C., and Luo, Y . (2023). Exploring large language models for knowledge\ngraph completion.\nZhang, J. (2023). Graph-toolformer: To empower llms with graph reasoning ability via prompt\naugmented by chatgpt. arXiv preprint arXiv:2304.11116 .\nZhang, Y ., Zhang, Y ., Gan, Y ., Yao, L., and Wang, C. (2024). Causal graph discovery with retrieval-\naugmented generation based large language models. arXiv preprint arXiv:2402.15301 .\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,\nE., et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\nInformation Processing Systems , 36.\n15","chunk_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":987,"entities":[{"name":"DUAN, N.","type":"PERSON","description":"Duan, N. is a researcher who co-authored a paper on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"CHEN, W.","type":"PERSON","description":"Chen, W. is a researcher who co-authored a paper on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"SU, D.","type":"PERSON","description":"Su, D. is a researcher who co-authored a paper on Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"XU, Y.","type":"PERSON","description":"Xu, Y. is a researcher who co-authored a paper on Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"YANG, Y.","type":"PERSON","description":"Yang, Y. is a researcher who co-authored a paper on MultiHop-RAG, benchmarking retrieval-augmented generation for multi-hop queries.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"TANG, Y.","type":"PERSON","description":"Tang, Y. is a researcher who co-authored a paper on MultiHop-RAG, benchmarking retrieval-augmented generation for multi-hop queries.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"TOUVRON, H.","type":"PERSON","description":"Touvron, H. is a researcher who co-authored a paper on Llama 2, which includes open foundation and fine-tuned chat models.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"MARTIN, L.","type":"PERSON","description":"Martin, L. is a researcher who co-authored a paper on Llama 2, which includes open foundation and fine-tuned chat models.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"TRAJANOSKA, M.","type":"PERSON","description":"Trajanoska, M. is a researcher who co-authored a paper on enhancing knowledge graph construction using large language models.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"TRIVEDI, H.","type":"PERSON","description":"Trivedi, H. is a researcher who co-authored a paper on interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"WANG, J.","type":"PERSON","description":"Wang, J. is a researcher who co-authored a preliminary study on whether ChatGPT is a good NLG evaluator.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"WANG, S.","type":"PERSON","description":"Wang, S. is a researcher who co-authored a paper on evaluating federated search in the context of retrieval-augmented generation.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"YAO, L.","type":"PERSON","description":"Yao, L. is a researcher who co-authored a paper exploring large language models for knowledge graph completion.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"ZHANG, J.","type":"PERSON","description":"Zhang, J. is a researcher who co-authored a paper on Graph-toolformer, empowering LLMs with graph reasoning ability via prompts augmented by ChatGPT.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"ZHANG, Y.","type":"PERSON","description":"Zhang, Y. is a researcher who co-authored a paper on causal graph discovery with retrieval-augmented generation based large language models.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"ZHENG, L.","type":"PERSON","description":"Zheng, L. is a researcher who co-authored a paper judging LLMs as judges with MT-bench and Chatbot Arena.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"CAIRE-COVID","type":"SYSTEM","description":"Caire-covid is a question answering and query-focused multi-document summarization system designed for managing COVID-19 scholarly information.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"MULTIHOP-RAG","type":"SYSTEM","description":"MultiHop-RAG is a benchmarking system for retrieval-augmented generation focused on multi-hop queries.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"LLAMA 2","type":"SYSTEM","description":"Llama 2 is a system that includes open foundation and fine-tuned chat models for various applications.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"KNOWLEDGE GRAPH","type":"CONCEPT","description":"Knowledge graphs are structured representations of knowledge that can be enhanced using large language models.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"GRAPH-TOOLFORMER","type":"SYSTEM","description":"Graph-toolformer is a system designed to empower large language models with graph reasoning abilities through prompts augmented by ChatGPT.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"CHATGPT","type":"","description":"","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"SIDDIQUE, F. B.","type":"PERSON","description":"Siddique, F. B. is a researcher who co-authored a paper on Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"FUNG, P.","type":"PERSON","description":"Fung, P. is a researcher who co-authored a paper on Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"BENGIO, Y.","type":"PERSON","description":"Bengio, Y. is a researcher who co-authored a paper on HotpotQA, a dataset for diverse, explainable multi-hop question answering.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"SALAKHUTDINOV, R.","type":"PERSON","description":"Salakhutdinov, R. is a researcher who co-authored a paper on HotpotQA, a dataset for diverse, explainable multi-hop question answering.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"MANNING, C. D.","type":"PERSON","description":"Manning, C. D. is a researcher who co-authored a paper on HotpotQA, a dataset for diverse, explainable multi-hop question answering.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"XIAO, J.","type":"PERSON","description":"Xiao, J. is a researcher who co-authored a paper on recent advances in document summarization.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"LI, Z.","type":"PERSON","description":"Li, Z. is a researcher who co-authored a paper on knowledge graph prompting for multi-document question answering.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"DERR, T.","type":"PERSON","description":"Derr, T. is a researcher who co-authored a paper on knowledge graph prompting for multi-document question answering.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"ZHANG, R.","type":"PERSON","description":"Zhang, R. is a researcher who co-authored a paper on knowledge graph prompting for multi-document question answering.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"KRAMTSOVA, E.","type":"PERSON","description":"Khramtsova, E. is a researcher who co-authored a paper on evaluating federated search in the context of retrieval-augmented generation.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"ZHUANG, S.","type":"PERSON","description":"Zhuang, S. is a researcher who co-authored a paper on evaluating federated search in the context of retrieval-augmented generation.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"ZUCCON, G.","type":"PERSON","description":"Zuccon, G. is a researcher who co-authored a paper on evaluating federated search in the context of retrieval-augmented generation.","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"BAREZI, E. J.","type":"","description":"","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"HOTSPOTQA","type":"","description":"","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"DOCUMENT SUMMARIZATION","type":"","description":"","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"DUAN, N.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Duan, N. is a researcher who co-authored a paper on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"CHEN, W.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen, W. is a researcher who co-authored a paper on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"SU, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Su, D. is a researcher who co-authored a paper on Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"XU, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu, Y. is a researcher who co-authored a paper on Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"YANG, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang, Y. is a researcher who co-authored a paper on MultiHop-RAG, benchmarking retrieval-augmented generation for multi-hop queries.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"TANG, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tang, Y. is a researcher who co-authored a paper on MultiHop-RAG, benchmarking retrieval-augmented generation for multi-hop queries.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"TOUVRON, H.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Touvron, H. is a researcher who co-authored a paper on Llama 2, which includes open foundation and fine-tuned chat models.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"MARTIN, L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Martin, L. is a researcher who co-authored a paper on Llama 2, which includes open foundation and fine-tuned chat models.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"TRAJANOSKA, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Trajanoska, M. is a researcher who co-authored a paper on enhancing knowledge graph construction using large language models.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"TRIVEDI, H.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Trivedi, H. is a researcher who co-authored a paper on interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"WANG, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang, J. is a researcher who co-authored a preliminary study on whether ChatGPT is a good NLG evaluator.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"WANG, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang, S. is a researcher who co-authored a paper on evaluating federated search in the context of retrieval-augmented generation.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"YAO, L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao, L. is a researcher who co-authored a paper exploring large language models for knowledge graph completion.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"ZHANG, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang, J. is a researcher who co-authored a paper on Graph-toolformer, empowering LLMs with graph reasoning ability via prompts augmented by ChatGPT.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"ZHANG, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang, Y. is a researcher who co-authored a paper on causal graph discovery with retrieval-augmented generation based large language models.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"ZHENG, L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zheng, L. is a researcher who co-authored a paper judging LLMs as judges with MT-bench and Chatbot Arena.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"CAIRE-COVID\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">Caire-covid is a question answering and query-focused multi-document summarization system designed for managing COVID-19 scholarly information.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"MULTIHOP-RAG\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">MultiHop-RAG is a benchmarking system for retrieval-augmented generation focused on multi-hop queries.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"LLAMA 2\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">Llama 2 is a system that includes open foundation and fine-tuned chat models for various applications.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"KNOWLEDGE GRAPH\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Knowledge graphs are structured representations of knowledge that can be enhanced using large language models.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"GRAPH-TOOLFORMER\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">Graph-toolformer is a system designed to empower large language models with graph reasoning abilities through prompts augmented by ChatGPT.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"CHATGPT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"SIDDIQUE, F. B.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Siddique, F. B. is a researcher who co-authored a paper on Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"FUNG, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fung, P. is a researcher who co-authored a paper on Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"BENGIO, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bengio, Y. is a researcher who co-authored a paper on HotpotQA, a dataset for diverse, explainable multi-hop question answering.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"SALAKHUTDINOV, R.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Salakhutdinov, R. is a researcher who co-authored a paper on HotpotQA, a dataset for diverse, explainable multi-hop question answering.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"MANNING, C. D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Manning, C. D. is a researcher who co-authored a paper on HotpotQA, a dataset for diverse, explainable multi-hop question answering.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"XIAO, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiao, J. is a researcher who co-authored a paper on recent advances in document summarization.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"LI, Z.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Li, Z. is a researcher who co-authored a paper on knowledge graph prompting for multi-document question answering.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"DERR, T.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Derr, T. is a researcher who co-authored a paper on knowledge graph prompting for multi-document question answering.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"ZHANG, R.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang, R. is a researcher who co-authored a paper on knowledge graph prompting for multi-document question answering.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"KRAMTSOVA, E.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Khramtsova, E. is a researcher who co-authored a paper on evaluating federated search in the context of retrieval-augmented generation.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"ZHUANG, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhuang, S. is a researcher who co-authored a paper on evaluating federated search in the context of retrieval-augmented generation.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"ZUCCON, G.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zuccon, G. is a researcher who co-authored a paper on evaluating federated search in the context of retrieval-augmented generation.<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"BAREZI, E. J.\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"HOTSPOTQA\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"DOCUMENT SUMMARIZATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <edge source=\"DUAN, N.\" target=\"CHEN, W.\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Duan, N. and Chen, W. co-authored a paper on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"SU, D.\" target=\"XU, Y.\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Su, D. and Xu, Y. co-authored a paper on Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"SU, D.\" target=\"BAREZI, E. J.\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Su, D. and Barezi, E. J. co-authored a paper on Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"SU, D.\" target=\"SIDDIQUE, F. B.\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Su, D. and Siddique, F. B. co-authored a paper on Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"SU, D.\" target=\"FUNG, P.\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Su, D. and Fung, P. co-authored a paper on Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"YANG, Y.\" target=\"TANG, Y.\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yang, Y. and Tang, Y. co-authored a paper on MultiHop-RAG, benchmarking retrieval-augmented generation for multi-hop queries.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"TOUVRON, H.\" target=\"MARTIN, L.\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Touvron, H. and Martin, L. co-authored a paper on Llama 2, which includes open foundation and fine-tuned chat models.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"TRAJANOSKA, M.\" target=\"KNOWLEDGE GRAPH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Trajanoska, M. co-authored a paper on enhancing knowledge graph construction using large language models.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"TRIVEDI, H.\" target=\"KNOWLEDGE GRAPH\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Trivedi, H. co-authored a paper on interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions, relevant to knowledge graphs.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"WANG, J.\" target=\"CHATGPT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Wang, J. co-authored a preliminary study on whether ChatGPT is a good NLG evaluator, indicating its relevance in natural language generation.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"WANG, S.\" target=\"KNOWLEDGE GRAPH\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Wang, S. co-authored a paper evaluating federated search in the context of retrieval-augmented generation, which relates to knowledge graphs.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"WANG, S.\" target=\"KRAMTSOVA, E.\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Khramtsova, E. and Wang, S. co-authored a paper on evaluating federated search in the context of retrieval-augmented generation.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"WANG, S.\" target=\"ZHUANG, S.\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Zhuang, S. and Wang, S. co-authored a paper on evaluating federated search in the context of retrieval-augmented generation.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"WANG, S.\" target=\"ZUCCON, G.\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Zuccon, G. and Wang, S. co-authored a paper on evaluating federated search in the context of retrieval-augmented generation.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"YAO, L.\" target=\"KNOWLEDGE GRAPH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yao, L. co-authored a paper exploring large language models for knowledge graph completion, directly related to knowledge graphs.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"ZHANG, J.\" target=\"CHATGPT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Zhang, J. co-authored a paper on Graph-toolformer, which empowers LLMs with graph reasoning ability via prompts augmented by ChatGPT.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"ZHANG, Y.\" target=\"KNOWLEDGE GRAPH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Zhang, Y. co-authored a paper on causal graph discovery with retrieval-augmented generation based large language models, relevant to knowledge graphs.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"ZHENG, L.\" target=\"CHATGPT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Zheng, L. co-authored a paper judging LLMs as judges with MT-bench and Chatbot Arena, which involves ChatGPT.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"LI, Z.\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Li, Z. co-authored a paper on knowledge graph prompting for multi-document question answering, directly related to knowledge graphs.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"DERR, T.\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Derr, T. co-authored a paper on knowledge graph prompting for multi-document question answering, directly related to knowledge graphs.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"ZHANG, R.\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Zhang, R. co-authored a paper on knowledge graph prompting for multi-document question answering, directly related to knowledge graphs.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"BENGIO, Y.\" target=\"HOTSPOTQA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Bengio, Y. co-authored a paper on HotpotQA, a dataset for diverse, explainable multi-hop question answering.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"SALAKHUTDINOV, R.\" target=\"HOTSPOTQA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Salakhutdinov, R. co-authored a paper on HotpotQA, a dataset for diverse, explainable multi-hop question answering.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"MANNING, C. D.\" target=\"HOTSPOTQA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Manning, C. D. co-authored a paper on HotpotQA, a dataset for diverse, explainable multi-hop question answering.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>    <edge source=\"XIAO, J.\" target=\"DOCUMENT SUMMARIZATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Xiao, J. co-authored a paper on recent advances in document summarization.<\/data>      <data key=\"d5\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"93cb0d0456e0822b5fe30a3e627405f8","chunk":"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in\nLanguage Models\nAndy Zhou1 2Kai Yan1Michal Shlapentokh-Rothman1Haohan Wang1Yu-Xiong Wang1\nAbstract\nWhile language models (LMs) have shown po-\ntential across a range of decision-making tasks,\ntheir reliance on simple acting processes limits\ntheir broad deployment as autonomous agents. In\nthis paper, we introduce Language Agent Tree\nSearch (LATS) \u2013 the first general framework that\nsynergizes the capabilities of LMs in reasoning,\nacting, and planning. By leveraging the in-context\nlearning ability of LMs, we integrate Monte Carlo\nTree Search into LATS to enable LMs as agents,\nalong with LM-powered value functions and\nself-reflections for proficient exploration and en-\nhanced decision-making. A key feature of our ap-\nproach is the incorporation of an environment for\nexternal feedback, which offers a more deliberate\nand adaptive problem-solving mechanism that sur-\npasses the constraints of existing techniques. Our\nexperimental evaluation across diverse domains,\nincluding programming, interactive question-\nanswering (QA), web navigation, and math, val-\nidates the effectiveness and generality of LATS\nin decision-making while maintaining compet-\nitive or improved reasoning performance. No-\ntably, LATS achieves state-of-the-art pass@1 ac-\ncuracy (92.7%) for programming on HumanEval\nwith GPT-4 and demonstrates gradient-free per-\nformance (average score of 75.9) comparable to\ngradient-based fine-tuning for web navigation on\nWebShop with GPT-3.5. Code can be found\nathttps:\/\/github.com\/lapisrocks\/\nLanguageAgentTreeSearch .\n1. Introduction\nGeneral autonomous agents capable of reasoning and\ndecision-making in a variety of environments (Wooldridge\n1University of Illinois Urbana-Champaign.2Lapis Labs. Corre-\nspondence to: Andy Zhou <andyz3@illinois.edu >.\nProceedings of the 41stInternational Conference on Machine\nLearning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\nFigure 1. Overview of LATS. Serving as a unified framework,\nLATS leverages an external environment and an MCTS-based\nsearch algorithm to improve reasoning and decision-making.\nand Jennings, 1995) have been of longstanding interest in\nthe field of artificial intelligence. While this has tradition-\nally been studied in reinforcement learning, the recent rise\nof language models (LMs) (Brown et al., 2020; Chowdh-\nery et al., 2023; Touvron et al., 2023; OpenAI, 2023) with\nstrong reasoning and general adaptability offers an alter-\nnative paradigm. Not only have LMs excelled in standard\nnatural language processing (NLP) tasks such as summariza-\ntion (Nallapati et al., 2016) and language inference (Bow-\nman et al., 2015), but they have also been adapted to an\nincreasingly diverse set of tasks that often require advanced\ncommon-sense reasoning or quantitative skills (Cobbe et al.,\n2021; Saparov and He, 2023). In addition, LMs are capable\nof performing in complex environments that involve knowl-\nedge and reasoning, such as web navigation (Yao et al.,\n2022; Deng et al., 2023), tool-use (Schick et al., 2023), and\nopen-ended games (Fan et al., 2022).\nReasoning and acting abilities have been further improved\nby prompting techniques that augment LMs with feedback\nor observations from an external environment, as exempli-\nfied by ReAct (Yao et al., 2023b) and other work (Gao et al.,\n2023; Shinn et al., 2023). This eliminates the need to rely en-\ntirely on the base abilities of LMs, enhancing them through\nexternal tools or semantic feedback. Despite such strengths,\nthese methods are reflexive and fall short of humans\u2019 deliber-\nate and thoughtful decision-making characteristics to solve\nproblems (Sloman, 1996; Evans, 2010). In particular, they\nfail to consider multiple reasoning paths or to plan ahead.\nRecent search-guided LM work (Xie et al., 2023; Yao et al.,\n2023a; Hao et al., 2023) addresses this issue by searching\nover multiple reasoning chains. While enabling planning,\n1arXiv:2310.04406v3  [cs.AI]  6 Jun 2024Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nsuch methods operate in isolation, lacking the incorporation\nof external feedback that can improve reasoning.\nTo overcome these challenges, we propose Language Agent\nTree Search (LATS) \u2013 a unified framework for decision-\nmaking and reasoning with language models. As illustrated\nin Fig. 1, LATS synergizes LM reasoning, acting, and plan-\nning strategies by expanding ReAct (Yao et al., 2023b) into a\nsearch over a combinatorial space of possible reasoning and\nacting steps. This effort is nontrivial \u2013 adapting search algo-\nrithms to language agents and shifting from non-interactive\ntasks to interactive ones requires a substantial novel design\non nodes, prompts, and search algorithms. In particular,\nnodes and prompts must effectively store and retrieve ex-\nternal feedback, with the search algorithm incorporating\nthis information into useful heuristics for value assignment.\nIndeed, our empirical evaluation, as demonstrated on Hot-\nPotQA (Yang et al., 2018)","chunk_id":"93cb0d0456e0822b5fe30a3e627405f8","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LANGUAGE AGENT TREE SEARCH (LATS)","type":"TECHNIQUE","description":"Language Agent Tree Search (LATS) is a framework that integrates reasoning, acting, and planning capabilities of language models to enhance decision-making processes.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"LANGUAGE MODELS (LMS)","type":"TECHNOLOGY","description":"Language models (LMs) are AI systems designed to understand and generate human language, capable of performing various tasks including reasoning and decision-making.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"MONTE CARLO TREE SEARCH (MCTS)","type":"ALGORITHM","description":"Monte Carlo Tree Search (MCTS) is a search algorithm used to make decisions in AI, particularly in game playing and decision-making tasks.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"EXTERNAL ENVIRONMENT","type":"CONTEXT","description":"The external environment provides feedback to language models, enhancing their problem-solving capabilities and decision-making processes.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"PROGRAMMING","type":"DOMAIN","description":"Programming is a domain where language models can be applied to solve coding tasks and challenges.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"INTERACTIVE QUESTION-ANSWERING (QA)","type":"DOMAIN","description":"Interactive question-answering (QA) is a domain where language models engage in dialogue to answer user queries.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"WEB NAVIGATION","type":"DOMAIN","description":"Web navigation involves using language models to assist users in finding information on the internet.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"MATH","type":"DOMAIN","description":"Math refers to tasks that involve numerical reasoning and problem-solving, which language models can perform.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"HUMAN EVAL","type":"EVALUATION SET","description":"HumanEval is a benchmark used to evaluate the performance of programming models, particularly in coding tasks.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a state-of-the-art language model developed by OpenAI, known for its advanced capabilities in natural language processing and reasoning.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a version of OpenAI's language model that is used for various applications, including web navigation.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"REACT","type":"TECHNIQUE","description":"ReAct is a prompting technique that enhances language models by incorporating feedback from external environments.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"WOOLDRIDGE AND JENNINGS (1995)","type":"PERSON","description":"Wooldridge and Jennings (1995) is a reference to foundational work in the field of artificial intelligence regarding autonomous agents.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"BROWN ET AL. (2020)","type":"PERSON","description":"Brown et al. (2020) is a reference to a study that discusses the capabilities of language models.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"CHOWDHERY ET AL. (2023)","type":"PERSON","description":"Chowdhery et al. (2023) is a reference to a study that discusses advancements in language models.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"TOUVRON ET AL. (2023)","type":"PERSON","description":"Touvron et al. (2023) is a reference to a study that discusses advancements in language models.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"OPENAI (2023)","type":"ORGANIZATION","description":"OpenAI is an artificial intelligence research organization known for developing advanced language models like GPT-4.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"NALLAPATI ET AL. (2016)","type":"PERSON","description":"Nallapati et al. (2016) is a reference to a study that discusses summarization tasks in natural language processing.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"BOWMAN ET AL. (2015)","type":"PERSON","description":"Bowman et al. (2015) is a reference to a study that discusses language inference tasks in natural language processing.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"COBBE ET AL. (2021)","type":"PERSON","description":"Cobbe et al. (2021) is a reference to a study that discusses the capabilities of language models in reasoning.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"SAPAROV AND HE (2023)","type":"PERSON","description":"Saparov and He (2023) is a reference to a study that discusses the capabilities of language models in reasoning.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"YAO ET AL. (2022)","type":"PERSON","description":"Yao et al. (2022) is a reference to a study that discusses language models in web navigation.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"DENG ET AL. (2023)","type":"PERSON","description":"Deng et al. (2023) is a reference to a study that discusses language models in web navigation.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"SCHICK ET AL. (2023)","type":"PERSON","description":"Schick et al. (2023) is a reference to a study that discusses tool-use capabilities of language models.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"FAN ET AL. (2022)","type":"PERSON","description":"Fan et al. (2022) is a reference to a study that discusses language models in open-ended games.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"XIE ET AL. (2023)","type":"PERSON","description":"Xie et al. (2023) is a reference to a study that discusses search-guided language models.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"YAO ET AL. (2023A)","type":"PERSON","description":"Yao et al. (2023A) is a reference to a study that discusses search-guided language models.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"YAO ET AL. (2023B)","type":"PERSON","description":"Yao et al. (2023B) is a reference to a study that discusses the ReAct technique.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"GAO ET AL. (2023)","type":"PERSON","description":"Gao et al. (2023) is a reference to a study that discusses prompting techniques for language models.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"SHINN ET AL. (2023)","type":"PERSON","description":"Shinn et al. (2023) is a reference to a study that discusses prompting techniques for language models.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"SLOMAN (1996)","type":"PERSON","description":"Sloman (1996) is a reference to a study that discusses human-like decision-making characteristics.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"EVANS (2010)","type":"PERSON","description":"Evans (2010) is a reference to a study that discusses human-like decision-making characteristics.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"AUTONOMOUS AGENTS","type":"CONCEPT","description":"Autonomous agents are systems capable of performing tasks without human intervention, often utilizing reasoning and decision-making capabilities.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"REINFORCEMENT LEARNING","type":"TECHNIQUE","description":"Reinforcement learning is a type of machine learning where agents learn to make decisions by receiving rewards or penalties based on their actions.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"COMBINATORIAL SPACE","type":"CONCEPT","description":"Combinatorial space refers to the set of all possible combinations of actions or decisions that can be made in a given context.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"HEURISTICS","type":"CONCEPT","description":"Heuristics are strategies or rules of thumb that simplify decision-making processes, often used to make quick judgments.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"EVALUATION","type":"PROCESS","description":"Evaluation is the process of assessing the performance or effectiveness of a model or technique in specific tasks or domains.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"EXPLORATION","type":"PROCESS","description":"Exploration refers to the process of investigating or trying out different options or paths in decision-making.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"PROBLEM-SOLVING","type":"PROCESS","description":"Problem-solving is the process of finding solutions to complex issues or challenges, often requiring reasoning and decision-making.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"STATE-OF-THE-ART","type":"CONCEPT","description":"State-of-the-art refers to the highest level of development or performance achieved in a particular field or technology.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"GRADIENT-BASED FINE-TUNING","type":"TECHNIQUE","description":"Gradient-based fine-tuning is a method used to improve the performance of models by adjusting their parameters based on gradient descent techniques.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"GRADIENT-FREE PERFORMANCE","type":"CONCEPT","description":"Gradient-free performance refers to the ability of a model to perform tasks without relying on gradient-based optimization methods.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"EFFECTIVENESS","type":"CONCEPT","description":"Effectiveness refers to the degree to which a model or technique achieves its intended outcomes or goals.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"ADAPTIVE MECHANISM","type":"CONCEPT","description":"An adaptive mechanism is a system or process that adjusts its behavior based on feedback or changes in the environment.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"IN-CONTEXT LEARNING","type":"TECHNIQUE","description":"In-context learning is a method where models learn from examples provided in the context of a task, enhancing their performance on similar tasks.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"FEEDBACK","type":"PROCESS","description":"Feedback is information provided to a model or agent about its performance, used to improve future actions or decisions.","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"PROFICIENT EXPLORATION","type":"PROCESS","description":"Proficient exploration refers to the ability of a model or agent to effectively investigate and evaluate different options in decision-making.)<|COMPLETE|>","source_id":"93cb0d0456e0822b5fe30a3e627405f8"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LANGUAGE AGENT TREE SEARCH (LATS)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Language Agent Tree Search (LATS) is a framework that integrates reasoning, acting, and planning capabilities of language models to enhance decision-making processes.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"LANGUAGE MODELS (LMS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language models (LMs) are AI systems designed to understand and generate human language, capable of performing various tasks including reasoning and decision-making.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"MONTE CARLO TREE SEARCH (MCTS)\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">Monte Carlo Tree Search (MCTS) is a search algorithm used to make decisions in AI, particularly in game playing and decision-making tasks.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"EXTERNAL ENVIRONMENT\">      <data key=\"d0\">CONTEXT<\/data>      <data key=\"d1\">The external environment provides feedback to language models, enhancing their problem-solving capabilities and decision-making processes.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"PROGRAMMING\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Programming is a domain where language models can be applied to solve coding tasks and challenges.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"INTERACTIVE QUESTION-ANSWERING (QA)\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Interactive question-answering (QA) is a domain where language models engage in dialogue to answer user queries.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"WEB NAVIGATION\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Web navigation involves using language models to assist users in finding information on the internet.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"MATH\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Math refers to tasks that involve numerical reasoning and problem-solving, which language models can perform.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"HUMAN EVAL\">      <data key=\"d0\">EVALUATION SET<\/data>      <data key=\"d1\">HumanEval is a benchmark used to evaluate the performance of programming models, particularly in coding tasks.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a state-of-the-art language model developed by OpenAI, known for its advanced capabilities in natural language processing and reasoning.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a version of OpenAI's language model that is used for various applications, including web navigation.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">ReAct is a prompting technique that enhances language models by incorporating feedback from external environments.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"WOOLDRIDGE AND JENNINGS (1995)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wooldridge and Jennings (1995) is a reference to foundational work in the field of artificial intelligence regarding autonomous agents.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"BROWN ET AL. (2020)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brown et al. (2020) is a reference to a study that discusses the capabilities of language models.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"CHOWDHERY ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chowdhery et al. (2023) is a reference to a study that discusses advancements in language models.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"TOUVRON ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Touvron et al. (2023) is a reference to a study that discusses advancements in language models.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"OPENAI (2023)\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">OpenAI is an artificial intelligence research organization known for developing advanced language models like GPT-4.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"NALLAPATI ET AL. (2016)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nallapati et al. (2016) is a reference to a study that discusses summarization tasks in natural language processing.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"BOWMAN ET AL. (2015)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bowman et al. (2015) is a reference to a study that discusses language inference tasks in natural language processing.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"COBBE ET AL. (2021)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cobbe et al. (2021) is a reference to a study that discusses the capabilities of language models in reasoning.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"SAPAROV AND HE (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Saparov and He (2023) is a reference to a study that discusses the capabilities of language models in reasoning.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"YAO ET AL. (2022)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. (2022) is a reference to a study that discusses language models in web navigation.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"DENG ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Deng et al. (2023) is a reference to a study that discusses language models in web navigation.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"SCHICK ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Schick et al. (2023) is a reference to a study that discusses tool-use capabilities of language models.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"FAN ET AL. (2022)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fan et al. (2022) is a reference to a study that discusses language models in open-ended games.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"XIE ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xie et al. (2023) is a reference to a study that discusses search-guided language models.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"YAO ET AL. (2023A)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. (2023A) is a reference to a study that discusses search-guided language models.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"YAO ET AL. (2023B)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. (2023B) is a reference to a study that discusses the ReAct technique.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"GAO ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gao et al. (2023) is a reference to a study that discusses prompting techniques for language models.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"SHINN ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn et al. (2023) is a reference to a study that discusses prompting techniques for language models.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"SLOMAN (1996)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sloman (1996) is a reference to a study that discusses human-like decision-making characteristics.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"EVANS (2010)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Evans (2010) is a reference to a study that discusses human-like decision-making characteristics.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"AUTONOMOUS AGENTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Autonomous agents are systems capable of performing tasks without human intervention, often utilizing reasoning and decision-making capabilities.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"REINFORCEMENT LEARNING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Reinforcement learning is a type of machine learning where agents learn to make decisions by receiving rewards or penalties based on their actions.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"COMBINATORIAL SPACE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Combinatorial space refers to the set of all possible combinations of actions or decisions that can be made in a given context.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"HEURISTICS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Heuristics are strategies or rules of thumb that simplify decision-making processes, often used to make quick judgments.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"EVALUATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Evaluation is the process of assessing the performance or effectiveness of a model or technique in specific tasks or domains.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"EXPLORATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Exploration refers to the process of investigating or trying out different options or paths in decision-making.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"PROBLEM-SOLVING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Problem-solving is the process of finding solutions to complex issues or challenges, often requiring reasoning and decision-making.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"STATE-OF-THE-ART\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">State-of-the-art refers to the highest level of development or performance achieved in a particular field or technology.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"GRADIENT-BASED FINE-TUNING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Gradient-based fine-tuning is a method used to improve the performance of models by adjusting their parameters based on gradient descent techniques.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"GRADIENT-FREE PERFORMANCE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Gradient-free performance refers to the ability of a model to perform tasks without relying on gradient-based optimization methods.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"EFFECTIVENESS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Effectiveness refers to the degree to which a model or technique achieves its intended outcomes or goals.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"ADAPTIVE MECHANISM\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">An adaptive mechanism is a system or process that adjusts its behavior based on feedback or changes in the environment.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"IN-CONTEXT LEARNING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">In-context learning is a method where models learn from examples provided in the context of a task, enhancing their performance on similar tasks.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"FEEDBACK\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Feedback is information provided to a model or agent about its performance, used to improve future actions or decisions.<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"PROFICIENT EXPLORATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Proficient exploration refers to the ability of a model or agent to effectively investigate and evaluate different options in decision-making.)&lt;|COMPLETE|&gt;<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"LANGUAGE MODELS (LMS)\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">LATS utilizes language models to enhance reasoning, acting, and planning capabilities in decision-making tasks.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"MONTE CARLO TREE SEARCH (MCTS)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS incorporates MCTS to improve decision-making processes in language models.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"EXTERNAL ENVIRONMENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS leverages external feedback from the environment to enhance its decision-making capabilities.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"PROGRAMMING\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS is evaluated in the programming domain, demonstrating its effectiveness in coding tasks.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"INTERACTIVE QUESTION-ANSWERING (QA)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS is evaluated in the interactive QA domain, showcasing its capabilities in answering user queries.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"WEB NAVIGATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS is evaluated in web navigation, demonstrating its ability to assist users in finding information online.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"MATH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS is evaluated in the math domain, showcasing its capabilities in numerical reasoning tasks.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"HUMAN EVAL\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">LATS achieves state-of-the-art performance on the HumanEval benchmark for programming tasks.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"GPT-4\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">LATS demonstrates high accuracy in programming tasks when evaluated with GPT-4.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"GPT-3.5\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS shows comparable performance in web navigation tasks when evaluated with GPT-3.5.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"REACT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS expands upon the ReAct technique to incorporate search over reasoning and acting steps.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"WOOLDRIDGE AND JENNINGS (1995)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Wooldridge and Jennings' work provides foundational insights into the development of autonomous agents, relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"SLOMAN (1996)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Sloman (1996) discusses human-like decision-making characteristics, relevant to the design of LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"EVANS (2010)\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Evans (2010) discusses human-like decision-making characteristics, relevant to the design of LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"BROWN ET AL. (2020)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Brown et al. (2020) discusses the capabilities of language models, which are utilized in LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"CHOWDHERY ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Chowdhery et al. (2023) discusses advancements in language models, relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"TOUVRON ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Touvron et al. (2023) discusses advancements in language models, relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"OPENAI (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">OpenAI's work in language models is foundational to the development of LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"NALLAPATI ET AL. (2016)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Nallapati et al. (2016) discusses summarization tasks, showcasing capabilities of language models relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"BOWMAN ET AL. (2015)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Bowman et al. (2015) discusses language inference tasks, showcasing capabilities of language models relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"COBBE ET AL. (2021)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Cobbe et al. (2021) discusses reasoning capabilities of language models, relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"SAPAROV AND HE (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Saparov and He (2023) discusses reasoning capabilities of language models, relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"YAO ET AL. (2022)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Yao et al. (2022) discusses web navigation tasks, showcasing capabilities of language models relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"DENG ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Deng et al. (2023) discusses web navigation tasks, showcasing capabilities of language models relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"SCHICK ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Schick et al. (2023) discusses tool-use capabilities of language models, relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"FAN ET AL. (2022)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Fan et al. (2022) discusses open-ended games, showcasing capabilities of language models relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"XIE ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Xie et al. (2023) discusses search-guided language models, relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"YAO ET AL. (2023A)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Yao et al. (2023A) discusses search-guided language models, relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"YAO ET AL. (2023B)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Yao et al. (2023B) discusses the ReAct technique, relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"GAO ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Gao et al. (2023) discusses prompting techniques for language models, relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"SHINN ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Shinn et al. (2023) discusses prompting techniques for language models, relevant to LATS.<\/data>      <data key=\"d5\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"f8e7ed806916bf15245bcb4d52570c26","chunk":" steps. This effort is nontrivial \u2013 adapting search algo-\nrithms to language agents and shifting from non-interactive\ntasks to interactive ones requires a substantial novel design\non nodes, prompts, and search algorithms. In particular,\nnodes and prompts must effectively store and retrieve ex-\nternal feedback, with the search algorithm incorporating\nthis information into useful heuristics for value assignment.\nIndeed, our empirical evaluation, as demonstrated on Hot-\nPotQA (Yang et al., 2018) in Sec. 5.1, reveals that a simple\ncombination of existing methods is inadequate, even failing\nto surpass internal reasoning performance, despite having\naccess to the ground truth answer from the environment.\nOurkey insight underpinning LATS is adapting Monte Carlo\nTree Search (MCTS), inspired by its success in model-based\nreinforcement learning (Silver et al., 2017) and the obser-\nvation that many LM tasks allow reverting to earlier steps ,\nto language agents, repurposing pretrained LMs as agents\nwith LM-powered value functions and self-reflections for\ncleverer exploration. Leveraging the general capabilities\nand in-context learning abilities of modern LMs, we use\nlanguage as an interface between each component, allowing\nLATS to adapt planning to environmental conditions with-\nout additional training . To the best of our knowledge, LATS\nisthe first framework that incorporates reasoning, acting,\nand planning to enhance LM performance. Notably, LATS\ndoubles the performance of ReAct (Yao et al., 2023b) on\nHotPotQA (Yang et al., 2018) and raises the average score\nby22.1on WebShop (Yao et al., 2022) with GPT-3.5. When\nused with GPT-4, LATS achieves a 92.7Pass@1 rate on\nHumanEval (Chen et al., 2021), setting the state of the art.\nOurcontributions are the following: 1) We introduce LATS,\na framework based on Monte Carlo Tree Search to construct\nthe best trajectory from sampled actions, enabling more flex-\nible and adaptive problem-solving compared with reflexive\nprompting methods. 2) We propose a novel value function\nthat guides the search process and incorporates successful\nheuristics such as self-refinement and self-consistency. 3)\nBy integrating external feedback and self-reflection, LATS\nenhances model sensibility and enables agents to learn from\nexperience, surpassing reasoning-based search methods.\nThrough experiments across diverse domains, including pro-\ngramming, interactive question-answering (QA), web navi-\ngation, and math, we demonstrate the versatility of LATS\nfor enhancing autonomous reasoning and decision-making.2. Related Work\nLMs for reasoning. For LMs, reasoning involves decom-\nposing complex inputs into sequential intermediate steps\ntowards a final answer (Cobbe et al., 2021), demonstrated\nwith chain-of-thought (CoT) prompting (Wei et al., 2022)\nand its variants (Wei et al., 2022; Kojima et al., 2022; Wang\net al., 2022). However, these methods, which create chains\nautoregressively in a single step, often suffer from error\npropagation as the number of steps increases (Guo et al.,\n2018; Chen et al., 2023b), due to compound errors. Various\nadvancements aim to mitigate this issue; some approaches,\nsuch as self-consistency (Wang et al., 2022), employ ma-\njority voting over sampled chains, while others focus on\nmulti-step decomposition, such as least-to-most prompt-\ning (Zhou et al., 2022). Recently, CoT has been improved\nwith search algorithms (Yao et al., 2023a; Hao et al., 2023;\nBesta et al., 2023) that can sample trajectories more effec-\ntively. Tree-of-thought (ToT) prompting (Yao et al., 2023a)\nuses DFS or BFS-based (depth\/breadth-first) search guided\nby an LM-generated heuristic, while reasoning via planning\n(RAP) (Hao et al., 2023) uses MCTS with rollouts simu-\nlated by LMs. However, they rely solely on LM internal\nknowledge and cannot adapt to useful external feedback.\nLMs for acting. The strong reasoning and common-sense\nabilities of LMs have been further adapted for decision-\nmaking or acting tasks as a policy model in interactive\nenvironments. In robotics, LMs have been employed as\nhigh-level controllers of control policies (Ahn et al., 2022;\nHuang et al., 2022; Driess et al., 2023). Similar work (Baker\net al., 2022; Wang et al., 2023) has also adapted LM agents\nto complex multimodal games such as Minecraft (Guss et al.,\n2019; Fan et al., 2022). LMs are particularly useful in text-\nbased environments (Liu et al., 2018; Shridhar et al., 2020;\nLiu et al., 2024), where acting-based prompting techniques\nsuch as ReAct (Yao et al., 2023b) have seen success. Sim-\nilar to CoT, ReAct is limited by its simplicity and cannot\neffectively adapt to environment conditions. Many exten-\nsions have been proposed to address this issue, including\nself-refine (Madaan et al., 2023) and Reflexion (Shinn et al.,\n2023), which use self-improvement","chunk_id":"f8e7ed806916bf15245bcb4d52570c26","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LATS","type":"FRAMEWORK","description":"LATS is a framework that incorporates reasoning, acting, and planning to enhance the performance of language models (LMs) using Monte Carlo Tree Search (MCTS).","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"MONTE CARLO TREE SEARCH (MCTS)","type":"TECHNIQUE","description":"MCTS is a search algorithm used in decision-making processes, particularly in reinforcement learning, to evaluate possible actions and their outcomes.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"LANGUAGE MODELS (LMS)","type":"TECHNOLOGY","description":"Language models are AI systems designed to understand and generate human language, often used for tasks like text generation, question answering, and more.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a dataset used for evaluating question-answering systems, requiring reasoning over multiple documents to answer questions.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a version of the Generative Pre-trained Transformer model developed by OpenAI, known for its advanced language understanding and generation capabilities.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a more advanced version of the Generative Pre-trained Transformer model, offering improved performance in various language tasks.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"REACT","type":"TECHNIQUE","description":"ReAct is a prompting technique for language models that combines reasoning and acting to improve performance in interactive tasks.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"SELF-REFINEMENT","type":"TECHNIQUE","description":"Self-refinement is a method used to improve the performance of models by allowing them to learn from their own outputs and experiences.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"SELF-CONSISTENCY","type":"TECHNIQUE","description":"Self-consistency is a technique that employs majority voting over multiple outputs to enhance the reliability of model predictions.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"CHAIN-OF-THOUGHT (COT)","type":"TECHNIQUE","description":"CoT prompting is a method that encourages language models to generate answers by breaking down complex problems into sequential steps.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"PROGRAMMING","type":"DOMAIN","description":"Programming is a domain where language models can be applied to assist in writing code, debugging, and other related tasks.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"INTERACTIVE QUESTION-ANSWERING (QA)","type":"DOMAIN","description":"Interactive QA is a domain where language models are used to answer questions in real-time, often requiring reasoning and context understanding.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"WEB NAVIGATION","type":"DOMAIN","description":"Web navigation involves using language models to assist users in finding information and navigating online resources.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"MATH","type":"DOMAIN","description":"Math is a domain where language models can assist in solving mathematical problems and providing explanations.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"SILVER ET AL. (2017)","type":"PERSON","description":"Silver et al. (2017) is a reference to a study that discusses the success of Monte Carlo Tree Search in model-based reinforcement learning.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"COBBE ET AL. (2021)","type":"PERSON","description":"Cobbe et al. (2021) is a reference to a study that discusses reasoning in language models and the decomposition of complex inputs.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"WEI ET AL. (2022)","type":"PERSON","description":"Wei et al. (2022) is a reference to a study that discusses chain-of-thought prompting and its variants in language models.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"GUO ET AL. (2018)","type":"PERSON","description":"Guo et al. (2018) is a reference to a study that addresses error propagation in reasoning tasks with language models.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"CHEN ET AL. (2021)","type":"PERSON","description":"Chen et al. (2021) is a reference to a study that discusses the HumanEval dataset used for evaluating programming tasks.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"YAO ET AL. (2023A)","type":"PERSON","description":"Yao et al. (2023a) is a reference to a study that discusses improvements in search algorithms for language models.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"HAO ET AL. (2023)","type":"PERSON","description":"Hao et al. (2023) is a reference to a study that discusses reasoning via planning using Monte Carlo Tree Search.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"BESTA ET AL. (2023)","type":"PERSON","description":"Besta et al. (2023) is a reference to a study that discusses advancements in search algorithms for language models.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"AHN ET AL. (2022)","type":"PERSON","description":"Ahn et al. (2022) is a reference to a study that discusses the application of language models in robotics as high-level controllers.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"HUANG ET AL. (2022)","type":"PERSON","description":"Huang et al. (2022) is a reference to a study that discusses the use of language models in robotics.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"DRIESS ET AL. (2023)","type":"PERSON","description":"Driess et al. (2023) is a reference to a study that discusses the application of language models in robotics.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"BAKER ET AL. (2022)","type":"PERSON","description":"Baker et al. (2022) is a reference to a study that discusses the adaptation of language models to complex multimodal games.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"WANG ET AL. (2023)","type":"PERSON","description":"Wang et al. (2023) is a reference to a study that discusses the adaptation of language models in interactive environments.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"GUSS ET AL. (2019)","type":"PERSON","description":"Guss et al. (2019) is a reference to a study that discusses the application of language models in games like Minecraft.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"FAN ET AL. (2022)","type":"PERSON","description":"Fan et al. (2022) is a reference to a study that discusses the use of language models in complex multimodal games.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"LIU ET AL. (2018)","type":"PERSON","description":"Liu et al. (2018) is a reference to a study that discusses the application of language models in text-based environments.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"SHRIDHAR ET AL. (2020)","type":"PERSON","description":"Shridhar et al. (2020) is a reference to a study that discusses the use of language models in interactive environments.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"LIU ET AL. (2024)","type":"PERSON","description":"Liu et al. (2024) is a reference to a study that discusses the application of language models in text-based environments.","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"YANG ET AL. (2018)","type":"","description":"","source_id":"f8e7ed806916bf15245bcb4d52570c26"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LATS\">      <data key=\"d0\">FRAMEWORK<\/data>      <data key=\"d1\">LATS is a framework that incorporates reasoning, acting, and planning to enhance the performance of language models (LMs) using Monte Carlo Tree Search (MCTS).<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"MONTE CARLO TREE SEARCH (MCTS)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">MCTS is a search algorithm used in decision-making processes, particularly in reinforcement learning, to evaluate possible actions and their outcomes.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"LANGUAGE MODELS (LMS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language models are AI systems designed to understand and generate human language, often used for tasks like text generation, question answering, and more.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a dataset used for evaluating question-answering systems, requiring reasoning over multiple documents to answer questions.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a version of the Generative Pre-trained Transformer model developed by OpenAI, known for its advanced language understanding and generation capabilities.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a more advanced version of the Generative Pre-trained Transformer model, offering improved performance in various language tasks.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">ReAct is a prompting technique for language models that combines reasoning and acting to improve performance in interactive tasks.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"SELF-REFINEMENT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-refinement is a method used to improve the performance of models by allowing them to learn from their own outputs and experiences.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"SELF-CONSISTENCY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-consistency is a technique that employs majority voting over multiple outputs to enhance the reliability of model predictions.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT (COT)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">CoT prompting is a method that encourages language models to generate answers by breaking down complex problems into sequential steps.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"PROGRAMMING\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Programming is a domain where language models can be applied to assist in writing code, debugging, and other related tasks.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"INTERACTIVE QUESTION-ANSWERING (QA)\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Interactive QA is a domain where language models are used to answer questions in real-time, often requiring reasoning and context understanding.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"WEB NAVIGATION\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Web navigation involves using language models to assist users in finding information and navigating online resources.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"MATH\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Math is a domain where language models can assist in solving mathematical problems and providing explanations.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"SILVER ET AL. (2017)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Silver et al. (2017) is a reference to a study that discusses the success of Monte Carlo Tree Search in model-based reinforcement learning.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"COBBE ET AL. (2021)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cobbe et al. (2021) is a reference to a study that discusses reasoning in language models and the decomposition of complex inputs.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"WEI ET AL. (2022)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wei et al. (2022) is a reference to a study that discusses chain-of-thought prompting and its variants in language models.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"GUO ET AL. (2018)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guo et al. (2018) is a reference to a study that addresses error propagation in reasoning tasks with language models.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"CHEN ET AL. (2021)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen et al. (2021) is a reference to a study that discusses the HumanEval dataset used for evaluating programming tasks.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"YAO ET AL. (2023A)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. (2023a) is a reference to a study that discusses improvements in search algorithms for language models.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"HAO ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao et al. (2023) is a reference to a study that discusses reasoning via planning using Monte Carlo Tree Search.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"BESTA ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Besta et al. (2023) is a reference to a study that discusses advancements in search algorithms for language models.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"AHN ET AL. (2022)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ahn et al. (2022) is a reference to a study that discusses the application of language models in robotics as high-level controllers.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"HUANG ET AL. (2022)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Huang et al. (2022) is a reference to a study that discusses the use of language models in robotics.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"DRIESS ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Driess et al. (2023) is a reference to a study that discusses the application of language models in robotics.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"BAKER ET AL. (2022)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Baker et al. (2022) is a reference to a study that discusses the adaptation of language models to complex multimodal games.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"WANG ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang et al. (2023) is a reference to a study that discusses the adaptation of language models in interactive environments.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"GUSS ET AL. (2019)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guss et al. (2019) is a reference to a study that discusses the application of language models in games like Minecraft.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"FAN ET AL. (2022)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fan et al. (2022) is a reference to a study that discusses the use of language models in complex multimodal games.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"LIU ET AL. (2018)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu et al. (2018) is a reference to a study that discusses the application of language models in text-based environments.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"SHRIDHAR ET AL. (2020)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shridhar et al. (2020) is a reference to a study that discusses the use of language models in interactive environments.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"LIU ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu et al. (2024) is a reference to a study that discusses the application of language models in text-based environments.<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"YANG ET AL. (2018)\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <edge source=\"LATS\" target=\"MONTE CARLO TREE SEARCH (MCTS)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS is based on MCTS, utilizing its principles to enhance the decision-making capabilities of language models.<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"LANGUAGE MODELS (LMS)\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">LATS enhances the performance of language models by integrating reasoning, acting, and planning.<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HOTPOTQA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS has been empirically evaluated on the HotPotQA dataset, demonstrating its effectiveness in question-answering tasks.<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-3.5\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS achieves improved performance when used with GPT-3.5, showcasing its adaptability to different language models.<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-4\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">LATS achieves a high Pass@1 rate on HumanEval when used with GPT-4, indicating its advanced capabilities.<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REACT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS offers a more flexible and adaptive problem-solving approach compared to the ReAct technique.<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SELF-REFINEMENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS incorporates self-refinement to enhance model sensibility and learning from experience.<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SELF-CONSISTENCY\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS utilizes self-consistency to improve the reliability of its outputs during reasoning tasks.<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"CHAIN-OF-THOUGHT (COT)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS builds upon the principles of CoT prompting to enhance reasoning capabilities in language models.<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PROGRAMMING\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS demonstrates versatility in enhancing programming tasks through its reasoning and decision-making capabilities.<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"INTERACTIVE QUESTION-ANSWERING (QA)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS improves performance in interactive QA tasks by integrating reasoning and planning.<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"WEB NAVIGATION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS can assist in web navigation tasks by providing intelligent responses based on user queries.<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"MATH\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">LATS enhances the ability of language models to solve mathematical problems through its advanced reasoning techniques.<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"YANG ET AL. (2018)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS is evaluated using the HotPotQA dataset, which was introduced by Yang et al. (2018).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SILVER ET AL. (2017)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS is inspired by the success of MCTS discussed by Silver et al. (2017).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"COBBE ET AL. (2021)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS builds upon the reasoning principles discussed by Cobbe et al. (2021).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"WEI ET AL. (2022)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS incorporates ideas from chain-of-thought prompting as discussed by Wei et al. (2022).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GUO ET AL. (2018)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS addresses the error propagation issues highlighted by Guo et al. (2018).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"CHEN ET AL. (2021)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS demonstrates improved performance on tasks evaluated by the HumanEval dataset discussed by Chen et al. (2021).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"YAO ET AL. (2023A)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS utilizes advancements in search algorithms discussed by Yao et al. (2023a).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HAO ET AL. (2023)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS incorporates planning techniques discussed by Hao et al. (2023).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"BESTA ET AL. (2023)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS builds upon advancements in search algorithms discussed by Besta et al. (2023).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"AHN ET AL. (2022)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS is related to the application of language models in robotics as discussed by Ahn et al. (2022).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HUANG ET AL. (2022)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS is connected to the use of language models in robotics as discussed by Huang et al. (2022).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"DRIESS ET AL. (2023)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS relates to the application of language models in robotics as discussed by Driess et al. (2023).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"BAKER ET AL. (2022)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS is relevant to the adaptation of language models in complex multimodal games as discussed by Baker et al. (2022).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"WANG ET AL. (2023)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS is connected to the adaptation of language models in interactive environments as discussed by Wang et al. (2023).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GUSS ET AL. (2019)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS is related to the application of language models in games like Minecraft as discussed by Guss et al. (2019).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"FAN ET AL. (2022)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS is relevant to the use of language models in complex multimodal games as discussed by Fan et al. (2022).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"LIU ET AL. (2018)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS is connected to the application of language models in text-based environments as discussed by Liu et al. (2018).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SHRIDHAR ET AL. (2020)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS relates to the use of language models in interactive environments as discussed by Shridhar et al. (2020).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"LIU ET AL. (2024)\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">LATS is relevant to the application of language models in text-based environments as discussed by Liu et al. (2024).<\/data>      <data key=\"d5\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"c95e02c0dca4a4a36b701cbc7dd14da6","chunk":"2024), where acting-based prompting techniques\nsuch as ReAct (Yao et al., 2023b) have seen success. Sim-\nilar to CoT, ReAct is limited by its simplicity and cannot\neffectively adapt to environment conditions. Many exten-\nsions have been proposed to address this issue, including\nself-refine (Madaan et al., 2023) and Reflexion (Shinn et al.,\n2023), which use self-improvement to enhance reasoning\nand decision-making, and AdaPlanner (Sun et al., 2023),\nwhich incorporates both positive and negative feedback.\nHowever, these methods focus on refining an individual tra-\njectory and do not consider alternative choices at each step.\nIn addition, recent work (Huang et al., 2024) has suggested\nthat LMs cannot self-correct their internal reasoning, mak-\ning it critical to use external feedback. Alternatively, to pure\ndecision-making environments, the reasoning and practical\nabilities of LMs have been enhanced by providing access\nto external tools, such as APIs, search engines, calculators,\nand other models (Schick et al., 2023; Shen et al., 2023;\nSur\u00b4\u0131s et al., 2023). We summarize prior work in Tab. 1.\nTree-based search. Tree-based search, where multiple\n2Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nApproach Reasoning Acting Planning Self- External\nReflection Memory\nCoT (Wei et al., 2022) \u2713 \u00d7 \u00d7 \u00d7 \u00d7\nReAct (Yao et al., 2023b) \u2713 \u2713 \u00d7 \u00d7 \u00d7\nToT (Yao et al., 2023a) \u2713 \u00d7 \u2713 \u2713 \u2713\nRAP (Hao et al., 2023) \u2713 \u00d7 \u2713 \u00d7 \u2713\nSelf-Refine (Madaan et al., 2023) \u2713 \u00d7 \u00d7 \u2713 \u00d7\nBeam Search (Xie et al., 2023) \u2713 \u00d7 \u00d7 \u2713 \u00d7\nReflexion (Shinn et al., 2023) \u2713 \u2713 \u00d7 \u2713 \u2713\nLATS (Ours) \u2713 \u2713 \u2713 \u2713 \u2713\nTable 1. Summary of related work on reasoning, acting, and planning. LATS is the firstwork incorporating designs from allthree domains,\nallowing broad applicability in all corresponding tasks. We refer to reasoning as LM internal reasoning, acting as external decision-making,\nplanning as the use of a search algorithm, self-reflection as the use of LM-generated feedback, and external memory as storing past text\ncontext for future updates of the solution.\nbranches of outcomes are explored during search, is widely\nused in many planning algorithms (Swiechowski et al., 2021;\nLaValle, 1998) and reinforcement learning (RL) (Hafner\net al., 2019; Du et al., 2023; Wu et al., 2023) algorithms for\nits good exploration-exploitation trade-off. Note that though\ntree-based search necessitates an environment model that\ncan expand from an arbitrary state (V odopivec et al., 2017),\noften requiring extra training in RL (Hafner et al., 2023),\nsuch a problem does not exist for most LM tasks. This is\nbecause we can conveniently revert to any state by setting\nthe input to be the context and the corresponding previous\noutput from the LM for many tasks. Thus, we operate on the\ntree-based framework and use MCTS (Swiechowski et al.,\n2021) to fully unlock the potential of LMs. In addition, we\navoid the cost of training a value function over language\ndescriptions by leveraging the in-context learning (Brown\net al., 2020) abilities of LMs. Concurrent work (Liu et al.,\n2023) also explores combining search algorithms with LM\nagents but uses an off-the-shelf search algorithm, which\nmay not be optimal for LMs. Finally, following Yao et al.\n(2023a) and Hao et al. (2023), we note that we use planning\nandsearch algorithms interchangeably in this paper.\n3. Preliminaries\n3.1. Problem Setting and Prompting\nWe first define our problem and outline a few established\nmethods that leverage language models for reasoning or\ndecision-making. In LM reasoning or decision making,\nwe are given an input xin natural language and a pre-\ntrained language model p\u03b8(x)parameterized by \u03b8; our goal\nis to generate a final output y\u223cp\u03b8(x)that corresponds\nto the answer (reasoning) or completes the task (decision-\nmaking). Both xandyare language sequences , which are\ncomprised of a list of tokens (the basic elements of natural\nlanguage, often words), denoted as x= (x[1], . . . , x [lx])\nandy= (y[1], . . . , y [ly])where lxandlyare the length.The LM decodes text autoregressively, i.e., without other\ninputs, the probability for an LM to generate a sequence y\nis given by p\u03b8(x) =Qlx\ni=1p\u03b8(x[i]|x[1. . . i\u22121]). Usually,\nto improve reasoning, prompts are provided along with the\ninput x, which are specific instructions or few-shot input-\noutput examples. We denote the generic process where an\ninput prompt IO(x)is transformed into an output yby LM:\ny\u223cp\u03b8(prompt IO(x)).\nChain-of-thought (CoT) prompting (Wei et al., 2022)\ncaters to scenarios where the direct mapping from xtoyis\nintricate, e.g","chunk_id":"c95e02c0dca4a4a36b701cbc7dd14da6","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"REACT","type":"TECHNIQUE","description":"ReAct is an acting-based prompting technique that has shown success in enhancing decision-making in language models, but it is limited by its simplicity and inability to adapt to environmental conditions.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"CHAIN-OF-THOUGHT (COT)","type":"TECHNIQUE","description":"Chain-of-thought (CoT) prompting is a method that helps language models reason through complex tasks by providing structured prompts.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"SELF-REFINE","type":"TECHNIQUE","description":"Self-refine is an extension of prompting techniques that uses self-improvement to enhance reasoning and decision-making in language models.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"REFLEXION","type":"TECHNIQUE","description":"Reflexion is a prompting technique that incorporates self-improvement to enhance reasoning and decision-making, addressing limitations of simpler methods.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"ADAPLANNER","type":"TECHNIQUE","description":"AdaPlanner is a method that incorporates both positive and negative feedback to improve decision-making in language models.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"LANGUAGE MODELS (LMS)","type":"TECHNOLOGY","description":"Language models (LMs) are AI systems designed to understand and generate human-like text based on input prompts.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"EXTERNAL TOOLS","type":"TECHNOLOGY","description":"External tools include APIs, search engines, calculators, and other models that enhance the reasoning and practical abilities of language models.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"MCTS","type":"TECHNIQUE","description":"Monte Carlo Tree Search (MCTS) is a search algorithm used to explore multiple branches of outcomes in decision-making processes.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"TREE-BASED SEARCH","type":"TECHNIQUE","description":"Tree-based search is a method used in planning algorithms and reinforcement learning that explores multiple branches of outcomes for effective decision-making.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"PROBLEM SETTING AND PROMPTING","type":"CONCEPT","description":"This section outlines the problem setting for using language models in reasoning and decision-making, including the role of prompts.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"KURATOV ET AL. (2023)","type":"PERSON","description":"Kuratov et al. (2023) is a reference to a study discussing the effectiveness of various prompting techniques in language models.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"YAO ET AL. (2023)","type":"PERSON","description":"Yao et al. (2023) is a reference to a study that discusses the ReAct technique and its applications in language models.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"HAO ET AL. (2023)","type":"PERSON","description":"Hao et al. (2023) is a reference to a study that explores the use of planning and search algorithms in language models.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"LIU ET AL. (2023)","type":"PERSON","description":"Liu et al. (2023) is a reference to a study that examines the combination of search algorithms with language model agents.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"DECISION-MAKING","type":"CONCEPT","description":"Decision-making is the process by which language models generate outputs based on input prompts, often involving reasoning and planning.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"REASONING","type":"CONCEPT","description":"Reasoning is the cognitive process employed by language models to derive conclusions or make decisions based on given inputs.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"PLANNING","type":"CONCEPT","description":"Planning involves the use of algorithms to determine a sequence of actions or decisions that a language model should take to achieve a specific goal.","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"TOOL USE","type":"","description":"","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"REACT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">ReAct is an acting-based prompting technique that has shown success in enhancing decision-making in language models, but it is limited by its simplicity and inability to adapt to environmental conditions.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT (COT)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain-of-thought (CoT) prompting is a method that helps language models reason through complex tasks by providing structured prompts.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-refine is an extension of prompting techniques that uses self-improvement to enhance reasoning and decision-making in language models.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Reflexion is a prompting technique that incorporates self-improvement to enhance reasoning and decision-making, addressing limitations of simpler methods.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"ADAPLANNER\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">AdaPlanner is a method that incorporates both positive and negative feedback to improve decision-making in language models.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"LANGUAGE MODELS (LMS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language models (LMs) are AI systems designed to understand and generate human-like text based on input prompts.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"EXTERNAL TOOLS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">External tools include APIs, search engines, calculators, and other models that enhance the reasoning and practical abilities of language models.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"MCTS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Monte Carlo Tree Search (MCTS) is a search algorithm used to explore multiple branches of outcomes in decision-making processes.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"TREE-BASED SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Tree-based search is a method used in planning algorithms and reinforcement learning that explores multiple branches of outcomes for effective decision-making.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"PROBLEM SETTING AND PROMPTING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">This section outlines the problem setting for using language models in reasoning and decision-making, including the role of prompts.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"KURATOV ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kuratov et al. (2023) is a reference to a study discussing the effectiveness of various prompting techniques in language models.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"YAO ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. (2023) is a reference to a study that discusses the ReAct technique and its applications in language models.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"HAO ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao et al. (2023) is a reference to a study that explores the use of planning and search algorithms in language models.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"LIU ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu et al. (2023) is a reference to a study that examines the combination of search algorithms with language model agents.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"DECISION-MAKING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Decision-making is the process by which language models generate outputs based on input prompts, often involving reasoning and planning.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"REASONING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Reasoning is the cognitive process employed by language models to derive conclusions or make decisions based on given inputs.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"PLANNING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Planning involves the use of algorithms to determine a sequence of actions or decisions that a language model should take to achieve a specific goal.<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <edge source=\"REACT\" target=\"CHAIN-OF-THOUGHT (COT)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">ReAct and CoT are both prompting techniques used to enhance reasoning and decision-making in language models, but they have different approaches and limitations.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"REACT\" target=\"SELF-REFINE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Self-refine is an extension proposed to improve upon the limitations of ReAct, focusing on self-improvement in reasoning and decision-making.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"REACT\" target=\"ADAPLANNER\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">AdaPlanner incorporates feedback mechanisms that can enhance the decision-making process in a manner similar to ReAct.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"REACT\" target=\"KURATOV ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Kuratov et al. (2023) provides insights relevant to the effectiveness of the ReAct technique in language models.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"REACT\" target=\"YAO ET AL. (2023)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Yao et al. (2023) discusses the ReAct technique and its applications in enhancing language model performance.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"REFLEXION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Reflexion builds on the self-improvement concept introduced by Self-Refine, enhancing reasoning and decision-making capabilities.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"EXTERNAL TOOLS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Language models can enhance their reasoning and practical abilities by utilizing external tools such as APIs and search engines.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"PROBLEM SETTING AND PROMPTING\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The problem setting and prompting techniques are essential for effectively utilizing language models in reasoning and decision-making tasks.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"LIU ET AL. (2023)\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Liu et al. (2023) examines the integration of search algorithms with language model agents, contributing to the understanding of their capabilities.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"TOOL USE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The use of external tools significantly enhances the functionality and performance of language models in various tasks.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"MCTS\" target=\"TREE-BASED SEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">MCTS is a specific implementation of tree-based search techniques used for exploring multiple outcomes in decision-making processes.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"MCTS\" target=\"HAO ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Hao et al. (2023) explores the use of planning and search algorithms, including MCTS, in language models.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"DECISION-MAKING\" target=\"REASONING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Reasoning is a fundamental component of decision-making processes in language models, guiding the generation of outputs based on inputs.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"DECISION-MAKING\" target=\"PLANNING\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Planning techniques are often employed to enhance the decision-making capabilities of language models, allowing for more structured outputs.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"REASONING\" target=\"TOOL USE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Utilizing external tools can improve the reasoning capabilities of language models by providing additional information and context.<\/data>      <data key=\"d5\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"9bb90746134619cad9a3e649b8b35f24","chunk":" . i\u22121]). Usually,\nto improve reasoning, prompts are provided along with the\ninput x, which are specific instructions or few-shot input-\noutput examples. We denote the generic process where an\ninput prompt IO(x)is transformed into an output yby LM:\ny\u223cp\u03b8(prompt IO(x)).\nChain-of-thought (CoT) prompting (Wei et al., 2022)\ncaters to scenarios where the direct mapping from xtoyis\nintricate, e.g., when xis from a mathematical query or chal-\nlenging question. It hinges on creating thoughts z1, . . . , z l\nthat act as stepping stones between xandy; each thought zi\nis a language sequence. To employ CoT prompting, thoughts\nare extracted sequentially as zi\u223cpCoT\n\u03b8(x, z1\u00b7\u00b7\u00b7i\u22121), with\nthe final output being y\u223cpCoT\n\u03b8(x, z1\u00b7\u00b7\u00b7l).\nTree-of-thought (ToT) prompting (Yao et al., 2023a) ex-\ntends CoT prompting by exploring multiple reasoning paths\nover thoughts. It frames problems as a search over a tree,\nwhere each node s= [x, z1\u00b7i]represents a partial solution\nstate comprising the original input xand the thought se-\nquence z1\u00b7\u00b7\u00b7i. Thoughts ziare generated by proposal or\nsampling with CoT zi\u223cpCoT\n\u03b8(x, z1\u00b7\u00b7\u00b7i\u22121). Search algo-\nrithms like depth-first (DFS) or breadth-first (BFS) search\nare used to systematically explore the tree, guided by heuris-\ntics based on LM evaluations V(s)of each state.\nReAct (Yao et al., 2023b) extends language models to\ntasks where the mapping from xtoyis enhanced by or\nrequires interactions with an external environment, such\nas a game or API. This technique constructs an action\nspace \u02c6A=A\u222aZthat adds permissible actions a\u2208A\nto the reasoning traces z\u2208Zfrom CoT. Observations o\nfrom the environment are used to improve both reasoning\nand acting. To solve problems with ReAct, after each ob-\nservation, actions are generated from p\u03b8sequentially as\nai\u223cpReAct\n\u03b8 (x, o1\u00b7\u00b7\u00b7i\u22121, a1\u00b7\u00b7\u00b7i\u22121), with the final output be-\n3Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\ningy\u223cpReAct\n\u03b8 (x, o1\u00b7\u00b7\u00b7l, a1\u00b7\u00b7\u00b7l). In this paper, consistent\nwith other LM agent methods such as ReAct and Reflexion\n(Shinn et al., 2023), we focus on decision-making tasks\nwhere reverting between iterations is feasible .\nWhile the previously described prompting techniques im-\nprove LM performance on reasoning tasks, they falter on\ndifficult tasks that involve multifaceted decision-making\ndue to several shortcomings: 1) Flexibility : Base prompting\ndesigns (CoT or ReAct) autoregressively sample from the\nLM, neglecting potential alternative continuations from spe-\ncific states. 2) Sensibility : Reasoning-based methods (CoT,\nRAP (Hao et al., 2023), or ToT) rely solely on the inter-\nnal representations of the LM and cannot consider external\nobservations. This dependency risks fact hallucination and\nerror propagation while setting a performance ceiling. 3)\nAdaptability : Current planning strategies (RAP or ToT) use\nsimple search algorithms such as BFS or cannot leverage\nenvironmental feedback to improve planning. Additionally,\nthe agent is static and cannot reuse previous experience or\nlearn from trial and error. While RAP also adopts MCTS, it\nis constrained to tasks where the LM can become a world\nmodel and accurately predict states. These shortcomings\nlimit the ability of LMs to be deployed as general problem-\nsolving agents and form the motivation for LATS.\n3.2. Monte Carlo Tree Search (MCTS)\nMonte Carlo Tree Search (MCTS) is a heuristic search al-\ngorithm that is proved successful on many decision-making\nenvironments, such as Atari (Ye et al., 2021) and Go (Silver\net al., 2016). MCTS builds a decision tree where every node\nin the tree is a state and edge is an action. MCTS runs for k\nepisodes; for each episode, it starts from the root (i.e., initial\nstate) and iteratively conducts two steps to expand the tree:\n1)Expansion , where multiple children states sare explored\nfrom the current parent state pby sampling nactions, and 2)\nSelection , where the children with the highest UCT (Upper\nConfidence bounds applied to Trees) (Kocsis and Szepesv \u00b4ari,\n2006) value is selected for expansion by the next iteration.\nThe UCT of a child state sis calculated as follows:\nUCT (s) =V(s) +ws\nlnN(p)\nN(s), (1)\nwhere N(s)is the number of visits to a node s,V(s)is the\nvalue function (expected return) from the subtree of s,wis\nthe exploration weight, and pis the parent node of s. When\nthe end of an episode is reached, a backpropagation is car-\nried out: the return ris used for updating every V(s)along\nthe path with the formula V(s) =Vold(s)(N(s)\u22121)+r\nN(s), where\nVold(s)is the old value function. Normally, the major short-\ncoming of","chunk_id":"9bb90746134619cad9a3e649b8b35f24","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"CHAIN-OF-THOUGHT PROMPTING","type":"TECHNIQUE","description":"Chain-of-thought (CoT) prompting is a method that enhances reasoning by creating intermediate thoughts that connect input queries to outputs, particularly in complex scenarios like mathematical problems.","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"TREE-OF-THOUGHT PROMPTING","type":"TECHNIQUE","description":"Tree-of-thought (ToT) prompting extends CoT prompting by exploring multiple reasoning paths in a tree structure, where each node represents a partial solution state.","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"REACT","type":"TECHNIQUE","description":"ReAct is a technique that enhances language models by incorporating interactions with external environments, allowing for improved reasoning and acting in decision-making tasks.","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"MONTE CARLO TREE SEARCH (MCTS)","type":"ALGORITHM","description":"Monte Carlo Tree Search (MCTS) is a heuristic search algorithm used in decision-making environments, building a decision tree to explore states and actions iteratively.","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"DECISION-MAKING TASKS","type":"TASK TYPE","description":"Decision-making tasks involve scenarios where choices must be made based on reasoning, often requiring complex problem-solving strategies.","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"HEURISTICS","type":"CONCEPT","description":"Heuristics are strategies or methods used to guide decision-making and problem-solving, often based on experience or rules of thumb.","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"ERROR PROPAGATION","type":"CONCEPT","description":"Error propagation refers to the phenomenon where errors in reasoning or decision-making can lead to further inaccuracies in subsequent steps or outputs.","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"OUTPUT","type":"DATA FORMAT","description":"Output refers to the result generated by a language model in response to a given input prompt, often representing the final answer or solution.","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"LANGUAGE MODEL (LM)","type":"TECHNOLOGY","description":"A language model (LM) is an AI system designed to understand and generate human-like text based on input data and prompts.","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"THOUGHTS","type":"CONCEPT","description":"Thoughts are intermediate language sequences generated during the reasoning process, serving as steps between the input and output in prompting techniques.","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"SEARCH ALGORITHMS","type":"CONCEPT","description":"Search algorithms are methods used to explore possible solutions or paths in decision-making processes, such as depth-first search (DFS) and breadth-first search (BFS).","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"DECISION TREE","type":"DATA STRUCTURE","description":"A decision tree is a model used to represent decisions and their possible consequences, structured as a tree with nodes representing states and edges representing actions.","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"INPUT PROMPT","type":"","description":"","source_id":"9bb90746134619cad9a3e649b8b35f24"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"CHAIN-OF-THOUGHT PROMPTING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain-of-thought (CoT) prompting is a method that enhances reasoning by creating intermediate thoughts that connect input queries to outputs, particularly in complex scenarios like mathematical problems.<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"TREE-OF-THOUGHT PROMPTING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Tree-of-thought (ToT) prompting extends CoT prompting by exploring multiple reasoning paths in a tree structure, where each node represents a partial solution state.<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">ReAct is a technique that enhances language models by incorporating interactions with external environments, allowing for improved reasoning and acting in decision-making tasks.<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"MONTE CARLO TREE SEARCH (MCTS)\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">Monte Carlo Tree Search (MCTS) is a heuristic search algorithm used in decision-making environments, building a decision tree to explore states and actions iteratively.<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"DECISION-MAKING TASKS\">      <data key=\"d0\">TASK TYPE<\/data>      <data key=\"d1\">Decision-making tasks involve scenarios where choices must be made based on reasoning, often requiring complex problem-solving strategies.<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"HEURISTICS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Heuristics are strategies or methods used to guide decision-making and problem-solving, often based on experience or rules of thumb.<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"ERROR PROPAGATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Error propagation refers to the phenomenon where errors in reasoning or decision-making can lead to further inaccuracies in subsequent steps or outputs.<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"OUTPUT\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Output refers to the result generated by a language model in response to a given input prompt, often representing the final answer or solution.<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"LANGUAGE MODEL (LM)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A language model (LM) is an AI system designed to understand and generate human-like text based on input data and prompts.<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"THOUGHTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Thoughts are intermediate language sequences generated during the reasoning process, serving as steps between the input and output in prompting techniques.<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"SEARCH ALGORITHMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Search algorithms are methods used to explore possible solutions or paths in decision-making processes, such as depth-first search (DFS) and breadth-first search (BFS).<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"DECISION TREE\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">A decision tree is a model used to represent decisions and their possible consequences, structured as a tree with nodes representing states and edges representing actions.<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"INPUT PROMPT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <edge source=\"CHAIN-OF-THOUGHT PROMPTING\" target=\"REACT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Chain-of-thought prompting and ReAct both aim to improve reasoning capabilities in language models, but ReAct incorporates external interactions for enhanced performance.<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT PROMPTING\" target=\"TREE-OF-THOUGHT PROMPTING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Tree-of-thought prompting builds upon chain-of-thought prompting by exploring multiple reasoning paths, enhancing the complexity of the reasoning process.<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT PROMPTING\" target=\"ERROR PROPAGATION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Error propagation can occur in chain-of-thought prompting if earlier reasoning steps lead to incorrect conclusions, affecting the final output.<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT PROMPTING\" target=\"LANGUAGE MODEL (LM)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The language model utilizes chain-of-thought prompting to enhance its reasoning capabilities by generating intermediate thoughts.<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT PROMPTING\" target=\"THOUGHTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Thoughts are generated as part of the chain-of-thought prompting process, acting as stepping stones between input and output.<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"REACT\" target=\"DECISION-MAKING TASKS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">ReAct is applied in decision-making tasks where interaction with an external environment is necessary for effective reasoning and action.<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"MONTE CARLO TREE SEARCH (MCTS)\" target=\"DECISION-MAKING TASKS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">MCTS is specifically designed for decision-making tasks, providing a structured approach to explore possible actions and outcomes.<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"MONTE CARLO TREE SEARCH (MCTS)\" target=\"HEURISTICS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Heuristics guide the search process in MCTS, helping to evaluate the potential of different states and actions in the decision tree.<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"MONTE CARLO TREE SEARCH (MCTS)\" target=\"SEARCH ALGORITHMS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Search algorithms are integral to MCTS, guiding the exploration of the decision tree to find optimal actions.<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"MONTE CARLO TREE SEARCH (MCTS)\" target=\"DECISION TREE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">MCTS builds a decision tree to represent states and actions in decision-making environments, facilitating the search for solutions.<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"OUTPUT\" target=\"INPUT PROMPT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The input prompt is transformed into an output by the language model, representing the model's response to the provided instructions or examples.<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"c234cb83764b899335af0950677ad024","chunk":" (expected return) from the subtree of s,wis\nthe exploration weight, and pis the parent node of s. When\nthe end of an episode is reached, a backpropagation is car-\nried out: the return ris used for updating every V(s)along\nthe path with the formula V(s) =Vold(s)(N(s)\u22121)+r\nN(s), where\nVold(s)is the old value function. Normally, the major short-\ncoming of MCTS is that it requires an environment model to\nundo previous steps and form a searching tree, which could\nbe a strong assumption. However, this limitation does not\nexist for many LM tasks, as we can conveniently reset toany step by simply copy-pasting historical text input. Such\na special property is the key motivation of our work.\n4. Unifying Reasoning, Acting, and Planning\n4.1. LM Agent\nDepending on the base prompting framework design, LATS\nsupports sequential reasoning or decision-making tasks. At\ntime step t, an agent receives an observation ot\u2208Ofrom\nthe environment and takes an action at\u2208Afollowing some\npolicy \u03c0(at|x, o1\u00b7\u00b7\u00b7t\u22121, a1\u00b7\u00b7\u00b7t\u22121). We initialize the agent\nwithp\u03b8to leverage the useful language representations of\nan LM as a base decision-maker. We follow the ReAct in-\nstantiation, in which the action space \u02c6A=A\u222aZconsists\nof both the space of permissible actions Aand the language\nspace of reasoning traces Z. Actions directly affect the envi-\nronment and result in observation, while thoughts are used\nto formalize decisions by organizing information, planning\nfuture actions, or injecting internal knowledge. The exact\ninstantiation of the action space depends on the particular\nenvironment \u2013 for decision-making tasks actions might con-\nsist of commands on a website, while for reasoning tasks\nthe action space might be limited to a few external tools or\nAPIs. In environments without feedback, such as reasoning\ntasks, we use CoT as the base prompting framework.\nInstead of greedily decoding one trajectory or solution, we\nsample nactions from p\u03b8using the current state. This is\nbased on the intuition that for complex decision-making\ntasks, there is likely to be a range of potential trajectories or\nreasoning paths that are correct (Evans, 2010). Sampling a\ndiverse set of candidates at each step mitigates the stochastic\nnature of LM text generation and enables greater exploration\nin both the decision-making and reasoning space. We wrap\np\u03b8within our proposed search algorithm to deliberately\nconstruct the best trajectory from sampled actions.\n4.2. LATS\nThe main component of LATS is a search algorithm that\ncontrols the problem-solving process with planning. To find\nthe most promising trajectory and systemically balance ex-\nploration with exploitation, we adopt a variant of MCTS that\nframes decision-making as a tree search, in which each node\ns= [x, a1\u00b7\u00b7\u00b7i, o1\u00b7\u00b7\u00b7i]represents a state comprising the origi-\nnal input x, action sequence a1\u00b7i, and observation sequence\no1\u00b7i, where iis a token in the text sequence.\nOur main technical contribution is adapting MCTS to lan-\nguage agents . LATS repurposes p\u03b8as an agent, state evalua-\ntor, and feedback generator, leveraging the useful language\nrepresentations of modern LMs to facilitate planning. While\nstandard MCTS and RAP (Hao et al., 2023) rely on internal\ndynamics models to facilitate simulation, LATS uses envi-\nronment interaction and does not require a world model. As\n4Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nFigure 2. Overview of the six operations in LATS. A node is selected ,expanded ,evaluated , then simulated until a terminal node is reached,\nand then the resulting value is backpropagated . If the trajectory fails, a reflection is generated and used as additional context for future\ntrials. These operations are performed in succession until the budget is reached or the task is successful.\ndepicted in Fig. 2, LATS consists of a series of operations\n\u2013selection, expansion, evaluation, simulation, backpropa-\ngation, and reflection \u2013 performed in succession until the\ntask is successfully completed or a computational limit is\nreached after sampling ktrajectories. The full pseudocode\nof LATS can be found in Sec. A in the Appendix.\nSelection. In the first operation, the algorithm identifies\na segment of the current tree most suitable for subsequent\nexpansion. Starting from the root node, denoted as the initial\nstates0, a child node is selected at each tree level until a leaf\nnode is reached. To balance exploration and exploitation,\nwe use the UCT algorithm as shown in Eq. 1.\nExpansion. After selecting a node, the second operation\nexpands the tree by sampling nactions from p\u03b8, as described\nin the prior section. The environment receives each action\nand returns corresponding feedback as an observation. This\nresults in nnew child nodes added to the tree. This tree is\nstored in an external long-term memory structure.\nEvaluation. The third operation assigns a scalar value to\neach new child node for selection and backpropagation.\nThis value effectively quantifies the agent\u2019s progress in task\ncompletion, serving as a heuristic to steer the search algo-\nrithm towards the most promising regions of the tree. As\nLATS does not involve training, we propose a novel value\nfunction for this setting based on two components: (1) a\nself-generated LM score and (2)","chunk_id":"c234cb83764b899335af0950677ad024","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"MCTS","type":"TECHNIQUE","description":"Monte Carlo Tree Search (MCTS) is a heuristic search algorithm used for decision-making processes, particularly in environments requiring exploration and exploitation strategies.","source_id":"c234cb83764b899335af0950677ad024"},{"name":"LM AGENT","type":"TECHNOLOGY","description":"The LM Agent is a decision-making entity that utilizes language models to perform reasoning and action-taking tasks based on observations from its environment.","source_id":"c234cb83764b899335af0950677ad024"},{"name":"LATS","type":"TECHNIQUE","description":"LATS (Language Agent Tree Search) is a search algorithm that integrates reasoning, acting, and planning in language models, adapting MCTS for language agents.","source_id":"c234cb83764b899335af0950677ad024"},{"name":"POLICY \u03a0","type":"CONCEPT","description":"The policy \u03c0 defines the strategy that the LM Agent follows to take actions based on its observations and previous actions.","source_id":"c234cb83764b899335af0950677ad024"},{"name":"ENVIRONMENT","type":"CONCEPT","description":"The environment is the context in which the LM Agent operates, providing observations and feedback based on the actions taken by the agent.","source_id":"c234cb83764b899335af0950677ad024"},{"name":"UCT ALGORITHM","type":"TECHNIQUE","description":"The Upper Confidence Bound for Trees (UCT) algorithm is used in MCTS to balance exploration and exploitation during the selection of nodes in the search tree.","source_id":"c234cb83764b899335af0950677ad024"},{"name":"REFLECTION","type":"PROCESS","description":"Reflection is a process in LATS where feedback from failed trajectories is used to inform future decision-making and improve performance.","source_id":"c234cb83764b899335af0950677ad024"},{"name":"VALUE FUNCTION","type":"CONCEPT","description":"The value function is a mathematical representation that quantifies the expected return of states in the search tree, guiding the search algorithm towards promising areas.","source_id":"c234cb83764b899335af0950677ad024"},{"name":"OBSERVATION","type":"DATA FORMAT","description":"An observation is the information received by the LM Agent from the environment after taking an action, which influences subsequent decisions.","source_id":"c234cb83764b899335af0950677ad024"},{"name":"ACTIONS","type":"DATA FORMAT","description":"Actions are the decisions made by the LM Agent based on its policy, which directly affect the environment and lead to new observations.","source_id":"c234cb83764b899335af0950677ad024"},{"name":"SEARCH ALGORITHM","type":"TECHNIQUE","description":"A search algorithm is a systematic method used to explore possible actions and states in order to find optimal solutions or trajectories in decision-making tasks.","source_id":"c234cb83764b899335af0950677ad024"},{"name":"BACKPROPAGATION","type":"","description":"","source_id":"c234cb83764b899335af0950677ad024"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"MCTS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Monte Carlo Tree Search (MCTS) is a heuristic search algorithm used for decision-making processes, particularly in environments requiring exploration and exploitation strategies.<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"LM AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The LM Agent is a decision-making entity that utilizes language models to perform reasoning and action-taking tasks based on observations from its environment.<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LATS (Language Agent Tree Search) is a search algorithm that integrates reasoning, acting, and planning in language models, adapting MCTS for language agents.<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"POLICY &#928;\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The policy &#960; defines the strategy that the LM Agent follows to take actions based on its observations and previous actions.<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"ENVIRONMENT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The environment is the context in which the LM Agent operates, providing observations and feedback based on the actions taken by the agent.<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"UCT ALGORITHM\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Upper Confidence Bound for Trees (UCT) algorithm is used in MCTS to balance exploration and exploitation during the selection of nodes in the search tree.<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"REFLECTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Reflection is a process in LATS where feedback from failed trajectories is used to inform future decision-making and improve performance.<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"VALUE FUNCTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The value function is a mathematical representation that quantifies the expected return of states in the search tree, guiding the search algorithm towards promising areas.<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"OBSERVATION\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">An observation is the information received by the LM Agent from the environment after taking an action, which influences subsequent decisions.<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"ACTIONS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Actions are the decisions made by the LM Agent based on its policy, which directly affect the environment and lead to new observations.<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"SEARCH ALGORITHM\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A search algorithm is a systematic method used to explore possible actions and states in order to find optimal solutions or trajectories in decision-making tasks.<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"BACKPROPAGATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <edge source=\"MCTS\" target=\"LATS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS adapts MCTS to create a search algorithm specifically designed for language agents, integrating reasoning and planning capabilities.<\/data>      <data key=\"d5\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"MCTS\" target=\"UCT ALGORITHM\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The UCT algorithm is a fundamental component of MCTS, used to guide the selection of nodes in the search tree based on exploration and exploitation balance.<\/data>      <data key=\"d5\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"ENVIRONMENT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The LM Agent interacts with the environment, receiving observations and feedback that guide its decision-making process.<\/data>      <data key=\"d5\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"LATS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS utilizes the LM Agent as a core component to facilitate decision-making and planning through language model representations.<\/data>      <data key=\"d5\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"POLICY &#928;\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The policy &#960; is employed by the LM Agent to determine the actions it should take based on its observations and previous actions.<\/data>      <data key=\"d5\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"ACTIONS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The LM Agent takes actions based on its observations and policy, which influence the state of the environment.<\/data>      <data key=\"d5\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REFLECTION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Reflection is a key operation in LATS that helps improve the search algorithm by using feedback from previous attempts to enhance future performance.<\/data>      <data key=\"d5\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SEARCH ALGORITHM\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">LATS is a specific type of search algorithm that integrates reasoning, acting, and planning for language models, enhancing decision-making capabilities.<\/data>      <data key=\"d5\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION\" target=\"BACKPROPAGATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Backpropagation is used to update the value function based on the returns received, ensuring that the search algorithm learns from past experiences.<\/data>      <data key=\"d5\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"OBSERVATION\" target=\"ACTIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Each action taken by the LM Agent results in an observation from the environment, which is crucial for the agent's decision-making process.<\/data>      <data key=\"d5\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"02ef0185bbeaaef92c3a8ee18b7a38cf","chunk":" long-term memory structure.\nEvaluation. The third operation assigns a scalar value to\neach new child node for selection and backpropagation.\nThis value effectively quantifies the agent\u2019s progress in task\ncompletion, serving as a heuristic to steer the search algo-\nrithm towards the most promising regions of the tree. As\nLATS does not involve training, we propose a novel value\nfunction for this setting based on two components: (1) a\nself-generated LM score and (2) a self-consistency score.\nInspired by ToT, we repurpose p\u03b8into a value function by\nprompting it to reason about a given state. To obtain a scalar\nvalue, we instruct p\u03b8to end its reasoning trace with a score\nindicating the correctness of the trajectory. Our key distinc-\ntion from ToT is that we obtain this value after obtaining\nthe environmental feedback, improving value assignment.\nThis also enables scaling to more challenging environments,as it is difficult for LMs to improve their responses with-\nout external feedback (Huang et al., 2024). Additionally,\nto further improve value assignment, we introduce an ad-\nditional heuristic based on self-consistency (Wang et al.,\n2022), in which actions sampled multiple times at the same\nstate tend to be more accurate. This results in the overall\nvalue function:\nV(s) =\u03bb\u2217LM(s) + (1\u2212\u03bb)\u2217SC(s), (2)\nwhere \u03bbis a hyperparameter. Notably, our method offers\nenhanced flexibility over programmed heuristics (Campbell\net al., 2002) and greater efficiency than learned heuristics\n(Silver et al., 2017).\nSimulation. The fourth operation expands the currently se-\nlected node until a terminal state is reached. At each depth\nlevel, we sample and evaluate nodes with the same opera-\ntions but prioritize nodes of the highest value. Reaching a\nterminal state provides objective feedback on the correct-\nness of a trajectory. If the task is completed successfully,\nthen LATS terminates the search. If the solution is partially\nsuccessful or unsuccessful, then we perform two additional\noperations as described below. The success of a trajectory is\ndetermined by the design of the specific environment, such\nas finalizing a purchase in web navigation environments.\nBackpropagation. This operation updates the values of the\ntree based on the outcome of a trajectory. For each node\ns0, s1, . . . , s lin the trajectory from root (initial state s0)\nof the searching tree to leaf (terminal state sl), its value is\nupdated to reflect the outcome of the simulation by N(si) =\nN(si\u22121)+1 andV(si) =V(si\u22121)N(si\u22121)+r\nN(si), where ris the\nreward. These updated values are used in the UCT formula\n(Eq. 1) to guide the selection of the next node.\nReflection. In addition to the environmental feedback, we\nleverage self-reflection to further refine the decision-making\n5Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nPrompt Method HotpotQA (EM) \u2191\nBase LM 0.32\nCoT (Wei et al., 2022) 0.34\nCoT - SC (Wang et al., 2022) 0.38\nToT (Yao et al., 2023a) 0.55\nRAP (Hao et al., 2023) 0.60\nRAP ( n= 10 ) 0.60\nLATS (CoT) 0.62\nTable 2. GPT-3.5 reasoning -based prompting results on HotpotQA.\nLATS achieves the highest exact match (EM) for reasoning. We\nsample n= 5nodes during expansion and k= 50 trajectories.\nprocess (Shinn et al., 2023; Madaan et al., 2023). Upon\nencountering an unsuccessful terminal node, p\u03b8is prompted\nwith the trajectory and final reward to provide a verbal self-\nreflection that summarizes the errors in the reasoning or\nacting process and proposes superior alternatives. We store\nboth failed trajectories and corresponding reflections in the\nmemory. In subsequent iterations, these are integrated as\nadditional context to the agent and value function, refining\nboth through in-context learning. This imparts a semantic\ngradient signal more useful than a scalar value, enabling\nthe agent to learn from trial and error without the cost of\nexpensive optimization such as reinforcement learning.\nDiscussion. Conceptually, LATS has several notable advan-\ntages as a general framework for reasoning and decision-\nmaking with LM agents. (1) Generality : LATS supports\nboth reasoning and decision-making tasks by defining a\nshared space of thoughts and actions. (2) Deliberation :\nLeveraging MCTS and LM value function in LATS en-\nsures a principled search that selects options with high value\nwhile exploring promising alternatives. (3) Adaptability :\nIncorporating external feedback through observations and\nself-reflection in LATS enables greater adaptation during\nproblem-solving. (4) Flexibility : LATS can accommodate\ndifferent scenarios, environments, and resource stipulations\nby modifying state design and tree dimensions. (5) Modu-\nlarity : The base LM agent, reflection generator, and value\nfunction can be independently altered and adapted to indi-\nvidual LM properties.\n5. Experiments\nTo demonstrate the general applicability of LATS, we eval-\nuate our method on a variety of domains that require rea-\nsoning and acting: programming (Chen et al., 2021; Austin\net al., 202","chunk_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LATS","type":"TECHNIQUE","description":"LATS is a novel framework for language models that integrates reasoning, acting, and planning through a tree search mechanism, enhancing decision-making and adaptability in various environments.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"VALUE FUNCTION","type":"CONCEPT","description":"The value function in LATS quantifies the agent's progress in task completion, combining a self-generated LM score and a self-consistency score to guide decision-making.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"SELF-CONSISTENCY SCORE","type":"CONCEPT","description":"The self-consistency score is a heuristic used in LATS to improve value assignment by evaluating actions sampled multiple times at the same state for accuracy.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"HYPERPARAMETER","type":"CONCEPT","description":"A hyperparameter in LATS, denoted as \u03bb, is used to balance the contributions of the LM score and self-consistency score in the overall value function.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"MCTS","type":"TECHNIQUE","description":"Monte Carlo Tree Search (MCTS) is a search algorithm utilized in LATS to explore and select options with high value while considering promising alternatives.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"ENVIRONMENTAL FEEDBACK","type":"CONCEPT","description":"Environmental feedback in LATS provides objective assessments of the correctness of trajectories, influencing the agent's learning and decision-making.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"SELF-REFLECTION","type":"CONCEPT","description":"Self-reflection in LATS allows the agent to analyze unsuccessful outcomes and propose better alternatives, enhancing learning from errors.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"TRIAL AND ERROR","type":"CONCEPT","description":"Trial and error is a learning process in LATS where the agent learns from past experiences without the need for expensive optimization methods like reinforcement learning.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a language model that serves as the base for LATS, providing reasoning capabilities and serving as a benchmark for performance evaluation.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"HOTPOTQA","type":"DATASET","description":"HotpotQA is a dataset used to evaluate the reasoning capabilities of language models, including LATS, by measuring exact match scores.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"SCALAR VALUE","type":"CONCEPT","description":"A scalar value in LATS is assigned to each new child node to quantify the agent's progress in task completion, serving as a heuristic for guiding the search algorithm.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"SEARCH ALGORITHM","type":"CONCEPT","description":"The search algorithm in LATS is responsible for navigating the decision tree, utilizing heuristics and feedback to select the most promising paths for task completion.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"TERMINAL STATE","type":"CONCEPT","description":"A terminal state in LATS is the endpoint of a trajectory where the outcome of the task is evaluated, providing feedback for the agent's learning process.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"TRAJECTORY","type":"CONCEPT","description":"A trajectory in LATS refers to the path taken by the agent through the decision tree, from the initial state to a terminal state, which is evaluated for success or failure.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"REWARD","type":"CONCEPT","description":"A reward in LATS is a feedback signal received by the agent upon reaching a terminal state, influencing the value updates of nodes in the search tree.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"UCT FORMULA","type":"CONCEPT","description":"The Upper Confidence Bound for Trees (UCT) formula is used in LATS to guide the selection of the next node based on updated values and exploration strategies.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"PROGRAMMED HEURISTICS","type":"CONCEPT","description":"Programmed heuristics are predefined strategies used in decision-making processes, which LATS aims to improve upon with its flexible and adaptive approach.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"LEARNED HEURISTICS","type":"CONCEPT","description":"Learned heuristics are strategies developed through training, which LATS seeks to outperform in terms of efficiency and adaptability in various scenarios.","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"AGENT","type":"","description":"","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LATS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LATS is a novel framework for language models that integrates reasoning, acting, and planning through a tree search mechanism, enhancing decision-making and adaptability in various environments.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"VALUE FUNCTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The value function in LATS quantifies the agent's progress in task completion, combining a self-generated LM score and a self-consistency score to guide decision-making.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"SELF-CONSISTENCY SCORE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The self-consistency score is a heuristic used in LATS to improve value assignment by evaluating actions sampled multiple times at the same state for accuracy.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"HYPERPARAMETER\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A hyperparameter in LATS, denoted as &#955;, is used to balance the contributions of the LM score and self-consistency score in the overall value function.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"MCTS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Monte Carlo Tree Search (MCTS) is a search algorithm utilized in LATS to explore and select options with high value while considering promising alternatives.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"ENVIRONMENTAL FEEDBACK\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Environmental feedback in LATS provides objective assessments of the correctness of trajectories, influencing the agent's learning and decision-making.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"SELF-REFLECTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Self-reflection in LATS allows the agent to analyze unsuccessful outcomes and propose better alternatives, enhancing learning from errors.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"TRIAL AND ERROR\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Trial and error is a learning process in LATS where the agent learns from past experiences without the need for expensive optimization methods like reinforcement learning.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model that serves as the base for LATS, providing reasoning capabilities and serving as a benchmark for performance evaluation.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotpotQA is a dataset used to evaluate the reasoning capabilities of language models, including LATS, by measuring exact match scores.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"SCALAR VALUE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A scalar value in LATS is assigned to each new child node to quantify the agent's progress in task completion, serving as a heuristic for guiding the search algorithm.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"SEARCH ALGORITHM\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The search algorithm in LATS is responsible for navigating the decision tree, utilizing heuristics and feedback to select the most promising paths for task completion.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"TERMINAL STATE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A terminal state in LATS is the endpoint of a trajectory where the outcome of the task is evaluated, providing feedback for the agent's learning process.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"TRAJECTORY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A trajectory in LATS refers to the path taken by the agent through the decision tree, from the initial state to a terminal state, which is evaluated for success or failure.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"REWARD\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A reward in LATS is a feedback signal received by the agent upon reaching a terminal state, influencing the value updates of nodes in the search tree.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"UCT FORMULA\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The Upper Confidence Bound for Trees (UCT) formula is used in LATS to guide the selection of the next node based on updated values and exploration strategies.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"PROGRAMMED HEURISTICS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Programmed heuristics are predefined strategies used in decision-making processes, which LATS aims to improve upon with its flexible and adaptive approach.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"LEARNED HEURISTICS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Learned heuristics are strategies developed through training, which LATS seeks to outperform in terms of efficiency and adaptability in various scenarios.<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"AGENT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <edge source=\"LATS\" target=\"VALUE FUNCTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS utilizes a value function to quantify the agent's progress in task completion, guiding its decision-making process.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"MCTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS employs MCTS to explore and select high-value options during the decision-making process.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ENVIRONMENTAL FEEDBACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Environmental feedback is integral to LATS, providing assessments that influence the agent's learning and decision-making.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SELF-REFLECTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Self-reflection is a mechanism in LATS that allows the agent to learn from unsuccessful outcomes and improve future performance.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"TRIAL AND ERROR\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS incorporates a trial and error approach, enabling the agent to learn from past experiences without costly optimization methods.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-3.5\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">GPT-3.5 serves as the foundational language model for LATS, providing the necessary reasoning capabilities for its operations.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PROGRAMMED HEURISTICS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS aims to improve upon programmed heuristics by offering a more flexible and adaptive decision-making framework.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"LEARNED HEURISTICS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">LATS seeks to outperform learned heuristics in terms of efficiency and adaptability in various problem-solving scenarios.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION\" target=\"SELF-CONSISTENCY SCORE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The self-consistency score is a component of the value function in LATS, contributing to the overall assessment of the agent's performance.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION\" target=\"AGENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The agent in LATS utilizes the value function to assess its progress and make informed decisions during task execution.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION\" target=\"REWARD\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The reward received at a terminal state is used to update the value function, influencing the agent's future actions and decisions.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"AGENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The agent utilizes self-reflection to analyze its performance and improve its decision-making process based on past experiences.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"GPT-3.5\" target=\"HOTPOTQA\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">GPT-3.5 is evaluated on the HotpotQA dataset to measure its reasoning capabilities and performance in tasks.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"SEARCH ALGORITHM\" target=\"AGENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The agent employs a search algorithm to navigate through the decision tree, optimizing its path based on feedback and heuristics.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"SEARCH ALGORITHM\" target=\"TERMINAL STATE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The search algorithm in LATS aims to reach a terminal state, where the outcome of the task can be evaluated for success or failure.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"SEARCH ALGORITHM\" target=\"UCT FORMULA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The UCT formula guides the search algorithm in LATS, helping to select the next node based on updated values and exploration strategies.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"TERMINAL STATE\" target=\"TRAJECTORY\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">A trajectory in LATS culminates in a terminal state, providing feedback that influences future decision-making processes.<\/data>      <data key=\"d5\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"fb9cb0c0984d44c3da881886ed637e55","chunk":"ulations\nby modifying state design and tree dimensions. (5) Modu-\nlarity : The base LM agent, reflection generator, and value\nfunction can be independently altered and adapted to indi-\nvidual LM properties.\n5. Experiments\nTo demonstrate the general applicability of LATS, we eval-\nuate our method on a variety of domains that require rea-\nsoning and acting: programming (Chen et al., 2021; Austin\net al., 2022), HotPotQA (Yang et al., 2018), WebShop (Yao\net al., 2022), and Game of 24 (Yao et al., 2023a).Prompt Method HotpotQA (EM) \u2191\nReAct (Yao et al., 2023b) 0.32\nReAct (best of k) 0.38\nReflexion (Shinn et al., 2023) 0.51\nToT (ReAct) 0.39\nRAP (ReAct) 0.54\nLATS (ReAct) 0.63\nLATS ( n= 3) 0.58\nLATS ( n= 10 ) 0.65\nLATS (CoT + ReAct) 0.71\nTable 3. GPT-3.5 acting -based prompting results on HotpotQA.\nLATS achieves the highest exact match (EM) for acting. We\nsample n= 5nodes and use k= 50 trajectories. We also evaluate\nsampling ReAct ktimes and using both CoT and ReAct base\nprompting designs for LATS, which achieves the best performance.\nNote that LATS outperforms ToT and RAP with ReAct prompting,\nwhich are the simple adaptations of search algorithms to decision-\nmaking.\n5.1. HotPotQA\nFor a task that can be approached with both reasoning-based\nand acting-based strategies, we consider HotPotQA (Yang\net al., 2018), a multi-hop question-answering benchmark\nthat requires retrieval over two or more Wikipedia passages.\nFor the action space, in addition to LM thoughts, we follow\nthe setup from Yao et al. (2023b), which provides the agent\nwith API calls to search and retrieve information. The output\nof these API calls and self-generated reflections form the\nobservation space. Note that consistent with previous work\n(Yao et al., 2023b; Shinn et al., 2023), we use an oracle\nsetup for HotPotQA, in which the environment provides\nfeedback about the answer\u2019s correctness upon receiving an\nanswer. This enables a fair comparison between our method\nand baselines in scenarios where the quality of feedback is\nhigh, allowing us to focus our evaluation on how well the\nagent incorporates external feedback. We use a subset of\n100 questions and three few-shot examples for each method.\nFor ToT, we use DFS as the base search algorithm. For all\nmethods that involve sampling, including LATS, we sample\nk= 50 trajectories. More details are in Appendix Sec. D.\nWe evaluate internal reasoning strategies by removing ac-\ntions and observations from the context, corresponding to\nCoT (Wei et al., 2022) and its variants, CoT-SC (Wang et al.,\n2022), ToT (Yao et al., 2023a), and RAP (Hao et al., 2023).\nThese methods rely solely on the agent\u2019s existing knowledge\nto answer the question. We further consider acting-based\nmethods ReAct, Reflexion, and LATS, which augment the\nagent with the interactive API environment and primarily\nevaluate its information retrieval abilities. We also design\na simple integration of search algorithms with LM agents,\nextending ToT and RAP with ReAct prompting to handle\n6Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nPrompt Method Model Pass@1 \u2191\nCoT (Wei et al., 2022) GPT-3.5 46.9\nReAct (Yao et al., 2023b) GPT-3.5 56.9\nReflexion (Shinn et al., 2023) GPT-3.5 68.1\nToT (Yao et al., 2023a) GPT-3.5 54.4\nRAP (Hao et al., 2023) GPT-3.5 63.1\nLATS (ReAct) GPT-3.5 83.8\nBase LM GPT-4 80.1\nReflexion GPT-4 91.0\nLATS (ReAct) GPT-4 92.7\nTable 4. GPT-3.5 and GPT-4 Pass@1 accuracy on HumanEval.\nPrompting with LATS achieves the best performance. We sample\n5 solutions during expansion for 8 iterations.\nexternal observations. In addition, while LATS is designed\nfor scenarios where external feedback can enhance reason-\ning, we also implement a reasoning-only version with CoT\nas the base prompting framework. Moreover, we combine\ninternal and external reasoning in LATS by first prompting\nwith a CoT-based prompt and then switching to a ReAct-\nbased prompt upon failure. This is closer to how humans\nmight approach this task by using tools to retrieve additional\ninformation only when the answer is not already known.\nResults. We observe in Tab. 2 and Tab. 3 that both in-\nternal reasoning and external retrieval strategies perform\nwell on HotPotQA. Due to their large-scale training corpus,\nmodern LMs already encode factual","chunk_id":"fb9cb0c0984d44c3da881886ed637e55","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LATS","type":"TECHNIQUE","description":"LATS is a method designed for reasoning and acting in various domains, utilizing language models to enhance performance in tasks such as question answering and programming.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a multi-hop question-answering benchmark that requires retrieval over multiple Wikipedia passages to answer questions.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a language model developed by OpenAI, used in various prompting methods to evaluate performance in tasks like HotPotQA.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a more advanced language model developed by OpenAI, showing improved performance in tasks compared to its predecessor, GPT-3.5.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"REACT","type":"METHOD","description":"ReAct is a prompting method that combines reasoning and acting strategies to enhance the performance of language models in various tasks.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"REFLEXION","type":"METHOD","description":"Reflexion is a prompting method that focuses on enhancing the reasoning capabilities of language models through self-reflection.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"TO","type":"METHOD","description":"ToT is a prompting method that adapts search algorithms for decision-making in language models.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"RAP","type":"METHOD","description":"RAP is a prompting method that integrates reasoning and acting strategies to improve language model performance.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"API CALLS","type":"TECHNIQUE","description":"API calls are used in the context of language models to search and retrieve information, enhancing the model's ability to answer questions.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"CO","type":"METHOD","description":"CoT is a prompting method that focuses on reasoning based on the existing knowledge of language models.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"YAO ET AL. (2023)","type":"PERSON","description":"Yao et al. (2023) is a reference to a study that discusses various prompting methods and their performance in language models.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"SHINN ET AL. (2023)","type":"PERSON","description":"Shinn et al. (2023) is a reference to a study that evaluates the performance of language models using the Reflexion method.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"CHEN ET AL. (2021)","type":"PERSON","description":"Chen et al. (2021) is a reference to a study that discusses the application of language models in programming tasks.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"AUSTIN ET AL. (2022)","type":"PERSON","description":"Austin et al. (2022) is a reference to a study that evaluates language models in various domains.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"YANG ET AL. (2018)","type":"PERSON","description":"Yang et al. (2018) is a reference to a study that discusses the HotPotQA benchmark.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"YAO ET AL. (2022)","type":"PERSON","description":"Yao et al. (2022) is a reference to a study that evaluates language models in the WebShop domain.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"YAO ET AL. (2023A)","type":"PERSON","description":"Yao et al. (2023a) is a reference to a study that discusses the Game of 24 and its relation to language models.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"EXTERNAL FEEDBACK","type":"CONCEPT","description":"External feedback refers to information provided to the language model agent about the correctness of its answers, which can enhance its reasoning capabilities.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"INTERNAL REASONING","type":"CONCEPT","description":"Internal reasoning involves the language model's ability to generate answers based solely on its existing knowledge without external input.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"OBSERVATION SPACE","type":"CONCEPT","description":"The observation space consists of the outputs from API calls and self-generated reflections that inform the language model's decision-making process.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"SAMPLE TRAJECTORIES","type":"DATA FORMAT","description":"Sample trajectories are paths or sequences of actions taken by the language model agent during its decision-making process, used for evaluation.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"FEEDBACK CORRECTNESS","type":"CONCEPT","description":"Feedback correctness refers to the accuracy of the information provided to the language model agent regarding the correctness of its answers.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"PROMPTING DESIGNS","type":"CONCEPT","description":"Prompting designs are structured formats or strategies used to guide the language model in generating responses or performing tasks.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"DECISION-MAKING","type":"CONCEPT","description":"Decision-making involves the process by which the language model agent selects actions based on reasoning and available information.","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"LANGUAGE MODEL AGENT","type":"","description":"","source_id":"fb9cb0c0984d44c3da881886ed637e55"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LATS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LATS is a method designed for reasoning and acting in various domains, utilizing language models to enhance performance in tasks such as question answering and programming.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a multi-hop question-answering benchmark that requires retrieval over multiple Wikipedia passages to answer questions.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model developed by OpenAI, used in various prompting methods to evaluate performance in tasks like HotPotQA.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a more advanced language model developed by OpenAI, showing improved performance in tasks compared to its predecessor, GPT-3.5.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">ReAct is a prompting method that combines reasoning and acting strategies to enhance the performance of language models in various tasks.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">Reflexion is a prompting method that focuses on enhancing the reasoning capabilities of language models through self-reflection.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"TO\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">ToT is a prompting method that adapts search algorithms for decision-making in language models.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"RAP\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">RAP is a prompting method that integrates reasoning and acting strategies to improve language model performance.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"API CALLS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">API calls are used in the context of language models to search and retrieve information, enhancing the model's ability to answer questions.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"CO\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">CoT is a prompting method that focuses on reasoning based on the existing knowledge of language models.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"YAO ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. (2023) is a reference to a study that discusses various prompting methods and their performance in language models.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"SHINN ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn et al. (2023) is a reference to a study that evaluates the performance of language models using the Reflexion method.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"CHEN ET AL. (2021)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen et al. (2021) is a reference to a study that discusses the application of language models in programming tasks.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"AUSTIN ET AL. (2022)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Austin et al. (2022) is a reference to a study that evaluates language models in various domains.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"YANG ET AL. (2018)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang et al. (2018) is a reference to a study that discusses the HotPotQA benchmark.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"YAO ET AL. (2022)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. (2022) is a reference to a study that evaluates language models in the WebShop domain.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"YAO ET AL. (2023A)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. (2023a) is a reference to a study that discusses the Game of 24 and its relation to language models.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"EXTERNAL FEEDBACK\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">External feedback refers to information provided to the language model agent about the correctness of its answers, which can enhance its reasoning capabilities.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"INTERNAL REASONING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Internal reasoning involves the language model's ability to generate answers based solely on its existing knowledge without external input.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"OBSERVATION SPACE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The observation space consists of the outputs from API calls and self-generated reflections that inform the language model's decision-making process.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"SAMPLE TRAJECTORIES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Sample trajectories are paths or sequences of actions taken by the language model agent during its decision-making process, used for evaluation.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"FEEDBACK CORRECTNESS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Feedback correctness refers to the accuracy of the information provided to the language model agent regarding the correctness of its answers.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"PROMPTING DESIGNS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Prompting designs are structured formats or strategies used to guide the language model in generating responses or performing tasks.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"DECISION-MAKING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Decision-making involves the process by which the language model agent selects actions based on reasoning and available information.<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"LANGUAGE MODEL AGENT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <edge source=\"LATS\" target=\"HOTPOTQA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS is evaluated on the HotPotQA benchmark to demonstrate its effectiveness in reasoning and acting tasks.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-3.5\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS uses GPT-3.5 for its prompting methods to evaluate performance in various tasks, including HotPotQA.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-4\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS also utilizes GPT-4, showing improved performance in tasks compared to GPT-3.5.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REACT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS incorporates the ReAct method to enhance its performance in reasoning and acting tasks.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REFLEXION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS can be compared with the Reflexion method to evaluate its reasoning capabilities.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"TO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS extends the ToT method by integrating it with ReAct prompting for better decision-making.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"RAP\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS integrates aspects of the RAP method to improve its reasoning and acting strategies.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"YAO ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Yao et al. (2023) provides insights relevant to the performance of LATS in various tasks.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SHINN ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Shinn et al. (2023) offers evaluations that can be compared with LATS's performance in reasoning tasks.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"CHEN ET AL. (2021)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Chen et al. (2021) discusses applications relevant to LATS's performance in programming tasks.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"AUSTIN ET AL. (2022)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Austin et al. (2022) evaluates language models in domains that may include applications of LATS.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"YAO ET AL. (2022)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Yao et al. (2022) provides insights into the WebShop domain that may relate to LATS's applications.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"YAO ET AL. (2023A)\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Yao et al. (2023a) discusses the Game of 24, which may relate to LATS's reasoning capabilities.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"API CALLS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">In the context of HotPotQA, API calls are utilized to enhance the agent's ability to retrieve information for answering questions.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"YANG ET AL. (2018)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yang et al. (2018) is a foundational reference for the HotPotQA benchmark, discussing its design and evaluation.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"EXTERNAL FEEDBACK\" target=\"LANGUAGE MODEL AGENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The language model agent utilizes external feedback to improve its reasoning and decision-making capabilities.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"INTERNAL REASONING\" target=\"LANGUAGE MODEL AGENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The language model agent employs internal reasoning to generate answers based on its existing knowledge.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"OBSERVATION SPACE\" target=\"LANGUAGE MODEL AGENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The observation space provides critical information to the language model agent for making informed decisions.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"SAMPLE TRAJECTORIES\" target=\"LANGUAGE MODEL AGENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Sample trajectories are used by the language model agent to evaluate its performance in decision-making tasks.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"FEEDBACK CORRECTNESS\" target=\"LANGUAGE MODEL AGENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Feedback correctness is essential for the language model agent to assess its performance and improve its responses.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"PROMPTING DESIGNS\" target=\"LANGUAGE MODEL AGENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Prompting designs guide the language model agent in generating appropriate responses for various tasks.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"DECISION-MAKING\" target=\"LANGUAGE MODEL AGENT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Decision-making is a core function of the language model agent, determining its actions based on reasoning and available data.<\/data>      <data key=\"d5\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"99d90aededb61e04241516ed9ec656cc","chunk":" in LATS by first prompting\nwith a CoT-based prompt and then switching to a ReAct-\nbased prompt upon failure. This is closer to how humans\nmight approach this task by using tools to retrieve additional\ninformation only when the answer is not already known.\nResults. We observe in Tab. 2 and Tab. 3 that both in-\nternal reasoning and external retrieval strategies perform\nwell on HotPotQA. Due to their large-scale training corpus,\nmodern LMs already encode factual knowledge and can\noften directly answer the question correctly. While CoT can\nslightly enhance performance on questions requiring rea-\nsoning, larger gains are observed with search methods ToT\nand RAP (Tab. 2, Row 4, 5), which can sample and explore\nmore outputs. We observe similar results for acting-based\nmethods. LATS surpasses ReAct, even when sampling the\nsame number of trajectories, by expanding more nodes with\nprincipled search. This is demonstrated when modifying\nn, the number of nodes expanded during each iteration. In-\ncreasing ncan consistently improve performance, although\nat greater computational and inference costs. LATS also\noutperforms RAP on internal reasoning, but has higher per-\nformance on the decision-making setting of HotPotQA than\nthe reasoning setting. Contrary to LATS, the ReAct versions\nof ToT and RAP (Tab. 3, Row 4, 5) perform even worse than\nthe reasoning-only setting of HotPotQA, which indicates\nthat the acting-based setting is more challenging and adap-\ntation of search algorithms to decision-making scenarios\nis non-trivial . Combining internal and external reasoning\nin LATS results in the highest performance, indicating the\nimportance of external feedback in augmenting reasoning\neven in tasks where the base LM can already perform.Prompt Method Pass@1 \u2191\nCoT (Wei et al., 2022) 54.9\nReAct (Wei et al., 2022) 67.0\nReflexion (Shinn et al., 2023) 70.0\nToT (Yao et al., 2023a) 65.8\nRAP (Hao et al., 2023) 71.4\nLATS (ReAct) 81.1\nTable 5. GPT-3.5 Pass@1 accuracy on MBPP. Prompting with\nLATS achieves the highest performance. We sample 5 solutions\nduring expansion for 8 iterations.\n5.2. Programming\nTo demonstrate the importance of external observations\nfor complex reasoning tasks, we evaluate the baselines\nand LATS on programming with HumanEval (Chen et al.,\n2021)1and MBPP (Austin et al., 2022). Both datasets mea-\nsure the correctness of synthesized programs in Python from\nnatural language docstrings. We use individual solutions\nas the action space and test suite and compiler feedback as\nthe external observation. We follow Chen et al. (2023a) and\nuse an LM to generate a synthetic test suite of syntactically\nvalid \u201cassert\u201d statements for each question. For each step,\nthe solution is evaluated on this test suite, and the results,\nincluding successful and failed tests and compiler output,\nare added to the context as an observation.\nFor this task, the reasoning and acting baselines share an\naction space, but acting methods are able to incorporate\nobservations as additional context. For LATS, since each\naction corresponds to a complete solution, we skip the sim-\nulation step of LATS and directly use the percentage of\npassed tests as the backpropagated reward. We use k= 8\niterations, set the number of generated tests at 4, and sam-\nplen= 5solutions during expansion. After the search is\ncompleted, we select the solution with the highest value and\nevaluate it on the real test suite for the pass@1 accuracy\nevaluation. More details can be found in Appendix Sec. D.\nResults. Tab. 4 and Tab. 5 show that both search and seman-\ntic feedback are crucial for better performance. Despite not\nusing observations, ToT and RAP are competitive with Re-\nflexion. LATS has the highest performance on both datasets.\nRAP uses a search algorithm similar to LATS, which reveals\nthe importance of external feedback for difficult reasoning\ntasks such as programming. With GPT-4, using LATS sets\nthe state of the art for HumanEval, validating that LATS can\nbe used with more advanced LMs for higher performance.\n1Some baselines use 161 questions from HumanEval. We\nuse all 164 questions for LATS and find minimal performance\ndifferences, so we report baselines for both settings.\n7Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nMethod Score\u2191SR\u2191\nReAct (Yao et al., 2023b) 53.8 28.0\nReAct (best of k) 59.1 32.0\nReflexion (Shinn et al., 2023) 64.2 35.0\nLATS (ReAct) 75.9 38.0\nIL(Yao et al., 2022) 59.9 29.1\nIL+RL (Yao et al., 2022) 62.4 28.7\nFine-tuning (Furuta et al., 2024) 67.5 45.0\nExpert 82.1 59.6\nTable 6. Score and success rate (SR) on WebShop. Results are\norganized into prompting, RL-based training, and human","chunk_id":"99d90aededb61e04241516ed9ec656cc","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LATS","type":"TECHNIQUE","description":"LATS is a method that combines internal reasoning and external retrieval strategies to enhance performance in tasks like HotPotQA and programming challenges.","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"REACT","type":"TECHNIQUE","description":"ReAct is a prompting method that adapts based on the success of previous attempts, used in conjunction with LATS for improved performance.","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"COT","type":"TECHNIQUE","description":"CoT (Chain of Thought) is a prompting technique that encourages reasoning in language models, slightly enhancing performance on reasoning tasks.","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"TOT","type":"TECHNIQUE","description":"ToT (Tree of Thought) is a search method that samples and explores outputs to improve performance in reasoning tasks.","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"RAP","type":"TECHNIQUE","description":"RAP (Retrieval-Augmented Prompting) is a method that incorporates external retrieval strategies to enhance reasoning capabilities.","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a question-answering dataset that evaluates the ability of models to reason and retrieve information from multiple sources.","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"HUMANEVAL","type":"DATASET","description":"HumanEval is a dataset used to assess the correctness of synthesized Python programs from natural language descriptions.","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"MBPP","type":"DATASET","description":"MBPP (Multi-Choice Programming Problems) is a dataset that measures the performance of models in generating correct Python code from natural language prompts.","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"GPT-3.5","type":"MODEL","description":"GPT-3.5 is a language model developed by OpenAI, used for various natural language processing tasks, including programming and reasoning.","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is a more advanced language model developed by OpenAI, known for its enhanced capabilities in reasoning and programming tasks.","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"IL","type":"TECHNIQUE","description":"IL (Imitation Learning) is a method that trains models to mimic expert behavior in various tasks, including reasoning and acting.","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"IL+RL","type":"TECHNIQUE","description":"IL+RL (Imitation Learning with Reinforcement Learning) combines imitation learning with reinforcement learning to improve model performance in complex tasks.","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"EXTERNAL OBSERVATIONS","type":"DATA FORMAT","description":"External observations refer to additional context or feedback used to enhance the reasoning capabilities of models during tasks.","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"SYNTHETIC TEST SUITE","type":"DATA FORMAT","description":"A synthetic test suite consists of generated test cases used to evaluate the correctness of programming solutions.","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"REFLEXION","type":"","description":"","source_id":"99d90aededb61e04241516ed9ec656cc"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LATS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LATS is a method that combines internal reasoning and external retrieval strategies to enhance performance in tasks like HotPotQA and programming challenges.<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">ReAct is a prompting method that adapts based on the success of previous attempts, used in conjunction with LATS for improved performance.<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"COT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">CoT (Chain of Thought) is a prompting technique that encourages reasoning in language models, slightly enhancing performance on reasoning tasks.<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"TOT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">ToT (Tree of Thought) is a search method that samples and explores outputs to improve performance in reasoning tasks.<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"RAP\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">RAP (Retrieval-Augmented Prompting) is a method that incorporates external retrieval strategies to enhance reasoning capabilities.<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a question-answering dataset that evaluates the ability of models to reason and retrieve information from multiple sources.<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"HUMANEVAL\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HumanEval is a dataset used to assess the correctness of synthesized Python programs from natural language descriptions.<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"MBPP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MBPP (Multi-Choice Programming Problems) is a dataset that measures the performance of models in generating correct Python code from natural language prompts.<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5 is a language model developed by OpenAI, used for various natural language processing tasks, including programming and reasoning.<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is a more advanced language model developed by OpenAI, known for its enhanced capabilities in reasoning and programming tasks.<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"IL\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">IL (Imitation Learning) is a method that trains models to mimic expert behavior in various tasks, including reasoning and acting.<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"IL+RL\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">IL+RL (Imitation Learning with Reinforcement Learning) combines imitation learning with reinforcement learning to improve model performance in complex tasks.<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"EXTERNAL OBSERVATIONS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">External observations refer to additional context or feedback used to enhance the reasoning capabilities of models during tasks.<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"SYNTHETIC TEST SUITE\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">A synthetic test suite consists of generated test cases used to evaluate the correctness of programming solutions.<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <edge source=\"LATS\" target=\"REACT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS utilizes the ReAct prompting method to adapt its approach based on previous successes or failures in reasoning tasks.<\/data>      <data key=\"d5\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"COT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS incorporates CoT prompting to enhance reasoning performance in tasks like HotPotQA.<\/data>      <data key=\"d5\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"TOT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS employs ToT methods to sample and explore outputs, improving its reasoning capabilities.<\/data>      <data key=\"d5\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"RAP\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS integrates RAP strategies to enhance its performance in reasoning tasks through external retrieval.<\/data>      <data key=\"d5\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HOTPOTQA\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">LATS is evaluated on the HotPotQA dataset to measure its effectiveness in answering complex questions.<\/data>      <data key=\"d5\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HUMANEVAL\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">LATS is tested on the HumanEval dataset to assess its ability to generate correct Python code from natural language descriptions.<\/data>      <data key=\"d5\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"MBPP\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">LATS is evaluated on the MBPP dataset to measure its performance in programming tasks.<\/data>      <data key=\"d5\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-3.5\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS achieves high performance using the GPT-3.5 model for reasoning and programming tasks.<\/data>      <data key=\"d5\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-4\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">LATS sets state-of-the-art performance on HumanEval when used with the GPT-4 model.<\/data>      <data key=\"d5\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REFLEXION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS incorporates the Reflexion method to improve reasoning performance through feedback mechanisms.<\/data>      <data key=\"d5\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"IL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS utilizes Imitation Learning techniques to enhance its reasoning and acting capabilities.<\/data>      <data key=\"d5\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"IL+RL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS employs Imitation Learning with Reinforcement Learning to optimize its performance in complex reasoning tasks.<\/data>      <data key=\"d5\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"EXTERNAL OBSERVATIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS integrates external observations to augment its reasoning process and improve task performance.<\/data>      <data key=\"d5\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SYNTHETIC TEST SUITE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">LATS uses a synthetic test suite to evaluate the correctness of generated programming solutions.<\/data>      <data key=\"d5\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"594449768ae2dea9b2efbe677075096b","chunk":"ao et al., 2022) 59.9 29.1\nIL+RL (Yao et al., 2022) 62.4 28.7\nFine-tuning (Furuta et al., 2024) 67.5 45.0\nExpert 82.1 59.6\nTable 6. Score and success rate (SR) on WebShop. Results are\norganized into prompting, RL-based training, and human perfor-\nmance. For the same number of iterations, LATS improves both\nscore and SR and surpasses RL-based training.\n5.3. WebShop\nFor a complex decision-making environment with practi-\ncal applications, we consider WebShop (Yao et al., 2022),\nan online shopping environment composed of a website\nwith 1.18M real-world products and 12k human instructions.\nAgents must navigate a website through a variety of com-\nmands to purchase an item matching a user specification.\nWe use the preconstructed action space of search and click\ncommands and browser feedback and reflections for the\nobservation. The performance is gauged using two metrics:\nan average score, reflecting the percentage of user-specified\nattributes met by the selected product, and a success rate,\nindicating the frequency with which the chosen product ful-\nfills all given conditions. We compare against acting-based\nprompting methods and RL-based approaches. We evaluate\non 50 instructions, expand n= 5children for LATS, and set\nk= 30 for LATS, ReAct (best of k), and Reflexion. More\ndetails and prompts are in Appendix Sec. D and Sec. G.\nResults. We find in Tab. 6 that GPT-3.5 with ReAct is\ncompetitive to imitation learning (IL) and can exceed re-\ninforcement learning techniques with stronger prompting\nstrategies. Sampling k= 30 trajectories with ReAct and\nReflexion results in a similar performance, suggesting the se-\nmantic feedback is not as helpful in complex environments\nlike WebShop. Similar to Shinn et al. (2023), we find that\ngenerated reflections are often generic and do not provide\nuseful feedback, resulting in a tendency for the agent to\nbecome stuck in local minima. However, using LATS in-\ndeed results in a noticeable improvement, indicating a more\neffective exploration for the same number of iterations.\n5.4. Ablation Study and Additional Analysis\nWe further test the reasoning ability of LATS on Game of 24,\nand also conduct additional experiments on HotPotQA to\ndemonstrate the effect of each component of LATS (resultsPrompt Method Game of 24 (Success Rate) \u2191\nCoT (Wei et al., 2022) 0.08\nReflexion (Shinn et al., 2023) 0.12\nToT (Yao et al., 2023a) 0.20\nRAP (Hao et al., 2023) 0.40\nLATS (CoT) 0.44\nTable 7. Results on Game of 24 with GPT-3.5. We sample n= 5\nnodes and k= 30 trajectories.\nPrompt Method HotPotQA (EM) \u2191\nToT (ReAct) 0.39\nRAP (ReAct) 0.54\nLATS (No LM Heuristic) 0.37\nLATS (DFS) 0.42\nLATS (No Reflection) 0.58\nLATS (ReAct) 0.63\nTable 8. Ablation results on LATS and baseline variants in Hot-\nPotQA. We use ReAct as the base prompt and sample n= 5\nchildren and k= 50 trajectories. LATS requires every component\nand operation for optimal performance.\nshown in Tab. 8). More ablations for token consumption on\nHotPotQA are in Tab. 9 in Appendix Sec. C.\nReasoning on Game of 24. To show how LATS can be\napplied to purely internal reasoning tasks, we additionally\nevaluate on Game of 24 (Yao et al., 2023a), a mathematical\nreasoning task where the agent must construct 24 out of a\nset of numbers and basic operations. We use CoT as the\nbase prompting design and employ the same operations as\nin other settings. We find in Tab. 7 that LATS outperforms\nprevious methods proposed specifically for reasoning. This\nis due to our proposed value function, which incorporates\nself-consistency as an additional heuristic.\nSelf-reflection. LATS uses self-reflection to provide addi-\ntional semantic signals for the agent. In Tab. 8 (Row 5, 6),\nwe observe a 0.05performance drop when self-reflection\nis removed from LATS, validating its usefulness. This is a\nsmaller gain than the 0.19gain that Reflexion has over Re-\nAct as shown in Tab. 3, suggesting overlap between the ques-\ntions where an answer can be improved by self-reflection\nand search. This variant outperforms RAP (ReAct), reflect-\ning our improvements to MCTS.\nSearch algorithm. MCTS is a more principled search algo-\nrithm than variants like A* (Zhuang et al., 2023) or DFS and\nis the basis for observed performance gains. We observe\nthe effects of using DFS, and incorporate the LM-based\nheuristic used in ToT in which branches with low values are\npruned. This removes the selection and backpropagation\noperations, and we observe a 0","chunk_id":"594449768ae2dea9b2efbe677075096b","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LATS","type":"TECHNIQUE","description":"LATS is a method that improves decision-making performance in complex environments by utilizing advanced prompting strategies and self-reflection techniques.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"WEB SHOP","type":"ENVIRONMENT","description":"WebShop is an online shopping environment consisting of a large number of real-world products and human instructions, where agents navigate to fulfill user specifications.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a language model that can be used for various tasks, including decision-making and reasoning in complex environments like WebShop.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"REACT","type":"TECHNIQUE","description":"ReAct is a prompting method used in conjunction with language models to enhance performance in decision-making tasks.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"REFLEXION","type":"TECHNIQUE","description":"Reflexion is a technique that provides semantic feedback to agents, aimed at improving their decision-making capabilities.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"COACHING OF THOUGHT (COT)","type":"TECHNIQUE","description":"CoT is a prompting method that guides agents in reasoning tasks, helping them to arrive at solutions through structured thinking.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"GAME OF 24","type":"TASK","description":"Game of 24 is a mathematical reasoning task where agents must create the number 24 using a set of numbers and basic operations.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"HOTPOTQA","type":"TASK","description":"HotPotQA is a question-answering task that evaluates the reasoning ability of models based on their performance in answering complex questions.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"MCTS","type":"ALGORITHM","description":"MCTS (Monte Carlo Tree Search) is a search algorithm used to make decisions in complex environments, providing principled performance gains.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"DFS","type":"ALGORITHM","description":"DFS (Depth-First Search) is a search algorithm that explores as far as possible along each branch before backtracking, used in decision-making tasks.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"RL","type":"TECHNIQUE","description":"RL (Reinforcement Learning) is a type of machine learning where agents learn to make decisions by receiving rewards or penalties based on their actions.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"FURUTA ET AL. (2024)","type":"PERSON","description":"Furuta et al. (2024) is a reference to a study that discusses fine-tuning methods in machine learning.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"YAO ET AL. (2022)","type":"PERSON","description":"Yao et al. (2022) is a reference to a study that explores the WebShop environment and its applications in decision-making.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"SHINN ET AL. (2023)","type":"PERSON","description":"Shinn et al. (2023) is a reference to a study that discusses the performance of agents using semantic feedback in decision-making tasks.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"HAO ET AL. (2023)","type":"PERSON","description":"Hao et al. (2023) is a reference to a study that evaluates various prompting methods in decision-making tasks.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"TOOL OF THOUGHT (TOT)","type":"TECHNIQUE","description":"ToT is a prompting method that aids agents in reasoning tasks by providing structured guidance.","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"IL","type":"","description":"","source_id":"594449768ae2dea9b2efbe677075096b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LATS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LATS is a method that improves decision-making performance in complex environments by utilizing advanced prompting strategies and self-reflection techniques.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"WEB SHOP\">      <data key=\"d0\">ENVIRONMENT<\/data>      <data key=\"d1\">WebShop is an online shopping environment consisting of a large number of real-world products and human instructions, where agents navigate to fulfill user specifications.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model that can be used for various tasks, including decision-making and reasoning in complex environments like WebShop.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">ReAct is a prompting method used in conjunction with language models to enhance performance in decision-making tasks.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Reflexion is a technique that provides semantic feedback to agents, aimed at improving their decision-making capabilities.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"COACHING OF THOUGHT (COT)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">CoT is a prompting method that guides agents in reasoning tasks, helping them to arrive at solutions through structured thinking.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"GAME OF 24\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Game of 24 is a mathematical reasoning task where agents must create the number 24 using a set of numbers and basic operations.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">HotPotQA is a question-answering task that evaluates the reasoning ability of models based on their performance in answering complex questions.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"MCTS\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">MCTS (Monte Carlo Tree Search) is a search algorithm used to make decisions in complex environments, providing principled performance gains.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"DFS\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">DFS (Depth-First Search) is a search algorithm that explores as far as possible along each branch before backtracking, used in decision-making tasks.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"RL\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">RL (Reinforcement Learning) is a type of machine learning where agents learn to make decisions by receiving rewards or penalties based on their actions.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"FURUTA ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Furuta et al. (2024) is a reference to a study that discusses fine-tuning methods in machine learning.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"YAO ET AL. (2022)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. (2022) is a reference to a study that explores the WebShop environment and its applications in decision-making.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"SHINN ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn et al. (2023) is a reference to a study that discusses the performance of agents using semantic feedback in decision-making tasks.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"HAO ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao et al. (2023) is a reference to a study that evaluates various prompting methods in decision-making tasks.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"TOOL OF THOUGHT (TOT)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">ToT is a prompting method that aids agents in reasoning tasks by providing structured guidance.<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"IL\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <edge source=\"LATS\" target=\"WEB SHOP\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS is applied in the WebShop environment to enhance decision-making performance for agents navigating the online shopping platform.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-3.5\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">GPT-3.5 is utilized within the LATS framework to improve decision-making and reasoning capabilities in various tasks.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REACT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS incorporates ReAct as a base prompting method to enhance its performance in decision-making tasks.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REFLEXION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS uses Reflexion to provide additional semantic signals that improve the agent's decision-making process.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"COACHING OF THOUGHT (COT)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS employs CoT as a prompting design to guide agents in reasoning tasks effectively.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GAME OF 24\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS is evaluated on the Game of 24 task to demonstrate its reasoning capabilities in mathematical challenges.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HOTPOTQA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS is tested on HotPotQA to assess its performance in complex question-answering scenarios.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"MCTS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">LATS utilizes MCTS as a principled search algorithm to enhance its decision-making performance in complex environments.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"DFS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">LATS incorporates DFS as a search strategy to explore decision-making paths in its framework.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"IL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS is compared against IL to evaluate its performance in decision-making tasks.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"RL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS is compared against RL-based training methods to assess its effectiveness in improving scores and success rates.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"FURUTA ET AL. (2024)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Furuta et al. (2024) provides insights relevant to the fine-tuning methods that can be applied in LATS.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HAO ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Hao et al. (2023) provides comparative analysis relevant to the performance of LATS in decision-making tasks.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"TOOL OF THOUGHT (TOT)\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">LATS incorporates ToT as a prompting method to enhance reasoning capabilities in decision-making tasks.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"WEB SHOP\" target=\"YAO ET AL. (2022)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Yao et al. (2022) discusses the WebShop environment, providing foundational knowledge for its application in decision-making tasks.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"REFLEXION\" target=\"SHINN ET AL. (2023)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Shinn et al. (2023) evaluates the effectiveness of Reflexion in enhancing agent performance in decision-making tasks.<\/data>      <data key=\"d5\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"faa2bd677c7f052136479e0175da3e5b","chunk":"ing our improvements to MCTS.\nSearch algorithm. MCTS is a more principled search algo-\nrithm than variants like A* (Zhuang et al., 2023) or DFS and\nis the basis for observed performance gains. We observe\nthe effects of using DFS, and incorporate the LM-based\nheuristic used in ToT in which branches with low values are\npruned. This removes the selection and backpropagation\noperations, and we observe a 0.21drop in performance in\n8Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nMethod Performance \u2191Sample complexity \u2193Token Consumption \u2193\nReAct (Best k= 250 ) 0.42 O(k) -\nCoT-SC ( n= 1, k= 250 ) 0.40 O(k) -\nLATS ( n= 1, k= 50 ) 0.48 O(k) -\nToT (ReAct, n= 5, k= 50 ) 0.49 O(kn) 210,215\nRAP (ReAct, n= 5, k= 50 ) 0.54 O(kn) 176,500\nLATS ( n= 5, k= 50 ) 0.63 O(kn) 173,290\nTable 9. Performance, sample complexity of different methods, average number of nodes expanded, and token consumption upon success\nby methods with tree-based search. nis the number of children nodes expanded at every step and kis the number of trajectories. LATS\nhas the same sample complexity as other methods with tree-based search and expands less nodes upon success, which indicates lower\ntoken cost.\nMethod k HotPotQA \u2191# of Nodes \u2193\nToT 10 0.34 33.97\nRAP 10 0.44 31.53\nLATS 10 0.44 28.42\nToT 30 0.39 47.54\nRAP 30 0.50 37.71\nLATS 30 0.52 34.12\nToT 50 0.49 84.05\nRAP 50 0.54 70.60\nLATS 50 0.61 66.65\nTable 10. Comparison of the cost of different methods on Hot-\nPotQA. LATS achieves the highest accuracy and the lowest av-\nerage number of nodes\/states required for success at various k\ntrajectories sampled.\nTab. 8 (Row 4) when sampling the same number of nodes\nbut outperforms ToT (ReAct). Despite also benefiting from\nground-truth feedback, LATS uses it better than ToT and\nRAP and can outperform these methods. We also find in\nTab. 8 (Row 3) that LM scoring, the main component of our\nvalue function, is crucial for leveraging external feedback\nand strong performance.\nSample complexity and token consumption. One pos-\nsible concern of LATS is that the tree-structured search\nmight consume much more tokens than existing methods.\nTo further study the computational cost of LATS compared\nto prior methods, we examine the sample complexity (i.e.,\nasymptotic token cost) of all methods considered in this\npaper and count the average number of nodes expanded\nby our method and other tree-structured methods (ToT and\nRAP) upon successful search on HotPotQA. We present the\nresults in Tab. 9 and Tab. 10, which show that our method\nhas the same sample complexity as other tree-based search\nmethods and requires fewer overall tokens and states. The\ntoken cost gap will be even larger when taking failed trajec-\ntories into account, since our method has a higher success\nrate and reaches the computational budget limit less often.\nThis is also true when sampling a smaller number of trajec-\ntories; on average, LATS requires 3.55 fewer nodes thanRAP and 12.12 fewer nodes than ToT. These findings un-\nderscore our improvements to MCTS and adaptation to LM\nagents, resulting in a more principled and efficient search\nmechanism.\n6. Conclusion\nThis work introduces Language Agent Tree Search (LATS),\nthe first framework to unify reasoning, acting, and plan-\nning for enhanced LM problem-solving. LATS addresses\nkey limitations of prior prompting techniques by deliber-\nately constructing trajectories with search algorithms, in-\ncorporating external feedback, and enabling agents to learn\nfrom experience. Our evaluation demonstrates the ability\nof LATS to harness LM capabilities for various decision-\nmaking tasks while maintaining its reasoning ability without\nadditional training . The proposed synergies between search,\ninteraction, and reflection offer a versatile approach to au-\ntonomous decision-making, highlighting the potential of\nLMs as generalist agents.\nLimitations and future directions. LATS has two main\nlimitations that should be considered before its application.\nFirst, it has a higher computational cost compared to simpler\nprompting methods like ReAct or Reflexion, which may\nlimit its practicality in certain situations. Second, LATS\nassumes the ability to revert to earlier states in decision-\nmaking environments, which may not be universally ap-\nplicable in all possible environments. Despite these limi-\ntations, it is worth noting that LATS still achieves better\nperformance and efficiency compared to similar methods,\nand the number of nodes expanded at each step provides a\ntrade-off between performance and efficiency. Additionally,\nwe expect inference-time compute costs to decrease over\ntime, thereby increasing the usefulness of LATS and other\n\u201cSystem-2\u201d LM approaches. Finally, the reversion prop-\nerty is feasible in many real-world applications, opening up\n","chunk_id":"faa2bd677c7f052136479e0175da3e5b","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LANGUAGE AGENT TREE SEARCH (LATS)","type":"TECHNIQUE","description":"LATS is a framework designed to unify reasoning, acting, and planning for enhanced problem-solving in language models, addressing limitations of prior prompting techniques by incorporating search algorithms and external feedback.","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"MCTS","type":"ALGORITHM","description":"Monte Carlo Tree Search (MCTS) is a principled search algorithm that serves as the foundation for performance improvements in various decision-making tasks.","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"DFS","type":"ALGORITHM","description":"Depth-First Search (DFS) is a search algorithm that explores as far as possible along each branch before backtracking, used in comparison with MCTS.","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"REACT","type":"METHOD","description":"ReAct is a prompting method that utilizes reasoning and action in decision-making tasks, serving as a baseline for comparison with LATS.","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"RAP","type":"METHOD","description":"RAP is a prompting method that combines reasoning and action, used for comparison with LATS in terms of performance and efficiency.","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"HOT-POTQA","type":"DATASET","description":"HotPotQA is a dataset used for evaluating the performance of various methods in question answering tasks, particularly in the context of tree-based search methods.","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TOKEN CONSUMPTION","type":"METRIC","description":"Token consumption refers to the number of tokens used during the execution of a method, which is a critical factor in evaluating the efficiency of search algorithms.","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"SAMPLE COMPLEXITY","type":"METRIC","description":"Sample complexity refers to the number of samples or tokens required for a method to achieve a certain level of performance, important for assessing the efficiency of algorithms.","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"NODES EXPANDED","type":"METRIC","description":"Nodes expanded refers to the number of nodes processed during the search, which is a critical factor in evaluating the efficiency of search algorithms.","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TRAJECTORIES","type":"DATA FORMAT","description":"Trajectories are sequences of actions or decisions taken by the algorithm during the search process, used to evaluate performance and efficiency.","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"GROUND-TRUTH FEEDBACK","type":"DATA FORMAT","description":"Ground-truth feedback is the actual correct information used to guide the learning process of the algorithm, enhancing its performance.","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"COMPUTATIONAL COST","type":"METRIC","description":"Computational cost refers to the resources required to execute a method, which is a significant consideration when evaluating LATS against simpler methods.","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"LATS PERFORMANCE","type":"","description":"","source_id":"faa2bd677c7f052136479e0175da3e5b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LANGUAGE AGENT TREE SEARCH (LATS)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LATS is a framework designed to unify reasoning, acting, and planning for enhanced problem-solving in language models, addressing limitations of prior prompting techniques by incorporating search algorithms and external feedback.<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"MCTS\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">Monte Carlo Tree Search (MCTS) is a principled search algorithm that serves as the foundation for performance improvements in various decision-making tasks.<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"DFS\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">Depth-First Search (DFS) is a search algorithm that explores as far as possible along each branch before backtracking, used in comparison with MCTS.<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">ReAct is a prompting method that utilizes reasoning and action in decision-making tasks, serving as a baseline for comparison with LATS.<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"RAP\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">RAP is a prompting method that combines reasoning and action, used for comparison with LATS in terms of performance and efficiency.<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"HOT-POTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a dataset used for evaluating the performance of various methods in question answering tasks, particularly in the context of tree-based search methods.<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TOKEN CONSUMPTION\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Token consumption refers to the number of tokens used during the execution of a method, which is a critical factor in evaluating the efficiency of search algorithms.<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"SAMPLE COMPLEXITY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Sample complexity refers to the number of samples or tokens required for a method to achieve a certain level of performance, important for assessing the efficiency of algorithms.<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"NODES EXPANDED\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Nodes expanded refers to the number of nodes processed during the search, which is a critical factor in evaluating the efficiency of search algorithms.<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TRAJECTORIES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Trajectories are sequences of actions or decisions taken by the algorithm during the search process, used to evaluate performance and efficiency.<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"GROUND-TRUTH FEEDBACK\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Ground-truth feedback is the actual correct information used to guide the learning process of the algorithm, enhancing its performance.<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"COMPUTATIONAL COST\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Computational cost refers to the resources required to execute a method, which is a significant consideration when evaluating LATS against simpler methods.<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"LATS PERFORMANCE\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"MCTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS improves upon MCTS by incorporating search algorithms and external feedback for better decision-making performance.<\/data>      <data key=\"d5\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"REACT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS is evaluated against ReAct to demonstrate its enhanced performance and efficiency in decision-making tasks.<\/data>      <data key=\"d5\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"RAP\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS is compared with RAP to showcase its advantages in terms of performance and computational efficiency.<\/data>      <data key=\"d5\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"HOT-POTQA\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">LATS is evaluated using the HotPotQA dataset to assess its performance in question answering tasks.<\/data>      <data key=\"d5\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"LATS PERFORMANCE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS shows improved performance metrics, indicating its effectiveness in comparison to other methods like ReAct and RAP.<\/data>      <data key=\"d5\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"NODES EXPANDED\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS requires fewer nodes expanded compared to other methods, demonstrating its efficiency in search operations.<\/data>      <data key=\"d5\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"TRAJECTORIES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS constructs trajectories to enhance its decision-making capabilities, integrating search algorithms effectively.<\/data>      <data key=\"d5\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"GROUND-TRUTH FEEDBACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS utilizes ground-truth feedback to improve its learning and performance in decision-making tasks.<\/data>      <data key=\"d5\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"COMPUTATIONAL COST\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">LATS has a higher computational cost compared to simpler methods, which may limit its practicality in certain situations.<\/data>      <data key=\"d5\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"MCTS\" target=\"DFS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">MCTS is compared to DFS as a more principled search algorithm, highlighting differences in performance and methodology.<\/data>      <data key=\"d5\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"TOKEN CONSUMPTION\" target=\"SAMPLE COMPLEXITY\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Token consumption and sample complexity are metrics used to evaluate the efficiency and performance of search algorithms like LATS, ReAct, and RAP.<\/data>      <data key=\"d5\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"4ae237a491bc8a84cc720e40c59a7464","chunk":" possible environments. Despite these limi-\ntations, it is worth noting that LATS still achieves better\nperformance and efficiency compared to similar methods,\nand the number of nodes expanded at each step provides a\ntrade-off between performance and efficiency. Additionally,\nwe expect inference-time compute costs to decrease over\ntime, thereby increasing the usefulness of LATS and other\n\u201cSystem-2\u201d LM approaches. Finally, the reversion prop-\nerty is feasible in many real-world applications, opening up\nnew opportunities in the LM decision-making community.\nFuture directions include scaling LATS to more complex\nenvironments or multi-agent frameworks and improving ef-\nficiency to reduce costs. A more detailed discussion about\nthe limitations of LATS can be found in Appendix Sec. B.\n9Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nImpact Statement\nLATS is a framework that enhances LM performance\nthrough interactions with an environment. This improve-\nment in autonomous decision-making may facilitate harm-\nful uses of LMs. On the other hand, LATS enhances in-\nterpretability and the potential for greater alignment, as it\ninvolves high-level linguistic reasoning and actions through\nseveral rounds of decision-making and reflection rather than\nrelying on autoregressive generation. Finally, enhancing the\ncapabilities of LM agents may raise security risks, such as\nexecuting malware. We encourage further research to fully\nunderstand and mitigate the risks of LMs.\nAcknowledgements\nWe thank Daniel Campos for useful feedback on earlier ver-\nsions of this paper. This work was supported in part by NSF\nGrant 2106825, NIFA Award 2020-67021-32799, the Jump\nARCHES endowment through the Health Care Engineering\nSystems Center at Illinois and the OSF Foundation, and the\nIBM-Illinois Discovery Accelerator Institute. This work\nused NVIDIA GPUs at NCSA Delta through allocations\nCIS220014, CIS230012, and CIS230218 from the ACCESS\nprogram.\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Cheb-\notar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan\nFu, Keerthana Gopalakrishnan, Karol Hausman, Alex\nHerzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian\nIchter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano,\nKyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian,\nDmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter\nPastor, Jornell Quiambao, Kanishka Rao, Jarek Retting-\nhouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers,\nClayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei\nXia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and\nAndy Zeng. Do as I can, not as I say: Grounding language\nin robotic affordances. In CoRL , 2022.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, and Charles Sut-\nton. Program synthesis with large language models. In\nNeurIPS , 2022.\nBowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga,\nJie Tang, Adrien Ecoffet, Brandon Houghton, Raul\nSampedro, and Jeff Clune. Video pretraining (VPT):\nLearning to act by watching unlabeled online videos. In\nNeurIPS , 2022.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger-stenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski, Pi-\notr Nyczyk, and Torsten Hoefler. Graph of thoughts:\nSolving elaborate problems with large language models.\narXiv:2308.09687 , 2023.\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and\nChristopher D Manning. A large annotated corpus for\nlearning natural language inference. In EMNLP , 2015.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. Language models are few-shot learners. In\nNeurIPS , 2020.\nMurray Campbell, A Joseph Hoane Jr, and Feng-hsiung\nHsu. Deep blue. Artificial intelligence , 2002.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi\nLin, Jian-Guang Lou, and Weizhu Chen. CodeT: Code\ngeneration with generated tests. In ICLR , 202","chunk_id":"4ae237a491bc8a84cc720e40c59a7464","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LATS","type":"FRAMEWORK","description":"LATS is a framework designed to enhance language model (LM) performance through interactions with various environments, focusing on autonomous decision-making and interpretability.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"SYSTEM-2 LM APPROACHES","type":"TECHNIQUE","description":"System-2 LM approaches refer to advanced methods in language modeling that emphasize reasoning and decision-making over simple autoregressive generation.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"DECISION-MAKING COMMUNITY","type":"COMMUNITY","description":"The decision-making community encompasses researchers and practitioners focused on improving decision-making processes using language models.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"DANIEL CAMPOS","type":"PERSON","description":"Daniel Campos is an individual acknowledged for providing useful feedback on earlier versions of the paper discussing LATS.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"NSF GRANT 2106825","type":"FUNDING","description":"NSF Grant 2106825 is a financial support awarded for research related to language models and their applications.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"NIFA AWARD 2020-67021-32799","type":"FUNDING","description":"NIFA Award 2020-67021-32799 is a financial support awarded for research related to language models and their applications.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"IBM-ILLINOIS DISCOVERY ACCELERATOR INSTITUTE","type":"INSTITUTION","description":"The IBM-Illinois Discovery Accelerator Institute is an institution that supports research and development in advanced computing and AI technologies.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"NVIDIA GPUS","type":"TECHNOLOGY","description":"NVIDIA GPUs are high-performance graphics processing units used for computational tasks, including those in language model research.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"ACCESS PROGRAM","type":"PROGRAM","description":"The ACCESS program provides allocations for computational resources to support research initiatives, including those involving language models.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"MICHAEL AHN","type":"PERSON","description":"Michael Ahn is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"ANTHONY BROHAN","type":"PERSON","description":"Anthony Brohan is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"NOAH BROWN","type":"PERSON","description":"Noah Brown is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"YEVGEN CHEBOTAR","type":"PERSON","description":"Yevgen Chebotar is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"OMAR CORTES","type":"PERSON","description":"Omar Cortes is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"BYRON DAVID","type":"PERSON","description":"Byron David is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"CHELSEA FINN","type":"PERSON","description":"Chelsea Finn is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"CHUYUAN FU","type":"PERSON","description":"Chuyuan Fu is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"KEERTHANA GOPALAKRISHNAN","type":"PERSON","description":"Keerthana Gopalakrishnan is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"KAROL HAUSMAN","type":"PERSON","description":"Karol Hausman is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"ALEX HERZOG","type":"PERSON","description":"Alex Herzog is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"DANIEL HO","type":"PERSON","description":"Daniel Ho is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"JASMINE HSU","type":"PERSON","description":"Jasmine Hsu is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"JULIAN IBARZ","type":"PERSON","description":"Julian Ibarz is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"BRIAN ICHTER","type":"PERSON","description":"Brian Ichter is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"ALEX IRPAN","type":"PERSON","description":"Alex Irpan is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"ERIC JANG","type":"PERSON","description":"Eric Jang is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"ROSARIO JAUREGUI RUANO","type":"PERSON","description":"Rosario Jauregui Ruano is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"KYLE JEFFREY","type":"PERSON","description":"Kyle Jeffrey is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"SALLY JESMONTH","type":"PERSON","description":"Sally Jesmonth is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"NIKHIL J JOSHI","type":"PERSON","description":"Nikhil J Joshi is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"RYAN JULIAN","type":"PERSON","description":"Ryan Julian is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"DMITRY KALASHNIKOV","type":"PERSON","description":"Dmitry Kalashnikov is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"YUHENG KUANG","type":"PERSON","description":"Yuheng Kuang is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"KUANG-HUEI LEE","type":"PERSON","description":"Kuang-Huei Lee is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"SERGEY LEVINE","type":"PERSON","description":"Sergey Levine is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"YAO LU","type":"PERSON","description":"Yao Lu is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"LINDA LUU","type":"PERSON","description":"Linda Luu is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"CAROLINA PARADA","type":"PERSON","description":"Carolina Parada is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"PETER PASTOR","type":"PERSON","description":"Peter Pastor is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"JORNELL QUIAMBAO","type":"PERSON","description":"Jornell Quiambao is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"KANISHKA RAO","type":"PERSON","description":"Kanishka Rao is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"JAREK RETTINGHOUSE","type":"PERSON","description":"Jarek Rettinghouse is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"DIEGO REYES","type":"PERSON","description":"Diego Reyes is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"PIERRE SERMANET","type":"PERSON","description":"Pierre Sermanet is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"NICOLAS SIEVERS","type":"PERSON","description":"Nicolas Sievers is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"CLAYTON TAN","type":"PERSON","description":"Clayton Tan is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"ALEXANDER TOSHEV","type":"PERSON","description":"Alexander Toshev is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"VINCENT VANHOUCKE","type":"PERSON","description":"Vincent Vanhoucke is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"FEI XIA","type":"PERSON","description":"Fei Xia is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"TED XIAO","type":"PERSON","description":"Ted Xiao is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"PENG XU","type":"PERSON","description":"Peng Xu is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"SICHUN XU","type":"PERSON","description":"Sichun Xu is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"MENGYUAN YAN","type":"PERSON","description":"Mengyuan Yan is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"ANDY ZENG","type":"PERSON","description":"Andy Zeng is a researcher who co-authored a paper on grounding language in robotic affordances.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"JORNELL QUIAMBAO<|(\"ENTITY\"","type":"","description":"","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"PROGRAM SYNTHESIS","type":"","description":"","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"VIDEO PRETRAINING (VPT)","type":"TECHNIQUE","description":"Video pretraining (VPT) is a technique that involves training models to learn to act by observing unlabeled online videos, enhancing their performance in action-related tasks.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"GRAPH OF THOUGHTS","type":"TECHNIQUE","description":"The Graph of Thoughts is a method for solving complex problems using large language models, focusing on structured reasoning and decision-making.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"NATURAL LANGUAGE INFERENCE","type":"TECHNIQUE","description":"Natural language inference is a task in natural language processing that involves determining the logical relationship between sentences, often used for training language models.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"DEEP BLUE","type":"TECHNOLOGY","description":"Deep Blue is a chess-playing computer developed by IBM, known for its ability to compete against and defeat human chess champions.","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"CODET","type":"TECHNIQUE","description":"CodeT is a technique for code generation that includes generating tests to ensure the correctness of the generated code.","source_id":"4ae237a491bc8a84cc720e40c59a7464"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LATS\">      <data key=\"d0\">FRAMEWORK<\/data>      <data key=\"d1\">LATS is a framework designed to enhance language model (LM) performance through interactions with various environments, focusing on autonomous decision-making and interpretability.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"SYSTEM-2 LM APPROACHES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">System-2 LM approaches refer to advanced methods in language modeling that emphasize reasoning and decision-making over simple autoregressive generation.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"DECISION-MAKING COMMUNITY\">      <data key=\"d0\">COMMUNITY<\/data>      <data key=\"d1\">The decision-making community encompasses researchers and practitioners focused on improving decision-making processes using language models.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"DANIEL CAMPOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel Campos is an individual acknowledged for providing useful feedback on earlier versions of the paper discussing LATS.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"NSF GRANT 2106825\">      <data key=\"d0\">FUNDING<\/data>      <data key=\"d1\">NSF Grant 2106825 is a financial support awarded for research related to language models and their applications.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"NIFA AWARD 2020-67021-32799\">      <data key=\"d0\">FUNDING<\/data>      <data key=\"d1\">NIFA Award 2020-67021-32799 is a financial support awarded for research related to language models and their applications.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"IBM-ILLINOIS DISCOVERY ACCELERATOR INSTITUTE\">      <data key=\"d0\">INSTITUTION<\/data>      <data key=\"d1\">The IBM-Illinois Discovery Accelerator Institute is an institution that supports research and development in advanced computing and AI technologies.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"NVIDIA GPUS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">NVIDIA GPUs are high-performance graphics processing units used for computational tasks, including those in language model research.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"ACCESS PROGRAM\">      <data key=\"d0\">PROGRAM<\/data>      <data key=\"d1\">The ACCESS program provides allocations for computational resources to support research initiatives, including those involving language models.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"MICHAEL AHN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Ahn is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"ANTHONY BROHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anthony Brohan is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"NOAH BROWN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Noah Brown is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"YEVGEN CHEBOTAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yevgen Chebotar is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"OMAR CORTES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Omar Cortes is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"BYRON DAVID\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Byron David is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"CHELSEA FINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chelsea Finn is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"CHUYUAN FU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chuyuan Fu is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"KEERTHANA GOPALAKRISHNAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Keerthana Gopalakrishnan is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"KAROL HAUSMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karol Hausman is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"ALEX HERZOG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex Herzog is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"DANIEL HO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel Ho is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"JASMINE HSU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jasmine Hsu is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"JULIAN IBARZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Julian Ibarz is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"BRIAN ICHTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brian Ichter is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"ALEX IRPAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex Irpan is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"ERIC JANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Jang is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"ROSARIO JAUREGUI RUANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rosario Jauregui Ruano is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"KYLE JEFFREY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kyle Jeffrey is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"SALLY JESMONTH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sally Jesmonth is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"NIKHIL J JOSHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nikhil J Joshi is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"RYAN JULIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ryan Julian is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"DMITRY KALASHNIKOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dmitry Kalashnikov is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"YUHENG KUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuheng Kuang is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"KUANG-HUEI LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kuang-Huei Lee is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"SERGEY LEVINE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sergey Levine is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"YAO LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao Lu is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"LINDA LUU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linda Luu is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"CAROLINA PARADA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carolina Parada is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"PETER PASTOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter Pastor is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"JORNELL QUIAMBAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jornell Quiambao is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"KANISHKA RAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kanishka Rao is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"JAREK RETTINGHOUSE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jarek Rettinghouse is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"DIEGO REYES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Diego Reyes is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"PIERRE SERMANET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pierre Sermanet is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"NICOLAS SIEVERS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicolas Sievers is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"CLAYTON TAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clayton Tan is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"ALEXANDER TOSHEV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alexander Toshev is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"VINCENT VANHOUCKE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vincent Vanhoucke is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"FEI XIA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fei Xia is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"TED XIAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ted Xiao is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"PENG XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peng Xu is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"SICHUN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sichun Xu is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"MENGYUAN YAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mengyuan Yan is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"ANDY ZENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andy Zeng is a researcher who co-authored a paper on grounding language in robotic affordances.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"JORNELL QUIAMBAO&lt;|(&quot;ENTITY&quot;\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"PROGRAM SYNTHESIS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"VIDEO PRETRAINING (VPT)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Video pretraining (VPT) is a technique that involves training models to learn to act by observing unlabeled online videos, enhancing their performance in action-related tasks.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"GRAPH OF THOUGHTS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Graph of Thoughts is a method for solving complex problems using large language models, focusing on structured reasoning and decision-making.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"NATURAL LANGUAGE INFERENCE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Natural language inference is a task in natural language processing that involves determining the logical relationship between sentences, often used for training language models.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"DEEP BLUE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Deep Blue is a chess-playing computer developed by IBM, known for its ability to compete against and defeat human chess champions.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"CODET\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">CodeT is a technique for code generation that includes generating tests to ensure the correctness of the generated code.<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <edge source=\"LATS\" target=\"SYSTEM-2 LM APPROACHES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS is an example of a System-2 LM approach that enhances decision-making through interactions with environments.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"DECISION-MAKING COMMUNITY\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS contributes to the decision-making community by improving autonomous decision-making processes using language models.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"DANIEL CAMPOS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Daniel Campos provided feedback on the development of LATS, indicating his involvement in the research process.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"NSF GRANT 2106825\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">NSF Grant 2106825 supports research related to the development and application of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"NIFA AWARD 2020-67021-32799\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">NIFA Award 2020-67021-32799 supports research related to the development and application of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"IBM-ILLINOIS DISCOVERY ACCELERATOR INSTITUTE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The IBM-Illinois Discovery Accelerator Institute supports research initiatives like LATS, enhancing language model capabilities.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"NVIDIA GPUS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">NVIDIA GPUs are utilized in the implementation of LATS for computational tasks, enhancing its performance.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ACCESS PROGRAM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The ACCESS program provides computational resources that support the research and development of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"MICHAEL AHN\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Michael Ahn is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ANTHONY BROHAN\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Anthony Brohan is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"NOAH BROWN\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Noah Brown is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"YEVGEN CHEBOTAR\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Yevgen Chebotar is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"OMAR CORTES\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Omar Cortes is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"BYRON DAVID\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Byron David is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"CHELSEA FINN\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Chelsea Finn is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"CHUYUAN FU\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Chuyuan Fu is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"KEERTHANA GOPALAKRISHNAN\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Keerthana Gopalakrishnan is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"KAROL HAUSMAN\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Karol Hausman is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ALEX HERZOG\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Alex Herzog is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"DANIEL HO\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Daniel Ho is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"JASMINE HSU\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Jasmine Hsu is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"JULIAN IBARZ\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Julian Ibarz is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"BRIAN ICHTER\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Brian Ichter is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ALEX IRPAN\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Alex Irpan is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ERIC JANG\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Eric Jang is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ROSARIO JAUREGUI RUANO\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Rosario Jauregui Ruano is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"KYLE JEFFREY\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Kyle Jeffrey is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SALLY JESMONTH\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Sally Jesmonth is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"NIKHIL J JOSHI\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Nikhil J Joshi is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"RYAN JULIAN\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Ryan Julian is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"DMITRY KALASHNIKOV\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Dmitry Kalashnikov is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"YUHENG KUANG\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Yuheng Kuang is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"KUANG-HUEI LEE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Kuang-Huei Lee is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SERGEY LEVINE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Sergey Levine is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"YAO LU\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Yao Lu is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"LINDA LUU\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Linda Luu is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"CAROLINA PARADA\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Carolina Parada is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PETER PASTOR\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Peter Pastor is a contributor to research on grounding language in robotic affordances, which relates to the applications of LATS.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PROGRAM SYNTHESIS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS can be applied in program synthesis to enhance decision-making processes through interactions with programming environments.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"VIDEO PRETRAINING (VPT)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS may utilize techniques from video pretraining to improve its performance in action-related tasks within language models.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GRAPH OF THOUGHTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS can incorporate the Graph of Thoughts methodology to enhance structured reasoning in decision-making processes.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"NATURAL LANGUAGE INFERENCE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS can benefit from natural language inference techniques to improve its understanding of logical relationships in decision-making.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"DEEP BLUE\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">LATS shares similarities with Deep Blue in terms of enhancing decision-making capabilities through advanced computational techniques.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"LATS\" target=\"CODET\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">LATS can leverage techniques from CodeT for generating and testing code in decision-making scenarios.<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>    <edge source=\"JORNELL QUIAMBAO&lt;|(&quot;ENTITY&quot;\" target=\"PROGRAM SYNTHESIS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">TECHNIQUE<\/data>      <data key=\"d5\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"7a48515e86161237c03c9a8373197126","chunk":"\nAmodei. Language models are few-shot learners. In\nNeurIPS , 2020.\nMurray Campbell, A Joseph Hoane Jr, and Feng-hsiung\nHsu. Deep blue. Artificial intelligence , 2002.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi\nLin, Jian-Guang Lou, and Weizhu Chen. CodeT: Code\ngeneration with generated tests. In ICLR , 2023a.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde, Jared Kaplan, Harrison Edwards, Yura\nBurda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy\nKhlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,\nLukasz Kaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, David W. Cum-\nmings, Matthias Plappert, Fotios Chantzis, Elizabeth\nBarnes, Ariel Herbert-V oss, William H. Guss, Alex\nNichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nAndrew Carr, Jan Leike, Joshua Achiam, Vedant Misra,\nEvan Morikawa, Alec Radford, Matthew M. Knight,\nMiles Brundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. Evaluating large lan-\nguage models trained on code. arXiv:2107.03374 , 2021.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W.\nCohen. Program of thoughts prompting: disentangling\ncomputation from reasoning for numerical reasoning\ntasks. TMLR , 2023b. ISSN 2835-8856.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\n10Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope,\nJames Bradbury, Jacob Austin, Michael Isard, Guy Gur-\nAri, Pengcheng Yin, Toju Duke, Anselm Levskaya, San-\njay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fedus,\nDenny Zhou, Daphne Ippolito, David Luan, Hyeontaek\nLim, Barret Zoph, Alexander Spiridonov, Ryan Sepa-\nssi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai,\nMarie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou,\nXuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat,\nMichele Catasta, Jason Wei, Kathy Meier-Hellstern, Dou-\nglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM:\nScaling language modeling with pathways. JMLR , 24\n(240):1\u2013113, 2023.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark\nChen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, Christo-\npher Hesse, and John Schulman. Training verifiers to\nsolve math word problems. arXiv:2110.14168 , 2021.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel\nStevens, Boshi Wang, Huan Sun, and Yu Su. Mind2Web:\nTowards a generalist agent for the web. In NeurIPS\nDatasets and Benchmarks Track , 2023.\nDanny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\nworth, Sergey Levine, Vincent Vanhoucke, Karol Haus-\nman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mor-\ndatch, and Pete Florence. PaLM-E: An embodied multi-\nmodal language model. In ICML , 2023.\nYilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir\nNachum, Joshua B. Tenenbaum, Dale Schuurmans, and\nPieter Abbeel. Learning universal policies via text-guided\nvideo generation. In NeurIPS , 2023.\nJonathan St BT Evans. Intuition and reasoning: A dual-\nprocess perspective. Psychological Inquiry , pages ","chunk_id":"7a48515e86161237c03c9a8373197126","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"AMODEI","type":"PERSON","description":"Dario Amodei is a researcher known for his work on language models and their capabilities as few-shot learners.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"MURRAY CAMPBELL","type":"PERSON","description":"Murray Campbell is a researcher associated with the development of the Deep Blue chess program.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JOSEPH HOANE JR","type":"PERSON","description":"Joseph Hoane Jr is a researcher involved in the development of the Deep Blue chess program.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"FENG-HSIUNG HSU","type":"PERSON","description":"Feng-hsiung Hsu is a researcher associated with the Deep Blue chess program.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"BEI CHEN","type":"PERSON","description":"Bei Chen is a researcher who co-authored a paper on CodeT, a code generation model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"FENGJI ZHANG","type":"PERSON","description":"Fengji Zhang is a researcher who co-authored a paper on CodeT, a code generation model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ANH NGUYEN","type":"PERSON","description":"Anh Nguyen is a researcher who co-authored a paper on CodeT, a code generation model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"DAOGUANG ZAN","type":"PERSON","description":"Daoguang Zan is a researcher who co-authored a paper on CodeT, a code generation model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ZEQI LIN","type":"PERSON","description":"Zeqi Lin is a researcher who co-authored a paper on CodeT, a code generation model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JIAN-GUANG LOU","type":"PERSON","description":"Jian-Guang Lou is a researcher who co-authored a paper on CodeT, a code generation model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"WEIZHU CHEN","type":"PERSON","description":"Weizhu Chen is a researcher who co-authored a paper on CodeT, a code generation model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"MARK CHEN","type":"PERSON","description":"Mark Chen is a researcher involved in evaluating large language models trained on code.\nMark Chen is a researcher involved in training verifiers to solve math word problems.","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"JERRY TWOREK","type":"PERSON","description":"Jerry Tworek is a researcher involved in evaluating large language models trained on code.\nJerry Tworek is a researcher involved in training verifiers to solve math word problems.","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"HEEWOO JUN","type":"PERSON","description":"Heewoo Jun is a researcher involved in training verifiers to solve math word problems.\nHeewoo Jun is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"QIMING YUAN","type":"PERSON","description":"Qiming Yuan is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"HENRIQUE PONDE","type":"PERSON","description":"Henrique Ponde is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JARED KAPLAN","type":"PERSON","description":"Jared Kaplan is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"HARRISON EDWARDS","type":"PERSON","description":"Harrison Edwards is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"YURA BURDA","type":"PERSON","description":"Yura Burda is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"NICHOLAS JOSEPH","type":"PERSON","description":"Nicholas Joseph is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"GREG BROCKMAN","type":"PERSON","description":"Greg Brockman is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ALEX RAY","type":"PERSON","description":"Alex Ray is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"RAUL PURI","type":"PERSON","description":"Raul Puri is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"GRETCHEN KRUEGER","type":"PERSON","description":"Gretchen Krueger is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"MICHAEL PETROV","type":"PERSON","description":"Michael Petrov is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"HEIDY KHLAAF","type":"PERSON","description":"Heidy Khlaaf is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"GIRISH SASTRY","type":"PERSON","description":"Girish Sastry is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"PAMELA MISHKIN","type":"PERSON","description":"Pamela Mishkin is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"BROOKE CHAN","type":"PERSON","description":"Brooke Chan is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"SCOTT GRAY","type":"PERSON","description":"Scott Gray is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"NICK RYDER","type":"PERSON","description":"Nick Ryder is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"MIKHAIL PAVLOV","type":"PERSON","description":"Mikhail Pavlov is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ALETHEA POWER","type":"PERSON","description":"Alethea Power is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"LUKASZ KAISER","type":"PERSON","description":"Lukasz Kaiser is a researcher involved in evaluating large language models trained on code.\nLukasz Kaiser is a researcher involved in training verifiers to solve math word problems.","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"MOHAMMAD BAVARIAN","type":"PERSON","description":"Mohammad Bavarian is a researcher involved in evaluating large language models trained on code.\nMohammad Bavarian is a researcher involved in training verifiers to solve math word problems.","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"CLEMENS WINTER","type":"PERSON","description":"Clemens Winter is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"PHILIPPE TILLET","type":"PERSON","description":"Philippe Tillet is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"FELIPE PETROSKI SUCH","type":"PERSON","description":"Felipe Petroski Such is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"DAVID W. CUMMINGS","type":"PERSON","description":"David W. Cummings is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"MATTHIAS PLAPPERT","type":"PERSON","description":"Matthias Plappert is a researcher involved in training verifiers to solve math word problems.\nMatthias Plappert is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"FOTIOS CHANTZIS","type":"PERSON","description":"Fotios Chantzis is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ELIZABETH BARNES","type":"PERSON","description":"Elizabeth Barnes is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ARIEL HERBERT-VOSS","type":"PERSON","description":"Ariel Herbert-Voss is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"WILLIAM H. GUSS","type":"PERSON","description":"William H. Guss is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ALEX NICOL","type":"PERSON","description":"Alex Nicol is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"IGOR BABUSHKIN","type":"PERSON","description":"Igor Babushkin is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"SUCHIR BALAJI","type":"PERSON","description":"Suchir Balaji is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"SHANTANU JAIN","type":"PERSON","description":"Shantanu Jain is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ANDREW CARR","type":"PERSON","description":"Andrew Carr is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JAN LEIKE","type":"PERSON","description":"Jan Leike is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JOSHUA ACHIAM","type":"PERSON","description":"Joshua Achiam is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"VEDANT MISRA","type":"PERSON","description":"Vedant Misra is a researcher involved in evaluating large language models trained on code.\nVedant Misra is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"EVAN MORIKAWA","type":"PERSON","description":"Evan Morikawa is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ALEC RADFORD","type":"PERSON","description":"Alec Radford is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"MATTHEW M. KNIGHT","type":"PERSON","description":"Matthew M. Knight is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"MILES BRUNDAGE","type":"PERSON","description":"Miles Brundage is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"MIRA MURATI","type":"PERSON","description":"Mira Murati is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"KATIE MAYER","type":"PERSON","description":"Katie Mayer is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"PETER WELINDER","type":"PERSON","description":"Peter Welinder is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"BOB MCGREW","type":"PERSON","description":"Bob McGrew is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"DARIO AMODEI","type":"PERSON","description":"Dario Amodei is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"SAM MCCANDLISH","type":"PERSON","description":"Sam McCandlish is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ILYA SUTSKEVER","type":"PERSON","description":"Ilya Sutskever is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"WOJCIECH ZAREMBA","type":"PERSON","description":"Wojciech Zaremba is a researcher involved in evaluating large language models trained on code.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"WENHU CHEN","type":"PERSON","description":"Wenhu Chen is a researcher who co-authored a paper on program of thoughts prompting for numerical reasoning tasks.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"XUEGUANG MA","type":"PERSON","description":"Xueguang Ma is a researcher who co-authored a paper on program of thoughts prompting for numerical reasoning tasks.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"WILLIAM W. COHEN","type":"PERSON","description":"William W. Cohen is a researcher who co-authored a paper on program of thoughts prompting for numerical reasoning tasks.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"AAKANKSHA CHOWDHERY","type":"PERSON","description":"Aakanksha Chowdhery is a researcher involved in the development of PaLM-E, an embodied multimodal language model.\nAakanksha Chowdhery is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"SHARAN NARANG","type":"PERSON","description":"Sharan Narang is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JACOB DEVLIN","type":"PERSON","description":"Jacob Devlin is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"MAARTEN BOSMA","type":"PERSON","description":"Maarten Bosma is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"GAURAV MISHRA","type":"PERSON","description":"Gaurav Mishra is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ADAM ROBERTS","type":"PERSON","description":"Adam Roberts is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"PAUL BARHAM","type":"PERSON","description":"Paul Barham is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"HYUNG WON CHUNG","type":"PERSON","description":"Hyung Won Chung is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"CHARLES SUTTON","type":"PERSON","description":"Charles Sutton is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"SEBASTIAN GEHRMANN","type":"PERSON","description":"Sebastian Gehrmann is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"PARKER SCHUH","type":"PERSON","description":"Parker Schuh is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"KENSEN SHI","type":"PERSON","description":"Kensen Shi is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"SASHA TSVYASHCHENKO","type":"PERSON","description":"Sasha Tsvyashchenko is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JOSHUA MAYNEZ","type":"PERSON","description":"Joshua Maynez is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ABHISHEK RAO","type":"PERSON","description":"Abhishek Rao is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"YI TAY","type":"PERSON","description":"Yi Tay is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"NOAM SHAZEER","type":"PERSON","description":"Noam Shazeer is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"VINODKUMAR PRABHAKARAN","type":"PERSON","description":"Vinodkumar Prabhakaran is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"EMILY REIF","type":"PERSON","description":"Emily Reif is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"NAN DU","type":"PERSON","description":"Nan Du is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"BEN HUTCHINSON","type":"PERSON","description":"Ben Hutchinson is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"REINER POPE","type":"PERSON","description":"Reiner Pope is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JAMES BRADBURY","type":"PERSON","description":"James Bradbury is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JACOB AUSTIN","type":"PERSON","description":"Jacob Austin is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"MICHAEL ISARD","type":"PERSON","description":"Michael Isard is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"GUY GUR-ARI","type":"PERSON","description":"Guy Gur-Ari is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"PENGCHENG YIN","type":"PERSON","description":"Pengcheng Yin is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"TOJU DUKE","type":"PERSON","description":"Toju Duke is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ANSELM LEVSKAYA","type":"PERSON","description":"Anselm Levskaya is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"SANJAY GHEMAWAT","type":"PERSON","description":"Sanjay Ghemawat is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"SUNIPA DEV","type":"PERSON","description":"Sunipa Dev is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"HENRYK MICHALIWSKI","type":"PERSON","description":"Henryk Michalewski is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"XAVIER GARCIA","type":"PERSON","description":"Xavier Garcia is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"KEVIN ROBINSON","type":"PERSON","description":"Kevin Robinson is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"LIAM FEDUS","type":"PERSON","description":"Liam Fedus is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"DENNY ZHOU","type":"PERSON","description":"Denny Zhou is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"DAPHNE IPPOLITO","type":"PERSON","description":"Daphne Ippolito is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"DAVID LUAN","type":"PERSON","description":"David Luan is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"HYEONTAEK LIM","type":"PERSON","description":"Hyeontaek Lim is a researcher involved in the development of PaLM, a language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"BARRET ZOPH","type":"PERSON","description":"Barret Zoph is a researcher involved(\"entity\"","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"VINEET KOSARAJU","type":"PERSON","description":"Vineet Kosaraju is a researcher involved in training verifiers to solve math word problems.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JACOB HILTON","type":"PERSON","description":"Jacob Hilton is a researcher involved in training verifiers to solve math word problems.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"REIICHIRO NAKANO","type":"PERSON","description":"Reiichiro Nakano is a researcher involved in training verifiers to solve math word problems.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"CHRISTOPHER HESSE","type":"PERSON","description":"Christopher Hesse is a researcher involved in training verifiers to solve math word problems.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JOHN SCHULMAN","type":"PERSON","description":"John Schulman is a researcher involved in training verifiers to solve math word problems.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"XIANG DENG","type":"PERSON","description":"Xiang Deng is a researcher involved in the development of Mind2Web, a generalist agent for the web.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"YU GU","type":"PERSON","description":"Yu Gu is a researcher involved in the development of Mind2Web, a generalist agent for the web.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"BOYUAN ZHENG","type":"PERSON","description":"Boyuan Zheng is a researcher involved in the development of Mind2Web, a generalist agent for the web.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"SHIJIE CHEN","type":"PERSON","description":"Shijie Chen is a researcher involved in the development of Mind2Web, a generalist agent for the web.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"SAMUEL STEVENS","type":"PERSON","description":"Samuel Stevens is a researcher involved in the development of Mind2Web, a generalist agent for the web.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"BOSHI WANG","type":"PERSON","description":"Boshi Wang is a researcher involved in the development of Mind2Web, a generalist agent for the web.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"HUAN SUN","type":"PERSON","description":"Huan Sun is a researcher involved in the development of Mind2Web, a generalist agent for the web.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"YU SU","type":"PERSON","description":"Yu Su is a researcher involved in the development of Mind2Web, a generalist agent for the web.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"DANNY DRIES","type":"PERSON","description":"Danny Driess is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"FEI XIA","type":"PERSON","description":"Fei Xia is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"MEHDI S. M. SAJJADI","type":"PERSON","description":"Mehdi S. M. Sajjadi is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"COREY LYNCH","type":"PERSON","description":"Corey Lynch is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"BRIAN ICHTER","type":"PERSON","description":"Brian Ichter is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"AYZAAN WAHID","type":"PERSON","description":"Ayzaan Wahid is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JONATHAN TOMPHSON","type":"PERSON","description":"Jonathan Tompson is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"QUAN VUONG","type":"PERSON","description":"Quan Vuong is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"TIANHE YU","type":"PERSON","description":"Tianhe Yu is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"WENLONG HUANG","type":"PERSON","description":"Wenlong Huang is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"YEVGEN CHEBOTAR","type":"PERSON","description":"Yevgen Chebotar is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"PIERRE SERMANET","type":"PERSON","description":"Pierre Sermanet is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"DANIEL DUCKWORTH","type":"PERSON","description":"Daniel Duckworth is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"SERGEY LEVINE","type":"PERSON","description":"Sergey Levine is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"VINCENT VANHOUCKE","type":"PERSON","description":"Vincent Vanhoucke is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"KAROL HAUSMAN","type":"PERSON","description":"Karol Hausman is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"MARC TOUSSAINT","type":"PERSON","description":"Marc Toussaint is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"KLAUS GREFF","type":"PERSON","description":"Klaus Greff is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ANDY ZENG","type":"PERSON","description":"Andy Zeng is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"IGOR MORDATCH","type":"PERSON","description":"Igor Mordatch is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"PETE FLORENCE","type":"PERSON","description":"Pete Florence is a researcher involved in the development of PaLM-E, an embodied multimodal language model.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"YILUN DU","type":"PERSON","description":"Yilun Du is a researcher involved in learning universal policies via text-guided video generation.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"MENGJIAO YANG","type":"PERSON","description":"Mengjiao Yang is a researcher involved in learning universal policies via text-guided video generation.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"BO DAI","type":"PERSON","description":"Bo Dai is a researcher involved in learning universal policies via text-guided video generation.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"HANJUN DAI","type":"PERSON","description":"Hanjun Dai is a researcher involved in learning universal policies via text-guided video generation.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"OFIR NACHUM","type":"PERSON","description":"Ofir Nachum is a researcher involved in learning universal policies via text-guided video generation.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JOSHUA B. TENENBAUM","type":"PERSON","description":"Joshua B. Tenenbaum is a researcher involved in learning universal policies via text-guided video generation.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"DALE SCHUURMANS","type":"PERSON","description":"Dale Schuurmans is a researcher involved in learning universal policies via text-guided video generation.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"PIETER ABBEEL","type":"PERSON","description":"Pieter Abbeel is a researcher involved in learning universal policies via text-guided video generation.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JONATHAN ST BT EVANS","type":"PERSON","description":"Jonathan St BT Evans is a researcher known for his work on intuition and reasoning from a dual-process perspective.","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"PSYCHOLOGICAL INQUIRY","type":"PUBLICATION","description":"Psychological Inquiry is a journal that publishes research on psychological topics, including intuition and reasoning.","source_id":"7a48515e86161237c03c9a8373197126"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AMODEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dario Amodei is a researcher known for his work on language models and their capabilities as few-shot learners.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"MURRAY CAMPBELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Murray Campbell is a researcher associated with the development of the Deep Blue chess program.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JOSEPH HOANE JR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joseph Hoane Jr is a researcher involved in the development of the Deep Blue chess program.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"FENG-HSIUNG HSU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Feng-hsiung Hsu is a researcher associated with the Deep Blue chess program.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"BEI CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bei Chen is a researcher who co-authored a paper on CodeT, a code generation model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"FENGJI ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fengji Zhang is a researcher who co-authored a paper on CodeT, a code generation model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ANH NGUYEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anh Nguyen is a researcher who co-authored a paper on CodeT, a code generation model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"DAOGUANG ZAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daoguang Zan is a researcher who co-authored a paper on CodeT, a code generation model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ZEQI LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zeqi Lin is a researcher who co-authored a paper on CodeT, a code generation model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JIAN-GUANG LOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jian-Guang Lou is a researcher who co-authored a paper on CodeT, a code generation model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"WEIZHU CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weizhu Chen is a researcher who co-authored a paper on CodeT, a code generation model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"MARK CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mark Chen is a researcher involved in evaluating large language models trained on code.Mark Chen is a researcher involved in training verifiers to solve math word problems.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JERRY TWOREK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jerry Tworek is a researcher involved in evaluating large language models trained on code.Jerry Tworek is a researcher involved in training verifiers to solve math word problems.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HEEWOO JUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heewoo Jun is a researcher involved in training verifiers to solve math word problems.Heewoo Jun is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QIMING YUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qiming Yuan is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"HENRIQUE PONDE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Henrique Ponde is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JARED KAPLAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jared Kaplan is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"HARRISON EDWARDS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Harrison Edwards is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"YURA BURDA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yura Burda is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"NICHOLAS JOSEPH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicholas Joseph is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"GREG BROCKMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Greg Brockman is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ALEX RAY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex Ray is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"RAUL PURI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Raul Puri is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"GRETCHEN KRUEGER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gretchen Krueger is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"MICHAEL PETROV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Petrov is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"HEIDY KHLAAF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heidy Khlaaf is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"GIRISH SASTRY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Girish Sastry is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"PAMELA MISHKIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pamela Mishkin is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"BROOKE CHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brooke Chan is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"SCOTT GRAY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Scott Gray is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"NICK RYDER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nick Ryder is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"MIKHAIL PAVLOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mikhail Pavlov is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ALETHEA POWER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alethea Power is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"LUKASZ KAISER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lukasz Kaiser is a researcher involved in evaluating large language models trained on code.Lukasz Kaiser is a researcher involved in training verifiers to solve math word problems.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MOHAMMAD BAVARIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mohammad Bavarian is a researcher involved in evaluating large language models trained on code.Mohammad Bavarian is a researcher involved in training verifiers to solve math word problems.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CLEMENS WINTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clemens Winter is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"PHILIPPE TILLET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Philippe Tillet is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"FELIPE PETROSKI SUCH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Felipe Petroski Such is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"DAVID W. CUMMINGS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David W. Cummings is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"MATTHIAS PLAPPERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthias Plappert is a researcher involved in training verifiers to solve math word problems.Matthias Plappert is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FOTIOS CHANTZIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fotios Chantzis is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ELIZABETH BARNES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Elizabeth Barnes is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ARIEL HERBERT-VOSS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ariel Herbert-Voss is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"WILLIAM H. GUSS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">William H. Guss is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ALEX NICOL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex Nicol is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"IGOR BABUSHKIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Igor Babushkin is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"SUCHIR BALAJI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Suchir Balaji is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"SHANTANU JAIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shantanu Jain is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ANDREW CARR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrew Carr is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JAN LEIKE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jan Leike is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JOSHUA ACHIAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshua Achiam is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"VEDANT MISRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vedant Misra is a researcher involved in evaluating large language models trained on code.Vedant Misra is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"EVAN MORIKAWA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Evan Morikawa is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ALEC RADFORD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alec Radford is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"MATTHEW M. KNIGHT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthew M. Knight is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"MILES BRUNDAGE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Miles Brundage is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"MIRA MURATI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mira Murati is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"KATIE MAYER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Katie Mayer is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"PETER WELINDER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter Welinder is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"BOB MCGREW\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bob McGrew is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"DARIO AMODEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dario Amodei is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"SAM MCCANDLISH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sam McCandlish is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ILYA SUTSKEVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ilya Sutskever is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"WOJCIECH ZAREMBA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wojciech Zaremba is a researcher involved in evaluating large language models trained on code.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"WENHU CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wenhu Chen is a researcher who co-authored a paper on program of thoughts prompting for numerical reasoning tasks.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"XUEGUANG MA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xueguang Ma is a researcher who co-authored a paper on program of thoughts prompting for numerical reasoning tasks.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"WILLIAM W. COHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">William W. Cohen is a researcher who co-authored a paper on program of thoughts prompting for numerical reasoning tasks.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"AAKANKSHA CHOWDHERY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aakanksha Chowdhery is a researcher involved in the development of PaLM-E, an embodied multimodal language model.Aakanksha Chowdhery is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHARAN NARANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sharan Narang is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JACOB DEVLIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Devlin is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"MAARTEN BOSMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maarten Bosma is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"GAURAV MISHRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gaurav Mishra is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ADAM ROBERTS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adam Roberts is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"PAUL BARHAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Paul Barham is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"HYUNG WON CHUNG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hyung Won Chung is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"CHARLES SUTTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Charles Sutton is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"SEBASTIAN GEHRMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sebastian Gehrmann is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"PARKER SCHUH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Parker Schuh is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"KENSEN SHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kensen Shi is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"SASHA TSVYASHCHENKO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sasha Tsvyashchenko is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JOSHUA MAYNEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshua Maynez is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ABHISHEK RAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Abhishek Rao is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"YI TAY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi Tay is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"NOAM SHAZEER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Noam Shazeer is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"VINODKUMAR PRABHAKARAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vinodkumar Prabhakaran is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"EMILY REIF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Emily Reif is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"NAN DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nan Du is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"BEN HUTCHINSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ben Hutchinson is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"REINER POPE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Reiner Pope is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JAMES BRADBURY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">James Bradbury is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JACOB AUSTIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Austin is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"MICHAEL ISARD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Isard is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"GUY GUR-ARI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guy Gur-Ari is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"PENGCHENG YIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pengcheng Yin is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"TOJU DUKE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Toju Duke is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ANSELM LEVSKAYA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anselm Levskaya is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"SANJAY GHEMAWAT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sanjay Ghemawat is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"SUNIPA DEV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sunipa Dev is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"HENRYK MICHALIWSKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Henryk Michalewski is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"XAVIER GARCIA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xavier Garcia is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"KEVIN ROBINSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kevin Robinson is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"LIAM FEDUS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liam Fedus is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"DENNY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Denny Zhou is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"DAPHNE IPPOLITO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daphne Ippolito is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"DAVID LUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David Luan is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"HYEONTAEK LIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hyeontaek Lim is a researcher involved in the development of PaLM, a language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"BARRET ZOPH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Barret Zoph is a researcher involved(\"entity\"<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"VINEET KOSARAJU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vineet Kosaraju is a researcher involved in training verifiers to solve math word problems.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JACOB HILTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Hilton is a researcher involved in training verifiers to solve math word problems.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"REIICHIRO NAKANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Reiichiro Nakano is a researcher involved in training verifiers to solve math word problems.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"CHRISTOPHER HESSE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Christopher Hesse is a researcher involved in training verifiers to solve math word problems.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JOHN SCHULMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">John Schulman is a researcher involved in training verifiers to solve math word problems.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"XIANG DENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiang Deng is a researcher involved in the development of Mind2Web, a generalist agent for the web.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"YU GU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu Gu is a researcher involved in the development of Mind2Web, a generalist agent for the web.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"BOYUAN ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Boyuan Zheng is a researcher involved in the development of Mind2Web, a generalist agent for the web.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"SHIJIE CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shijie Chen is a researcher involved in the development of Mind2Web, a generalist agent for the web.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"SAMUEL STEVENS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Samuel Stevens is a researcher involved in the development of Mind2Web, a generalist agent for the web.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"BOSHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Boshi Wang is a researcher involved in the development of Mind2Web, a generalist agent for the web.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"HUAN SUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Huan Sun is a researcher involved in the development of Mind2Web, a generalist agent for the web.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"YU SU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu Su is a researcher involved in the development of Mind2Web, a generalist agent for the web.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"DANNY DRIES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Danny Driess is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"FEI XIA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fei Xia is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"MEHDI S. M. SAJJADI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mehdi S. M. Sajjadi is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"COREY LYNCH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Corey Lynch is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"BRIAN ICHTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brian Ichter is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"AYZAAN WAHID\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ayzaan Wahid is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JONATHAN TOMPHSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jonathan Tompson is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"QUAN VUONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quan Vuong is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"TIANHE YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tianhe Yu is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"WENLONG HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wenlong Huang is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"YEVGEN CHEBOTAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yevgen Chebotar is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"PIERRE SERMANET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pierre Sermanet is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"DANIEL DUCKWORTH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel Duckworth is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"SERGEY LEVINE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sergey Levine is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"VINCENT VANHOUCKE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vincent Vanhoucke is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"KAROL HAUSMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karol Hausman is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"MARC TOUSSAINT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marc Toussaint is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"KLAUS GREFF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Klaus Greff is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ANDY ZENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andy Zeng is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"IGOR MORDATCH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Igor Mordatch is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"PETE FLORENCE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pete Florence is a researcher involved in the development of PaLM-E, an embodied multimodal language model.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"YILUN DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yilun Du is a researcher involved in learning universal policies via text-guided video generation.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"MENGJIAO YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mengjiao Yang is a researcher involved in learning universal policies via text-guided video generation.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"BO DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bo Dai is a researcher involved in learning universal policies via text-guided video generation.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"HANJUN DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hanjun Dai is a researcher involved in learning universal policies via text-guided video generation.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"OFIR NACHUM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ofir Nachum is a researcher involved in learning universal policies via text-guided video generation.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JOSHUA B. TENENBAUM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshua B. Tenenbaum is a researcher involved in learning universal policies via text-guided video generation.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"DALE SCHUURMANS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dale Schuurmans is a researcher involved in learning universal policies via text-guided video generation.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"PIETER ABBEEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pieter Abbeel is a researcher involved in learning universal policies via text-guided video generation.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JONATHAN ST BT EVANS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jonathan St BT Evans is a researcher known for his work on intuition and reasoning from a dual-process perspective.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"PSYCHOLOGICAL INQUIRY\">      <data key=\"d0\">PUBLICATION<\/data>      <data key=\"d1\">Psychological Inquiry is a journal that publishes research on psychological topics, including intuition and reasoning.<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"68e5573b596d253a03047b1e41988598","chunk":" An embodied multi-\nmodal language model. In ICML , 2023.\nYilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir\nNachum, Joshua B. Tenenbaum, Dale Schuurmans, and\nPieter Abbeel. Learning universal policies via text-guided\nvideo generation. In NeurIPS , 2023.\nJonathan St BT Evans. Intuition and reasoning: A dual-\nprocess perspective. Psychological Inquiry , pages 313 \u2013\n326, 2010.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar,\nYuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\nYuke Zhu, and Anima Anandkumar. MineDojo: Building\nopen-ended embodied agents with internet-scale knowl-\nedge. In NeurIPS Datasets and Benchmarks Track , 2022.\nHiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Mat-\nsuo, Shixiang Shane Gu, and Izzeddin Gur. Multimodal\nweb navigation with instruction-finetuned foundation\nmodels. In ICLR , 2024.Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei\nLiu, Yiming Yang, Jamie Callan, and Graham Neubig.\nPAL: Program-aided language models. In ICML , 2023.\nJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and\nJun Wang. Long text generation via adversarial training\nwith leaked information. In AAAI , 2018.\nWilliam H. Guss, Brandon Houghton, Nicholay Topin,\nPhillip Wang, Cayden Codel, Manuela Veloso, and Rus-\nlan Salakhutdinov. MineRL: A large-scale dataset of\nMinecraft demonstrations. In IJCAI , 2019.\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Ville-\ngas, David Ha, Honglak Lee, and James Davidson. Learn-\ning latent dynamics for planning from pixels. In ICML ,\n2019.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timo-\nthy Lillicrap. Mastering diverse domains through world\nmodels. arXiv:2301.04104 , 2023.\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen\nWang, Daisy Zhe Wang, and Zhiting Hu. Reasoning\nwith language model is planning with world model. In\nEMNLP , 2023.\nJie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven\nZheng, Adams Wei Yu, Xinying Song, and Denny Zhou.\nLarge language models cannot self-correct reasoning yet.\nInICLR , 2024.\nWenlong Huang, F. Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Peter R. Florence, Andy Zeng, Jonathan Tompson,\nIgor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah\nBrown, Tomas Jackson, Linda Luu, Sergey Levine, Karol\nHausman, and Brian Ichter. Inner monologue: Embodied\nreasoning through planning with language models. In\nCoRL , 2022.\nLevente Kocsis and Csaba Szepesv \u00b4ari. Bandit based monte-\ncarlo planning. In ECML , 2006.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka\nMatsuo, and Yusuke Iwasawa. Large language models\nare zero-shot reasoners. In NeurIPS , 2022.\nSteven M. LaValle. Rapidly-exploring random trees : A\nnew tool for path planning. The Annual Research Report ,\n1998.\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin\nShi, and Percy Liang. Reinforcement learning on web\ninterfaces using workflow-guided exploration. In ICLR ,\n2018.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu\nLei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men,\n11Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nKejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng,\nZhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun\nZhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong,\nand Jie Tang. AgentBench: Evaluating LLMs as agents.\nInICLR , 2024.\nZhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi\nKe, Boyi Liu, and Zhaoran Wang. Reason for fu-\nture, act for now: A principled framework for au-\ntonomous LLM agents with provable sample efficiency.\narXiv:2309.17382 , 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hal-\nlinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\nDziri, Shrimai Prabhumoye, Yiming Yang, Shashank\nGupta, Bodhisattwa Prasad Majumder, Katherine Her-\nmann, Sean Welle","chunk_id":"68e5573b596d253a03047b1e41988598","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"YILUN DU","type":"PERSON","description":"Yilun Du is a researcher involved in the development of an embodied multi-modal language model, contributing to advancements in AI and machine learning.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"MENGJIAO YANG","type":"PERSON","description":"Mengjiao Yang is a researcher who co-authored a paper on embodied multi-modal language models, focusing on integrating various modalities in AI.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"BO DAI","type":"PERSON","description":"Bo Dai is a researcher contributing to the field of multi-modal language models, particularly in the context of embodied agents.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"HANJUN DAI","type":"PERSON","description":"Hanjun Dai is a researcher involved in the development of multi-modal language models, focusing on their applications in AI.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"OFIR NACHUM","type":"PERSON","description":"Ofir Nachum is a researcher who has contributed to the field of multi-modal language models and their applications in AI.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"JOSHUA B. TENENBAUM","type":"PERSON","description":"Joshua B. Tenenbaum is a prominent researcher in AI, known for his work on cognitive models and multi-modal learning.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"DALE SCHUURMANS","type":"PERSON","description":"Dale Schuurmans is a researcher contributing to advancements in AI, particularly in multi-modal language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"PIETER ABBEEL","type":"PERSON","description":"Pieter Abbeel is a researcher known for his work in AI and robotics, contributing to the development of multi-modal language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"MINE DOJO","type":"PROJECT","description":"MineDojo is a project focused on building open-ended embodied agents using internet-scale knowledge, contributing to advancements in AI.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"INSTRUCTION-FINETUNED FOUNDATION MODELS","type":"TECHNIQUE","description":"Instruction-finetuned foundation models are AI models that have been fine-tuned to follow specific instructions, enhancing their performance in various tasks.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"PAL","type":"PROJECT","description":"PAL (Program-aided language models) is a project aimed at improving language models through programmatic assistance, enhancing their capabilities.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"MINE RL","type":"PROJECT","description":"MineRL is a large-scale dataset of Minecraft demonstrations, used for training AI agents in complex environments.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"AGENTBENCH","type":"PROJECT","description":"AgentBench is a framework for evaluating large language models (LLMs) as agents, assessing their performance in various tasks.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"REINFORCEMENT LEARNING","type":"TECHNIQUE","description":"Reinforcement learning is a machine learning technique where agents learn to make decisions by receiving rewards or penalties based on their actions.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"DUAL-PROCESS PERSPECTIVE","type":"THEORY","description":"The dual-process perspective is a psychological theory that explains reasoning and intuition as two distinct cognitive processes.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"LARGE LANGUAGE MODELS","type":"TECHNOLOGY","description":"Large language models are advanced AI systems designed to understand and generate human-like text based on vast amounts of data.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"LINXI FAN","type":"PERSON","description":"Linxi Fan is a researcher involved in the development of MineDojo, focusing on building open-ended embodied agents.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"GUANZHI WANG","type":"PERSON","description":"Guanzhi Wang is a researcher contributing to the MineDojo project, focusing on embodied agents with internet-scale knowledge.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"YUNFAN JIANG","type":"PERSON","description":"Yunfan Jiang is a researcher involved in the MineDojo project, contributing to the development of embodied agents.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"AJAY MANDLEKAR","type":"PERSON","description":"Ajay Mandlekar is a researcher contributing to the MineDojo project, focusing on building embodied agents.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"YUNCONG YANG","type":"PERSON","description":"Yuncong Yang is a researcher involved in the MineDojo project, contributing to the development of embodied agents.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"HAOYI ZHU","type":"PERSON","description":"Haoyi Zhu is a researcher contributing to the MineDojo project, focusing on building embodied agents.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"ANDREW TANG","type":"PERSON","description":"Andrew Tang is a researcher involved in the MineDojo project, contributing to the development of embodied agents.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"DE-AN HUANG","type":"PERSON","description":"De-An Huang is a researcher contributing to the MineDojo project, focusing on building embodied agents.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"YUKE ZHU","type":"PERSON","description":"Yuke Zhu is a researcher involved in the MineDojo project, contributing to the development of embodied agents.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"ANIMA ANANDKUMAR","type":"PERSON","description":"Anima Anandkumar is a researcher contributing to the MineDojo project, focusing on building embodied agents.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"HIROKI FURUTA","type":"PERSON","description":"Hiroki Furuta is a researcher involved in multimodal web navigation with instruction-finetuned foundation models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"KUANG-HUEI LEE","type":"PERSON","description":"Kuang-Huei Lee is a researcher contributing to multimodal web navigation with instruction-finetuned foundation models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"YUTAKA MATSUNO","type":"PERSON","description":"Yutaka Matsuno is a researcher involved in multimodal web navigation with instruction-finetuned foundation models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"SHIXIANG SHANE GU","type":"PERSON","description":"Shixiang Shane Gu is a researcher contributing to multimodal web navigation with instruction-finetuned foundation models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"IZZEDDIN GUR","type":"PERSON","description":"Izzeddin Gur is a researcher involved in multimodal web navigation with instruction-finetuned foundation models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"LUYU GAO","type":"PERSON","description":"Luyu Gao is a researcher contributing to the development of PAL, focusing on program-aided language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"AMAN MAADAAN","type":"PERSON","description":"Aman Madaan is a researcher involved in the development of PAL, focusing on program-aided language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"SHUYAN ZHOU","type":"PERSON","description":"Shuyan Zhou is a researcher contributing to the development of PAL, focusing on program-aided language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"URI ALON","type":"PERSON","description":"Uri Alon is a researcher involved in the development of PAL, focusing on program-aided language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"PENGFEI LIU","type":"PERSON","description":"Pengfei Liu is a researcher contributing to the development of PAL, focusing on program-aided language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"YIMING YANG","type":"PERSON","description":"Yiming Yang is a researcher involved in the development of PAL, focusing on program-aided language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"JAMIE CALLAN","type":"PERSON","description":"Jamie Callan is a researcher contributing to the development of PAL, focusing on program-aided language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"GRAHAM NEUBIG","type":"PERSON","description":"Graham Neubig is a researcher involved in the development of PAL, focusing on program-aided language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"DANIJAR HAFNER","type":"PERSON","description":"Danijar Hafner is a researcher involved in learning latent dynamics for planning from pixels.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"TIMOTHY LILLICRAP","type":"PERSON","description":"Timothy Lillicrap is a researcher contributing to learning latent dynamics for planning from pixels.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"IAN FISCHER","type":"PERSON","description":"Ian Fischer is a researcher involved in learning latent dynamics for planning from pixels.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"RUBEN VILLEGAS","type":"PERSON","description":"Ruben Villegas is a researcher contributing to learning latent dynamics for planning from pixels.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"DAVID HA","type":"PERSON","description":"David Ha is a researcher involved in learning latent dynamics for planning from pixels.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"HONGLAK LEE","type":"PERSON","description":"Honglak Lee is a researcher contributing to learning latent dynamics for planning from pixels.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"JAMES DAVIDSON","type":"PERSON","description":"James Davidson is a researcher involved in learning latent dynamics for planning from pixels.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"SHIBO HAO","type":"PERSON","description":"Shibo Hao is a researcher contributing to reasoning with language models in planning with world models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"YI GU","type":"PERSON","description":"Yi Gu is a researcher involved in reasoning with language models in planning with world models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"HAODI MA","type":"PERSON","description":"Haodi Ma is a researcher contributing to reasoning with language models in planning with world models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"JOSHUA JIAHUA HONG","type":"PERSON","description":"Joshua Jiahua Hong is a researcher involved in reasoning with language models in planning with world models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"ZHEN WANG","type":"PERSON","description":"Zhen Wang is a researcher contributing to reasoning with language models in planning with world models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"DAISY ZHE WANG","type":"PERSON","description":"Daisy Zhe Wang is a researcher involved in reasoning with language models in planning with world models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"ZHITING HU","type":"PERSON","description":"Zhiting Hu is a researcher contributing to reasoning with language models in planning with world models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"JIE HUANG","type":"PERSON","description":"Jie Huang is a researcher involved in large language models and their reasoning capabilities.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"XINYUN CHEN","type":"PERSON","description":"Xinyun Chen is a researcher contributing to the study of large language models and their reasoning capabilities.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"SWAROOP MISHRA","type":"PERSON","description":"Swaroop Mishra is a researcher involved in large language models and their reasoning capabilities.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"HUAIXIU STEVEN ZHENG","type":"PERSON","description":"Huaixiu Steven Zheng is a researcher contributing to the study of large language models and their reasoning capabilities.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"ADAMS WEI YU","type":"PERSON","description":"Adams Wei Yu is a researcher involved in large language models and their reasoning capabilities.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"XINYING SONG","type":"PERSON","description":"Xinying Song is a researcher contributing to the study of large language models and their reasoning capabilities.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"DENNY ZHOU","type":"PERSON","description":"Denny Zhou is a researcher involved in large language models and their reasoning capabilities.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"WENLONG HUANG","type":"PERSON","description":"Wenlong Huang is a researcher contributing to embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"F. XIA","type":"PERSON","description":"F. Xia is a researcher involved in embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"TED XIAO","type":"PERSON","description":"Ted Xiao is a researcher contributing to embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"HARRIS CHAN","type":"PERSON","description":"Harris Chan is a researcher involved in embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"JACKY LIANG","type":"PERSON","description":"Jacky Liang is a researcher contributing to embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"PETER R. FLORENCE","type":"PERSON","description":"Peter R. Florence is a researcher involved in embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"ANDY ZENG","type":"PERSON","description":"Andy Zeng is a researcher contributing to embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"JONATHAN TOMPSON","type":"PERSON","description":"Jonathan Tompson is a researcher involved in embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"IGOR MORDATCH","type":"PERSON","description":"Igor Mordatch is a researcher contributing to embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"YEVGEN CHEBOTAR","type":"PERSON","description":"Yevgen Chebotar is a researcher involved in embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"PIERRE SERMANET","type":"PERSON","description":"Pierre Sermanet is a researcher contributing to embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"NOAH BROWN","type":"PERSON","description":"Noah Brown is a researcher involved in embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"TOMAS JACKSON","type":"PERSON","description":"Tomas Jackson is a researcher contributing to embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"LINDA LUU","type":"PERSON","description":"Linda Luu is a researcher involved in embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"SERGEY LEVINE","type":"PERSON","description":"Sergey Levine is a researcher contributing to embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"KAROL HAUSMAN","type":"PERSON","description":"Karol Hausman is a researcher involved in embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"BRIAN ICHTER","type":"PERSON","description":"Brian Ichter is a researcher contributing to embodied reasoning through planning with language models.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"LEVANTE KOCSIS","type":"PERSON","description":"Levente Kocsis is a researcher known for his work on bandit-based Monte Carlo planning.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"CSABA SZEPESV\u00c1RI","type":"PERSON","description":"Csaba Szepesv\u00e1ri is a researcher contributing to bandit-based Monte Carlo planning.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"TAKESHI KOJIMA","type":"PERSON","description":"Takeshi Kojima is a researcher involved in large language models and their reasoning capabilities.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"MACHEL REID","type":"PERSON","description":"Machel Reid is a researcher contributing to large language models and their reasoning capabilities.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"YUSUKE IWASAWA","type":"PERSON","description":"Yusuke Iwasawa is a researcher involved in large language models and their reasoning capabilities.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"ZHIHAN LIU","type":"PERSON","description":"Zhihan Liu is a researcher contributing to the study of autonomous LLM agents with provable sample efficiency.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"HAO HU","type":"PERSON","description":"Hao Hu is a researcher involved in the study of autonomous LLM agents with provable sample efficiency.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"SHENAO ZHANG","type":"PERSON","description":"Shenao Zhang is a researcher contributing to the study of autonomous LLM agents with provable sample efficiency.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"HONGYI GUO","type":"PERSON","description":"Hongyi Guo is a researcher involved in the study of autonomous LLM agents with provable sample efficiency.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"SHUQI KE","type":"PERSON","description":"Shuqi Ke is a researcher contributing to the study of autonomous LLM agents with provable sample efficiency.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"BOYI LIU","type":"PERSON","description":"Boyi Liu is a researcher involved in the study of autonomous LLM agents with provable sample efficiency.","source_id":"68e5573b596d253a03047b1e41988598"},{"name":"ZHAORAN WANG","type":"PERSON","description":"Zhaoran Wang is a researcher contributing to the study of autonomous LLM agents with provable sample efficiency.)<|COMPLETE|>","source_id":"68e5573b596d253a03047b1e41988598"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"YILUN DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yilun Du is a researcher involved in the development of an embodied multi-modal language model, contributing to advancements in AI and machine learning.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"MENGJIAO YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mengjiao Yang is a researcher who co-authored a paper on embodied multi-modal language models, focusing on integrating various modalities in AI.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"BO DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bo Dai is a researcher contributing to the field of multi-modal language models, particularly in the context of embodied agents.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"HANJUN DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hanjun Dai is a researcher involved in the development of multi-modal language models, focusing on their applications in AI.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"OFIR NACHUM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ofir Nachum is a researcher who has contributed to the field of multi-modal language models and their applications in AI.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"JOSHUA B. TENENBAUM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshua B. Tenenbaum is a prominent researcher in AI, known for his work on cognitive models and multi-modal learning.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"DALE SCHUURMANS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dale Schuurmans is a researcher contributing to advancements in AI, particularly in multi-modal language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"PIETER ABBEEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pieter Abbeel is a researcher known for his work in AI and robotics, contributing to the development of multi-modal language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"MINE DOJO\">      <data key=\"d0\">PROJECT<\/data>      <data key=\"d1\">MineDojo is a project focused on building open-ended embodied agents using internet-scale knowledge, contributing to advancements in AI.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"INSTRUCTION-FINETUNED FOUNDATION MODELS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Instruction-finetuned foundation models are AI models that have been fine-tuned to follow specific instructions, enhancing their performance in various tasks.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"PAL\">      <data key=\"d0\">PROJECT<\/data>      <data key=\"d1\">PAL (Program-aided language models) is a project aimed at improving language models through programmatic assistance, enhancing their capabilities.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"MINE RL\">      <data key=\"d0\">PROJECT<\/data>      <data key=\"d1\">MineRL is a large-scale dataset of Minecraft demonstrations, used for training AI agents in complex environments.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"AGENTBENCH\">      <data key=\"d0\">PROJECT<\/data>      <data key=\"d1\">AgentBench is a framework for evaluating large language models (LLMs) as agents, assessing their performance in various tasks.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"REINFORCEMENT LEARNING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Reinforcement learning is a machine learning technique where agents learn to make decisions by receiving rewards or penalties based on their actions.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"DUAL-PROCESS PERSPECTIVE\">      <data key=\"d0\">THEORY<\/data>      <data key=\"d1\">The dual-process perspective is a psychological theory that explains reasoning and intuition as two distinct cognitive processes.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"LARGE LANGUAGE MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Large language models are advanced AI systems designed to understand and generate human-like text based on vast amounts of data.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"LINXI FAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linxi Fan is a researcher involved in the development of MineDojo, focusing on building open-ended embodied agents.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"GUANZHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guanzhi Wang is a researcher contributing to the MineDojo project, focusing on embodied agents with internet-scale knowledge.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"YUNFAN JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yunfan Jiang is a researcher involved in the MineDojo project, contributing to the development of embodied agents.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"AJAY MANDLEKAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ajay Mandlekar is a researcher contributing to the MineDojo project, focusing on building embodied agents.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"YUNCONG YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuncong Yang is a researcher involved in the MineDojo project, contributing to the development of embodied agents.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"HAOYI ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Haoyi Zhu is a researcher contributing to the MineDojo project, focusing on building embodied agents.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"ANDREW TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrew Tang is a researcher involved in the MineDojo project, contributing to the development of embodied agents.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"DE-AN HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">De-An Huang is a researcher contributing to the MineDojo project, focusing on building embodied agents.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"YUKE ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuke Zhu is a researcher involved in the MineDojo project, contributing to the development of embodied agents.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"ANIMA ANANDKUMAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anima Anandkumar is a researcher contributing to the MineDojo project, focusing on building embodied agents.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"HIROKI FURUTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hiroki Furuta is a researcher involved in multimodal web navigation with instruction-finetuned foundation models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"KUANG-HUEI LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kuang-Huei Lee is a researcher contributing to multimodal web navigation with instruction-finetuned foundation models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"YUTAKA MATSUNO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yutaka Matsuno is a researcher involved in multimodal web navigation with instruction-finetuned foundation models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"SHIXIANG SHANE GU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shixiang Shane Gu is a researcher contributing to multimodal web navigation with instruction-finetuned foundation models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"IZZEDDIN GUR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Izzeddin Gur is a researcher involved in multimodal web navigation with instruction-finetuned foundation models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"LUYU GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luyu Gao is a researcher contributing to the development of PAL, focusing on program-aided language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"AMAN MAADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aman Madaan is a researcher involved in the development of PAL, focusing on program-aided language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"SHUYAN ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shuyan Zhou is a researcher contributing to the development of PAL, focusing on program-aided language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"URI ALON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Uri Alon is a researcher involved in the development of PAL, focusing on program-aided language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"PENGFEI LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pengfei Liu is a researcher contributing to the development of PAL, focusing on program-aided language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"YIMING YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiming Yang is a researcher involved in the development of PAL, focusing on program-aided language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"JAMIE CALLAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jamie Callan is a researcher contributing to the development of PAL, focusing on program-aided language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"GRAHAM NEUBIG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Graham Neubig is a researcher involved in the development of PAL, focusing on program-aided language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"DANIJAR HAFNER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Danijar Hafner is a researcher involved in learning latent dynamics for planning from pixels.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"TIMOTHY LILLICRAP\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Timothy Lillicrap is a researcher contributing to learning latent dynamics for planning from pixels.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"IAN FISCHER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ian Fischer is a researcher involved in learning latent dynamics for planning from pixels.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"RUBEN VILLEGAS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ruben Villegas is a researcher contributing to learning latent dynamics for planning from pixels.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"DAVID HA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David Ha is a researcher involved in learning latent dynamics for planning from pixels.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"HONGLAK LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Honglak Lee is a researcher contributing to learning latent dynamics for planning from pixels.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"JAMES DAVIDSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">James Davidson is a researcher involved in learning latent dynamics for planning from pixels.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"SHIBO HAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shibo Hao is a researcher contributing to reasoning with language models in planning with world models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"YI GU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi Gu is a researcher involved in reasoning with language models in planning with world models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"HAODI MA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Haodi Ma is a researcher contributing to reasoning with language models in planning with world models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"JOSHUA JIAHUA HONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshua Jiahua Hong is a researcher involved in reasoning with language models in planning with world models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"ZHEN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhen Wang is a researcher contributing to reasoning with language models in planning with world models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"DAISY ZHE WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daisy Zhe Wang is a researcher involved in reasoning with language models in planning with world models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"ZHITING HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiting Hu is a researcher contributing to reasoning with language models in planning with world models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"JIE HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jie Huang is a researcher involved in large language models and their reasoning capabilities.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"XINYUN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xinyun Chen is a researcher contributing to the study of large language models and their reasoning capabilities.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"SWAROOP MISHRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Swaroop Mishra is a researcher involved in large language models and their reasoning capabilities.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"HUAIXIU STEVEN ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Huaixiu Steven Zheng is a researcher contributing to the study of large language models and their reasoning capabilities.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"ADAMS WEI YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adams Wei Yu is a researcher involved in large language models and their reasoning capabilities.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"XINYING SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xinying Song is a researcher contributing to the study of large language models and their reasoning capabilities.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"DENNY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Denny Zhou is a researcher involved in large language models and their reasoning capabilities.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"WENLONG HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wenlong Huang is a researcher contributing to embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"F. XIA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">F. Xia is a researcher involved in embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"TED XIAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ted Xiao is a researcher contributing to embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"HARRIS CHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Harris Chan is a researcher involved in embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"JACKY LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacky Liang is a researcher contributing to embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"PETER R. FLORENCE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter R. Florence is a researcher involved in embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"ANDY ZENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andy Zeng is a researcher contributing to embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"JONATHAN TOMPSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jonathan Tompson is a researcher involved in embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"IGOR MORDATCH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Igor Mordatch is a researcher contributing to embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"YEVGEN CHEBOTAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yevgen Chebotar is a researcher involved in embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"PIERRE SERMANET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pierre Sermanet is a researcher contributing to embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"NOAH BROWN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Noah Brown is a researcher involved in embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"TOMAS JACKSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tomas Jackson is a researcher contributing to embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"LINDA LUU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linda Luu is a researcher involved in embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"SERGEY LEVINE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sergey Levine is a researcher contributing to embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"KAROL HAUSMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karol Hausman is a researcher involved in embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"BRIAN ICHTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brian Ichter is a researcher contributing to embodied reasoning through planning with language models.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"LEVANTE KOCSIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Levente Kocsis is a researcher known for his work on bandit-based Monte Carlo planning.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"CSABA SZEPESV&#193;RI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Csaba Szepesv&#225;ri is a researcher contributing to bandit-based Monte Carlo planning.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"TAKESHI KOJIMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Takeshi Kojima is a researcher involved in large language models and their reasoning capabilities.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"MACHEL REID\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Machel Reid is a researcher contributing to large language models and their reasoning capabilities.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"YUSUKE IWASAWA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yusuke Iwasawa is a researcher involved in large language models and their reasoning capabilities.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"ZHIHAN LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhihan Liu is a researcher contributing to the study of autonomous LLM agents with provable sample efficiency.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"HAO HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao Hu is a researcher involved in the study of autonomous LLM agents with provable sample efficiency.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"SHENAO ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shenao Zhang is a researcher contributing to the study of autonomous LLM agents with provable sample efficiency.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"HONGYI GUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hongyi Guo is a researcher involved in the study of autonomous LLM agents with provable sample efficiency.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"SHUQI KE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shuqi Ke is a researcher contributing to the study of autonomous LLM agents with provable sample efficiency.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"BOYI LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Boyi Liu is a researcher involved in the study of autonomous LLM agents with provable sample efficiency.<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <node id=\"ZHAORAN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhaoran Wang is a researcher contributing to the study of autonomous LLM agents with provable sample efficiency.)&lt;|COMPLETE|&gt;<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>    <\/node>    <edge source=\"YILUN DU\" target=\"MINE DOJO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Yilun Du is a contributor to the MineDojo project, focusing on building embodied agents with internet-scale knowledge.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"YILUN DU\" target=\"YILUN DU\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Yilun Du is involved in the development of an embodied multi-modal language model, which is related to the MineDojo project.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"YILUN DU\" target=\"MENGJIAO YANG\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Mengjiao Yang collaborates with Yilun Du on the development of multi-modal language models, contributing to the same research area.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"YILUN DU\" target=\"BO DAI\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Bo Dai collaborates with Yilun Du on the development of multi-modal language models, contributing to the same research area.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"YILUN DU\" target=\"HANJUN DAI\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Hanjun Dai collaborates with Yilun Du on the development of multi-modal language models, contributing to the same research area.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"YILUN DU\" target=\"OFIR NACHUM\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Ofir Nachum collaborates with Yilun Du on the development of multi-modal language models, contributing to the same research area.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"YILUN DU\" target=\"JOSHUA B. TENENBAUM\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Joshua B. Tenenbaum collaborates with Yilun Du on the development of multi-modal language models, contributing to the same research area.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"YILUN DU\" target=\"DALE SCHUURMANS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Dale Schuurmans collaborates with Yilun Du on the development of multi-modal language models, contributing to the same research area.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"YILUN DU\" target=\"PIETER ABBEEL\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Pieter Abbeel collaborates with Yilun Du on the development of multi-modal language models, contributing to the same research area.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"MENGJIAO YANG\" target=\"MINE DOJO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Mengjiao Yang is a contributor to the MineDojo project, focusing on building embodied agents with internet-scale knowledge.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"BO DAI\" target=\"MINE DOJO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Bo Dai is a contributor to the MineDojo project, focusing on building embodied agents with internet-scale knowledge.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"HANJUN DAI\" target=\"MINE DOJO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Hanjun Dai is a contributor to the MineDojo project, focusing on building embodied agents with internet-scale knowledge.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"OFIR NACHUM\" target=\"MINE DOJO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Ofir Nachum is a contributor to the MineDojo project, focusing on building embodied agents with internet-scale knowledge.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"JOSHUA B. TENENBAUM\" target=\"MINE DOJO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Joshua B. Tenenbaum is a contributor to the MineDojo project, focusing on building embodied agents with internet-scale knowledge.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"DALE SCHUURMANS\" target=\"MINE DOJO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Dale Schuurmans is a contributor to the MineDojo project, focusing on building embodied agents with internet-scale knowledge.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"PIETER ABBEEL\" target=\"MINE DOJO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Pieter Abbeel is a contributor to the MineDojo project, focusing on building embodied agents with internet-scale knowledge.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"MINE DOJO\" target=\"MINE RL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">MineRL is a dataset used in the MineDojo project for training AI agents in complex environments.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"PAL\" target=\"LARGE LANGUAGE MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">PAL aims to enhance large language models through programmatic assistance, improving their capabilities.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"AGENTBENCH\" target=\"LARGE LANGUAGE MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AgentBench evaluates large language models as agents, assessing their performance in various tasks.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"AGENTBENCH\" target=\"REINFORCEMENT LEARNING\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Reinforcement learning techniques are evaluated within the AgentBench framework to assess agent performance.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>    <edge source=\"DUAL-PROCESS PERSPECTIVE\" target=\"LARGE LANGUAGE MODELS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The dual-process perspective can be applied to understand reasoning in large language models, linking cognitive theories to AI.<\/data>      <data key=\"d5\">68e5573b596d253a03047b1e41988598<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"2d4672dfb7bd4283f0b5f23ab4f26653","chunk":" efficiency.\narXiv:2309.17382 , 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hal-\nlinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\nDziri, Shrimai Prabhumoye, Yiming Yang, Shashank\nGupta, Bodhisattwa Prasad Majumder, Katherine Her-\nmann, Sean Welleck, Amir Yazdanbakhsh, and Peter\nClark. Self-refine: Iterative refinement with self-feedback.\nInNeurIPS , 2023.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar\nGulcehre, and Bing Xiang. Abstractive text summariza-\ntion using sequence-to-sequence RNNs and beyond. In\nSpecial Interest Group on Natural Language Learning ,\n2016.\nOpenAI. GPT-4 technical report. arXiv:2303.08774 , 2023.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill\nQian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou,\nMark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.\nToolLLM: Facilitating large language models to master\n16000+ real-world APIs. In ICLR , 2024.\nAbulhair Saparov and He He. Language models are greedy\nreasoners: A systematic formal analysis of chain-of-\nthought. In ICLR , 2023.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess `\u0131, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-\ncedda, and Thomas Scialom. Toolformer: Language\nmodels can teach themselves to use tools. In NeurIPS ,\n2023.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. HuggingGPT: Solving\nAI tasks with ChatGPT and its friends in Hugging Face.\nInNeurIPS , 2023.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin\nGopinath, Karthik Narasimhan, and Shunyu Yao. Reflex-\nion: Language agents with verbal reinforcement learning.\nInNeurIPS , 2023.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C \u02c6ot\u00b4e,\nYonatan Bisk, Adam Trischler, and Matthew Hausknecht.\nALFWorld: Aligning text and embodied environments\nfor interactive learning. In ICLR , 2020.David Silver, Aja Huang, Chris J. Maddison, Arthur Guez,\nL. Sifre, George van den Driessche, Julian Schrittwieser,\nIoannis Antonoglou, Vedavyas Panneershelvam, Marc\nLanctot, Sander Dieleman, Dominik Grewe, John Nham,\nNal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,\nMadeleine Leach, Koray Kavukcuoglu, Thore Graepel,\nand Demis Hassabis. Mastering the game of Go with deep\nneural networks and tree search. Nature , 529:484\u2013489,\n2016.\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez,\nL. Sifre, George van den Driessche, Julian Schrittwieser,\nIoannis Antonoglou, Vedavyas Panneershelvam, Marc\nLanctot, Sander Dieleman, Dominik Grewe, John Nham,\nNal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,\nMadeleine Leach, Koray Kavukcuoglu, Thore Graepel,\nand Demis Hassabis. Mastering chess and Shogi by self-\nplay with a general reinforcement learning algorithm.\narXiv:1712.01815 , 2017.\nSteven A. Sloman. The empirical case for two systems of\nreasoning. Psychological Bulletin , 119:3\u201322, 1996.\nHaotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and\nChao Zhang. AdaPlanner: Adaptive planning from feed-\nback with language models. In NeurIPS , 2023.\nD\u00b4\u0131dac Sur \u00b4\u0131s, Sachit Menon, and Carl V ondrick. ViperGPT:\nVisual inference via Python execution for reasoning. In\nICCV , 2023.\nMaciej Swiechowski, Konrad Godlewski, Bartosz Sawicki,\nand Jacek Ma\u2019ndziuk. Monte Carlo tree search: A re-\nview of recent modifications and applications. Artificial\nIntelligence Review , 56:2497\u20132562, 2021.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, Daniel M. Bikel, Lukas Blecher, Cristian Cant \u00b4on\nFerrer, Moya Chen, Guillem Cucurull","chunk_id":"2d4672dfb7bd4283f0b5f23ab4f26653","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"AMAN MAADAAN","type":"PERSON","description":"Aman Madaan is a researcher involved in the development of the Self-refine method for iterative refinement with self-feedback.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"NIKET TANDON","type":"PERSON","description":"Niket Tandon is a researcher who co-authored the Self-refine paper, contributing to advancements in iterative refinement techniques.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"PRAKHAR GUPTA","type":"PERSON","description":"Prakhar Gupta is a researcher who co-authored the Self-refine paper, focusing on self-feedback mechanisms.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SKYLER HALLINAN","type":"PERSON","description":"Skyler Hallinan is a researcher who co-authored the Self-refine paper, contributing to the field of natural language processing.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"LUYU GAO","type":"PERSON","description":"Luyu Gao is a researcher who co-authored the Self-refine paper, focusing on iterative refinement techniques.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SARAH WIEGREFFE","type":"PERSON","description":"Sarah Wiegreffe is a researcher who co-authored the Self-refine paper, contributing to advancements in self-feedback methods.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"URI ALON","type":"PERSON","description":"Uri Alon is a researcher who co-authored the Self-refine paper, focusing on iterative refinement in language models.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"NOUHA DZIRI","type":"PERSON","description":"Nouha Dziri is a researcher who co-authored the Self-refine paper, contributing to the field of natural language processing.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SHRIMAI PRABHUMOYE","type":"PERSON","description":"Shrimai Prabhumoye is a researcher who co-authored the Self-refine paper, focusing on self-feedback mechanisms.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YIMING YANG","type":"PERSON","description":"Yiming Yang is a researcher who co-authored the Self-refine paper, contributing to advancements in iterative refinement techniques.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SHASHANK GUPTA","type":"PERSON","description":"Shashank Gupta is a researcher who co-authored the Self-refine paper, focusing on self-feedback methods.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"BODHISATTVA PRASAD MAJUMDER","type":"PERSON","description":"Bodhisattwa Prasad Majumder is a researcher who co-authored the Self-refine paper, contributing to the field of natural language processing.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"KATHERINE HERMANN","type":"PERSON","description":"Katherine Hermann is a researcher who co-authored the Self-refine paper, focusing on iterative refinement techniques.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SEAN WELLECK","type":"PERSON","description":"Sean Welleck is a researcher who co-authored the Self-refine paper, contributing to advancements in self-feedback methods.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"AMIR YAZDANBAKHSH","type":"PERSON","description":"Amir Yazdanbakhsh is a researcher who co-authored the Self-refine paper, focusing on iterative refinement in language models.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"PETER CLARK","type":"PERSON","description":"Peter Clark is a researcher who co-authored the Self-refine paper, contributing to the field of natural language processing.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"RAMESH NALLAPATI","type":"PERSON","description":"Ramesh Nallapati is a researcher who co-authored a paper on abstractive text summarization using sequence-to-sequence RNNs.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"BOWEN ZHOU","type":"PERSON","description":"Bowen Zhou is a researcher who co-authored a paper on abstractive text summarization, focusing on sequence-to-sequence models.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"CICERO DOS SANTOS","type":"PERSON","description":"Cicero dos Santos is a researcher who co-authored a paper on abstractive text summarization, contributing to advancements in RNNs.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"CAGLAR GULCEHRE","type":"PERSON","description":"Caglar Gulcehre is a researcher who co-authored a paper on abstractive text summarization, focusing on sequence-to-sequence techniques.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"BING XIANG","type":"PERSON","description":"Bing Xiang is a researcher who co-authored a paper on abstractive text summarization, contributing to the field of natural language processing.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"OPENAI","type":"ORGANIZATION","description":"OpenAI is an artificial intelligence research organization known for developing models like GPT-4 and contributing to advancements in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a state-of-the-art language model developed by OpenAI, known for its advanced capabilities in natural language processing.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YUJIA QIN","type":"PERSON","description":"Yujia Qin is a researcher who co-authored the ToolLLM paper, focusing on facilitating large language models to master real-world APIs.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SHIHAO LIANG","type":"PERSON","description":"Shihao Liang is a researcher who co-authored the ToolLLM paper, contributing to advancements in API integration with language models.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YINING YE","type":"PERSON","description":"Yining Ye is a researcher who co-authored the ToolLLM paper, focusing on enhancing language models' capabilities.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"KUNLUN ZHU","type":"PERSON","description":"Kunlun Zhu is a researcher who co-authored the ToolLLM paper, contributing to the integration of language models with APIs.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"LAN YAN","type":"PERSON","description":"Lan Yan is a researcher who co-authored the ToolLLM paper, focusing on facilitating language models' interactions with APIs.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YAXI LU","type":"PERSON","description":"Yaxi Lu is a researcher who co-authored the ToolLLM paper, contributing to advancements in language model capabilities.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YANKAI LIN","type":"PERSON","description":"Yankai Lin is a researcher who co-authored the ToolLLM paper, focusing on enhancing language models' API interactions.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"XIN CONG","type":"PERSON","description":"Xin Cong is a researcher who co-authored the ToolLLM paper, contributing to the integration of language models with real-world APIs.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"XIANGRU TANG","type":"PERSON","description":"Xiangru Tang is a researcher who co-authored the ToolLLM paper, focusing on facilitating language models' capabilities.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"BILL QIAN","type":"PERSON","description":"Bill Qian is a researcher who co-authored the ToolLLM paper, contributing to advancements in language model integration.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SIHAN ZHAO","type":"PERSON","description":"Sihan Zhao is a researcher who co-authored the ToolLLM paper, focusing on enhancing language models' API interactions.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"RUNCHU TIAN","type":"PERSON","description":"Runchu Tian is a researcher who co-authored the ToolLLM paper, contributing to the integration of language models with APIs.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"RUOBING XIE","type":"PERSON","description":"Ruobing Xie is a researcher who co-authored the ToolLLM paper, focusing on facilitating language models' capabilities.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"JIE ZHOU","type":"PERSON","description":"Jie Zhou is a researcher who co-authored the ToolLLM paper, contributing to advancements in language model integration.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MARK GERSTEIN","type":"PERSON","description":"Mark Gerstein is a researcher who co-authored the ToolLLM paper, focusing on enhancing language models' API interactions.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"DAHAI LI","type":"PERSON","description":"Dahai Li is a researcher who co-authored the ToolLLM paper, contributing to the integration of language models with real-world APIs.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ZHUYUAN LIU","type":"PERSON","description":"Zhiyuan Liu is a researcher who co-authored the ToolLLM paper, focusing on facilitating language models' capabilities.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MAOSONG SUN","type":"PERSON","description":"Maosong Sun is a researcher who co-authored the ToolLLM paper, contributing to advancements in language model integration.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ABULHAIR SAPAROV","type":"PERSON","description":"Abulhair Saparov is a researcher who co-authored a paper analyzing the reasoning capabilities of language models.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"HE HE","type":"PERSON","description":"He He is a researcher who co-authored a paper analyzing the reasoning capabilities of language models, focusing on chain-of-thought reasoning.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"TIMO SCHICK","type":"PERSON","description":"Timo Schick is a researcher who co-authored the Toolformer paper, focusing on language models teaching themselves to use tools.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"JANE DWIVEDI-YU","type":"PERSON","description":"Jane Dwivedi-Yu is a researcher who co-authored the Toolformer paper, contributing to advancements in language model capabilities.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ROBERTO DESSI","type":"PERSON","description":"Roberto Dess\u00ec is a researcher who co-authored the Toolformer paper, focusing on language models' self-teaching capabilities.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ROBERTA RAILEANU","type":"PERSON","description":"Roberta Raileanu is a researcher who co-authored the Toolformer paper, contributing to advancements in language model integration.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MARIA LOMELI","type":"PERSON","description":"Maria Lomeli is a researcher who co-authored the Toolformer paper, focusing on enhancing language models' capabilities.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"LUKE ZETTLEMOYER","type":"PERSON","description":"Luke Zettlemoyer is a researcher who co-authored the Toolformer paper, contributing to advancements in language model integration.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"NICOLA CANCEDDA","type":"PERSON","description":"Nicola Cancedda is a researcher who co-authored the Toolformer paper, focusing on language models' self-teaching capabilities.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"THOMAS SCIALOM","type":"PERSON","description":"Thomas Scialom is a researcher who co-authored the Toolformer paper, contributing to advancements in language model capabilities.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YONATAN BISK","type":"PERSON","description":"Yonatan Bisk is a researcher who co-authored the ALFWorld paper, focusing on aligning text and embodied environments for interactive learning.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ADAM TRISCHLER","type":"PERSON","description":"Adam Trischler is a researcher who co-authored the ALFWorld paper, contributing to advancements in interactive learning.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MATTHEW HAUSKNECHT","type":"PERSON","description":"Matthew Hausknecht is a researcher who co-authored the ALFWorld paper, focusing on aligning text and environments for learning.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"DAVID SILVER","type":"PERSON","description":"David Silver is a researcher known for his work on reinforcement learning and has co-authored papers on game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"AJA HUANG","type":"PERSON","description":"Aja Huang is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"CHRIS J. MADDISON","type":"PERSON","description":"Chris J. Maddison is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ARTHUR GUEZ","type":"PERSON","description":"Arthur Guez is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"L. SIFRE","type":"PERSON","description":"L. Sifre is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"GEORGE VAN DEN DRIESSCHE","type":"PERSON","description":"George van den Driessche is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"JULIAN SCHRITTWIESER","type":"PERSON","description":"Julian Schrittwieser is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"IOANNIS ANTONOGLOU","type":"PERSON","description":"Ioannis Antonoglou is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"VEDAVYAS PANNEERSHELVAM","type":"PERSON","description":"Vedavyas Panneershelvam is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MARC LANCTOT","type":"PERSON","description":"Marc Lanctot is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SANDER DIELEMAN","type":"PERSON","description":"Sander Dieleman is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"DOMINIK GREWE","type":"PERSON","description":"Dominik Grewe is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"JOHN NHAM","type":"PERSON","description":"John Nham is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"NAL KALCHBRENNER","type":"PERSON","description":"Nal Kalchbrenner is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ILYA SUTSKEVER","type":"PERSON","description":"Ilya Sutskever is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"TIMOTHY P. LILLICRAP","type":"PERSON","description":"Timothy P. Lillicrap is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MADELEINE LEACH","type":"PERSON","description":"Madeleine Leach is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"KORAY KAVUKCUOGLU","type":"PERSON","description":"Koray Kavukcuoglu is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"THORE GRAEPEL","type":"PERSON","description":"Thore Graepel is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"DEMIS HASABIS","type":"PERSON","description":"Demis Hassabis is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"HAOTIAN SUN","type":"PERSON","description":"Haotian Sun is a researcher who co-authored the AdaPlanner paper, focusing on adaptive planning with language models.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YUCHEN ZHUANG","type":"PERSON","description":"Yuchen Zhuang is a researcher who co-authored the AdaPlanner paper, contributing to advancements in adaptive planning.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"LINGKAI KONG","type":"PERSON","description":"Lingkai Kong is a researcher who co-authored the AdaPlanner paper, focusing on feedback mechanisms in planning.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"BO DAI","type":"PERSON","description":"Bo Dai is a researcher who co-authored the AdaPlanner paper, contributing to advancements in adaptive planning techniques.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"CHAO ZHANG","type":"PERSON","description":"Chao Zhang is a researcher who co-authored the AdaPlanner paper, focusing on adaptive planning with language models.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"DIDAC SURIS","type":"PERSON","description":"D\u00eddac Sur\u00eds is a researcher who co-authored the ViperGPT paper, focusing on visual inference via Python execution.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SACHIT MENON","type":"PERSON","description":"Sachit Menon is a researcher who co-authored the ViperGPT paper, contributing to advancements in visual inference.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"CARL VONDRICK","type":"PERSON","description":"Carl Vondrick is a researcher who co-authored the ViperGPT paper, focusing on reasoning through visual inference.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MACIEJ SWIECHOWSKI","type":"PERSON","description":"Maciej Swiechowski is a researcher who co-authored a review on Monte Carlo tree search and its applications.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"KONRAD GODLEWSKI","type":"PERSON","description":"Konrad Godlewski is a researcher who co-authored a review on Monte Carlo tree search, focusing on recent modifications.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"BARTOSZ SAWICKI","type":"PERSON","description":"Bartosz Sawicki is a researcher who co-authored a review on Monte Carlo tree search and its applications.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"JACEK MA\u2019NDZIUK","type":"PERSON","description":"Jacek Ma\u2019ndziuk is a researcher who co-authored a review on Monte Carlo tree search, focusing on its applications in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"HUGO TOUVRON","type":"PERSON","description":"Hugo Touvron is a researcher who co-authored a paper on visual inference and its applications in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"LOUIS MARTIN","type":"PERSON","description":"Louis Martin is a researcher who co-authored a paper on visual inference and its applications in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"KEVIN R. STONE","type":"PERSON","description":"Kevin R. Stone is a researcher who co-authored a paper on visual inference and its applications in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"PETER ALBERT","type":"PERSON","description":"Peter Albert is a researcher who co-authored a paper on visual inference and its applications in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"AMJAD ALMAHAIRI","type":"PERSON","description":"Amjad Almahairi is a researcher who co-authored a paper on visual inference and its applications in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YASMINE BABAIEI","type":"(\"ENTITY\"","description":"YASMINE BABAIEI","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"NIKOLAY BASHLYKOV","type":"PERSON","description":"Nikolay Bashlykov is a researcher who co-authored a paper on visual inference and its applications in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SOUMYA BATRA","type":"PERSON","description":"Soumya Batra is a researcher who co-authored a paper on visual inference and its applications in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"PRAJJWAL BHARGAVA","type":"PERSON","description":"Prajjwal Bhargava is a researcher who co-authored a paper on visual inference and its applications in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SHRUTI BHOSALE","type":"PERSON","description":"Shruti Bhosale is a researcher who co-authored a paper on visual inference and its applications in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"DANIEL M. BIKEL","type":"PERSON","description":"Daniel M. Bikel is a researcher who co-authored a paper on visual inference and its applications in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"LUKAS BLECHER","type":"PERSON","description":"Lukas Blecher is a researcher who co-authored a paper on visual inference and its applications in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"CRISTIAN CANT\u00d3N FERRER","type":"PERSON","description":"Cristian Cant\u00f3n Ferrer is a researcher who co-authored a paper on visual inference and its applications in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MOYA CHEN","type":"PERSON","description":"Moya Chen is a researcher who co-authored a paper on visual inference and its applications in AI.","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"GUILLEM CUCURULL","type":"PERSON","description":"Guillem Cucurull is a researcher who co-authored a paper on visual inference and its applications in AI.)<|COMPLETE|>","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AMAN MAADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aman Madaan is a researcher involved in the development of the Self-refine method for iterative refinement with self-feedback.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"NIKET TANDON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Niket Tandon is a researcher who co-authored the Self-refine paper, contributing to advancements in iterative refinement techniques.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"PRAKHAR GUPTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Prakhar Gupta is a researcher who co-authored the Self-refine paper, focusing on self-feedback mechanisms.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SKYLER HALLINAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Skyler Hallinan is a researcher who co-authored the Self-refine paper, contributing to the field of natural language processing.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"LUYU GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luyu Gao is a researcher who co-authored the Self-refine paper, focusing on iterative refinement techniques.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SARAH WIEGREFFE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sarah Wiegreffe is a researcher who co-authored the Self-refine paper, contributing to advancements in self-feedback methods.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"URI ALON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Uri Alon is a researcher who co-authored the Self-refine paper, focusing on iterative refinement in language models.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"NOUHA DZIRI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nouha Dziri is a researcher who co-authored the Self-refine paper, contributing to the field of natural language processing.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SHRIMAI PRABHUMOYE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shrimai Prabhumoye is a researcher who co-authored the Self-refine paper, focusing on self-feedback mechanisms.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YIMING YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiming Yang is a researcher who co-authored the Self-refine paper, contributing to advancements in iterative refinement techniques.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SHASHANK GUPTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shashank Gupta is a researcher who co-authored the Self-refine paper, focusing on self-feedback methods.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"BODHISATTVA PRASAD MAJUMDER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bodhisattwa Prasad Majumder is a researcher who co-authored the Self-refine paper, contributing to the field of natural language processing.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"KATHERINE HERMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Katherine Hermann is a researcher who co-authored the Self-refine paper, focusing on iterative refinement techniques.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SEAN WELLECK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sean Welleck is a researcher who co-authored the Self-refine paper, contributing to advancements in self-feedback methods.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"AMIR YAZDANBAKHSH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amir Yazdanbakhsh is a researcher who co-authored the Self-refine paper, focusing on iterative refinement in language models.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"PETER CLARK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter Clark is a researcher who co-authored the Self-refine paper, contributing to the field of natural language processing.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"RAMESH NALLAPATI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ramesh Nallapati is a researcher who co-authored a paper on abstractive text summarization using sequence-to-sequence RNNs.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"BOWEN ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bowen Zhou is a researcher who co-authored a paper on abstractive text summarization, focusing on sequence-to-sequence models.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"CICERO DOS SANTOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cicero dos Santos is a researcher who co-authored a paper on abstractive text summarization, contributing to advancements in RNNs.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"CAGLAR GULCEHRE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Caglar Gulcehre is a researcher who co-authored a paper on abstractive text summarization, focusing on sequence-to-sequence techniques.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"BING XIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bing Xiang is a researcher who co-authored a paper on abstractive text summarization, contributing to the field of natural language processing.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"OPENAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">OpenAI is an artificial intelligence research organization known for developing models like GPT-4 and contributing to advancements in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a state-of-the-art language model developed by OpenAI, known for its advanced capabilities in natural language processing.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YUJIA QIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yujia Qin is a researcher who co-authored the ToolLLM paper, focusing on facilitating large language models to master real-world APIs.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SHIHAO LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shihao Liang is a researcher who co-authored the ToolLLM paper, contributing to advancements in API integration with language models.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YINING YE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yining Ye is a researcher who co-authored the ToolLLM paper, focusing on enhancing language models' capabilities.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"KUNLUN ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kunlun Zhu is a researcher who co-authored the ToolLLM paper, contributing to the integration of language models with APIs.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"LAN YAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lan Yan is a researcher who co-authored the ToolLLM paper, focusing on facilitating language models' interactions with APIs.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YAXI LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yaxi Lu is a researcher who co-authored the ToolLLM paper, contributing to advancements in language model capabilities.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YANKAI LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yankai Lin is a researcher who co-authored the ToolLLM paper, focusing on enhancing language models' API interactions.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"XIN CONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xin Cong is a researcher who co-authored the ToolLLM paper, contributing to the integration of language models with real-world APIs.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"XIANGRU TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiangru Tang is a researcher who co-authored the ToolLLM paper, focusing on facilitating language models' capabilities.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"BILL QIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bill Qian is a researcher who co-authored the ToolLLM paper, contributing to advancements in language model integration.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SIHAN ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sihan Zhao is a researcher who co-authored the ToolLLM paper, focusing on enhancing language models' API interactions.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"RUNCHU TIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Runchu Tian is a researcher who co-authored the ToolLLM paper, contributing to the integration of language models with APIs.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"RUOBING XIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ruobing Xie is a researcher who co-authored the ToolLLM paper, focusing on facilitating language models' capabilities.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"JIE ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jie Zhou is a researcher who co-authored the ToolLLM paper, contributing to advancements in language model integration.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MARK GERSTEIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mark Gerstein is a researcher who co-authored the ToolLLM paper, focusing on enhancing language models' API interactions.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"DAHAI LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dahai Li is a researcher who co-authored the ToolLLM paper, contributing to the integration of language models with real-world APIs.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ZHUYUAN LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyuan Liu is a researcher who co-authored the ToolLLM paper, focusing on facilitating language models' capabilities.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MAOSONG SUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maosong Sun is a researcher who co-authored the ToolLLM paper, contributing to advancements in language model integration.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ABULHAIR SAPAROV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Abulhair Saparov is a researcher who co-authored a paper analyzing the reasoning capabilities of language models.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"HE HE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">He He is a researcher who co-authored a paper analyzing the reasoning capabilities of language models, focusing on chain-of-thought reasoning.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"TIMO SCHICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Timo Schick is a researcher who co-authored the Toolformer paper, focusing on language models teaching themselves to use tools.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"JANE DWIVEDI-YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jane Dwivedi-Yu is a researcher who co-authored the Toolformer paper, contributing to advancements in language model capabilities.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ROBERTO DESSI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Roberto Dess&#236; is a researcher who co-authored the Toolformer paper, focusing on language models' self-teaching capabilities.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ROBERTA RAILEANU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Roberta Raileanu is a researcher who co-authored the Toolformer paper, contributing to advancements in language model integration.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MARIA LOMELI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maria Lomeli is a researcher who co-authored the Toolformer paper, focusing on enhancing language models' capabilities.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"LUKE ZETTLEMOYER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luke Zettlemoyer is a researcher who co-authored the Toolformer paper, contributing to advancements in language model integration.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"NICOLA CANCEDDA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicola Cancedda is a researcher who co-authored the Toolformer paper, focusing on language models' self-teaching capabilities.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"THOMAS SCIALOM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Scialom is a researcher who co-authored the Toolformer paper, contributing to advancements in language model capabilities.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YONATAN BISK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yonatan Bisk is a researcher who co-authored the ALFWorld paper, focusing on aligning text and embodied environments for interactive learning.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ADAM TRISCHLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adam Trischler is a researcher who co-authored the ALFWorld paper, contributing to advancements in interactive learning.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MATTHEW HAUSKNECHT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthew Hausknecht is a researcher who co-authored the ALFWorld paper, focusing on aligning text and environments for learning.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"DAVID SILVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David Silver is a researcher known for his work on reinforcement learning and has co-authored papers on game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"AJA HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aja Huang is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"CHRIS J. MADDISON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chris J. Maddison is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ARTHUR GUEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arthur Guez is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"L. SIFRE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">L. Sifre is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"GEORGE VAN DEN DRIESSCHE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">George van den Driessche is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"JULIAN SCHRITTWIESER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Julian Schrittwieser is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"IOANNIS ANTONOGLOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ioannis Antonoglou is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"VEDAVYAS PANNEERSHELVAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vedavyas Panneershelvam is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MARC LANCTOT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marc Lanctot is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SANDER DIELEMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sander Dieleman is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"DOMINIK GREWE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dominik Grewe is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"JOHN NHAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">John Nham is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"NAL KALCHBRENNER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nal Kalchbrenner is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ILYA SUTSKEVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ilya Sutskever is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"TIMOTHY P. LILLICRAP\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Timothy P. Lillicrap is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MADELEINE LEACH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Madeleine Leach is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"KORAY KAVUKCUOGLU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Koray Kavukcuoglu is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"THORE GRAEPEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thore Graepel is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"DEMIS HASABIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Demis Hassabis is a researcher who co-authored papers on reinforcement learning and game mastery using deep neural networks.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"HAOTIAN SUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Haotian Sun is a researcher who co-authored the AdaPlanner paper, focusing on adaptive planning with language models.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YUCHEN ZHUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuchen Zhuang is a researcher who co-authored the AdaPlanner paper, contributing to advancements in adaptive planning.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"LINGKAI KONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lingkai Kong is a researcher who co-authored the AdaPlanner paper, focusing on feedback mechanisms in planning.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"BO DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bo Dai is a researcher who co-authored the AdaPlanner paper, contributing to advancements in adaptive planning techniques.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"CHAO ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chao Zhang is a researcher who co-authored the AdaPlanner paper, focusing on adaptive planning with language models.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"DIDAC SURIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">D&#237;dac Sur&#237;s is a researcher who co-authored the ViperGPT paper, focusing on visual inference via Python execution.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SACHIT MENON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sachit Menon is a researcher who co-authored the ViperGPT paper, contributing to advancements in visual inference.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"CARL VONDRICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carl Vondrick is a researcher who co-authored the ViperGPT paper, focusing on reasoning through visual inference.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MACIEJ SWIECHOWSKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maciej Swiechowski is a researcher who co-authored a review on Monte Carlo tree search and its applications.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"KONRAD GODLEWSKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Konrad Godlewski is a researcher who co-authored a review on Monte Carlo tree search, focusing on recent modifications.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"BARTOSZ SAWICKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bartosz Sawicki is a researcher who co-authored a review on Monte Carlo tree search and its applications.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"JACEK MA&#8217;NDZIUK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacek Ma&#8217;ndziuk is a researcher who co-authored a review on Monte Carlo tree search, focusing on its applications in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"HUGO TOUVRON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hugo Touvron is a researcher who co-authored a paper on visual inference and its applications in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"LOUIS MARTIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Louis Martin is a researcher who co-authored a paper on visual inference and its applications in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"KEVIN R. STONE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kevin R. Stone is a researcher who co-authored a paper on visual inference and its applications in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"PETER ALBERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter Albert is a researcher who co-authored a paper on visual inference and its applications in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"AMJAD ALMAHAIRI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amjad Almahairi is a researcher who co-authored a paper on visual inference and its applications in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YASMINE BABAIEI\">      <data key=\"d0\">(\"ENTITY\"<\/data>      <data key=\"d1\">YASMINE BABAIEI<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"NIKOLAY BASHLYKOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nikolay Bashlykov is a researcher who co-authored a paper on visual inference and its applications in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SOUMYA BATRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Soumya Batra is a researcher who co-authored a paper on visual inference and its applications in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"PRAJJWAL BHARGAVA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Prajjwal Bhargava is a researcher who co-authored a paper on visual inference and its applications in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SHRUTI BHOSALE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shruti Bhosale is a researcher who co-authored a paper on visual inference and its applications in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"DANIEL M. BIKEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel M. Bikel is a researcher who co-authored a paper on visual inference and its applications in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"LUKAS BLECHER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lukas Blecher is a researcher who co-authored a paper on visual inference and its applications in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"CRISTIAN CANT&#211;N FERRER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cristian Cant&#243;n Ferrer is a researcher who co-authored a paper on visual inference and its applications in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MOYA CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Moya Chen is a researcher who co-authored a paper on visual inference and its applications in AI.<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"GUILLEM CUCURULL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guillem Cucurull is a researcher who co-authored a paper on visual inference and its applications in AI.)&lt;|COMPLETE|&gt;<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"8180bf20b7577f3eee40df5991e2886d","chunk":":2497\u20132562, 2021.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, Daniel M. Bikel, Lukas Blecher, Cristian Cant \u00b4on\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Is-\nabel M. Kloumann, A. V . Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,\nTodor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin\nNie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, R. Subramanian, Xia Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Rodriguez,\nRobert Stojnic, Sergey Edunov, and Thomas Scialom.\n12Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nLlama 2: Open foundation and fine-tuned chat models.\narXiv:2307.09288 , 2023.\nTom V odopivec, Spyridon Samothrakis, and Branko Ster.\nOn Monte Carlo tree search and reinforcement learning.\nJournal of Artificial Intelligence Research , 60:881\u2013936,\n2017.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anand-\nkumar. V oyager: An open-ended embodied agent with\nlarge language models. arXiv:2305.16291 , 2023.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. Self-consistency improves\nchain of thought reasoning in language models. In ICLR ,\n2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of\nthought prompting elicits reasoning in large language\nmodels. In NeurIPS , 2022.\nMichael Wooldridge and Nicholas R Jennings. Intelligent\nagents: Theory and practice. The Knowledge Engineering\nReview , 10:115 \u2013 152, 1995.\nPhilipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter\nAbbeel, and Ken Goldberg. Daydreamer: World models\nfor physical robot learning. In CoRL , 2023.\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-\nYen Kan, Junxian He, and Qizhe Xie. Decomposition\nenhances reasoning via self-evaluation guided decoding.\narXiv:2305.00633 , 2023.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam W Cohen, Ruslan Salakhutdinov, and Christo-\npher D Manning. HotpotQA: A dataset for diverse, ex-\nplainable multi-hop question answering. In EMNLP ,\n2018.\nShunyu Yao, Howard Chen, John Yang, and Karthik R\nNarasimhan. WebShop: Towards scalable real-world web\ninteraction with grounded language agents. In NeurIPS ,\n2022.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: deliberate problem solving with large\nlanguage models. In NeurIPS , 2023a.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,\nKarthik Narasimhan, and Yuan Cao. ReAct: Synergizing\nreasoning and acting in language models. In ICLR , 2023b.\nWeirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel,\nand Yang Gao. Mastering Atari games with limited data.\nInNeurIPS , 2021.Denny Zhou, Nathanael Sch \u00a8arli, Le Hou, Jason Wei, Nathan\nScales, Xuezhi Wang, Dale Schuurmans, Olivier Bous-\nquet, Quoc Le, and Ed Chi. Least-to-most prompting\nenables complex reasoning in large language models. In\nICLR , 2022.\nYuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor\nBursztyn, Ryan A. Rossi, Somdeb Sarkhel, and Chao\nZhang. ToolChain*: Efficient action space navigation in\nlarge language models with A* search. In ICLR , ","chunk_id":"8180bf20b7577f3eee40df5991e2886d","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"HUGO TOUVRON","type":"PERSON","description":"Hugo Touvron is a researcher involved in the development of language models and related technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"LOUIS MARTIN","type":"PERSON","description":"Louis Martin is a researcher contributing to advancements in language models and AI technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"KEVIN R. STONE","type":"PERSON","description":"Kevin R. Stone is a researcher focused on language models and their applications.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PETER ALBERT","type":"PERSON","description":"Peter Albert is a researcher working on language models and AI methodologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"AMJAD ALMAHAIRI","type":"PERSON","description":"Amjad Almahairi is a researcher contributing to the field of language models and AI.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YASMINE BABAIE","type":"PERSON","description":"Yasmine Babaie is a researcher involved in the development of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"NIKOLAY BASHLYKOV","type":"PERSON","description":"Nikolay Bashlykov is a researcher contributing to advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SOUMYA BATRA","type":"PERSON","description":"Soumya Batra is a researcher focused on language models and AI technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PRAJJWAL BHARGAVA","type":"PERSON","description":"Prajjwal Bhargava is a researcher working on language models and their applications.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SHRUTI BHOSALE","type":"PERSON","description":"Shruti Bhosale is a researcher contributing to the field of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"DANIEL M. BIKEL","type":"PERSON","description":"Daniel M. Bikel is a researcher involved in language model development.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"LUKAS BLECHER","type":"PERSON","description":"Lukas Blecher is a researcher focused on advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"CRISTIAN CANT\u00d3N FERRER","type":"PERSON","description":"Cristian Cant\u00f3n Ferrer is a researcher contributing to language model technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MOYA CHEN","type":"PERSON","description":"Moya Chen is a researcher involved in the development of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"GUILLEM CUCURULL","type":"PERSON","description":"Guillem Cucurull is a researcher focused on language models and AI.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"DAVID ESIOSU","type":"PERSON","description":"David Esiosu is a researcher contributing to advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JUDE FERNANDES","type":"PERSON","description":"Jude Fernandes is a researcher working on language models and their applications.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JEREMY FU","type":"PERSON","description":"Jeremy Fu is a researcher involved in the development of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"WENYIN FU","type":"PERSON","description":"Wenyin Fu is a researcher focused on advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"BRIAN FULLER","type":"PERSON","description":"Brian Fuller is a researcher contributing to the field of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"CYNTHIA GAO","type":"PERSON","description":"Cynthia Gao is a researcher involved in language model development.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"VEDANUJ GOSWAMI","type":"PERSON","description":"Vedanuj Goswami is a researcher focused on language models and AI technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"NAMAN GOYAL","type":"PERSON","description":"Naman Goyal is a researcher contributing to advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ANTHONY S. HARTSHORN","type":"PERSON","description":"Anthony S. Hartshorn is a researcher working on language models and their applications.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SAGHAR HOSSEINI","type":"PERSON","description":"Saghar Hosseini is a researcher involved in the development of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"RUI HOU","type":"PERSON","description":"Rui Hou is a researcher focused on advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"HAKAN INAN","type":"PERSON","description":"Hakan Inan is a researcher contributing to the field of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MARCIN KARDAS","type":"PERSON","description":"Marcin Kardas is a researcher involved in language model development.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"VIKTOR KERKEZ","type":"PERSON","description":"Viktor Kerkez is a researcher focused on advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MADIAN KHABSA","type":"PERSON","description":"Madian Khabsa is a researcher contributing to language model technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ISABEL M. KLOUMANN","type":"PERSON","description":"Isabel M. Kloumann is a researcher involved in the development of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"A. V. KORENEV","type":"PERSON","description":"A. V. Korenev is a researcher focused on language models and AI.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PUNIT SINGH KOURA","type":"PERSON","description":"Punit Singh Koura is a researcher contributing to advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MARIE-ANNE LACHAUX","type":"PERSON","description":"Marie-Anne Lachaux is a researcher working on language models and their applications.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"THIBAUT LAVRIL","type":"PERSON","description":"Thibaut Lavril is a researcher involved in the development of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JENYA LEE","type":"PERSON","description":"Jenya Lee is a researcher focused on advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"DIANA LISKOVICH","type":"PERSON","description":"Diana Liskovich is a researcher contributing to the field of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YINGHAI LU","type":"PERSON","description":"Yinghai Lu is a researcher involved in language model development.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YUNING MAO","type":"PERSON","description":"Yuning Mao is a researcher focused on language models and AI technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"XAVIER MARTINET","type":"PERSON","description":"Xavier Martinet is a researcher contributing to advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"TODOR MIHAYLOV","type":"PERSON","description":"Todor Mihaylov is a researcher working on language models and their applications.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PUSHKAR MISHRA","type":"PERSON","description":"Pushkar Mishra is a researcher involved in the development of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"IGOR MOLYBOG","type":"PERSON","description":"Igor Molybog is a researcher focused on advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YIXIN NIE","type":"PERSON","description":"Yixin Nie is a researcher contributing to the field of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ANDREW POULTON","type":"PERSON","description":"Andrew Poulton is a researcher involved in language model development.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JEREMY REIZENSTEIN","type":"PERSON","description":"Jeremy Reizenstein is a researcher focused on advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"RASHI RUNGTA","type":"PERSON","description":"Rashi Rungta is a researcher contributing to the field of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"KALYAN SALADI","type":"PERSON","description":"Kalyan Saladi is a researcher involved in the development of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ALAN SCHELTON","type":"PERSON","description":"Alan Schelton is a researcher focused on advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"RUAN SILVA","type":"PERSON","description":"Ruan Silva is a researcher contributing to language model technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ERIC MICHAEL SMITH","type":"PERSON","description":"Eric Michael Smith is a researcher involved in the development of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"R. SUBRAMANIAN","type":"PERSON","description":"R. Subramanian is a researcher focused on advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"XIA TAN","type":"PERSON","description":"Xia Tan is a researcher contributing to the field of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"BINH TANG","type":"PERSON","description":"Binh Tang is a researcher involved in language model development.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ROSS TAYLOR","type":"PERSON","description":"Ross Taylor is a researcher focused on advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ADINA WILLIAMS","type":"PERSON","description":"Adina Williams is a researcher contributing to the field of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JIAN XIANG KUAN","type":"PERSON","description":"Jian Xiang Kuan is a researcher involved in the development of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PUXIN XU","type":"PERSON","description":"Puxin Xu is a researcher focused on advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ZHENGXU YAN","type":"PERSON","description":"Zhengxu Yan is a researcher contributing to the field of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ILIYAN ZAROV","type":"PERSON","description":"Iliyan Zarov is a researcher involved in language model development.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ANGELA FAN","type":"PERSON","description":"Angela Fan is a researcher contributing to the field of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MELANIE KAMBADUR","type":"PERSON","description":"Melanie Kambadur is a researcher involved in language model development.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SHARAN NARANG","type":"PERSON","description":"Sharan Narang is a researcher focused on advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"AURELIEN RODRIGUEZ","type":"PERSON","description":"Aurelien Rodriguez is a researcher contributing to the field of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ROBERT STOJNIC","type":"PERSON","description":"Robert Stojnic is a researcher involved in language model development.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SERGEY EDUNOV","type":"PERSON","description":"Sergey Edunov is a researcher involved in the development of language models.\nSergey Edunov is a researcher focused on advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"PERSON"},{"name":"THOMAS SCIALOM","type":"PERSON","description":"Thomas Scialom is a researcher contributing to the field of language models.\nThomas Scialom is a researcher contributing to advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"PERSON"},{"name":"LLAMA 2","type":"TECHNOLOGY","description":"Llama 2 is an open foundation and fine-tuned chat model developed for various applications in AI.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"LANGUAGE AGENT TREE SEARCH","type":"TECHNIQUE","description":"Language Agent Tree Search is a method that unifies reasoning, acting, and planning in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"TECHNIQUE"},{"name":"VOYAGER","type":"TECHNOLOGY","description":"Voyager is an open-ended embodied agent that utilizes large language models for various tasks.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SELF-CONSISTENCY","type":"TECHNIQUE","description":"Self-consistency is a method that improves chain of thought reasoning in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"CHAIN OF THOUGHT PROMPTING","type":"TECHNIQUE","description":"Chain of thought prompting is a technique that elicits reasoning in large language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"INTELLIGENT AGENTS","type":"CONCEPT","description":"Intelligent agents are systems that can reason, act, and plan, often discussed in the context of AI.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"DAYDREAMER","type":"TECHNOLOGY","description":"Daydreamer is a model for physical robot learning that utilizes world models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"DECOMPOSITION","type":"TECHNIQUE","description":"Decomposition is a method that enhances reasoning through self-evaluation guided decoding.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"HOTPOTQA","type":"DATASET","description":"HotpotQA is a dataset designed for diverse, explainable multi-hop question answering.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"WEBSHOP","type":"TECHNOLOGY","description":"WebShop is a system aimed at scalable real-world web interaction with grounded language agents.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"TREE OF THOUGHTS","type":"TECHNIQUE","description":"Tree of thoughts is a method for deliberate problem solving using large language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"REACT","type":"TECHNIQUE","description":"ReAct is a technique that synergizes reasoning and acting in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MASTERING ATARI GAMES","type":"RESEARCH","description":"Mastering Atari games with limited data is a research area focused on AI learning in gaming environments.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"LEAST-TO-MOST PROMPTING","type":"TECHNIQUE","description":"Least-to-most prompting is a method that enables complex reasoning in large language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"TOOLCHAIN*","type":"TECHNOLOGY","description":"ToolChain* is a system for efficient action space navigation in large language models using A* search.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"HOWARD CHEN","type":"PERSON","description":"Howard Chen is a researcher contributing to the development of language models and AI systems.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JOHN YANG","type":"PERSON","description":"John Yang is a researcher focused on advancements in language models and their applications.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"KARTHIK R NARASIMHAN","type":"PERSON","description":"Karthik R Narasimhan is a researcher involved in the development of language models and AI technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"DANIJAR HAFNER","type":"PERSON","description":"Danijar Hafner is a researcher contributing to advancements in AI and language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PIETER ABBEEL","type":"PERSON","description":"Pieter Abbeel is a researcher focused on AI technologies and language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"KEN GOLDBERG","type":"PERSON","description":"Ken Goldberg is a researcher involved in the development of AI systems and language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"XUEZHI WANG","type":"PERSON","description":"Xuezhi Wang is a researcher contributing to advancements in language models and AI technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JASON WEI","type":"PERSON","description":"Jason Wei is a researcher focused on language models and their applications.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"DALE SCHUURMANS","type":"PERSON","description":"Dale Schuurmans is a researcher involved in the development of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"QUOC LE","type":"PERSON","description":"Quoc Le is a researcher contributing to advancements in language models and AI technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ED CHI","type":"PERSON","description":"Ed Chi is a researcher focused on AI systems and language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"DENNY ZHOU","type":"PERSON","description":"Denny Zhou is a researcher involved in the development of language models and AI technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YUXI XIE","type":"PERSON","description":"Yuxi Xie is a researcher contributing to advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"KENJI KAWAGUCHI","type":"PERSON","description":"Kenji Kawaguchi is a researcher focused on language models and their applications.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YIRAN ZHAO","type":"PERSON","description":"Yiran Zhao is a researcher involved in the development of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"XU ZHAO","type":"PERSON","description":"Xu Zhao is a researcher contributing to advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MIN-YEN KAN","type":"PERSON","description":"Min-Yen Kan is a researcher focused on language models and their applications.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JUNXIAN HE","type":"PERSON","description":"Junxian He is a researcher involved in the development of language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"QIZHE XIE","type":"PERSON","description":"Qizhe Xie is a researcher contributing to advancements in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PHILIPP WU","type":"PERSON","description":"Philipp Wu is a researcher focused on AI technologies and language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MONTE CARLO TREE SEARCH","type":"TECHNIQUE","description":"Monte Carlo Tree Search is a method used in reinforcement learning and decision-making processes.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"REINFORCEMENT LEARNING","type":"TECHNIQUE","description":"Reinforcement Learning is a type of machine learning where agents learn to make decisions by receiving rewards or penalties.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"A* SEARCH","type":"TECHNIQUE","description":"A* Search is an algorithm used for pathfinding and graph traversal, commonly applied in AI.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MONTY CARLO TREE SEARCH","type":"TECHNIQUE","description":"Monte Carlo Tree Search is a heuristic search algorithm for decision processes.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"CHAIN OF THOUGHT REASONING","type":"TECHNIQUE","description":"Chain of Thought Reasoning is a method that enhances reasoning capabilities in language models.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PROBLEM SOLVING","type":"CONCEPT","description":"Problem Solving is a cognitive process aimed at finding solutions to complex issues, often discussed in AI contexts.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SCALABLE WEB INTERACTION","type":"CONCEPT","description":"Scalable Web Interaction refers to systems designed for efficient interaction with web services using AI.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"EXPLAINABLE AI","type":"CONCEPT","description":"Explainable AI refers to methods and techniques that make the decision-making processes of AI systems understandable to humans.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MULTI-HOP QUESTION ANSWERING","type":"CONCEPT","description":"Multi-hop Question Answering is a task in AI that involves answering questions that require reasoning across multiple pieces of information.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"EMNLP","type":"CONFERENCE","description":"EMNLP is a conference focused on natural language processing and computational linguistics.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ICLR","type":"CONFERENCE","description":"ICLR is a conference that focuses on learning representations and deep learning.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"NEURIPS","type":"CONFERENCE","description":"NeurIPS is a conference that covers advancements in neural information processing systems and machine learning.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"CORL","type":"CONFERENCE","description":"CoRL is a conference focused on robot learning and reinforcement learning.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"AI RESEARCH","type":"FIELD","description":"AI Research encompasses various studies and advancements in artificial intelligence technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"NLP","type":"FIELD","description":"Natural Language Processing (NLP) is a field of AI focused on the interaction between computers and human language.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ROBOTICS","type":"FIELD","description":"Robotics is a field of engineering and AI focused on the design and application of robots.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"AGENT-BASED SYSTEMS","type":"FIELD","description":"Agent-based systems are computational systems that use autonomous agents to perform tasks and solve problems.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"AI ETHICS","type":"FIELD","description":"AI Ethics is a field that examines the moral implications and societal impacts of artificial intelligence technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"DATASET","type":"CONCEPT","description":"A dataset is a collection of data used for analysis and training in machine learning and AI.","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"AI SYSTEMS","type":"CONCEPT","description":"AI Systems refer to computational systems that utilize artificial intelligence to perform tasks and make decisions.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI TECHNOLOGIES","type":"CONCEPT","description":"AI Technologies encompass various tools and methods used in the development and application of artificial intelligence.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI APPLICATIONS","type":"CONCEPT","description":"AI Applications refer to the practical uses of artificial intelligence in various fields and industries.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI MODELS","type":"CONCEPT","description":"AI Models are mathematical representations used to simulate and predict behaviors in artificial intelligence systems.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI TOOLS","type":"CONCEPT","description":"AI Tools are software and applications designed to assist in the development and implementation of AI technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI FRAMEWORKS","type":"CONCEPT","description":"AI Frameworks are structured environments that facilitate the development and deployment of AI applications.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI PLATFORMS","type":"CONCEPT","description":"AI Platforms are comprehensive solutions that provide tools and services for building and deploying AI applications.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI SOLUTIONS","type":"CONCEPT","description":"AI Solutions refer to specific implementations of AI technologies to address particular problems or needs.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI STRATEGIES","type":"CONCEPT","description":"AI Strategies are plans and approaches for effectively utilizing artificial intelligence in various contexts.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI INNOVATIONS","type":"CONCEPT","description":"AI Innovations refer to new and creative advancements in the field of artificial intelligence.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI TRENDS","type":"CONCEPT","description":"AI Trends are emerging patterns and directions in the development and application of artificial intelligence.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI CHALLENGES","type":"CONCEPT","description":"AI Challenges refer to the obstacles and issues faced in the development and implementation of AI technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI OPPORTUNITIES","type":"CONCEPT","description":"AI Opportunities are potential areas for growth and advancement in the field of artificial intelligence.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI RESEARCH DIRECTIONS","type":"CONCEPT","description":"AI Research Directions refer to the future paths and areas of focus in artificial intelligence research.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI COLLABORATIONS","type":"CONCEPT","description":"AI Collaborations are partnerships and joint efforts in the field of artificial intelligence research and development.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI COMMUNITIES","type":"CONCEPT","description":"AI Communities are groups of individuals and organizations focused on advancing artificial intelligence technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI EDUCATION","type":"CONCEPT","description":"AI Education refers to the teaching and learning of artificial intelligence concepts and technologies.\nAI Education refers to","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI TRAINING","type":"CONCEPT","description":"AI Training involves the process of teaching AI models to perform specific tasks using data.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI EVALUATION","type":"CONCEPT","description":"AI Evaluation refers to the assessment of AI models and systems to determine their effectiveness and performance.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI DEPLOYMENT","type":"CONCEPT","description":"AI Deployment is the process of implementing AI models and systems in real-world applications.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI MAINTENANCE","type":"CONCEPT","description":"AI Maintenance involves the ongoing support and updates of AI systems to ensure their functionality and relevance.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI REGULATIONS","type":"CONCEPT","description":"AI Regulations refer to the legal and ethical guidelines governing the use of artificial intelligence technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI STANDARDS","type":"CONCEPT","description":"AI Standards are established criteria and benchmarks for the development and application of AI technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI POLICIES","type":"CONCEPT","description":"AI Policies are guidelines and rules governing the use and development of artificial intelligence technologies.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI BEST PRACTICES","type":"CONCEPT","description":"AI Best Practices are recommended methods and techniques for effectively utilizing artificial intelligence.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"},{"name":"AI METHODOLOGIES","type":"CONCEPT","description":"AI Methodologies are systematic approaches to conducting research and development in artificial intelligence.","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"CONCEPT"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"HUGO TOUVRON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hugo Touvron is a researcher involved in the development of language models and related technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"LOUIS MARTIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Louis Martin is a researcher contributing to advancements in language models and AI technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"KEVIN R. STONE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kevin R. Stone is a researcher focused on language models and their applications.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PETER ALBERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter Albert is a researcher working on language models and AI methodologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"AMJAD ALMAHAIRI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amjad Almahairi is a researcher contributing to the field of language models and AI.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YASMINE BABAIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yasmine Babaie is a researcher involved in the development of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"NIKOLAY BASHLYKOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nikolay Bashlykov is a researcher contributing to advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SOUMYA BATRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Soumya Batra is a researcher focused on language models and AI technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PRAJJWAL BHARGAVA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Prajjwal Bhargava is a researcher working on language models and their applications.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SHRUTI BHOSALE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shruti Bhosale is a researcher contributing to the field of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"DANIEL M. BIKEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel M. Bikel is a researcher involved in language model development.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"LUKAS BLECHER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lukas Blecher is a researcher focused on advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"CRISTIAN CANT&#211;N FERRER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cristian Cant&#243;n Ferrer is a researcher contributing to language model technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MOYA CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Moya Chen is a researcher involved in the development of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"GUILLEM CUCURULL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guillem Cucurull is a researcher focused on language models and AI.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"DAVID ESIOSU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David Esiosu is a researcher contributing to advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JUDE FERNANDES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jude Fernandes is a researcher working on language models and their applications.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JEREMY FU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeremy Fu is a researcher involved in the development of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"WENYIN FU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wenyin Fu is a researcher focused on advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"BRIAN FULLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brian Fuller is a researcher contributing to the field of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"CYNTHIA GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cynthia Gao is a researcher involved in language model development.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"VEDANUJ GOSWAMI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vedanuj Goswami is a researcher focused on language models and AI technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"NAMAN GOYAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Naman Goyal is a researcher contributing to advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ANTHONY S. HARTSHORN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anthony S. Hartshorn is a researcher working on language models and their applications.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SAGHAR HOSSEINI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Saghar Hosseini is a researcher involved in the development of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"RUI HOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rui Hou is a researcher focused on advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"HAKAN INAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hakan Inan is a researcher contributing to the field of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MARCIN KARDAS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marcin Kardas is a researcher involved in language model development.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"VIKTOR KERKEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Viktor Kerkez is a researcher focused on advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MADIAN KHABSA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Madian Khabsa is a researcher contributing to language model technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ISABEL M. KLOUMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Isabel M. Kloumann is a researcher involved in the development of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"A. V. KORENEV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. V. Korenev is a researcher focused on language models and AI.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PUNIT SINGH KOURA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Punit Singh Koura is a researcher contributing to advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MARIE-ANNE LACHAUX\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marie-Anne Lachaux is a researcher working on language models and their applications.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"THIBAUT LAVRIL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thibaut Lavril is a researcher involved in the development of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JENYA LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jenya Lee is a researcher focused on advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"DIANA LISKOVICH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Diana Liskovich is a researcher contributing to the field of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YINGHAI LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yinghai Lu is a researcher involved in language model development.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YUNING MAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuning Mao is a researcher focused on language models and AI technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"XAVIER MARTINET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xavier Martinet is a researcher contributing to advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"TODOR MIHAYLOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Todor Mihaylov is a researcher working on language models and their applications.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PUSHKAR MISHRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pushkar Mishra is a researcher involved in the development of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"IGOR MOLYBOG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Igor Molybog is a researcher focused on advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YIXIN NIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yixin Nie is a researcher contributing to the field of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ANDREW POULTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrew Poulton is a researcher involved in language model development.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JEREMY REIZENSTEIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeremy Reizenstein is a researcher focused on advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"RASHI RUNGTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rashi Rungta is a researcher contributing to the field of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"KALYAN SALADI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kalyan Saladi is a researcher involved in the development of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ALAN SCHELTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alan Schelton is a researcher focused on advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"RUAN SILVA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ruan Silva is a researcher contributing to language model technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ERIC MICHAEL SMITH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Michael Smith is a researcher involved in the development of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"R. SUBRAMANIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">R. Subramanian is a researcher focused on advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"XIA TAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xia Tan is a researcher contributing to the field of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"BINH TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Binh Tang is a researcher involved in language model development.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ROSS TAYLOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ross Taylor is a researcher focused on advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ADINA WILLIAMS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adina Williams is a researcher contributing to the field of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JIAN XIANG KUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jian Xiang Kuan is a researcher involved in the development of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PUXIN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Puxin Xu is a researcher focused on advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ZHENGXU YAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhengxu Yan is a researcher contributing to the field of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ILIYAN ZAROV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Iliyan Zarov is a researcher involved in language model development.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ANGELA FAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Angela Fan is a researcher contributing to the field of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MELANIE KAMBADUR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Melanie Kambadur is a researcher involved in language model development.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SHARAN NARANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sharan Narang is a researcher focused on advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"AURELIEN RODRIGUEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aurelien Rodriguez is a researcher contributing to the field of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ROBERT STOJNIC\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Robert Stojnic is a researcher involved in language model development.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SERGEY EDUNOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sergey Edunov is a researcher involved in the development of language models.Sergey Edunov is a researcher focused on advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"THOMAS SCIALOM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Scialom is a researcher contributing to the field of language models.Thomas Scialom is a researcher contributing to advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LLAMA 2\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Llama 2 is an open foundation and fine-tuned chat model developed for various applications in AI.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"LANGUAGE AGENT TREE SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Language Agent Tree Search is a method that unifies reasoning, acting, and planning in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"VOYAGER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Voyager is an open-ended embodied agent that utilizes large language models for various tasks.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SELF-CONSISTENCY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-consistency is a method that improves chain of thought reasoning in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"CHAIN OF THOUGHT PROMPTING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain of thought prompting is a technique that elicits reasoning in large language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"INTELLIGENT AGENTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Intelligent agents are systems that can reason, act, and plan, often discussed in the context of AI.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"DAYDREAMER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Daydreamer is a model for physical robot learning that utilizes world models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"DECOMPOSITION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Decomposition is a method that enhances reasoning through self-evaluation guided decoding.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotpotQA is a dataset designed for diverse, explainable multi-hop question answering.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"WEBSHOP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">WebShop is a system aimed at scalable real-world web interaction with grounded language agents.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"TREE OF THOUGHTS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Tree of thoughts is a method for deliberate problem solving using large language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">ReAct is a technique that synergizes reasoning and acting in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MASTERING ATARI GAMES\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Mastering Atari games with limited data is a research area focused on AI learning in gaming environments.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"LEAST-TO-MOST PROMPTING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Least-to-most prompting is a method that enables complex reasoning in large language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"TOOLCHAIN*\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ToolChain* is a system for efficient action space navigation in large language models using A* search.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"HOWARD CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Howard Chen is a researcher contributing to the development of language models and AI systems.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JOHN YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">John Yang is a researcher focused on advancements in language models and their applications.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"KARTHIK R NARASIMHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karthik R Narasimhan is a researcher involved in the development of language models and AI technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"DANIJAR HAFNER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Danijar Hafner is a researcher contributing to advancements in AI and language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PIETER ABBEEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pieter Abbeel is a researcher focused on AI technologies and language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"KEN GOLDBERG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ken Goldberg is a researcher involved in the development of AI systems and language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"XUEZHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuezhi Wang is a researcher contributing to advancements in language models and AI technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JASON WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jason Wei is a researcher focused on language models and their applications.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"DALE SCHUURMANS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dale Schuurmans is a researcher involved in the development of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"QUOC LE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quoc Le is a researcher contributing to advancements in language models and AI technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ED CHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ed Chi is a researcher focused on AI systems and language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"DENNY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Denny Zhou is a researcher involved in the development of language models and AI technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YUXI XIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuxi Xie is a researcher contributing to advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"KENJI KAWAGUCHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kenji Kawaguchi is a researcher focused on language models and their applications.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YIRAN ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiran Zhao is a researcher involved in the development of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"XU ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu Zhao is a researcher contributing to advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MIN-YEN KAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Min-Yen Kan is a researcher focused on language models and their applications.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JUNXIAN HE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Junxian He is a researcher involved in the development of language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"QIZHE XIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qizhe Xie is a researcher contributing to advancements in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PHILIPP WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Philipp Wu is a researcher focused on AI technologies and language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MONTE CARLO TREE SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Monte Carlo Tree Search is a method used in reinforcement learning and decision-making processes.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"REINFORCEMENT LEARNING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Reinforcement Learning is a type of machine learning where agents learn to make decisions by receiving rewards or penalties.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"A* SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A* Search is an algorithm used for pathfinding and graph traversal, commonly applied in AI.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MONTY CARLO TREE SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Monte Carlo Tree Search is a heuristic search algorithm for decision processes.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"CHAIN OF THOUGHT REASONING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain of Thought Reasoning is a method that enhances reasoning capabilities in language models.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PROBLEM SOLVING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Problem Solving is a cognitive process aimed at finding solutions to complex issues, often discussed in AI contexts.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SCALABLE WEB INTERACTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Scalable Web Interaction refers to systems designed for efficient interaction with web services using AI.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"EXPLAINABLE AI\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Explainable AI refers to methods and techniques that make the decision-making processes of AI systems understandable to humans.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MULTI-HOP QUESTION ANSWERING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Multi-hop Question Answering is a task in AI that involves answering questions that require reasoning across multiple pieces of information.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"EMNLP\">      <data key=\"d0\">CONFERENCE<\/data>      <data key=\"d1\">EMNLP is a conference focused on natural language processing and computational linguistics.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ICLR\">      <data key=\"d0\">CONFERENCE<\/data>      <data key=\"d1\">ICLR is a conference that focuses on learning representations and deep learning.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"NEURIPS\">      <data key=\"d0\">CONFERENCE<\/data>      <data key=\"d1\">NeurIPS is a conference that covers advancements in neural information processing systems and machine learning.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"CORL\">      <data key=\"d0\">CONFERENCE<\/data>      <data key=\"d1\">CoRL is a conference focused on robot learning and reinforcement learning.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"AI RESEARCH\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">AI Research encompasses various studies and advancements in artificial intelligence technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"NLP\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Natural Language Processing (NLP) is a field of AI focused on the interaction between computers and human language.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ROBOTICS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Robotics is a field of engineering and AI focused on the design and application of robots.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"AGENT-BASED SYSTEMS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Agent-based systems are computational systems that use autonomous agents to perform tasks and solve problems.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"AI ETHICS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">AI Ethics is a field that examines the moral implications and societal impacts of artificial intelligence technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"DATASET\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A dataset is a collection of data used for analysis and training in machine learning and AI.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"AI SYSTEMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Systems refer to computational systems that utilize artificial intelligence to perform tasks and make decisions.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI TECHNOLOGIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Technologies encompass various tools and methods used in the development and application of artificial intelligence.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI APPLICATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Applications refer to the practical uses of artificial intelligence in various fields and industries.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI MODELS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Models are mathematical representations used to simulate and predict behaviors in artificial intelligence systems.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI TOOLS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Tools are software and applications designed to assist in the development and implementation of AI technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI FRAMEWORKS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Frameworks are structured environments that facilitate the development and deployment of AI applications.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI PLATFORMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Platforms are comprehensive solutions that provide tools and services for building and deploying AI applications.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI SOLUTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Solutions refer to specific implementations of AI technologies to address particular problems or needs.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI STRATEGIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Strategies are plans and approaches for effectively utilizing artificial intelligence in various contexts.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI INNOVATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Innovations refer to new and creative advancements in the field of artificial intelligence.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI TRENDS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Trends are emerging patterns and directions in the development and application of artificial intelligence.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI CHALLENGES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Challenges refer to the obstacles and issues faced in the development and implementation of AI technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI OPPORTUNITIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Opportunities are potential areas for growth and advancement in the field of artificial intelligence.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI RESEARCH DIRECTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Research Directions refer to the future paths and areas of focus in artificial intelligence research.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI COLLABORATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Collaborations are partnerships and joint efforts in the field of artificial intelligence research and development.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI COMMUNITIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Communities are groups of individuals and organizations focused on advancing artificial intelligence technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI EDUCATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Education refers to the teaching and learning of artificial intelligence concepts and technologies.AI Education refers to<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI TRAINING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Training involves the process of teaching AI models to perform specific tasks using data.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI EVALUATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Evaluation refers to the assessment of AI models and systems to determine their effectiveness and performance.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI DEPLOYMENT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Deployment is the process of implementing AI models and systems in real-world applications.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI MAINTENANCE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Maintenance involves the ongoing support and updates of AI systems to ensure their functionality and relevance.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI REGULATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Regulations refer to the legal and ethical guidelines governing the use of artificial intelligence technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI STANDARDS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Standards are established criteria and benchmarks for the development and application of AI technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI POLICIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Policies are guidelines and rules governing the use and development of artificial intelligence technologies.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI BEST PRACTICES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Best Practices are recommended methods and techniques for effectively utilizing artificial intelligence.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AI METHODOLOGIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI Methodologies are systematic approaches to conducting research and development in artificial intelligence.<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <edge source=\"HUGO TOUVRON\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Hugo Touvron is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"LOUIS MARTIN\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Louis Martin is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"KEVIN R. STONE\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Kevin R. Stone is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"PETER ALBERT\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Peter Albert is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"AMJAD ALMAHAIRI\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Amjad Almahairi is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"YASMINE BABAIE\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yasmine Babaie is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"NIKOLAY BASHLYKOV\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Nikolay Bashlykov is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"SOUMYA BATRA\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Soumya Batra is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"PRAJJWAL BHARGAVA\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Prajjwal Bhargava is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"SHRUTI BHOSALE\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Shruti Bhosale is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"DANIEL M. BIKEL\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Daniel M. Bikel is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"LUKAS BLECHER\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Lukas Blecher is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"CRISTIAN CANT&#211;N FERRER\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Cristian Cant&#243;n Ferrer is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"MOYA CHEN\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Moya Chen is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"GUILLEM CUCURULL\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Guillem Cucurull is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"DAVID ESIOSU\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">David Esiosu is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"JUDE FERNANDES\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jude Fernandes is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"JEREMY FU\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jeremy Fu is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"WENYIN FU\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Wenyin Fu is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"BRIAN FULLER\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Brian Fuller is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"CYNTHIA GAO\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Cynthia Gao is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"VEDANUJ GOSWAMI\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Vedanuj Goswami is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"NAMAN GOYAL\" target=\"LLAMA 2\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Naman Goyal is involved in the development of Llama 2, contributing to advancements in language models.<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>    <edge source=\"ANTHONY S. HARTSHORN\" target=\"LLAMA 2\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Anthony S. Hartshorn is involved in the development of Llama 2, contributing to advancements in language models(\"entity\"<\/data>      <data key=\"d6\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"42de130f5b6144472a86a4c8260a87c7","chunk":" Olivier Bous-\nquet, Quoc Le, and Ed Chi. Least-to-most prompting\nenables complex reasoning in large language models. In\nICLR , 2022.\nYuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor\nBursztyn, Ryan A. Rossi, Somdeb Sarkhel, and Chao\nZhang. ToolChain*: Efficient action space navigation in\nlarge language models with A* search. In ICLR , 2023.\n13Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nAppendix of LATS\nThe appendix is organized as follows. First in Sec. A, we\nshow the pseudocode of our proposed algorithm, LATS. In\nSec. B, we provide further discussion of the limitations of\nour method. In Sec. C, we present additional experimental\nresults. In Sec. D, we specify the environment details in our\nexperiments. Finally, we list our prompts used for the three\nenvironments in Sec. E (HotPotQA), Sec. F (Programming),\nand Sec. G (WebShop), respectively.\nA. LATS Pseudocode\nAlg. 1 shows the pseudocode of our algorithm LATS. Nodes\nare stored explicitly in the memory. Unless otherwise speci-\nfied, in all experiments, we set the number of sampled nodes\nton= 5 and the exploration weight to w= 1. We use\na self-consistency weight of \u03bb= 0.5for HotPotQA and\nGame of 24, and \u03bb= 0.8for Programming and WebShop.\nB. More Discussion on Limitations\nAs stated in Sec. 6, LATS has two main limitations:\nComputational cost. Although LATS can improve rea-\nsoning and decision-making, this arrives at a higher com-\nputational cost relative to simpler prompting methods like\nReAct or Reflexion. However, the following facts serve as\nmitigations to this issue:\n\u2022Asymptotically, our method has the same sample com-\nplexity as ToT (Yao et al., 2023a) and RAP (Hao et al.,\n2023), but achieves better performance, expands fewer\nnodes, and uses fewer tokens on average upon success.\nThis suggests that our method is not only stronger\nin problem-solving but also has higher efficiency. A\nfull analysis of the cost can be found in Tab. 9 in Ap-\npendix C.\n\u2022The number of nodes nexpanded at every step provides\na natural trade-off between performance and efficiency.\nIn fact, setting n= 1 makes the method as efficient\nas ReAct (Yao et al., 2023b) with multiple trials or\nCoT-SC (Wang et al., 2022).\nIn general, we recommend using LATS for difficult tasks\nlike programming or for situations where performance is\nprioritized over efficiency in practice. We hope that contin-\nued advancements in LMs will reduce costs and increase\nthe applicability of LATS.\nAdditionally, there exists a minor cost from querying the en-\nvironment, which we find to be trivial for the environments\nwe study. Most LM-based environments involve API-based\ntools, which are inexpensive and fast to use. It is also worthnoting that this is cheaper than the inference cost associ-\nated with using LMs as world models, as in previous search\napproaches (Hao et al., 2023; Liu et al., 2023).\nAssumption of environment reversion in decision-\nmaking. Since our method is based on Monte Carlo\nTree Search and is model-free, one limitation of LATS on\ndecision-making tasks is that it requires the agent to be\nable to revert to earlier states in the environments. How-\never, this reversion property is feasible in many real-world\nenvironments and applications (despite being not univer-\nsally applicable in all possible environments), including\nprogramming (HumanEval (Chen et al., 2021)), web search\n(WebShop (Yao et al., 2022)), text-based manipulation tasks\n(Alfworld (Shridhar et al., 2020)), and LMs with tool use\n(ToolBench (Qin et al., 2024)). Therefore, we believe that\nleveraging the reversion property is not a shortcoming but\nrather a feature that has not been explicitly given notice\nby the LM decision-making community \u2013 it opens up new\nopportunities in the emerging LM agent community.\nAdditionally, the benchmarks we use in this paper are rel-\natively simple and focused on decision-making compared\nto the complexity of real-world interactive environments.\nMoreover, some environments might not easily support roll-\nbacks to previous states. However, the design of LATS is\nflexible and can be adjusted to various resource constraints.\nUsing planning-based prompting methods like LATS in\nenvironments like Minecraft (Fan et al., 2022) and more rea-\nsoning benchmarks would be interesting avenues for future\nwork.\nC. Additional Ablations\nIn this section, we ablate various designs of LATS. Ex-\nperiments are conducted on HotPotQA with a maximum\nofk= 50 trajectories and sampling size of n= 5 and\nHumanEval with a maximum of k= 8trajectories and sam-\npling size of n= 5. The result for HotPotQA is shown in\nTab. 8 and HumanEval in Fig. 3.\nExploration weight. We find that there is lower perfor-\nmance on HotPotQA when the exploration weight win the\nselection formula is decreased to 0.5, suggesting that this\nreduces the effectiveness of the search. Increasing w","chunk_id":"42de130f5b6144472a86a4c8260a87c7","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"OLIVIER BOUSQUET","type":"PERSON","description":"Olivier Bousquet is a researcher known for his work on prompting methods in large language models, particularly in the context of complex reasoning.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"QUOC LE","type":"PERSON","description":"Quoc Le is a researcher who has contributed to the development of prompting techniques for large language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"ED CHI","type":"PERSON","description":"Ed Chi is a researcher involved in the study of reasoning in large language models, particularly through least-to-most prompting.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"YUCHEN ZHUANG","type":"PERSON","description":"Yuchen Zhuang is a researcher who co-authored a paper on efficient action space navigation in large language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"XIANG CHEN","type":"PERSON","description":"Xiang Chen is a researcher who contributed to the development of the ToolChain* method for action space navigation in large language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"TONG YU","type":"PERSON","description":"Tong Yu is a researcher involved in the ToolChain* project, focusing on navigation in large language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"SAAYAN MITRA","type":"PERSON","description":"Saayan Mitra is a researcher who co-authored the ToolChain* paper, contributing to action space navigation in language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"VICTOR BURSZTYN","type":"PERSON","description":"Victor Bursztyn is a researcher associated with the ToolChain* project, focusing on efficient navigation in large language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"RYAN A. ROSSI","type":"PERSON","description":"Ryan A. Rossi is a researcher who contributed to the ToolChain* project, which enhances action space navigation in language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"SOMDEB SARKHEL","type":"PERSON","description":"Somdeb Sarkhel is a researcher involved in the ToolChain* project, focusing on large language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"CHAO ZHANG","type":"PERSON","description":"Chao Zhang is a researcher who co-authored the ToolChain* paper, contributing to advancements in action space navigation.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"LATS","type":"TECHNIQUE","description":"LATS (Language Agent Tree Search) is an algorithm designed for unifying reasoning, acting, and planning in language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a dataset used for evaluating question answering systems, particularly in the context of reasoning tasks.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"PROGRAMMING","type":"DOMAIN","description":"Programming refers to the domain of writing code and developing software, often used as a benchmark for language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"WEBSHOP","type":"DATASET","description":"WebShop is a dataset used for evaluating language models in the context of web search tasks.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"MONTE CARLO TREE SEARCH","type":"TECHNIQUE","description":"Monte Carlo Tree Search is a heuristic search algorithm used for decision-making tasks, particularly in AI applications.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"REACT","type":"TECHNIQUE","description":"ReAct is a simpler prompting method used in language models for decision-making tasks.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"REFLEXION","type":"TECHNIQUE","description":"Reflexion is another prompting method that is simpler compared to LATS, used for decision-making in language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"TOOLBENCH","type":"TECHNOLOGY","description":"ToolBench is a framework for evaluating language models with tool use capabilities.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"HUMANEVAL","type":"DATASET","description":"HumanEval is a dataset used for evaluating programming tasks in language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"ALFWORLD","type":"DATASET","description":"Alfworld is a dataset used for evaluating text-based manipulation tasks in language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"KURATOV ET AL. (2024)","type":"PERSON","description":"Kuratov et al. (2024) is a reference to a study discussing the performance of longer text chunks in language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"YAO ET AL. (2023A)","type":"PERSON","description":"Yao et al. (2023a) is a reference to a study that compares the sample complexity of LATS with other methods.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"HAO ET AL. (2023)","type":"PERSON","description":"Hao et al. (2023) is a reference to a study discussing the performance of LATS in comparison to other methods.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"YAO ET AL. (2023B)","type":"PERSON","description":"Yao et al. (2023b) is a reference to a study that discusses the efficiency of the ReAct method.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"WANG ET AL. (2022)","type":"PERSON","description":"Wang et al. (2022) is a reference to a study discussing the CoT-SC method in language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"TOOLCHAIN*","type":"","description":"","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"A* SEARCH","type":"TECHNIQUE","description":"A* search is an algorithm used for pathfinding and graph traversal, which is applied in the context of action space navigation in language models.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"EXPERIMENTAL RESULTS","type":"DATA","description":"Experimental results refer to the data and findings obtained from testing the LATS algorithm across various tasks and environments.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"ENVIRONMENT DETAILS","type":"DATA","description":"Environment details provide information about the settings and conditions under which experiments with the LATS algorithm were conducted.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"PROMPTS","type":"DATA","description":"Prompts are specific queries or instructions used in the LATS algorithm to guide the language model's responses in different environments.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"LIMITATIONS","type":"DATA","description":"Limitations refer to the constraints and challenges faced by the LATS algorithm, particularly in decision-making tasks.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"COMPUTATIONAL COST","type":"DATA","description":"Computational cost refers to the resources required to run the LATS algorithm, which can be higher compared to simpler methods.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"SAMPLE COMPLEXITY","type":"DATA","description":"Sample complexity is a measure of the number of samples needed for the LATS algorithm to achieve a certain level of performance.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"TRIALS","type":"DATA","description":"Trials refer to the repeated tests conducted to evaluate the performance and efficiency of the LATS algorithm.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"RESOURCE CONSTRAINTS","type":"DATA","description":"Resource constraints refer to the limitations in computational resources that may affect the implementation of the LATS algorithm.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"DECISION-MAKING","type":"PROCESS","description":"Decision-making is the process by which the LATS algorithm evaluates options and selects actions based on its reasoning capabilities.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"REAL-WORLD ENVIRONMENTS","type":"DATA","description":"Real-world environments are practical applications where the LATS algorithm can be utilized, such as programming and web search tasks.","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"INTERACTIVE ENVIRONMENTS","type":"DATA","description":"Interactive environments are settings where the LATS algorithm can engage with users or systems to perform tasks.","source_id":"42de130f5b6144472a86a4c8260a87c7"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"OLIVIER BOUSQUET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Olivier Bousquet is a researcher known for his work on prompting methods in large language models, particularly in the context of complex reasoning.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"QUOC LE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quoc Le is a researcher who has contributed to the development of prompting techniques for large language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"ED CHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ed Chi is a researcher involved in the study of reasoning in large language models, particularly through least-to-most prompting.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"YUCHEN ZHUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuchen Zhuang is a researcher who co-authored a paper on efficient action space navigation in large language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"XIANG CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiang Chen is a researcher who contributed to the development of the ToolChain* method for action space navigation in large language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"TONG YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tong Yu is a researcher involved in the ToolChain* project, focusing on navigation in large language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"SAAYAN MITRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Saayan Mitra is a researcher who co-authored the ToolChain* paper, contributing to action space navigation in language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"VICTOR BURSZTYN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Victor Bursztyn is a researcher associated with the ToolChain* project, focusing on efficient navigation in large language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"RYAN A. ROSSI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ryan A. Rossi is a researcher who contributed to the ToolChain* project, which enhances action space navigation in language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"SOMDEB SARKHEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Somdeb Sarkhel is a researcher involved in the ToolChain* project, focusing on large language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"CHAO ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chao Zhang is a researcher who co-authored the ToolChain* paper, contributing to advancements in action space navigation.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LATS (Language Agent Tree Search) is an algorithm designed for unifying reasoning, acting, and planning in language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a dataset used for evaluating question answering systems, particularly in the context of reasoning tasks.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"PROGRAMMING\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Programming refers to the domain of writing code and developing software, often used as a benchmark for language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"WEBSHOP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">WebShop is a dataset used for evaluating language models in the context of web search tasks.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"MONTE CARLO TREE SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Monte Carlo Tree Search is a heuristic search algorithm used for decision-making tasks, particularly in AI applications.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">ReAct is a simpler prompting method used in language models for decision-making tasks.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Reflexion is another prompting method that is simpler compared to LATS, used for decision-making in language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"TOOLBENCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ToolBench is a framework for evaluating language models with tool use capabilities.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"HUMANEVAL\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HumanEval is a dataset used for evaluating programming tasks in language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"ALFWORLD\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Alfworld is a dataset used for evaluating text-based manipulation tasks in language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"KURATOV ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kuratov et al. (2024) is a reference to a study discussing the performance of longer text chunks in language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"YAO ET AL. (2023A)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. (2023a) is a reference to a study that compares the sample complexity of LATS with other methods.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"HAO ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao et al. (2023) is a reference to a study discussing the performance of LATS in comparison to other methods.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"YAO ET AL. (2023B)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. (2023b) is a reference to a study that discusses the efficiency of the ReAct method.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"WANG ET AL. (2022)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang et al. (2022) is a reference to a study discussing the CoT-SC method in language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"TOOLCHAIN*\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"A* SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A* search is an algorithm used for pathfinding and graph traversal, which is applied in the context of action space navigation in language models.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"EXPERIMENTAL RESULTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Experimental results refer to the data and findings obtained from testing the LATS algorithm across various tasks and environments.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"ENVIRONMENT DETAILS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Environment details provide information about the settings and conditions under which experiments with the LATS algorithm were conducted.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"PROMPTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Prompts are specific queries or instructions used in the LATS algorithm to guide the language model's responses in different environments.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"LIMITATIONS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Limitations refer to the constraints and challenges faced by the LATS algorithm, particularly in decision-making tasks.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"COMPUTATIONAL COST\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Computational cost refers to the resources required to run the LATS algorithm, which can be higher compared to simpler methods.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"SAMPLE COMPLEXITY\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Sample complexity is a measure of the number of samples needed for the LATS algorithm to achieve a certain level of performance.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"TRIALS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Trials refer to the repeated tests conducted to evaluate the performance and efficiency of the LATS algorithm.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"RESOURCE CONSTRAINTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Resource constraints refer to the limitations in computational resources that may affect the implementation of the LATS algorithm.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"DECISION-MAKING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Decision-making is the process by which the LATS algorithm evaluates options and selects actions based on its reasoning capabilities.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"REAL-WORLD ENVIRONMENTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Real-world environments are practical applications where the LATS algorithm can be utilized, such as programming and web search tasks.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"INTERACTIVE ENVIRONMENTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Interactive environments are settings where the LATS algorithm can engage with users or systems to perform tasks.<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <edge source=\"OLIVIER BOUSQUET\" target=\"LATS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Olivier Bousquet is a researcher who contributed to the development of the LATS algorithm for reasoning in language models.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"QUOC LE\" target=\"LATS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Quoc Le is a researcher involved in the development of the LATS algorithm for reasoning in language models.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"ED CHI\" target=\"LATS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Ed Chi is a researcher who contributed to the study of reasoning in language models through the LATS algorithm.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"YUCHEN ZHUANG\" target=\"TOOLCHAIN*\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yuchen Zhuang co-authored the ToolChain* paper, which focuses on efficient action space navigation in language models.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"XIANG CHEN\" target=\"TOOLCHAIN*\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Xiang Chen is a co-author of the ToolChain* paper, contributing to action space navigation in language models.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"TONG YU\" target=\"TOOLCHAIN*\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Tong Yu is a co-author of the ToolChain* paper, focusing on navigation in large language models.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"SAAYAN MITRA\" target=\"TOOLCHAIN*\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Saayan Mitra is a co-author of the ToolChain* paper, contributing to advancements in action space navigation.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"VICTOR BURSZTYN\" target=\"TOOLCHAIN*\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Victor Bursztyn is a co-author of the ToolChain* paper, focusing on efficient navigation in language models.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"RYAN A. ROSSI\" target=\"TOOLCHAIN*\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Ryan A. Rossi is a co-author of the ToolChain* paper, contributing to action space navigation in language models.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"SOMDEB SARKHEL\" target=\"TOOLCHAIN*\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Somdeb Sarkhel is a co-author of the ToolChain* paper, focusing on large language models.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"CHAO ZHANG\" target=\"TOOLCHAIN*\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chao Zhang is a co-author of the ToolChain* paper, contributing to advancements in action space navigation.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"MONTE CARLO TREE SEARCH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS utilizes Monte Carlo Tree Search as part of its decision-making process.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HOTPOTQA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS is evaluated using the HotPotQA dataset for reasoning tasks in language models.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PROGRAMMING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS is recommended for programming tasks where performance is prioritized over efficiency.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"WEBSHOP\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS is evaluated using the WebShop dataset for web search tasks in language models.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REACT\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS is compared to simpler prompting methods like ReAct in terms of computational cost and performance.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REFLEXION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LATS is compared to simpler prompting methods like Reflexion in terms of computational cost and performance.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HUMANEVAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS is evaluated using the HumanEval dataset for programming tasks in language models.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ALFWORLD\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS is applicable in environments like Alfworld for text-based manipulation tasks.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"TOOLBENCH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS can be evaluated in environments that utilize ToolBench for tool use capabilities.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"KURATOV ET AL. (2024)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Kuratov et al. (2024) provides insights relevant to the performance of the LATS algorithm.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"YAO ET AL. (2023A)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Yao et al. (2023a) provides insights relevant to the sample complexity of the LATS algorithm.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HAO ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Hao et al. (2023) provides insights relevant to the performance of the LATS algorithm compared to other methods.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"WANG ET AL. (2022)\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Wang et al. (2022) provides insights relevant to the performance of LATS in comparison to CoT-SC.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"A* SEARCH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The LATS algorithm employs A* search as part of its action space navigation strategy.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"EXPERIMENTAL RESULTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Experimental results are used to evaluate the effectiveness of the LATS algorithm in various tasks.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ENVIRONMENT DETAILS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Environment details are crucial for understanding the context in which the LATS algorithm operates.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PROMPTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Prompts are utilized within the LATS algorithm to guide the language model's responses in different scenarios.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"LIMITATIONS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Limitations highlight the challenges faced by the LATS algorithm in its application to decision-making tasks.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"COMPUTATIONAL COST\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Computational cost is a significant factor when evaluating the efficiency of the LATS algorithm compared to simpler methods.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SAMPLE COMPLEXITY\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Sample complexity is an important metric for assessing the performance of the LATS algorithm in various scenarios.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"TRIALS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Trials are conducted to test the performance and efficiency of the LATS algorithm in different environments.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"RESOURCE CONSTRAINTS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Resource constraints can impact the implementation and performance of the LATS algorithm in real-world applications.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"DECISION-MAKING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Decision-making is a core function of the LATS algorithm, enabling it to evaluate and select actions based on reasoning.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REAL-WORLD ENVIRONMENTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The LATS algorithm is designed to be applicable in real-world environments, enhancing its utility in practical tasks.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"INTERACTIVE ENVIRONMENTS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Interactive environments provide a context for the LATS algorithm to engage in tasks that require reasoning and decision-making.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"REACT\" target=\"YAO ET AL. (2023B)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Yao et al. (2023b) discusses the efficiency of the ReAct method, which is compared to LATS.<\/data>      <data key=\"d5\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"48e423e2baf2ed485872756f5b4d87d8","chunk":" 5 and\nHumanEval with a maximum of k= 8trajectories and sam-\npling size of n= 5. The result for HotPotQA is shown in\nTab. 8 and HumanEval in Fig. 3.\nExploration weight. We find that there is lower perfor-\nmance on HotPotQA when the exploration weight win the\nselection formula is decreased to 0.5, suggesting that this\nreduces the effectiveness of the search. Increasing wto2.0\ndoes not lead to a performance improvement, but we tend\nto observe faster convergence. The optimal setting depends\non the particular environment and complexity of the state\nspace.\nDepth. In our main experiments we use a maximum depth\nofd= 7on HotPotQA for all methods, following previous\nwork (Yao et al., 2023b). We ablate the effect on LATS after\nreducing it to d= 4. This results in only a slight drop in\nperformance. We find that most questions can be answered\nwithin four steps, and using a greater number of steps tends\n14Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nAlgorithm 1 LATS( s, p\u03b8, pV, pref, d, k, n, w, a, b )\nRequire: Initial state s, action generator p\u03b8, value function pV, reflection generator pref, number of generated actions n,\ndepth limit L, number of roll-outs K, context c, exploration weight w, and value function weight \u03bb\nInitialize action space A, observation space O\nInitialize the state-action value function pV:S\u00d7A7\u2192Rand visit counter N:S7\u2192Nto one\nfork\u21900, . . . , K \u22121do\nfort\u21900, . . . , L \u22121do\nifstnot terminal then \u25b7Expansion & Simulation\nfori\u21901, . . . , n do\nSample a(i)\nt\u223cp\u03b8(st)\nGeto(i)\ntfrom environment, s(i)\nt+1\u2190(c(i)\nt, o(i)\nt, a(i)\nt),c(i)\nt+1\u2190(o(i)\nt, a(i)\nt)\nEvaluate V(i)\nt\u223c\u03bb\u2217pV(s(i)\nt) + (1\u2212\u03bb)\u2217SC(s(i)\nt) \u25b7Evaluation\nV(st)\u2190V(i)\nt\nAdds(i)\ntto children\nend for\nend if\nifstis terminal then \u25b7Reflection\nGetrfrom environment\nifrnot success then\nreflection \u2190pref(ct)\nc\u2190reflection\nend if\nend if\nat\u2190arg max a\u2208e(st)h\nV(st) +wq\nlnN(st)\nN(st+1)i\n\u25b7Selection\nGet corresponding otfrom memory, st+1\u2190(ct, ot, at), ct+1\u2190(ot, at)\nN(st+1)\u2190N(st+1) + 1\nifatis an output action then break\nend for\nT\u2190the actual number of steps\nfort\u2190T\u22121, . . . , 0do \u25b7Backpropagation\nV(st)\u2190V(st)(N(st)\u22121)+r\nN(st)\nend for\nend for\nto force the agent into local minima and rarely improves\nsuccess.\nLM value function. The LM value function scores states\nbased on expected future reward. Without this heuristic,\nthe only signal to guide search would be from environment\nrewards for completed trajectories, which are scarce and\noften binary. When we remove the evaluation operation, we\nobserve a dramatic 0.26drop in performance.\nPerformance over time. To see the effects of increasing\nthe number of trajectories sampled, we change kto different\nvalues. We conduct this experiment on HumanEval, which\nhas a more noticeable difference due to sampling less tra-\njectories. The results are shown in Fig. 3, in which LATS\nscales better with more iterations than Reflexion.\nD. Environment Details\nD.1. HotPotQA\nHotPotQA (Yang et al., 2018) is a question-answering\ndataset that requires reasoning over multiple supporting\ndocuments to answer questions. It contains 113k Wikipedia-based question-answer pairs crafted by crowdworkers to\nbe diverse, multi-hop, and explainable. Questions cover a\nrange of types like entities, locations, dates, and comparison\nof shared properties between two entities. Crowdworkers\nalso provide supporting facts from the documents that justify\nthe answer. We use the HotPotQA benchmark setting with\nall the Wikipedia paragraphs to test retrieval. We use a ran-\ndomly selected subset of 100 questions for our experiments\nand a maximum depth limit of 6. Fig. 4 illustrates how\nReAct and LATS work on an example task of HotPotQA,\nand gives a qualitative example on how LATS outperforms\nReAct on the task. For value function hyperparameters, we\nuse\u03bb= 0.5for the LM score and self-consistency score.\nAction Space. We adopt the Wikipedia web API proposed\nin Yao et al. (2023b), with three types of actions to support\ninteractive information retrieval:\n(1)search [entity ], which returns the first 5 sentences\nfrom the corresponding entity wiki page if it exists,\nor else suggests top-5 similar entities from the Wikipedia\nsearch engine,\n(2)lookup [string ], which returns the next sentence in\n15Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nPrompt Method HotpotQA (EM) \u2191\nLATS ( w= 0.5) 0.55","chunk_id":"48e423e2baf2ed485872756f5b4d87d8","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a question-answering dataset that requires reasoning over multiple supporting documents to answer questions, containing 113k diverse, multi-hop, and explainable question-answer pairs crafted by crowdworkers.","source_id":"48e423e2baf2ed485872756f5b4d87d8"},{"name":"LATS","type":"TECHNIQUE","description":"LATS (Language Agent Tree Search) is an algorithm designed to unify reasoning, acting, and planning in language models, utilizing a structured approach to decision-making in complex environments.","source_id":"48e423e2baf2ed485872756f5b4d87d8"},{"name":"REFLEXION","type":"TECHNIQUE","description":"Reflexion is a method used in conjunction with LATS for decision-making, which is compared against LATS in terms of performance on tasks like HotPotQA.","source_id":"48e423e2baf2ed485872756f5b4d87d8"},{"name":"ENVIRONMENT","type":"CONCEPT","description":"The environment refers to the context or setting in which the LATS algorithm operates, including the state space and the dynamics of the tasks being performed.","source_id":"48e423e2baf2ed485872756f5b4d87d8"},{"name":"ACTION SPACE","type":"CONCEPT","description":"The action space defines the set of possible actions that can be taken by the LATS algorithm, including searching for entities and looking up strings in the Wikipedia API.","source_id":"48e423e2baf2ed485872756f5b4d87d8"},{"name":"WIKIPEDIA","type":"RESOURCE","description":"Wikipedia is a free online encyclopedia that provides a wealth of information, which is utilized by the LATS algorithm for interactive information retrieval.","source_id":"48e423e2baf2ed485872756f5b4d87d8"},{"name":"KURATOV ET AL. (2024)","type":"PERSON","description":"Kuratov et al. (2024) is a reference to a study or work that discusses the performance of algorithms in relation to text chunk processing.","source_id":"48e423e2baf2ed485872756f5b4d87d8"},{"name":"YAO ET AL. (2023)","type":"PERSON","description":"Yao et al. (2023) is a reference to a study or work that proposes the Wikipedia web API and discusses its application in the context of LATS.","source_id":"48e423e2baf2ed485872756f5b4d87d8"},{"name":"SAMPLING SIZE","type":"CONCEPT","description":"Sampling size refers to the number of trajectories or samples taken during the evaluation of an algorithm, impacting the performance and results observed.","source_id":"48e423e2baf2ed485872756f5b4d87d8"},{"name":"EXPLORATION WEIGHT","type":"PARAMETER","description":"Exploration weight is a parameter in the selection formula that influences the effectiveness of the search process in algorithms like LATS, affecting performance outcomes.","source_id":"48e423e2baf2ed485872756f5b4d87d8"},{"name":"DEPTH","type":"PARAMETER","description":"Depth is a parameter that limits the maximum number of steps an algorithm can take in a decision-making process, impacting the performance and efficiency of the search.","source_id":"48e423e2baf2ed485872756f5b4d87d8"},{"name":"VALUE FUNCTION","type":"CONCEPT","description":"The value function is a component of the LATS algorithm that scores states based on expected future rewards, guiding the search process.","source_id":"48e423e2baf2ed485872756f5b4d87d8"},{"name":"REWARD","type":"CONCEPT","description":"Reward refers to the feedback received from the environment based on the actions taken, which is used to evaluate the success of trajectories in algorithms.","source_id":"48e423e2baf2ed485872756f5b4d87d8"},{"name":"PERFORMANCE","type":"CONCEPT","description":"Performance refers to the effectiveness of an algorithm in completing tasks, often measured through metrics like accuracy or efficiency in answering questions.","source_id":"48e423e2baf2ed485872756f5b4d87d8"},{"name":"TRAJECTORIES","type":"","description":"","source_id":"48e423e2baf2ed485872756f5b4d87d8"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a question-answering dataset that requires reasoning over multiple supporting documents to answer questions, containing 113k diverse, multi-hop, and explainable question-answer pairs crafted by crowdworkers.<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LATS (Language Agent Tree Search) is an algorithm designed to unify reasoning, acting, and planning in language models, utilizing a structured approach to decision-making in complex environments.<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Reflexion is a method used in conjunction with LATS for decision-making, which is compared against LATS in terms of performance on tasks like HotPotQA.<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <node id=\"ENVIRONMENT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The environment refers to the context or setting in which the LATS algorithm operates, including the state space and the dynamics of the tasks being performed.<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <node id=\"ACTION SPACE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The action space defines the set of possible actions that can be taken by the LATS algorithm, including searching for entities and looking up strings in the Wikipedia API.<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <node id=\"WIKIPEDIA\">      <data key=\"d0\">RESOURCE<\/data>      <data key=\"d1\">Wikipedia is a free online encyclopedia that provides a wealth of information, which is utilized by the LATS algorithm for interactive information retrieval.<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <node id=\"KURATOV ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kuratov et al. (2024) is a reference to a study or work that discusses the performance of algorithms in relation to text chunk processing.<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <node id=\"YAO ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. (2023) is a reference to a study or work that proposes the Wikipedia web API and discusses its application in the context of LATS.<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <node id=\"SAMPLING SIZE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Sampling size refers to the number of trajectories or samples taken during the evaluation of an algorithm, impacting the performance and results observed.<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <node id=\"EXPLORATION WEIGHT\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Exploration weight is a parameter in the selection formula that influences the effectiveness of the search process in algorithms like LATS, affecting performance outcomes.<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <node id=\"DEPTH\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Depth is a parameter that limits the maximum number of steps an algorithm can take in a decision-making process, impacting the performance and efficiency of the search.<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <node id=\"VALUE FUNCTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The value function is a component of the LATS algorithm that scores states based on expected future rewards, guiding the search process.<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <node id=\"REWARD\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Reward refers to the feedback received from the environment based on the actions taken, which is used to evaluate the success of trajectories in algorithms.<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <node id=\"PERFORMANCE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Performance refers to the effectiveness of an algorithm in completing tasks, often measured through metrics like accuracy or efficiency in answering questions.<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <node id=\"TRAJECTORIES\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/node>    <edge source=\"HOTPOTQA\" target=\"LATS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">LATS is evaluated using the HotPotQA dataset to assess its performance in answering complex questions that require reasoning over multiple documents.<\/data>      <data key=\"d5\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REFLEXION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS and Reflexion are compared in terms of their performance on tasks like HotPotQA, with LATS showing superior results.<\/data>      <data key=\"d5\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ENVIRONMENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS operates within a defined environment that includes the state space and task dynamics, influencing its decision-making process.<\/data>      <data key=\"d5\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ACTION SPACE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS utilizes a specific action space that defines the actions it can take, such as searching for entities and looking up strings.<\/data>      <data key=\"d5\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"KURATOV ET AL. (2024)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Kuratov et al. (2024) provides insights relevant to the performance of LATS in processing text chunks and decision-making.<\/data>      <data key=\"d5\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"YAO ET AL. (2023)\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Yao et al. (2023) discusses the Wikipedia web API, which is integral to the functioning of LATS in its information retrieval tasks.<\/data>      <data key=\"d5\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"TRAJECTORIES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LATS evaluates its performance based on the trajectories it generates during its decision-making process.<\/data>      <data key=\"d5\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SAMPLING SIZE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The sampling size affects the performance of LATS, as different sizes can lead to varying results in task completion.<\/data>      <data key=\"d5\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"EXPLORATION WEIGHT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The exploration weight is a critical parameter in LATS that influences the effectiveness of its search strategy.<\/data>      <data key=\"d5\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"DEPTH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The depth parameter in LATS limits the number of steps taken during the search process, impacting overall performance.<\/data>      <data key=\"d5\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"VALUE FUNCTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The value function in LATS scores states to guide the search process based on expected future rewards.<\/data>      <data key=\"d5\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REWARD\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Rewards from the environment provide feedback to LATS, influencing its learning and decision-making process.<\/data>      <data key=\"d5\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PERFORMANCE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The performance of LATS is assessed based on its ability to successfully complete tasks, such as answering questions in HotPotQA.<\/data>      <data key=\"d5\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"ACTION SPACE\" target=\"WIKIPEDIA\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The action space of LATS includes actions that interact with the Wikipedia API to retrieve information about entities and strings.<\/data>      <data key=\"d5\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"fb2b4544aedd793e4d4ec3147320a51c","chunk":"interactive information retrieval:\n(1)search [entity ], which returns the first 5 sentences\nfrom the corresponding entity wiki page if it exists,\nor else suggests top-5 similar entities from the Wikipedia\nsearch engine,\n(2)lookup [string ], which returns the next sentence in\n15Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nPrompt Method HotpotQA (EM) \u2191\nLATS ( w= 0.5) 0.55\nLATS ( w= 2.0) 0.63\nLATS ( d= 4) 0.58\nLATS (CoT) 0.62\nLATS (No LM Heuristic) 0.37\nLATS ( w= 1.0,d= 7) 0.63\nTable 11. Ablation results on LATS and baseline variants in Hot-\nPotQA measured by Exact Match (EM). We test different depth d,\nexploration factor w, and versions of LATS using CoT and without\nthe LM value function. We sample n= 5andk= 50 trajectories.\nFigure 3. Performance over successive iterations on HumanEval\nwith GPT-3.5.\nthe page containing string ,\n(3)finish [answer ], which finishes the current task with\nanswer .\nThese API calls and free-form thoughts form the action\nspace for this environment.\nD.2. Programming\nThe HumanEval dataset (Chen et al., 2021) is a collection\nof 164 handwritten programming problems introduced to\nevaluate the functional correctness of models for synthe-\nsizing programs from natural language descriptions. Each\nproblem includes a function signature, docstring descrip-\ntion, reference implementation, and multiple unit tests, with\nan average of 7.7 tests per problem. The programming\ntasks assess comprehension of natural language, reasoning,\nalgorithms, and basic mathematics, at a difficulty level com-\nparable to simple software interview questions. Pass rates\nare evaluated with the pass@k metric, where k samples are\ngenerated per problem and a problem is considered solvedif any sample passes all tests. We use all 164 problems for\nour experiments and a maximum depth limit of 8. For the\nthree questions without sample test cases, we write our own.\nFor value function hyperparameters, we use \u03bb= 0.8for the\nLM score and self-consistency score. For GPT-3.5 we use\nsix internal tests, while for GPT-4 we use four internal tests.\nThe Mostly Basic Programming Problems (MBPP) (Austin\net al., 2022) benchmark contains 974 short Python functions\ndesigned to evaluate program synthesis techniques. The\ndataset was constructed by crowdsourcing from workers\nwith basic Python knowledge. Each data point consists of\na natural language description of a programming task, a\nreference solution implementation, and three test cases for\nfunctional correctness. The natural language prompts are\ntypically short, one-sentence descriptions. Solutions cover\ncommon programming constructs including mathematical\noperations, list processing, string manipulation, and usage\nof the Python standard library. On average, solutions are 6.8\nlines of code. The dataset is also supplemented with an ad-\nditional set of 426 problems that were manually verified for\nunambiguous specifications, standard function signatures,\nand accurate test cases. We use a randomly selected subset\nof 397 problems for our experiments. For value function\nhyperparameters, we use \u03bb= 0.8for the LM score and\nself-consistency score.\nD.3. WebShop\nWebShop (Yao et al., 2022) is an interactive web-based\nenvironment designed to evaluate agents on grounded\nlanguage understanding and decision-making. It simulates\nan e-commerce shopping task by providing agents with\nover 1 million real-world products scraped from Amazon,\nspanning 5 categories and 113 subcategories. These\nproducts contain rich linguistic information, with an\naverage text length of 262 words and a vocabulary size\nof 224k. In addition, there are over 800k unique product\noptions available for customization. The environment\nrenders webpages in two modes: HTML mode provides\npixel-level observations with interactive elements, while\nsimple mode converts the raw HTML into a structured text\nobservation more amenable for training agents. The action\nspace consists of query searches and button clicks, which\ntransition between 4-page types: search, results, item, and\nitem detail. Instructions are crowdsourced natural language\nspecifying product attributes and options, with a total of 12k\ncollected. Automatic rewards are computed by comparing\nthe product purchased by the agent against the attributes\nand options specified in the instruction, using both lexical\nmatching and semantic similarity metrics.\nThere are two evaluation metrics used in WebShop: (1) Task\nScore defined as (100\u00d7avg. reward ), which captures the\n16Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nFigure 4. Example trajectories on HotPotQA for ReAct ( left) and LATS ( right ). LATS can sample more actions and avoid failure from\nprevious mistakes by evaluating states with an LM to guide the search toward promising areas of the tree.\nType Argument State \u2192Next State\nsearch [Query ] Search \u2192Results\nchoose Back to search \u2217 \u2192 Search\nchoose Prev\/Next page Results \u2192Results\nchoose [Product title ] Results \u2192Item\nchoose [Option ] Item \u2192Item\nchoose Desc\/Overview Item \u2192Item-Detail\nchoose Previous Item-Detail \u2192Item\nchoose Buy Item \u2192Episode End\nTable 12. Action space of WebShop.\naverage reward obtained across episodes; and (2) Success\nRate (SR) defined as the portion","chunk_id":"fb2b4544aedd793e4d4ec3147320a51c","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"INTERACTIVE INFORMATION RETRIEVAL","type":"TECHNIQUE","description":"Interactive information retrieval is a method that allows users to search for entities and retrieve relevant information from a database or knowledge base, such as Wikipedia.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"ENTITY WIKI PAGE","type":"DATA FORMAT","description":"An entity wiki page is a dedicated page on Wikipedia that contains information about a specific entity, including its attributes and related topics.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"SEARCH","type":"FUNCTION","description":"Search is a function that retrieves the first five sentences from an entity's wiki page or suggests similar entities if the page does not exist.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"LOOKUP","type":"FUNCTION","description":"Lookup is a function that returns the next sentence in a page based on a given string.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"FINISH","type":"FUNCTION","description":"Finish is a function that completes the current task with a specified answer.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"HUMANEVAL DATASET","type":"DATASET","description":"The HumanEval dataset is a collection of programming problems used to evaluate the functional correctness of models that synthesize programs from natural language descriptions.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"MOSTLY BASIC PROGRAMMING PROBLEMS (MBPP)","type":"DATASET","description":"The Mostly Basic Programming Problems (MBPP) benchmark is a dataset of short Python functions designed to assess program synthesis techniques.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"WEBSHOP","type":"ENVIRONMENT","description":"WebShop is an interactive web-based environment that evaluates agents on grounded language understanding and decision-making in an e-commerce context.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"AGENTS","type":"AGENT","description":"Agents are automated systems that interact with the WebShop environment to perform tasks such as searching for products and making purchases.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"PRODUCTS","type":"DATA FORMAT","description":"Products are items available for purchase in the WebShop environment, containing various attributes and options for customization.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"E-COMMERCE","type":"CONCEPT","description":"E-commerce refers to the buying and selling of goods and services over the internet, which is simulated in the WebShop environment.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"FUNCTION","type":"","description":"","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"PROGRAMMING","type":"","description":"","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a dataset used for evaluating question answering systems, particularly in multi-hop reasoning tasks.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"EXACT MATCH (EM)","type":"METRIC","description":"Exact Match (EM) is a metric used to evaluate the performance of models by checking if the generated answer exactly matches the ground truth.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"PASS@K","type":"METRIC","description":"Pass@k is a metric that evaluates the success rate of a model by determining if any of the k generated samples pass all tests for a given problem.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"VALUE FUNCTION HYPERPARAMETERS","type":"PARAMETER","description":"Value function hyperparameters are settings used to optimize the performance of language models in various tasks, such as \u03bb values for scoring.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"GPT-3.5","type":"MODEL","description":"GPT-3.5 is a version of the Generative Pre-trained Transformer model developed by OpenAI, used for various natural language processing tasks.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is an advanced version of the Generative Pre-trained Transformer model developed by OpenAI, known for its enhanced capabilities in language understanding.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"PROGRAM SYNTHESIS","type":"CONCEPT","description":"Program synthesis is the process of automatically generating code from high-level specifications or natural language descriptions.","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"LATS","type":"","description":"","source_id":"fb2b4544aedd793e4d4ec3147320a51c"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"INTERACTIVE INFORMATION RETRIEVAL\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Interactive information retrieval is a method that allows users to search for entities and retrieve relevant information from a database or knowledge base, such as Wikipedia.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"ENTITY WIKI PAGE\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">An entity wiki page is a dedicated page on Wikipedia that contains information about a specific entity, including its attributes and related topics.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"SEARCH\">      <data key=\"d0\">FUNCTION<\/data>      <data key=\"d1\">Search is a function that retrieves the first five sentences from an entity's wiki page or suggests similar entities if the page does not exist.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"LOOKUP\">      <data key=\"d0\">FUNCTION<\/data>      <data key=\"d1\">Lookup is a function that returns the next sentence in a page based on a given string.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"FINISH\">      <data key=\"d0\">FUNCTION<\/data>      <data key=\"d1\">Finish is a function that completes the current task with a specified answer.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"HUMANEVAL DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">The HumanEval dataset is a collection of programming problems used to evaluate the functional correctness of models that synthesize programs from natural language descriptions.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"MOSTLY BASIC PROGRAMMING PROBLEMS (MBPP)\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">The Mostly Basic Programming Problems (MBPP) benchmark is a dataset of short Python functions designed to assess program synthesis techniques.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"WEBSHOP\">      <data key=\"d0\">ENVIRONMENT<\/data>      <data key=\"d1\">WebShop is an interactive web-based environment that evaluates agents on grounded language understanding and decision-making in an e-commerce context.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"AGENTS\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Agents are automated systems that interact with the WebShop environment to perform tasks such as searching for products and making purchases.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"PRODUCTS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Products are items available for purchase in the WebShop environment, containing various attributes and options for customization.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"E-COMMERCE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">E-commerce refers to the buying and selling of goods and services over the internet, which is simulated in the WebShop environment.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"FUNCTION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"PROGRAMMING\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a dataset used for evaluating question answering systems, particularly in multi-hop reasoning tasks.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"EXACT MATCH (EM)\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Exact Match (EM) is a metric used to evaluate the performance of models by checking if the generated answer exactly matches the ground truth.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"PASS@K\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Pass@k is a metric that evaluates the success rate of a model by determining if any of the k generated samples pass all tests for a given problem.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"VALUE FUNCTION HYPERPARAMETERS\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Value function hyperparameters are settings used to optimize the performance of language models in various tasks, such as &#955; values for scoring.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5 is a version of the Generative Pre-trained Transformer model developed by OpenAI, used for various natural language processing tasks.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is an advanced version of the Generative Pre-trained Transformer model developed by OpenAI, known for its enhanced capabilities in language understanding.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"PROGRAM SYNTHESIS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Program synthesis is the process of automatically generating code from high-level specifications or natural language descriptions.<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <edge source=\"INTERACTIVE INFORMATION RETRIEVAL\" target=\"SEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Interactive information retrieval utilizes the search function to retrieve information from entity wiki pages or suggest similar entities.<\/data>      <data key=\"d5\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"ENTITY WIKI PAGE\" target=\"SEARCH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The search function retrieves information from an entity wiki page if it exists.<\/data>      <data key=\"d5\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"ENTITY WIKI PAGE\" target=\"LOOKUP\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The lookup function accesses the content of an entity wiki page based on a specified string.<\/data>      <data key=\"d5\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"FINISH\" target=\"FUNCTION\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The finish function is part of the interactive information retrieval process, completing tasks with answers.<\/data>      <data key=\"d5\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"HUMANEVAL DATASET\" target=\"PROGRAMMING\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The HumanEval dataset is used to evaluate programming models, assessing their ability to synthesize code from natural language descriptions.<\/data>      <data key=\"d5\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"HUMANEVAL DATASET\" target=\"PASS@K\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The pass@k metric is used to evaluate the performance of models on the HumanEval dataset by measuring the success rate of generated samples.<\/data>      <data key=\"d5\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"HUMANEVAL DATASET\" target=\"PROGRAM SYNTHESIS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The HumanEval dataset is specifically designed to evaluate models on their ability to perform program synthesis from natural language descriptions.<\/data>      <data key=\"d5\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"MOSTLY BASIC PROGRAMMING PROBLEMS (MBPP)\" target=\"PROGRAMMING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The MBPP benchmark is designed to evaluate program synthesis techniques, similar to the HumanEval dataset.<\/data>      <data key=\"d5\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"AGENTS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">WebShop provides an environment for agents to perform tasks related to e-commerce, such as searching for and purchasing products.<\/data>      <data key=\"d5\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"PRODUCTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Products are the items available for interaction within the WebShop environment, forming the basis of the e-commerce simulation.<\/data>      <data key=\"d5\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"E-COMMERCE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">WebShop simulates e-commerce activities, allowing agents to engage in buying and selling products online.<\/data>      <data key=\"d5\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"LATS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS is evaluated using the HotPotQA dataset to assess its performance in multi-hop reasoning tasks.<\/data>      <data key=\"d5\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION HYPERPARAMETERS\" target=\"GPT-3.5\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">GPT-3.5 utilizes specific value function hyperparameters to optimize its performance in various tasks.<\/data>      <data key=\"d5\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION HYPERPARAMETERS\" target=\"GPT-4\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">GPT-4 also employs value function hyperparameters to enhance its performance in language processing tasks.<\/data>      <data key=\"d5\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"b8dd0300033963bb4a3e1bad37f8e7b9","chunk":" State\nsearch [Query ] Search \u2192Results\nchoose Back to search \u2217 \u2192 Search\nchoose Prev\/Next page Results \u2192Results\nchoose [Product title ] Results \u2192Item\nchoose [Option ] Item \u2192Item\nchoose Desc\/Overview Item \u2192Item-Detail\nchoose Previous Item-Detail \u2192Item\nchoose Buy Item \u2192Episode End\nTable 12. Action space of WebShop.\naverage reward obtained across episodes; and (2) Success\nRate (SR) defined as the portion of instructions where r= 1.\nThe reward is calculated based on the number of attributes\nsatisfied by the selected item. We use 50 environments for\nour experiments and a maximum depth limit of 15. For\nvalue function hyperparameters, we use \u03bb= 0.8for the LM\nscore and self-consistency score.\nD.4. Game of 24\nGame of 24 is a mathematical reasoning challenge where\nthe goal is to use basic arithmetic operations to construct\n24 out of 4 numbers. We follow the setup from Yao et al.\n(2023a), where we measure success if the agent produces aPrompt Method Game of 24 (Success Rate) \u2191\nLATS (CoT, \u03bb= 1) 0.40\nLATS (CoT) 0.44\nTable 13. Ablations on \u03bbin Game of 24 with GPT-3.5. \u03bb= 0.5\nused in the main paper outperforms \u03bb= 1, equivalent to removing\nself-consistency, which indicates that the self-consistency score\nimproves the performance of LATS.\ncorrect equation that equals 24 and uses each input number\nonly once. We report success rate over 50 games. We use\nk= 30 iterations and a maximum depth limit of 5. For value\nfunction hyperparameters, we use \u03bb= 0.5for the LM score\nand self-consistency score. Tab. 13 shows the performance\ncomparison between \u03bb= 0.5and\u03bb= 1, which validates\nour design of self-consistency term.\nE. HotPotQA Prompts\nE.1. Base Acting Prompt\nSolve a question answering task with interleaving Thought,\nAction, Observation steps. Thought can reason about the\ncurrent situation, and Action can be three types:\n(1) Search[entity], which searches the exact entity on\nWikipedia and returns the first paragraph if it exists. If\nnot, it will return some similar entities to search.\n17Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\n(2) Lookup[keyword], which returns the next sentence con-\ntaining keyword in the current passage.\n(3) Finish[answer], which returns the answer and finishes\nthe task.\nAfter each observation, provide the next Thought and next\nAction.\nHere are some examples:\nQuestion: Which magazine was started first Arthur\u2019s Maga-\nzine or First for Women?\nThought 1: I need to search Arthur\u2019s Magazine and First for\nWomen, and find which was started first.\nAction 1: Search[Arthur\u2019s Magazine]\nObservation 1: Arthur\u2019s Magazine was an American literary\nperiodical published in Philadelphia in the 19th century.\nEdited by Timothy Shay Arthur, it featured work by Edgar\nA. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G.\nSpear, and others.[1][2] In May 1846 it was merged into\nGodey\u2019s Lady\u2019s Book.[3]\nThought 2: Arthur\u2019s Magazine was started in 1844. I need\nto search First for Women next.\nAction 2: Search[First for Women]\nObservation 2: First for Women is a woman\u2019s magazine\npublished by Bauer Media Group in the USA.[1] The mag-\nazine was started in 1989. It is based in Englewood Cliffs,\nNew Jersey.[2] In 2011 the circulation of the magazine was\n1,310,696 copies.[3]\nThought 3: First for Women was started in 1989. 1844\n(Arthur\u2019s Magazine) <1989 (First for Women), so Arthur\u2019s\nMagazine was started first.\nAction 3: Finish[Arthur\u2019s Magazine]\n(examples)\nYou have attempted to answer the following question before\nand failed. The following reflection(s) give a plan to avoid\nfailing to answer the question in the same way you did\npreviously. Use them to improve your strategy of correctly\nanswering the given question.\n(trajectories)\n(input)\nE.2. Base Reasoning Prompt\nSolve a question answering task by having a Thought, then\nFinish with your answer. Thought can reason about the\ncurrent situation. Finish[answer] returns the answer and\nfinishes the task. You will be given context that you should\nuse to help you answer the question. Start your responsewith either Action or an indexed Thought\nHere are some examples:\nQuestion: What is the elevation range for the area that the\neastern sector of the Colorado orogeny extends into?\nLet\u2019s think step by step.\nThought 1: The eastern sector of Colorado orogeny extends\ninto the High Plains.\nThought 2: High Plains rise in elevation from around 1,800\nto 7,000 ft\nThought 3: The answer is 1,800 to 7,000 ft.\nAction: Finish[1,800 to 7,000 ft]\n(examples)\nPrevious trial: (trajectories)\n(input)\nE.3. Value Function Prompt\nAnalyze the trajectories of a solution to a question answering\ntask. The trajectories are labeled by environmental Obser-\nvations about the situation, Thoughts","chunk_id":"b8dd0300033963bb4a3e1bad37f8e7b9","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"WEB SHOP","type":"SYSTEM","description":"Web Shop is an online platform that allows users to search for products, view results, and make purchases through a structured interface.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ACTION SPACE","type":"CONCEPT","description":"The action space of Web Shop defines the various actions users can take while interacting with the platform, such as searching, selecting items, and viewing details.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"REWARD","type":"METRIC","description":"Reward is a metric used to evaluate the performance of the Web Shop system, calculated based on the number of attributes satisfied by the selected item.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"SUCCESS RATE (SR)","type":"METRIC","description":"Success Rate (SR) is a metric that measures the portion of instructions successfully executed by the system, defined as r=1.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"GAME OF 24","type":"GAME","description":"Game of 24 is a mathematical reasoning challenge where players use basic arithmetic operations to create the number 24 from four given numbers.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"LATS","type":"METHOD","description":"LATS is a method used in the Game of 24 to evaluate success rates based on different configurations of parameters.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a language model used in the context of the Game of 24 to analyze performance and success rates.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"HOTPOTQA PROMPTS","type":"TECHNIQUE","description":"HotPotQA prompts are structured instructions designed to guide question-answering tasks using reasoning and observation steps.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"THOUGHT","type":"CONCEPT","description":"Thought refers to the reasoning process that occurs during a question-answering task, guiding the actions taken to find an answer.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ACTION","type":"CONCEPT","description":"Action represents the steps taken in response to a Thought during a question-answering task, such as searching for information or finishing an answer.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"OBSERVATION","type":"CONCEPT","description":"Observation is the information gathered during the execution of an Action, which informs subsequent Thoughts and Actions.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ITEM-DETAIL","type":"DATA FORMAT","description":"Item-Detail provides comprehensive information about a specific item, including its description, attributes, and purchasing options.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"EPISODE","type":"CONCEPT","description":"Episode refers to a distinct instance or session of interaction within the Web Shop, where users can perform actions and receive rewards.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ENVIRONMENTS","type":"CONCEPT","description":"Environments refer to the different settings or scenarios in which experiments are conducted to evaluate the performance of the Web Shop system.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"VALUE FUNCTION HYPERPARAMETERS","type":"PARAMETERS","description":"Value function hyperparameters are specific settings used to optimize the performance of the Web Shop's decision-making algorithms.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"SELF-CONSISTENCY SCORE","type":"METRIC","description":"Self-consistency score is a metric used to evaluate the reliability of the results produced by the LATS method in the Game of 24.","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ITEM","type":"","description":"","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"WEB SHOP\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">Web Shop is an online platform that allows users to search for products, view results, and make purchases through a structured interface.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ACTION SPACE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The action space of Web Shop defines the various actions users can take while interacting with the platform, such as searching, selecting items, and viewing details.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"REWARD\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Reward is a metric used to evaluate the performance of the Web Shop system, calculated based on the number of attributes satisfied by the selected item.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"SUCCESS RATE (SR)\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Success Rate (SR) is a metric that measures the portion of instructions successfully executed by the system, defined as r=1.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"GAME OF 24\">      <data key=\"d0\">GAME<\/data>      <data key=\"d1\">Game of 24 is a mathematical reasoning challenge where players use basic arithmetic operations to create the number 24 from four given numbers.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">LATS is a method used in the Game of 24 to evaluate success rates based on different configurations of parameters.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model used in the context of the Game of 24 to analyze performance and success rates.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"HOTPOTQA PROMPTS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">HotPotQA prompts are structured instructions designed to guide question-answering tasks using reasoning and observation steps.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"THOUGHT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Thought refers to the reasoning process that occurs during a question-answering task, guiding the actions taken to find an answer.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ACTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Action represents the steps taken in response to a Thought during a question-answering task, such as searching for information or finishing an answer.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"OBSERVATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Observation is the information gathered during the execution of an Action, which informs subsequent Thoughts and Actions.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ITEM-DETAIL\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Item-Detail provides comprehensive information about a specific item, including its description, attributes, and purchasing options.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"EPISODE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Episode refers to a distinct instance or session of interaction within the Web Shop, where users can perform actions and receive rewards.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ENVIRONMENTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Environments refer to the different settings or scenarios in which experiments are conducted to evaluate the performance of the Web Shop system.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"VALUE FUNCTION HYPERPARAMETERS\">      <data key=\"d0\">PARAMETERS<\/data>      <data key=\"d1\">Value function hyperparameters are specific settings used to optimize the performance of the Web Shop's decision-making algorithms.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"SELF-CONSISTENCY SCORE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Self-consistency score is a metric used to evaluate the reliability of the results produced by the LATS method in the Game of 24.<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ITEM\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <edge source=\"WEB SHOP\" target=\"ACTION SPACE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The action space defines the various interactions users can perform within the Web Shop system, guiding their navigation and choices.<\/data>      <data key=\"d5\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEB SHOP\" target=\"REWARD\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The reward metric is used to evaluate the performance of the Web Shop based on user interactions and item selections.<\/data>      <data key=\"d5\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEB SHOP\" target=\"SUCCESS RATE (SR)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Success Rate (SR) measures the effectiveness of the Web Shop in executing user instructions successfully.<\/data>      <data key=\"d5\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEB SHOP\" target=\"ITEM\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Web Shop allows users to select and purchase various items based on their attributes and user preferences.<\/data>      <data key=\"d5\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEB SHOP\" target=\"EPISODE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Each episode represents a unique interaction session within the Web Shop, where users can engage with the system and perform actions.<\/data>      <data key=\"d5\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEB SHOP\" target=\"ENVIRONMENTS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Different environments are used to conduct experiments that assess the performance and effectiveness of the Web Shop system.<\/data>      <data key=\"d5\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEB SHOP\" target=\"VALUE FUNCTION HYPERPARAMETERS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Value function hyperparameters are utilized to fine-tune the decision-making processes within the Web Shop system.<\/data>      <data key=\"d5\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"GAME OF 24\" target=\"LATS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LATS is a method applied in the Game of 24 to assess success rates based on different parameter settings.<\/data>      <data key=\"d5\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-3.5\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-3.5 is utilized in the context of LATS to analyze and report performance metrics in the Game of 24.<\/data>      <data key=\"d5\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SELF-CONSISTENCY SCORE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The self-consistency score is evaluated as part of the LATS method to determine the accuracy and reliability of the results in the Game of 24.<\/data>      <data key=\"d5\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"HOTPOTQA PROMPTS\" target=\"THOUGHT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">HotPotQA prompts guide the reasoning process (Thought) during question-answering tasks, structuring the approach to finding answers.<\/data>      <data key=\"d5\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"THOUGHT\" target=\"ACTION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Thought informs the Action taken during a question-answering task, creating a logical flow in the problem-solving process.<\/data>      <data key=\"d5\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"ACTION\" target=\"OBSERVATION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Action leads to Observation, where information is gathered that influences subsequent Thoughts and Actions in the task.<\/data>      <data key=\"d5\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"ITEM-DETAIL\" target=\"ITEM\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Each item in the Web Shop has an associated Item-Detail that provides users with essential information for making purchasing decisions.<\/data>      <data key=\"d5\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"357f3442ba581c9d2bdf84d90509056f","chunk":" Plains rise in elevation from around 1,800\nto 7,000 ft\nThought 3: The answer is 1,800 to 7,000 ft.\nAction: Finish[1,800 to 7,000 ft]\n(examples)\nPrevious trial: (trajectories)\n(input)\nE.3. Value Function Prompt\nAnalyze the trajectories of a solution to a question answering\ntask. The trajectories are labeled by environmental Obser-\nvations about the situation, Thoughts that can reason about\nthe current situation, and Actions that can be three types:\n(1) Search[entity], which searches the exact entity on\nWikipedia and returns the first paragraph if it exists. If\nnot, it will return some similar entities to search.\n(2) Lookup[keyword], which returns the next sentence con-\ntaining keyword in the current passage.\n(3) Finish[answer], which returns the answer and finishes\nthe task.\nGiven a question and a trajectory, evaluate its correctness\nand provide your reasoning and analysis in detail. Focus\non the latest thought, action, and observation. Incomplete\ntrajectories can be correct if the thoughts and actions so\nfar are correct, even if the answer is not found yet. Do not\ngenerate additional thoughts or actions. Then at the last line\nconclude \u201cThus the correctness score is s\u201d, where s is an\ninteger from 1 to 10.\nQuestion: Which magazine was started first Arthur\u2019s Maga-\nzine or First for Women?\nThought 1: I need to search Arthur\u2019s Magazine and First for\nWomen, and find which was started first.\nAction 1: Search[Arthur\u2019s Magazine]\nObservation 1: Arthur\u2019s Magazine was an American literary\nperiodical published in Philadelphia in the 19th century.\nEdited by Timothy Shay Arthur, it featured work by Edgar\nA. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G.\n18Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nSpear, and others.[1][2] In May 1846 it was merged into\nGodey\u2019s Lady\u2019s Book.[3]\nThis trajectory is correct as it is reasonable to search for\nthe first magazine provided in the question. It is also better\nto have simple searches corresponding to a single entity,\nmaking this the best action.\nThus the correctness score is 10\n(other examples)\n(failed trajectories)\n(context)\nE.4. Reflection Prompt\nAnalyze the trajectories of a solution to a question-\nanswering task. The trajectories are labeled by environ-\nmental Observations about the situation, Thoughts that can\nreason about the current situation, and Actions that can be\nthree types:\n(1) Search[entity], which searches the exact entity on\nWikipedia and returns the first paragraph if it exists. If\nnot, it will return some similar entities to search.\n(2) Lookup[keyword], which returns the next sentence con-\ntaining keyword in the current passage.\n(3) Finish[answer], which returns the answer and finishes\nthe task.\nGiven a question and a trajectory, evaluate its correctness\nand provide your reasoning and analysis in detail. Focus\non the latest thought, action, and observation. Incomplete\ntrajectories can be correct if the thoughts and actions so\nfar are correct, even if the answer is not found yet. Do not\ngenerate additional thoughts or actions. Then at the last line\nconclude \u201cThus the correctness score is s\u201d, where s is an\ninteger from 1 to 10.\nQuestion: Which magazine was started first Arthur\u2019s Maga-\nzine or First for Women?\nThought 1: I need to search Arthur\u2019s Magazine and First for\nWomen, and find which was started first.\nAction 1: Search[Arthur\u2019s Magazine]\nObservation 1: Arthur\u2019s Magazine was an American literary\nperiodical published in Philadelphia in the 19th century.\nEdited by Timothy Shay Arthur, it featured work by Edgar\nA. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G.\nSpear, and others.[1][2] In May 1846 it was merged into\nGodey\u2019s Lady\u2019s Book.[3]\nThis trajectory is correct as it is reasonable to search for\nthe first magazine provided in the question. It is also better\nto have simple searches corresponding to a single entity,making this the best action.\nThus the correctness score is 10\n(other examples)\n(failed trajectories)\n(context)\nF. Programming Prompts\nF.1. HumanEval function implementation example\nSample function signature:\nd e f minSubArraySum ( nums ) :\nGiven an a r r a y of i n t e g e r s nums ,\nf i n d t h e minimum sum of any\nnon \u2212empty sub \u2212 a r r a y of nums .\nExample\nminSubArraySum ( [ \u2212 1 , \u22122 , \u22123]) == \u22126\nSample function body implementation:\nmin sum = f l o a t ( \u2019 i n f \u2019 )\nf o r i i n r a n g e ( l e n ( nums ) ) :\nc u r r e n t s u m = 0\nf o r j i n r a n g e ( i , l e n ( nums ) ) :\nc u r r e n t s u m += nums [ j ]\ni f c u r r e n t s u m <min sum :\nmin sum = c u r r e n t s u m\nr e t u r n min sum\nF.2. Base Acting\/Reasoning Prompt\nYou are an AI Python assistant. You will be given your\nprevious implementation of a function, a series of unit tests\nresults","chunk_id":"357f3442ba581c9d2bdf84d90509056f","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"PLAINS","type":"GEOGRAPHICAL FEATURE","description":"Plains are flat or gently rolling areas of land that rise in elevation, typically characterized by their expansive and open landscapes.","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"ELEVATION","type":"MEASUREMENT","description":"Elevation refers to the height of a geographical feature above a reference point, usually sea level, measured in feet or meters.","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"ARTHUR\u2019S MAGAZINE","type":"PUBLICATION","description":"Arthur\u2019s Magazine was an American literary periodical published in Philadelphia in the 19th century, known for featuring works by notable authors.","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"FIRST FOR WOMEN","type":"PUBLICATION","description":"First for Women is a magazine that focuses on topics relevant to women, including health, lifestyle, and personal stories.","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"EDGAR A. POE","type":"PERSON","description":"Edgar A. Poe was a notable author whose work was featured in Arthur\u2019s Magazine, known for his contributions to American literature.","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"J.H. INGRAHAM","type":"PERSON","description":"J.H. Ingraham was an author whose works were published in Arthur\u2019s Magazine, contributing to its literary significance.","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"SARAH JOSEPHA HALE","type":"PERSON","description":"Sarah Josepha Hale was a prominent writer featured in Arthur\u2019s Magazine, recognized for her contributions to literature and women\u2019s rights.","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"THOMAS G. SPEAR","type":"PERSON","description":"Thomas G. Spear was an author whose works appeared in Arthur\u2019s Magazine, adding to its diverse literary offerings.","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"TIMOTHY SHAY ARTHUR","type":"","description":"","source_id":"357f3442ba581c9d2bdf84d90509056f"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"PLAINS\">      <data key=\"d0\">GEOGRAPHICAL FEATURE<\/data>      <data key=\"d1\">Plains are flat or gently rolling areas of land that rise in elevation, typically characterized by their expansive and open landscapes.<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"ELEVATION\">      <data key=\"d0\">MEASUREMENT<\/data>      <data key=\"d1\">Elevation refers to the height of a geographical feature above a reference point, usually sea level, measured in feet or meters.<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"ARTHUR&#8217;S MAGAZINE\">      <data key=\"d0\">PUBLICATION<\/data>      <data key=\"d1\">Arthur&#8217;s Magazine was an American literary periodical published in Philadelphia in the 19th century, known for featuring works by notable authors.<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"FIRST FOR WOMEN\">      <data key=\"d0\">PUBLICATION<\/data>      <data key=\"d1\">First for Women is a magazine that focuses on topics relevant to women, including health, lifestyle, and personal stories.<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"EDGAR A. POE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Edgar A. Poe was a notable author whose work was featured in Arthur&#8217;s Magazine, known for his contributions to American literature.<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"J.H. INGRAHAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J.H. Ingraham was an author whose works were published in Arthur&#8217;s Magazine, contributing to its literary significance.<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"SARAH JOSEPHA HALE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sarah Josepha Hale was a prominent writer featured in Arthur&#8217;s Magazine, recognized for her contributions to literature and women&#8217;s rights.<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"THOMAS G. SPEAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas G. Spear was an author whose works appeared in Arthur&#8217;s Magazine, adding to its diverse literary offerings.<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"TIMOTHY SHAY ARTHUR\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <edge source=\"PLAINS\" target=\"ELEVATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Plains can rise in elevation, indicating a change in height from lower to higher altitudes, typically measured in feet.<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"ARTHUR&#8217;S MAGAZINE\" target=\"FIRST FOR WOMEN\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Both Arthur&#8217;s Magazine and First for Women are publications, but they differ in their historical context and target audience.<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"ARTHUR&#8217;S MAGAZINE\" target=\"TIMOTHY SHAY ARTHUR\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Timothy Shay Arthur served as the editor of Arthur&#8217;s Magazine, shaping its content and direction.<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"ARTHUR&#8217;S MAGAZINE\" target=\"EDGAR A. POE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Edgar A. Poe contributed literary works to Arthur&#8217;s Magazine, enhancing its reputation in the 19th century.<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"ARTHUR&#8217;S MAGAZINE\" target=\"J.H. INGRAHAM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">J.H. Ingraham's works were published in Arthur&#8217;s Magazine, contributing to its literary diversity.<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"ARTHUR&#8217;S MAGAZINE\" target=\"SARAH JOSEPHA HALE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Sarah Josepha Hale's contributions to Arthur&#8217;s Magazine helped promote women's literature in the 19th century.<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"ARTHUR&#8217;S MAGAZINE\" target=\"THOMAS G. SPEAR\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Thomas G. Spear's works were featured in Arthur&#8217;s Magazine, contributing to its literary content.<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"785ad59c6a37896a4676ec5c1689735f","chunk":" , l e n ( nums ) ) :\nc u r r e n t s u m += nums [ j ]\ni f c u r r e n t s u m <min sum :\nmin sum = c u r r e n t s u m\nr e t u r n min sum\nF.2. Base Acting\/Reasoning Prompt\nYou are an AI Python assistant. You will be given your\nprevious implementation of a function, a series of unit tests\nresults, and your self-reflection on your previous implemen-\ntation. Write your full implementation (restate the function\nsignature).\nExample 1:\n[previous impl]:\nd e f add ( a : i n t , b : i n t ) \u2212 >i n t :\n\u2018 \u2018 Given i n t e g e r s a and b ,\nr e t u r n t h e t o t a l v a l u e of a and b . \u2019 \u2019\nr e t u r n a \u2212 b\n[unit test results from previous impl]:\nTested passed:\nTests failed:\nassert add(1, 2) == 3 # output: -1\n19Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nassert add(1, 2) == 4 # output: -1\n[reflection on previous impl]:\nThe implementation failed the test cases where the input\nintegers are 1 and 2. The issue arises because the code does\nnot add the two integers together, but instead subtracts the\nsecond integer from the first. To fix this issue, we should\nchange the operator from \u2018-\u2019 to \u2018+\u2019 in the return statement.\nThis will ensure that the function returns the correct output\nfor the given input.\n[improved impl]:\nd e f add ( a : i n t , b : i n t ) \u2212 >i n t :\n\u2018 \u2018\nGiven i n t e g e r s a and b ,\nr e t u r n t h e t o t a l v a l u e of a and b .\n\u2019 \u2019\nr e t u r n a + b\nF.3. Reflection Prompt\nYou are a Python programming assistant. You will be given\na function implementation and a series of unit test results.\nYour goal is to write a few sentences to explain why your\nimplementation is wrong, as indicated by the tests. You\nwill need this as guidance when you try again later. Only\nprovide the few sentence description in your answer, not the\nimplementation. You will be given a few examples by the\nuser.\nExample 1:\n[previous impl]:\nd e f add ( a : i n t , b : i n t ) \u2212 >i n t :\n\u2018 \u2018\nGiven i n t e g e r s a and b ,\nr e t u r n t h e t o t a l v a l u e of a and b .\n\u2019 \u2019\nr e t u r n a \u2212 b\n[unit test results from previous impl]:\nTested passed:\nTests failed:\nassert add(1, 2) == 3 # output: -1\nassert add(1, 2) == 4 # output: -1\n[reflection on previous impl]:\nThe implementation failed the test cases where the input\nintegers are 1 and 2. The issue arises because the code does\nnot add the two integers together, but instead subtracts the\nsecond integer from the first. To fix this issue, we should\nchange the operator from \u2018-\u2019 to \u2018+\u2019 in the return statement.This will ensure that the function returns the correct output\nfor the given input.\nF.4. Test Case Generation Prompt\nYou are an AI coding assistant that can write unique, diverse,\nand intuitive unit tests for functions given the signature and\ndocstring.\n(input)\nG. WebShop Prompts\nG.1. Acting Prompt\nWebshop\nInstruction:\ni would like a 3 ounce bottle of bright citrus deodorant for\nsensitive skin, and price lower than 50.00 dollars\n[Search]\nAction: search[3 ounce bright citrus deodorant sensitive\nskin]\nObservation:\n[Back to Search]\nPage 1 (Total results: 50)\n[Next ] [B078GWRC1J]\nBright Citrus Deodorant by Earth Mama \u2014 Natural and Safe\nfor Sensitive Skin, Pregnancy and Breastfeeding, Contains\nOrganic Calendula 3-Ounce\n$10.99\n[B078GTKVXY]\nGinger Fresh Deodorant by Earth Mama \u2014 Natural and Safe\nfor Sensitive Skin, Pregnancy and Breastfeeding, Contains\nOrganic Calendula 3-Ounce\n$10.99\n[B08KBVJ4XN]\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant\nfor Men, Essential Oil-Based Scent, 24-Hour Odor Protec-\ntion, Cedar & Patchouli Blend, Gentle on Sensitive Skin\n(Mountain Sage, 2.7 oz, 2-Pack)\n$15.95\nAction: think[B078GWRC1J and B078GTKVXY are\nbright citrus deodorant less then 50 dollars. I can check\nB078GWRC1J first.]\n20Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nObservation: OK.\nAction: click[B078GWRC1J]\nObservation:\n[Back to Search]\n[Prev]\nscent [assorted scents][bright citrus][calming laven-\nder][ginger fresh][simply non-scents] size [travel set (4-\npack)][3 ounce (pack of 1)][3-ounce (2","chunk_id":"785ad59c6a37896a4676ec5c1689735f","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"ADD FUNCTION","type":"FUNCTION","description":"The add function is a Python function designed to take two integers as input and return their total value. However, the initial implementation incorrectly subtracts the second integer from the first instead of adding them together.","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"UNIT TEST RESULTS","type":"TESTING","description":"Unit test results provide feedback on the performance of the add function, indicating which tests passed and which failed, thus guiding improvements in the implementation.","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"REFLECTION ON IMPLEMENTATION","type":"ANALYSIS","description":"The reflection on the implementation discusses the errors in the add function, specifically the incorrect use of the subtraction operator instead of the addition operator, and suggests a fix.","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"IMPROVED IMPLEMENTATION","type":"FUNCTION","description":"The improved implementation of the add function corrects the previous error by changing the operator from subtraction to addition, ensuring the function returns the correct output for the given input.","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"WEB SHOP","type":"E-COMMERCE","description":"The Web Shop is an online platform where users can search for and purchase various products, such as deodorants, based on specific criteria like size and price.","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"BRIGHT CITRUS DEODORANT","type":"PRODUCT","description":"Bright Citrus Deodorant is a product designed for sensitive skin, available in a 3-ounce bottle, and is priced under $50.00.","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"TEST CASE","type":"TESTING","description":"A test case is a set of conditions or variables under which a tester will determine whether a function or system is working correctly.","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"SEARCH ACTION","type":"ACTION","description":"The search action is a command executed in the Web Shop to find products based on user-defined criteria, such as product type and price.","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"CLICK ACTION","type":"ACTION","description":"The click action is a command executed in the Web Shop to select a specific product from the search results for more details.","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"UNIT TEST","type":"","description":"","source_id":"785ad59c6a37896a4676ec5c1689735f"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ADD FUNCTION\">      <data key=\"d0\">FUNCTION<\/data>      <data key=\"d1\">The add function is a Python function designed to take two integers as input and return their total value. However, the initial implementation incorrectly subtracts the second integer from the first instead of adding them together.<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"UNIT TEST RESULTS\">      <data key=\"d0\">TESTING<\/data>      <data key=\"d1\">Unit test results provide feedback on the performance of the add function, indicating which tests passed and which failed, thus guiding improvements in the implementation.<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"REFLECTION ON IMPLEMENTATION\">      <data key=\"d0\">ANALYSIS<\/data>      <data key=\"d1\">The reflection on the implementation discusses the errors in the add function, specifically the incorrect use of the subtraction operator instead of the addition operator, and suggests a fix.<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"IMPROVED IMPLEMENTATION\">      <data key=\"d0\">FUNCTION<\/data>      <data key=\"d1\">The improved implementation of the add function corrects the previous error by changing the operator from subtraction to addition, ensuring the function returns the correct output for the given input.<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"WEB SHOP\">      <data key=\"d0\">E-COMMERCE<\/data>      <data key=\"d1\">The Web Shop is an online platform where users can search for and purchase various products, such as deodorants, based on specific criteria like size and price.<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"BRIGHT CITRUS DEODORANT\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Bright Citrus Deodorant is a product designed for sensitive skin, available in a 3-ounce bottle, and is priced under $50.00.<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"TEST CASE\">      <data key=\"d0\">TESTING<\/data>      <data key=\"d1\">A test case is a set of conditions or variables under which a tester will determine whether a function or system is working correctly.<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"SEARCH ACTION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">The search action is a command executed in the Web Shop to find products based on user-defined criteria, such as product type and price.<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"CLICK ACTION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">The click action is a command executed in the Web Shop to select a specific product from the search results for more details.<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"UNIT TEST\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <edge source=\"ADD FUNCTION\" target=\"UNIT TEST RESULTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The unit test results evaluate the performance of the add function, indicating whether it meets the expected outcomes.<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"ADD FUNCTION\" target=\"REFLECTION ON IMPLEMENTATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The reflection on the implementation analyzes the failures in the add function based on the unit test results, providing insights for improvement.<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"ADD FUNCTION\" target=\"IMPROVED IMPLEMENTATION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The improved implementation of the add function is a direct response to the issues identified in the previous implementation and its reflection.<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"ADD FUNCTION\" target=\"UNIT TEST\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Unit tests are designed to validate the functionality of the add function, ensuring it performs as intended.<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"UNIT TEST RESULTS\" target=\"UNIT TEST\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Unit tests are used to generate unit test results, which indicate the success or failure of the tests performed on a function.<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"WEB SHOP\" target=\"BRIGHT CITRUS DEODORANT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The Web Shop allows users to search for products like Bright Citrus Deodorant based on specific criteria such as scent and price.<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"WEB SHOP\" target=\"SEARCH ACTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The search action is performed within the Web Shop to locate products that meet specific user requirements.<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"WEB SHOP\" target=\"CLICK ACTION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The click action is performed within the Web Shop to view more details about a selected product from the search results.<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"6f486e20e3102c7a285e357d356417ad","chunk":"RC1J first.]\n20Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nObservation: OK.\nAction: click[B078GWRC1J]\nObservation:\n[Back to Search]\n[Prev]\nscent [assorted scents][bright citrus][calming laven-\nder][ginger fresh][simply non-scents] size [travel set (4-\npack)][3 ounce (pack of 1)][3-ounce (2-pack)] Bright Citrus\nDeodorant by Earth Mama \u2014 Natural and Safe for Sensi-\ntive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\nPrice: $10.99\nRating: N.A.\n[Description]\n[Features]\n[Reviews]\n[Buy Now]\nAction: think[For 3 ounce bottle of bright citrus deodorant\nfor sensitive skin, the item has options \u2018bright citrus\u2019 and \u20183\nounce (pack of 1)\u2019 and seems good to buy.]\nObservation: OK.\nAction: click[bright citrus]\nObservation: You have clicked bright citrus.\nAction: click[3 ounce (pack of 1)]\nObservation: You have clicked 3 ounce (pack of 1).\nAction: click[Buy Now]\nG.2. Value Function Prompt\nGiven an item to purchase and a trajectory that aims to buy\nan item that exactly matches the specification, which cor-\nresponds to the ideal score of 1.0, analyze the following\ntrajectory, then at the last line conclude, \u201cThus the correct-\nness score is s\u201d, where s is an integer from 1 to 10.\nHere are some examples\nWebshop\nInstruction:i am looking for dairy free and apple variety pack of chips,\nand price lower than 30.00 dollars\n[Search]\nAction: search[dairy free and apple variety pack of chips]\nObservation:\n[Back to Search]\nPage 1 (Total results: 50)\n[Next]\n[B07HRFJWP8]\nEnjoy Life Foods Soft Baked Ovals, Breakfast Bars, Nut\nFree Bars, Soy Free, Dairy Free, Non GMO, Gluten Free,\nVegan, Variety Pack, 4 Boxes (20 Bars Total)\n$100.0\n[B01KMHY5PG]\nEnjoy Life Soft Baked Chewy Bars, Variety Pack, Nut Free\nBars, Soy Free, Dairy Free, Gluten Free, 6 Boxes (30 Total\nBars)\n$21.49\n[B008D2X8C4]\nEnjoy Life Lentil Chips Variety Pack, Dairy Free Chips, Soy\nFree, Nut Free, Non GMO, Vegan, Gluten Free, 24 Bags\n(0.8 oz)\n$100.0\nAction: think[The first two items are not what I want. The\nthird item is the dairy free and apple variety pack of chips I\nwant.]\nObservation: OK.\nAction: click[B008D2X8C4]\nObservation:\n[Back to Search]\n[Prev]\nflavor name [dill and sour cream][garlic & parmesan][light\nsea salt][margherita pizza][thai chili lime][variety pack] size\n[0.8 ounce (pack of 24)][4 ounce (pack of 12)]\nPrice: $100.0\nRating: N.A.\n[Description]\n[Features]\n21Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\n[Reviews]\n[Buy Now]\nAction: think[The item has options \u2018variety pack\u2019 and \u20180.8\nounce (pack of 24)\u2019 and seems good to buy.]\nObservation: OK.\nAction: click[variety pack]\nObservation: You have clicked variety pack.\nAction: click[0.8 ounce (pack of 24)]\nObservation: You have clicked 0.8 ounce (pack of 24).\nAction: click[Buy Now]\nReflection: In this attempt, I was unsuccessful. I acciden-\ntally bought a product that was $100, which is more than my\nbudget of $30. Either way, the initial search results were not\ngood. Next time, I will do search[\u201cvariety pack of chip\u201d;]\nand then check if the results meet the dairy free and the $30\nbudget constraints. I will continue to refine my searches so\nthat I can find more products.\nThus the correctness score is 5\n(input)\nG.3. Reflection Prompt\nYou are an advanced reasoning agent that can improve based\non self-reflection. You will be given a previous reasoning\ntrial in which you were given access to a shopping website\nand a specific type of item to buy. You were given access\nto relevant context and an item to purchase. You were un-\nsuccessful in buying the correct item either because you did\nnot find an item meeting all of the required specifications\nor because you did not select the correct item. The ideal\nscore is 1.0, and anything less is incorrect. In a few sen-\ntences, Diagnose a possible reason for failure and devise a\nnew, concise, high-level plan that aims to mitigate the same\nfailure. Use complete sentences. Here are some examples:\nPrevious Trial Instruction: i am looking for dairy free and\napple variety pack of chips, and price lower than 30.00\ndollars [Search]\nAction: search[dairy free and apple variety pack of chips]\nObservation: [Back to Search] Page 1 (Total results: 50)\n[Next >] [B07HRFJWP8] Enjoy Life Foods Soft Baked\nOvals, Breakfast Bars, Nut Free Bars, Soy Free, Dairy Free,\nNon GMO, Gluten Free","chunk_id":"6f486e20e3102c7a285e357d356417ad","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LANGUAGE AGENT TREE SEARCH","type":"TECHNIQUE","description":"The Language Agent Tree Search is a method that unifies reasoning, acting, and planning in language models, facilitating decision-making processes in various tasks.","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"BRIGHT CITRUS DEODORANT","type":"PRODUCT","description":"Bright Citrus Deodorant by Earth Mama is a natural deodorant designed for sensitive skin, pregnancy, and breastfeeding, containing organic calendula and available in a 3-ounce size.","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"VALUE FUNCTION PROMPT","type":"TECHNIQUE","description":"The Value Function Prompt is a method used to analyze purchasing trajectories and evaluate their correctness based on specified criteria.","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"DAIRY FREE AND APPLE VARIETY PACK OF CHIPS","type":"PRODUCT","description":"The Dairy Free and Apple Variety Pack of Chips is a specific type of snack item that meets dietary restrictions and flavor preferences.","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"ENJOY LIFE FOODS","type":"BRAND","description":"Enjoy Life Foods is a brand that offers a variety of allergen-free snacks, including soft baked ovals and chewy bars, catering to dietary needs.","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"SEARCH ACTION","type":"ACTION","description":"The Search Action is a command executed to find specific items based on user-defined criteria in an online shopping context.","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"CORRECTNESS SCORE","type":"METRIC","description":"The Correctness Score is a numerical evaluation of how well a purchasing action meets the specified requirements, ranging from 1 to 10.","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"CALMING LAVENDER DEODORANT","type":"PRODUCT","description":"Calming Lavender Deodorant by Earth Mama is a natural deodorant designed for sensitive skin, pregnancy, and breastfeeding, containing organic ingredients and available in a 3-ounce size.","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"TRAVEL SET (4-PACK)","type":"PRODUCT","description":"The Travel Set (4-Pack) is a collection of various deodorant scents offered by Earth Mama, designed for convenience and portability.","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"3 OUNCE (PACK OF 1)","type":"PRODUCT","description":"The 3 Ounce (Pack of 1) refers to a single unit of deodorant, suitable for individual use, particularly for sensitive skin.","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"3-OUNCE (2-PACK)","type":"PRODUCT","description":"The 3-Ounce (2-Pack) is a collection of two units of deodorant, designed for users who prefer to have a backup or share with others.","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"NATURAL AND SAFE FOR SENSITIVE SKIN","type":"FEATURE","description":"Natural and safe for sensitive skin indicates that the product is formulated without harsh chemicals, making it suitable for individuals with skin sensitivities.","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"ORGANIC CALENDULA","type":"INGREDIENT","description":"Organic Calendula is a natural ingredient used in the formulation of the deodorant, known for its soothing properties.","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"GINGER FRESH DEODORANT","type":"","description":"","source_id":"6f486e20e3102c7a285e357d356417ad"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LANGUAGE AGENT TREE SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Language Agent Tree Search is a method that unifies reasoning, acting, and planning in language models, facilitating decision-making processes in various tasks.<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"BRIGHT CITRUS DEODORANT\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Bright Citrus Deodorant by Earth Mama is a natural deodorant designed for sensitive skin, pregnancy, and breastfeeding, containing organic calendula and available in a 3-ounce size.<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"VALUE FUNCTION PROMPT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Value Function Prompt is a method used to analyze purchasing trajectories and evaluate their correctness based on specified criteria.<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"DAIRY FREE AND APPLE VARIETY PACK OF CHIPS\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">The Dairy Free and Apple Variety Pack of Chips is a specific type of snack item that meets dietary restrictions and flavor preferences.<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"ENJOY LIFE FOODS\">      <data key=\"d0\">BRAND<\/data>      <data key=\"d1\">Enjoy Life Foods is a brand that offers a variety of allergen-free snacks, including soft baked ovals and chewy bars, catering to dietary needs.<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"SEARCH ACTION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">The Search Action is a command executed to find specific items based on user-defined criteria in an online shopping context.<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"CORRECTNESS SCORE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">The Correctness Score is a numerical evaluation of how well a purchasing action meets the specified requirements, ranging from 1 to 10.<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"CALMING LAVENDER DEODORANT\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Calming Lavender Deodorant by Earth Mama is a natural deodorant designed for sensitive skin, pregnancy, and breastfeeding, containing organic ingredients and available in a 3-ounce size.<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"TRAVEL SET (4-PACK)\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">The Travel Set (4-Pack) is a collection of various deodorant scents offered by Earth Mama, designed for convenience and portability.<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"3 OUNCE (PACK OF 1)\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">The 3 Ounce (Pack of 1) refers to a single unit of deodorant, suitable for individual use, particularly for sensitive skin.<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"3-OUNCE (2-PACK)\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">The 3-Ounce (2-Pack) is a collection of two units of deodorant, designed for users who prefer to have a backup or share with others.<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"NATURAL AND SAFE FOR SENSITIVE SKIN\">      <data key=\"d0\">FEATURE<\/data>      <data key=\"d1\">Natural and safe for sensitive skin indicates that the product is formulated without harsh chemicals, making it suitable for individuals with skin sensitivities.<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"ORGANIC CALENDULA\">      <data key=\"d0\">INGREDIENT<\/data>      <data key=\"d1\">Organic Calendula is a natural ingredient used in the formulation of the deodorant, known for its soothing properties.<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"GINGER FRESH DEODORANT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <edge source=\"LANGUAGE AGENT TREE SEARCH\" target=\"VALUE FUNCTION PROMPT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Language Agent Tree Search utilizes the Value Function Prompt to analyze and improve decision-making in purchasing scenarios.<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"DAIRY FREE AND APPLE VARIETY PACK OF CHIPS\">      <data key=\"d3\">4.0<\/data>      <data key=\"d4\">Both products are examples of items that can be searched for and evaluated in an online shopping context.<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"ENJOY LIFE FOODS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Enjoy Life Foods is a brand that may offer similar products to Bright Citrus Deodorant, focusing on allergen-free and natural ingredients.<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"GINGER FRESH DEODORANT\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both Bright Citrus and Ginger Fresh Deodorants are products offered by Earth Mama, designed for sensitive skin and pregnancy.<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"CALMING LAVENDER DEODORANT\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both Bright Citrus and Calming Lavender Deodorants are products offered by Earth Mama, designed for sensitive skin and pregnancy.<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"TRAVEL SET (4-PACK)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The Travel Set (4-Pack) may include Bright Citrus Deodorant as one of its options, catering to users who prefer various scents.<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"3 OUNCE (PACK OF 1)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The 3 Ounce (Pack of 1) refers to the size of the Bright Citrus Deodorant, indicating its individual packaging.<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"3-OUNCE (2-PACK)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The 3-Ounce (2-Pack) refers to a packaging option for the Bright Citrus Deodorant, providing users with two units for convenience.<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"NATURAL AND SAFE FOR SENSITIVE SKIN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Bright Citrus Deodorant is marketed as natural and safe for sensitive skin, appealing to users with skin sensitivities.<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"ORGANIC CALENDULA\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Organic Calendula is an ingredient in the Bright Citrus Deodorant, contributing to its soothing properties for sensitive skin.<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION PROMPT\" target=\"CORRECTNESS SCORE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Value Function Prompt is used to determine the Correctness Score of a purchasing trajectory based on how well it meets the specified criteria.<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"DAIRY FREE AND APPLE VARIETY PACK OF CHIPS\" target=\"SEARCH ACTION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Search Action is executed to find the Dairy Free and Apple Variety Pack of Chips based on user preferences.<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"CALMING LAVENDER DEODORANT\" target=\"GINGER FRESH DEODORANT\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both Ginger Fresh and Calming Lavender Deodorants are products offered by Earth Mama, designed for sensitive skin and pregnancy.<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"4ed5aa10872b585d02aa2daf4ff8f7fd","chunk":" i am looking for dairy free and\napple variety pack of chips, and price lower than 30.00\ndollars [Search]\nAction: search[dairy free and apple variety pack of chips]\nObservation: [Back to Search] Page 1 (Total results: 50)\n[Next >] [B07HRFJWP8] Enjoy Life Foods Soft Baked\nOvals, Breakfast Bars, Nut Free Bars, Soy Free, Dairy Free,\nNon GMO, Gluten Free, Vegan, Variety Pack, 4 Boxes(20 Bars Total) $100.0 [B01KMHY5PG] Enjoy Life Soft\nBaked Chewy Bars, Variety Pack, Nut Free Bars, Soy Free,\nDairy Free, Gluten Free, 6 Boxes (30 Total Bars) $21.49\n[B008D2X8C4] Enjoy Life Lentil Chips Variety Pack, Dairy\nFree Chips, Soy Free, Nut Free, Non GMO, Vegan, Gluten\nFree, 24 Bags (0.8 oz) $100.0\nAction: think[The first two items are not what I want. The\nthird item is the dairy free and apple variety pack of chips I\nwant.] Observation: OK.\nAction: click[B008D2X8C4] Observation: [Back to Search]\n[<Prev] flavor name [dill and sour cream][garlic & parme-\nsan][light sea salt][margherita pizza][thai chili lime][variety\npack] size [0.8 ounce (pack of 24)][4 ounce (pack of 12)]\nPrice: $100.0 Rating: N.A. [Description] [Features] [Re-\nviews] [Buy Now]\nAction: think[The item has options \u2018variety pack\u2019 and \u20180.8\nounce (pack of 24)\u2019 and seems good to buy.] Observation:\nOK.\nAction: click[variety pack] Observation: You have clicked\nvariety pack.\nAction: click[0.8 ounce (pack of 24)] Observation: You\nhave clicked 0.8 ounce (pack of 24).\nAction: click[Buy Now]\nSTATUS: FAIL\nReflection: In this attempt, I was unsuccessful. I acciden-\ntally bought a product that was $100, which is more than my\nbudget of $30. Either way, the initial search results were not\ngood. Next time, I will do search[\u201cvariety pack of chips\u201d]\nand then check if the results meet the dairy free and the $30\nbudget constraints. I will continue to refine my searches so\nthat I can find more products.\nPrevious Trial Instruction: i need gluten free vegetarian\nsmoked peppered bacon - 4 ounce (pack of 2), and price\nlower than 40.00 dollars [Search]\nAction: search[gluten free vegetarian smoked peppered ba-\ncon 4 ounce pack of 2] Observation: [Back to Search] Page\n1 (Total results: 50) [Next >] [B06Y96MXJV] Smoked\nBacon Sea Salt 3-Pack: Smoked Bacon Chipotle, Smoked\nBacon and Onion, Smoked Peppered Bacon - All-Natural\nSea Salts Slowly Smoked Over Alderwood - No Gluten, No\nMSG, Non-GMO (12 total oz.) $29.99 [B06Y96N1KG]\nSpicy Hot Pepper Sea Salt 3-Pack: Ghost Pepper, Jalapeno,\nHabanero - All Natural, Delicious Blends of Peppers & Sea\nSalt - Gluten-Free, Kosher, No MSG, Non-GMO (12 total\noz.) $29.99 [B07GJTKYJQ] Louisville Vegan Jerky - 5\nFlavor Variety Pack, Non-GMO Soy Protein, Gluten-Free\n(Black Pepper, Buffalo Dill, Pepperoni, Maple Bacon, and\nCarolina BBQ, 3 Ounces) $42.99\n22Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nAction: think[The first two items are not what I want. The\nthird item is not what I want, but it is the closest one.]\nObservation: OK.\nAction: click[B07GJTKYJQ] Observation: [Back to\nSearch] [ <Prev] Louisville Vegan Jerky - 5 Flavor Variety\nPack, Non-GMO Soy Protein, Gluten-Free (Black Pepper,\nBuffalo Dill, Pepperoni, Maple Bacon, and Carolina BBQ,\n3 Ounces) Price: $42.99 Rating: N.A. [Description] [Fea-\ntures] [Reviews] [Buy Now]\nAction: think[The price is too high and it is not what I want.]\nObservation: OK.\nAction: click[ <Prev] Observation: [Back to Search] Page\n1 (Total results: 50) [Next >] [B06Y96MXJV] Smoked\nBacon Sea Salt 3-Pack: Smoked Bacon Chipotle, Smoked\nBacon and Onion, Smoked Peppered Bacon - All-Natural\nSea Salts Slowly Smoked Over Alderwood - No Gluten,\nNo MSG, Non-GMO (12 total oz.) 29.99 [B06Y96N1KG]\nSpicy Hot Pepper Sea Salt 3-Pack: Ghost Pepper, Jalapeno,\nHabanero - All Natural, Delicious Blends of Peppers &\nSea Salt - Gluten-Free, Kosher, No MSG, Non-GMO (12\ntotal oz.) 29.99 [B07GJTKYJQ] Louisville Vegan Jerky - 5\nFlavor Variety Pack,","chunk_id":"4ed5aa10872b585d02aa2daf4ff8f7fd","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"DAIRY FREE AND APPLE VARIETY PACK OF CHIPS","type":"PRODUCT","description":"A specific type of snack that is both dairy-free and includes apple flavor, sought after for its dietary restrictions and flavor profile.","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"ENJOY LIFE FOODS","type":"BRAND","description":"Enjoy Life Foods is a brand that specializes in allergen-free snacks, including various types of bars and chips.","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"SOFT BAKED OVALS","type":"PRODUCT","description":"Soft Baked Ovals are a type of breakfast bar produced by Enjoy Life Foods, designed to be nut-free, soy-free, dairy-free, and gluten-free.","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"SOFT BAKED CHEWY BARS","type":"PRODUCT","description":"Soft Baked Chewy Bars are another product from Enjoy Life Foods, available in a variety pack and meeting dietary restrictions.","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"LENTIL CHIPS VARIETY PACK","type":"PRODUCT","description":"Lentil Chips Variety Pack is a snack option from Enjoy Life Foods that is dairy-free, soy-free, nut-free, and gluten-free.","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON","type":"PRODUCT","description":"A specific type of vegetarian bacon that is gluten-free and sought after for its flavor and dietary compliance.","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"LOUISVILLE VEGAN JERKY","type":"BRAND","description":"Louisville Vegan Jerky is a brand that offers a variety of vegan jerky products, including different flavor packs.","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"SMOKED BACON SEA SALT 3-PACK","type":"PRODUCT","description":"A product that includes three flavors of smoked bacon sea salt, marketed as gluten-free and non-GMO.","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"SPICY HOT PEPPER SEA SALT 3-PACK","type":"PRODUCT","description":"A product that includes three flavors of spicy hot pepper sea salt, also marketed as gluten-free and non-GMO.","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"0.8 OUNCE (PACK OF 24)","type":"PRODUCT","description":"A specific packaging size for snacks, indicating that the product contains 24 bags, each weighing 0.8 ounces.","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"PRICE","type":"VALUE","description":"The cost associated with a product, which is a critical factor in consumer purchasing decisions, often compared against budget constraints.","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"BUDGET","type":"VALUE","description":"The maximum amount of money a consumer is willing to spend on a product, influencing their purchasing choices.","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"VARIETY PACK","type":"","description":"","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"DAIRY FREE AND APPLE VARIETY PACK OF CHIPS\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">A specific type of snack that is both dairy-free and includes apple flavor, sought after for its dietary restrictions and flavor profile.<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"ENJOY LIFE FOODS\">      <data key=\"d0\">BRAND<\/data>      <data key=\"d1\">Enjoy Life Foods is a brand that specializes in allergen-free snacks, including various types of bars and chips.<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"SOFT BAKED OVALS\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Soft Baked Ovals are a type of breakfast bar produced by Enjoy Life Foods, designed to be nut-free, soy-free, dairy-free, and gluten-free.<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"SOFT BAKED CHEWY BARS\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Soft Baked Chewy Bars are another product from Enjoy Life Foods, available in a variety pack and meeting dietary restrictions.<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"LENTIL CHIPS VARIETY PACK\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Lentil Chips Variety Pack is a snack option from Enjoy Life Foods that is dairy-free, soy-free, nut-free, and gluten-free.<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">A specific type of vegetarian bacon that is gluten-free and sought after for its flavor and dietary compliance.<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"LOUISVILLE VEGAN JERKY\">      <data key=\"d0\">BRAND<\/data>      <data key=\"d1\">Louisville Vegan Jerky is a brand that offers a variety of vegan jerky products, including different flavor packs.<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"SMOKED BACON SEA SALT 3-PACK\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">A product that includes three flavors of smoked bacon sea salt, marketed as gluten-free and non-GMO.<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"SPICY HOT PEPPER SEA SALT 3-PACK\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">A product that includes three flavors of spicy hot pepper sea salt, also marketed as gluten-free and non-GMO.<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"0.8 OUNCE (PACK OF 24)\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">A specific packaging size for snacks, indicating that the product contains 24 bags, each weighing 0.8 ounces.<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"PRICE\">      <data key=\"d0\">VALUE<\/data>      <data key=\"d1\">The cost associated with a product, which is a critical factor in consumer purchasing decisions, often compared against budget constraints.<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"BUDGET\">      <data key=\"d0\">VALUE<\/data>      <data key=\"d1\">The maximum amount of money a consumer is willing to spend on a product, influencing their purchasing choices.<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"VARIETY PACK\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <edge source=\"DAIRY FREE AND APPLE VARIETY PACK OF CHIPS\" target=\"LENTIL CHIPS VARIETY PACK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The dairy-free and apple variety pack of chips is similar to the lentil chips variety pack, both being dairy-free snack options.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"DAIRY FREE AND APPLE VARIETY PACK OF CHIPS\" target=\"VARIETY PACK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The dairy-free and apple variety pack of chips is a specific example of a variety pack that includes multiple flavors or types of chips.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"DAIRY FREE AND APPLE VARIETY PACK OF CHIPS\" target=\"BUDGET\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The budget of $30 influences the decision to purchase the dairy-free and apple variety pack of chips.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE FOODS\" target=\"SOFT BAKED OVALS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Enjoy Life Foods produces Soft Baked Ovals, which are part of their allergen-free product line.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE FOODS\" target=\"SOFT BAKED CHEWY BARS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Enjoy Life Foods produces Soft Baked Chewy Bars, which are part of their allergen-free product line.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE FOODS\" target=\"LENTIL CHIPS VARIETY PACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Enjoy Life Foods produces the Lentil Chips Variety Pack, which is part of their allergen-free product line.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SOFT BAKED OVALS\" target=\"PRICE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The price of Soft Baked Ovals is a key factor for consumers when considering a purchase.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SOFT BAKED CHEWY BARS\" target=\"PRICE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The price of Soft Baked Chewy Bars is a key factor for consumers when considering a purchase.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"LENTIL CHIPS VARIETY PACK\" target=\"VARIETY PACK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Lentil Chips Variety Pack is a specific example of a variety pack that includes multiple flavors of lentil chips.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"LENTIL CHIPS VARIETY PACK\" target=\"PRICE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The price of the Lentil Chips Variety Pack is a key factor for consumers when considering a purchase.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON\" target=\"LOUISVILLE VEGAN JERKY\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both products cater to dietary restrictions, with the gluten-free vegetarian smoked peppered bacon being a meat alternative similar to the offerings from Louisville Vegan Jerky.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON\" target=\"PRICE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The price of gluten-free vegetarian smoked peppered bacon is a key factor for consumers when considering a purchase.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON\" target=\"BUDGET\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The budget of $40 influences the decision to purchase gluten-free vegetarian smoked peppered bacon.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"SMOKED BACON SEA SALT 3-PACK\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Louisville Vegan Jerky and the Smoked Bacon Sea Salt 3-Pack both offer flavor profiles that appeal to consumers looking for savory snacks.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"SPICY HOT PEPPER SEA SALT 3-PACK\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Louisville Vegan Jerky and the Spicy Hot Pepper Sea Salt 3-Pack both cater to consumers looking for spicy and flavorful snack options.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"PRICE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The price of Louisville Vegan Jerky is a key factor for consumers when considering a purchase.<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"5d356b8ff719763a38cecff22c4e17b7","chunk":" Non-GMO (12 total oz.) 29.99 [B06Y96N1KG]\nSpicy Hot Pepper Sea Salt 3-Pack: Ghost Pepper, Jalapeno,\nHabanero - All Natural, Delicious Blends of Peppers &\nSea Salt - Gluten-Free, Kosher, No MSG, Non-GMO (12\ntotal oz.) 29.99 [B07GJTKYJQ] Louisville Vegan Jerky - 5\nFlavor Variety Pack, Non-GMO Soy Protein, Gluten-Free\n(Black Pepper, Buffalo Dill, Pepperoni, Maple Bacon, and\nCarolina BBQ, 3 Ounces) 42.99\nAction: think[The first two items are not what I want.]\nObservation: OK.\nAction: click[Next >] Observation: Invalid action!\nAction: think[I need to refine my search.] Observation: OK.\nAction: click[ <Back to Search] Observation: Invalid ac-\ntion!\nAction: click[Back to Search]\nSTATUS: FAIL\nReflection: In this attempt, I was unsuccessful. The initial\nsearch results were not the products that I was looking for.\nNext time, I will do search[\u201cvegetarian bacon\u201d] for a broader\nsearch. I will check that the new results will fulfill the gluten\nfree and 4 ounce pack of 2 constraints. I will continue to\nrefine my searches so that I can find more products.\nPrevious trial: trajectory Reflection:\u201d\u2019\n23","chunk_id":"5d356b8ff719763a38cecff22c4e17b7","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":309,"entities":[{"name":"NON-GMO","type":"PRODUCT ATTRIBUTE","description":"Non-GMO indicates that the product does not contain genetically modified organisms, ensuring it meets certain dietary preferences.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"SPICY HOT PEPPER SEA SALT","type":"PRODUCT","description":"Spicy Hot Pepper Sea Salt is a seasoning product that includes blends of various peppers and sea salt, marketed as all-natural and gluten-free.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"LOUISVILLE VEGAN JERKY","type":"PRODUCT","description":"Louisville Vegan Jerky is a plant-based snack made from non-GMO soy protein, available in a variety pack with different flavors, and is gluten-free.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"GLUTEN-FREE","type":"PRODUCT ATTRIBUTE","description":"Gluten-free indicates that the product does not contain gluten, catering to individuals with gluten sensitivities or celiac disease.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"PEPPER FLAVORS","type":"PRODUCT ATTRIBUTE","description":"Pepper flavors refer to the various types of peppers used in products, such as Ghost Pepper, Jalapeno, and Habanero, contributing to the taste profile.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"SEARCH ACTION","type":"USER ACTION","description":"Search action refers to the user's attempt to find specific products online, indicating their preferences and constraints.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"REFLECTION","type":"USER ACTION","description":"Reflection is the user's consideration of their previous actions and outcomes, leading to adjustments in future search strategies.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"JALAPENO","type":"PRODUCT ATTRIBUTE","description":"Jalapeno is a medium-heat chili pepper commonly used in cooking, known for its distinct flavor and versatility in various dishes.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"HABANERO","type":"PRODUCT ATTRIBUTE","description":"Habanero is a very hot chili pepper that adds significant heat and flavor to food products.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"BLACK PEPPER","type":"PRODUCT ATTRIBUTE","description":"Black Pepper is a common spice made from the dried fruit of the pepper plant, used to enhance the flavor of various dishes.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"BUFFALO DILL","type":"PRODUCT ATTRIBUTE","description":"Buffalo Dill is a flavor variant of Louisville Vegan Jerky, combining the taste of buffalo sauce with dill seasoning.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"PEPPERONI","type":"PRODUCT ATTRIBUTE","description":"Pepperoni is a type of spicy Italian-American sausage, often used as a flavor in various food products.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"MAPLE BACON","type":"PRODUCT ATTRIBUTE","description":"Maple Bacon is a flavor variant that combines the sweetness of maple with the savory taste of bacon, used in Louisville Vegan Jerky.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"CAROLINA BBQ","type":"PRODUCT ATTRIBUTE","description":"Carolina BBQ is a flavor variant that reflects the barbecue style from the Carolinas, known for its tangy and smoky taste.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"4 OUNCE PACK","type":"PRODUCT ATTRIBUTE","description":"A 4 ounce pack refers to the size of the product packaging, indicating the quantity of the product contained within.","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"GHOST PEPPER","type":"","description":"","source_id":"5d356b8ff719763a38cecff22c4e17b7"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"NON-GMO\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">Non-GMO indicates that the product does not contain genetically modified organisms, ensuring it meets certain dietary preferences.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"SPICY HOT PEPPER SEA SALT\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Spicy Hot Pepper Sea Salt is a seasoning product that includes blends of various peppers and sea salt, marketed as all-natural and gluten-free.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"LOUISVILLE VEGAN JERKY\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Louisville Vegan Jerky is a plant-based snack made from non-GMO soy protein, available in a variety pack with different flavors, and is gluten-free.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"GLUTEN-FREE\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">Gluten-free indicates that the product does not contain gluten, catering to individuals with gluten sensitivities or celiac disease.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"PEPPER FLAVORS\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">Pepper flavors refer to the various types of peppers used in products, such as Ghost Pepper, Jalapeno, and Habanero, contributing to the taste profile.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"SEARCH ACTION\">      <data key=\"d0\">USER ACTION<\/data>      <data key=\"d1\">Search action refers to the user's attempt to find specific products online, indicating their preferences and constraints.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"REFLECTION\">      <data key=\"d0\">USER ACTION<\/data>      <data key=\"d1\">Reflection is the user's consideration of their previous actions and outcomes, leading to adjustments in future search strategies.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"JALAPENO\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">Jalapeno is a medium-heat chili pepper commonly used in cooking, known for its distinct flavor and versatility in various dishes.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"HABANERO\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">Habanero is a very hot chili pepper that adds significant heat and flavor to food products.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"BLACK PEPPER\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">Black Pepper is a common spice made from the dried fruit of the pepper plant, used to enhance the flavor of various dishes.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"BUFFALO DILL\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">Buffalo Dill is a flavor variant of Louisville Vegan Jerky, combining the taste of buffalo sauce with dill seasoning.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"PEPPERONI\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">Pepperoni is a type of spicy Italian-American sausage, often used as a flavor in various food products.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"MAPLE BACON\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">Maple Bacon is a flavor variant that combines the sweetness of maple with the savory taste of bacon, used in Louisville Vegan Jerky.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"CAROLINA BBQ\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">Carolina BBQ is a flavor variant that reflects the barbecue style from the Carolinas, known for its tangy and smoky taste.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"4 OUNCE PACK\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">A 4 ounce pack refers to the size of the product packaging, indicating the quantity of the product contained within.<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"GHOST PEPPER\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <edge source=\"NON-GMO\" target=\"SPICY HOT PEPPER SEA SALT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Spicy Hot Pepper Sea Salt is labeled as Non-GMO, indicating it meets specific dietary standards.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"NON-GMO\" target=\"LOUISVILLE VEGAN JERKY\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Louisville Vegan Jerky is also labeled as Non-GMO, appealing to health-conscious consumers.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT\" target=\"GLUTEN-FREE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Spicy Hot Pepper Sea Salt is marketed as gluten-free, making it suitable for those with gluten sensitivities.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT\" target=\"PEPPER FLAVORS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Spicy Hot Pepper Sea Salt includes various pepper flavors, enhancing its taste profile.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT\" target=\"GHOST PEPPER\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Spicy Hot Pepper Sea Salt includes Ghost Pepper as one of its flavor components, contributing to its spiciness.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT\" target=\"JALAPENO\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Spicy Hot Pepper Sea Salt may include Jalapeno as part of its pepper blend, enhancing its flavor profile.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT\" target=\"HABANERO\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Spicy Hot Pepper Sea Salt may include Habanero, adding to its heat and flavor complexity.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"GLUTEN-FREE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Louisville Vegan Jerky is gluten-free, catering to individuals with dietary restrictions.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"PEPPER FLAVORS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Louisville Vegan Jerky features flavors that may include pepper varieties, contributing to its overall flavor.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"BLACK PEPPER\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Louisville Vegan Jerky may contain Black Pepper as a seasoning, enhancing its overall flavor.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"BUFFALO DILL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Buffalo Dill is one of the flavor variants of Louisville Vegan Jerky, contributing to its diverse taste offerings.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"PEPPERONI\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Pepperoni is one of the flavor variants of Louisville Vegan Jerky, appealing to those who enjoy spicy flavors.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"MAPLE BACON\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Maple Bacon is a flavor variant of Louisville Vegan Jerky, combining sweet and savory elements.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"CAROLINA BBQ\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Carolina BBQ is a flavor variant of Louisville Vegan Jerky, reflecting regional barbecue flavors.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SEARCH ACTION\" target=\"REFLECTION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The user's search action leads to reflection on their experience, prompting them to refine their search strategy.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SEARCH ACTION\" target=\"4 OUNCE PACK\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The user is looking for products that fulfill the constraint of being in a 4 ounce pack, indicating a specific size preference.<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"c3d0436082aada237ee4bee645f16059","chunk":"2024-8-19\nAutomated Design of Agentic Systems\nShengran Hu1,2, Cong Lu1,2and Jeff Clune1,2,3\n1University of British Columbia,2Vector Institute,3Canada CIFAR AI Chair\nResearchers are investing substantial effort in developing powerful general-purpose agents, wherein\nFoundation Models are used as modules within agentic systems (e.g. Chain-of-Thought, Self-Reflection,\nToolformer). However, the history of machine learning teaches us that hand-designed solutions are\neventually replaced by learned solutions. We formulate a new research area, Automated Design of\nAgentic Systems (ADAS), which aims to automatically create powerful agentic system designs, including\ninventing novel building blocks and\/or combining them in new ways. We further demonstrate that there\nis an unexplored yet promising approach within ADAS where agents can be defined in code and new\nagents can be automatically discovered by a meta agent programming ever better ones in code. Given\nthat programming languages are Turing Complete, this approach theoretically enables the learning\nofany possible agentic system: including novel prompts, tool use, control flows, and combinations\nthereof. We present a simple yet effective algorithm named Meta Agent Search to demonstrate this idea,\nwhere a meta agent iteratively programs interesting new agents based on an ever-growing archive of\nprevious discoveries. Through extensive experiments across multiple domains including coding, science,\nand math, we show that our algorithm can progressively invent agents with novel designs that greatly\noutperform state-of-the-art hand-designed agents. Importantly, we consistently observe the surprising\nresult that agents invented by Meta Agent Search maintain superior performance even when transferred\nacross domains and models, demonstrating their robustness and generality. Provided we develop it\nsafely, our work illustrates the potential of an exciting new research direction toward automatically\ndesigning ever-more powerful agentic systems to benefit humanity.\n\/githubhttps:\/\/github.com\/ShengranHu\/ADAS\n1. Introduction\nFoundation Models (FMs) such as GPT (OpenAI, 2022, 2024) and Claude (Anthropic, 2024b) are\nquicklybeingadoptedaspowerfulgeneral-purposeagentsforagentictasksthatneedflexiblereasoning\nand planning (Wang et al., 2024). Despite recent advancements in FMs, solving problems reliably\noften requires an agent to be a compound agentic system with multiple components instead of a\nmonolithic model query (Rockt\u00e4schel, 2024; Zaharia et al., 2024). Additionally, to enable agents to\nsolve complex real-world tasks, they often need access to external tools such as search engines, code\nexecution, and database queries. As a result, many effective building blocks of agentic systems have\nbeen proposed, such as chain-of-thought planning and reasoning (Hu & Clune, 2024; Wei et al., 2022;\nYao et al., 2023), memory structures (Lewis et al., 2020; Zhang et al., 2024c), tool use (Qu et al.,\n2024; Schick et al., 2023), and self-reflection (Madaan et al., 2024; Shinn et al., 2023). Although\nthese agents have already seen significant success across various applications (Wang et al., 2024),\ndeveloping these building blocks and combining them into complex agentic systems often requires\ndomain-specific manual tuning and substantial effort from both researchers and engineers.\nHowever, the history of machine learning reveals a recurring theme: manually created arti-\nfacts become replaced by learned, more efficient solutions over time as we get more compute and\ndata (Clune, 2019). An early example is from computer vision, where hand-designed features like\nHOG (Dalal & Triggs, 2005) were eventually replaced by learned features from Convolutional Neural\nCorresponding author(s): Shengran Hu (srhu@cs.ubc.ca)arXiv:2408.08435v1  [cs.AI]  15 Aug 2024Automated Design of Agentic Systems\nSummary and motivation : \u201cBased on \nthe insights from previous agents \u2026\u201d,\nName: \u201cDivide and Conquer Agent\u201d,\nCode: \u201cdef forward(Task):\n \u2026\u2026\n return Answer\u201d\nMeta AgentNext interesting agent\nAgent ArchiveTest performance on tasks InputRefine until novel \nand error -free\nExamples of Discovered Agents\nMulti -step Peer Review AgentExperts\nAnswers\nReviewersT ask\nVerified Multimodal AgentT ask\nVisual \nParadigm\nVerifier\nVerified \nParadigmVisual \nAnalyzer\nCOTAnswerT askSub -problem \nDivision subsubsub\nsubsub\nExpertsAnswersEnsembleAnswer\nDivide and Conquer AgentReviewsand add to archiveNew Agent\n\u2026\nFigure 1|Overview of the proposed algorithm Meta Agent Search and examples of discovered\nagents. In our algorithm, we instruct the \u201cmeta\u201d agent to iteratively program new agents, test their\nperformance on tasks, add them to an archive of discovered agents, and use this archive to inform the\nmeta agent in subsequent iterations. We show three example agents across our runs, with all names\ngenerated by the meta agent. The detailed code of example agents can be found in Appendix F.\nNetworks (CNNs, Krizhevsky et al. (2012)). More recently, AutoML methods (Hutter et al., 2019)\nand AI-Generating Algorithms (AI-GAs, Clune (2019)) have also demonstrated the superiority of\nlearned AI systems compared to hand-designed AI systems. For example, the current best-performing\nCNN models come from Neural Architecture Search (Elsken","chunk_id":"c3d0436082aada237ee4bee645f16059","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"SHENGRAN HU","type":"PERSON","description":"Shengran Hu is a researcher affiliated with the University of British Columbia and the Vector Institute, contributing to the field of automated design of agentic systems.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"CONG LU","type":"PERSON","description":"Cong Lu is a researcher affiliated with the University of British Columbia and the Vector Institute, contributing to the field of automated design of agentic systems.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is a researcher affiliated with the University of British Columbia, the Vector Institute, and holds a Canada CIFAR AI Chair, focusing on agentic systems.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)","type":"RESEARCH AREA","description":"ADAS is a new research area aimed at automatically creating powerful agentic system designs, including novel building blocks and combinations.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"FOUNDATION MODELS (FMS)","type":"TECHNOLOGY","description":"Foundation Models are large-scale models like GPT and Claude that serve as powerful general-purpose agents for various tasks requiring flexible reasoning and planning.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"META AGENT SEARCH","type":"ALGORITHM","description":"Meta Agent Search is an algorithm that enables a meta agent to iteratively program new agents based on an archive of previous discoveries.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"CHAIN-OF-THOUGHT PLANNING","type":"TECHNIQUE","description":"Chain-of-thought planning is a technique used in agentic systems to enhance reasoning and decision-making processes.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"SELF-REFLECTION","type":"TECHNIQUE","description":"Self-reflection is a technique in agentic systems that allows agents to evaluate their own performance and improve over time.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"TOOL USE","type":"TECHNIQUE","description":"Tool use refers to the capability of agents to utilize external tools such as search engines and code execution to solve complex tasks.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"AGENT ARCHIVE","type":"DATA STRUCTURE","description":"The agent archive is a collection of previously discovered agents that informs the meta agent during the programming of new agents.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"MULTI-STEP PEER REVIEW AGENT","type":"AGENT","description":"The Multi-step Peer Review Agent is an example of an agent discovered through the Meta Agent Search algorithm, designed for reviewing tasks.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"VERIFIED MULTIMODAL AGENT","type":"AGENT","description":"The Verified Multimodal Agent is another example of an agent discovered through the Meta Agent Search algorithm, capable of handling visual tasks.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"DIVIDE AND CONQUER AGENT","type":"AGENT","description":"The Divide and Conquer Agent is an agent designed to break down tasks into subproblems for more efficient processing.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"ZAHARIA ET AL. (2024)","type":"PERSON","description":"Zaharia et al. (2024) is a reference to a study that emphasizes the importance of compound agentic systems in solving complex tasks.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"HOG","type":"TECHNIQUE","description":"HOG (Histogram of Oriented Gradients) is a hand-designed feature used in computer vision, which has been replaced by learned features over time.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"DALAL & TRIGGS (2005)","type":"PERSON","description":"Dalal & Triggs (2005) is a reference to a study that introduced the HOG feature in computer vision.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"CNNS","type":"TECHNOLOGY","description":"Convolutional Neural Networks (CNNs) are a type of deep learning model that have surpassed hand-designed features in performance for image processing tasks.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"HUTTER ET AL. (2019)","type":"PERSON","description":"Hutter et al. (2019) is a reference to a study discussing AutoML methods that demonstrate the superiority of learned AI systems.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"AI-GENERATING ALGORITHMS (AI-GAS)","type":"TECHNOLOGY","description":"AI-Generating Algorithms are methods that create AI systems automatically, showing better performance than hand-designed systems.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"NEURAL ARCHITECTURE SEARCH","type":"TECHNIQUE","description":"Neural Architecture Search is a method for automatically designing neural network architectures that outperform manually designed models.","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"ROCKT\u00c4SCHEL (2024)","type":"","description":"","source_id":"c3d0436082aada237ee4bee645f16059"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"SHENGRAN HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengran Hu is a researcher affiliated with the University of British Columbia and the Vector Institute, contributing to the field of automated design of agentic systems.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"CONG LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cong Lu is a researcher affiliated with the University of British Columbia and the Vector Institute, contributing to the field of automated design of agentic systems.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is a researcher affiliated with the University of British Columbia, the Vector Institute, and holds a Canada CIFAR AI Chair, focusing on agentic systems.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\">      <data key=\"d0\">RESEARCH AREA<\/data>      <data key=\"d1\">ADAS is a new research area aimed at automatically creating powerful agentic system designs, including novel building blocks and combinations.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"FOUNDATION MODELS (FMS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundation Models are large-scale models like GPT and Claude that serve as powerful general-purpose agents for various tasks requiring flexible reasoning and planning.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">Meta Agent Search is an algorithm that enables a meta agent to iteratively program new agents based on an archive of previous discoveries.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT PLANNING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain-of-thought planning is a technique used in agentic systems to enhance reasoning and decision-making processes.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"SELF-REFLECTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-reflection is a technique in agentic systems that allows agents to evaluate their own performance and improve over time.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Tool use refers to the capability of agents to utilize external tools such as search engines and code execution to solve complex tasks.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"AGENT ARCHIVE\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">The agent archive is a collection of previously discovered agents that informs the meta agent during the programming of new agents.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"MULTI-STEP PEER REVIEW AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Multi-step Peer Review Agent is an example of an agent discovered through the Meta Agent Search algorithm, designed for reviewing tasks.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"VERIFIED MULTIMODAL AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Verified Multimodal Agent is another example of an agent discovered through the Meta Agent Search algorithm, capable of handling visual tasks.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"DIVIDE AND CONQUER AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Divide and Conquer Agent is an agent designed to break down tasks into subproblems for more efficient processing.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"ZAHARIA ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zaharia et al. (2024) is a reference to a study that emphasizes the importance of compound agentic systems in solving complex tasks.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"HOG\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">HOG (Histogram of Oriented Gradients) is a hand-designed feature used in computer vision, which has been replaced by learned features over time.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"DALAL &amp; TRIGGS (2005)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dalal &amp; Triggs (2005) is a reference to a study that introduced the HOG feature in computer vision.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"CNNS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Convolutional Neural Networks (CNNs) are a type of deep learning model that have surpassed hand-designed features in performance for image processing tasks.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"HUTTER ET AL. (2019)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hutter et al. (2019) is a reference to a study discussing AutoML methods that demonstrate the superiority of learned AI systems.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"AI-GENERATING ALGORITHMS (AI-GAS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AI-Generating Algorithms are methods that create AI systems automatically, showing better performance than hand-designed systems.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Neural Architecture Search is a method for automatically designing neural network architectures that outperform manually designed models.<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"ROCKT&#196;SCHEL (2024)\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <edge source=\"SHENGRAN HU\" target=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Shengran Hu is a researcher contributing to the development of the Automated Design of Agentic Systems (ADAS) research area.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"CONG LU\" target=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Cong Lu is a researcher contributing to the development of the Automated Design of Agentic Systems (ADAS) research area.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"JEFF CLUNE\" target=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Jeff Clune is a researcher contributing to the development of the Automated Design of Agentic Systems (ADAS) research area.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"FOUNDATION MODELS (FMS)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Foundation Models are utilized as modules within agentic systems, which are a focus of the ADAS research area.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"META AGENT SEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search is an algorithm developed within the ADAS research area to create new agentic systems.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"CHAIN-OF-THOUGHT PLANNING\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Chain-of-thought planning is one of the effective building blocks proposed for agentic systems in the ADAS research area.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"SELF-REFLECTION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Self-reflection is another building block proposed for enhancing agentic systems within the ADAS research area.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"TOOL USE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Tool use is a critical component for agents to solve complex tasks, as discussed in the context of ADAS.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"ROCKT&#196;SCHEL (2024)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Rockt&#228;schel (2024) provides insights relevant to the development of compound agentic systems in the context of ADAS.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"ZAHARIA ET AL. (2024)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Zaharia et al. (2024) provides insights relevant to the development of compound agentic systems in the context of ADAS.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"AI-GENERATING ALGORITHMS (AI-GAS)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">AI-Generating Algorithms are relevant to the ADAS research area as they demonstrate the potential of learned AI systems.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Neural Architecture Search is a technique that can be applied within the ADAS research area to improve agentic system designs.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"AGENT ARCHIVE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The agent archive is utilized by the Meta Agent Search algorithm to inform the programming of new agents.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MULTI-STEP PEER REVIEW AGENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Multi-step Peer Review Agent is an example of an agent discovered through the Meta Agent Search algorithm.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"VERIFIED MULTIMODAL AGENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Verified Multimodal Agent is an example of an agent discovered through the Meta Agent Search algorithm.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"DIVIDE AND CONQUER AGENT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The Divide and Conquer Agent is an example of an agent discovered through the Meta Agent Search algorithm.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"HOG\" target=\"DALAL &amp; TRIGGS (2005)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Dalal &amp; Triggs (2005) introduced the HOG feature, which is a significant example of hand-designed features in computer vision.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"HOG\" target=\"CNNS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">HOG features have been surpassed by learned features from CNNs in the field of computer vision.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"HUTTER ET AL. (2019)\" target=\"AI-GENERATING ALGORITHMS (AI-GAS)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Hutter et al. (2019) discusses AutoML methods that highlight the effectiveness of AI-Generating Algorithms.<\/data>      <data key=\"d5\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"81c504ffbcc5ed882e234802135295ba","chunk":" The detailed code of example agents can be found in Appendix F.\nNetworks (CNNs, Krizhevsky et al. (2012)). More recently, AutoML methods (Hutter et al., 2019)\nand AI-Generating Algorithms (AI-GAs, Clune (2019)) have also demonstrated the superiority of\nlearned AI systems compared to hand-designed AI systems. For example, the current best-performing\nCNN models come from Neural Architecture Search (Elsken et al., 2019; Shen et al., 2023) instead\nof manual design; in LLM alignment, learned loss functions (Lu et al., 2024a) outperform most\nhand-designed ones such as DPO (Rafailov et al., 2024); The AI Scientist (Lu et al., 2024b) demon-\nstrates an automated research pipeline, including the development of novel ML algorithms; and\nan endless number of robotics learning environments can be automatically generated in works like\nOMNI-EPIC (Faldor et al., 2024), which demonstrate surprising creativity in generated environments\nand allow more efficient environment creation than the manual approach (see more examples in\nSection 5). Therefore, in this paper, we propose a new research question: Can we automate the design\nof agentic systems rather than relying on manual efforts?\nTo explore the above research question, we formulate a new research area we call Automated\nDesign of AgenticSystems (ADAS), which aims to automatically invent novel building blocks and\ndesign powerful agentic systems (Section 2). We argue that ADAS may prove to be the fastest path to\ndeveloping powerful agents, and show initial evidence that learned agents can greatly outperform\nhand-designed agents. Considering the tremendous number of building blocks yet to be discovered in\nagentic systems (Section 5), it would take a long time for our research community to discover them\nall. Even if we successfully discover most of the useful building blocks, combining them into effective\nagentic systems for massive real-world applications would still be challenging and time-consuming,\ngiven the many different ways the building blocks can combine and interact with each other. In\ncontrast, with ADAS, the building blocks and agents can be learned in an automated fashion. ADAS\n2Automated Design of Agentic Systems\nmay not only potentially save human effort in developing powerful agents but also could be a faster\npath to more effective solutions than manual design.\nAlthough a few existing works can be considered as ADAS methods, most of them focus only on\ndesigning prompts (Fernando et al., 2024; Yang et al., 2024), greatly limiting their ability to invent\nflexible design patterns in agents (Section 5). In this paper, we show that there is an unexplored\nyet promising approach to ADAS where we can define the entire agentic system in code and new\nagents can be automatically discovered by a \u201cmeta\u201d agent programming even better ones in code.\nGiven that most programming languages, such as Python, which we use in this paper, are Turing\nComplete (Boyer & Moore, 1983; Ladha, 2024), searching within a code space theoretically enables a\nADAS algorithm to discover anypossible agentic systems, including all components such as prompts,\ntool use, control flows, and more. Furthermore, with recent FMs being increasingly proficient in\ncoding, we can use FMs as a meta agent to create new agents in code for ADAS, enabling novel agents\nto be programmed in an automated manner.\nFollowing the aforementioned ideas, we present Meta Agent Search in this paper as one of the first\nalgorithms in ADAS that enables complete design in code space (Figure 1). The core concept of Meta\nAgent Search is to instruct a meta agent to iteratively create interestingly new agents, evaluate them,\nadd them to an archive that stores discovered agents, and use this archive to help the meta agent in\nsubsequent iterations create yet more interestingly new agents. Similar to existing open-endedness\nalgorithms that leverage human notions of interestingness (Lu et al., 2024c; Zhang et al., 2024a),\nwe encourage the meta agent to explore interesting (e.g., novel or worthwhile) agents. To validate\nthe proposed approach, we evaluate the proposed Meta Agent Search on: (1) the challenging ARC\nlogic puzzle task (Chollet, 2019) that aims to test the general intelligence of an AI system, (2) four\npopular benchmarks on reading comprehension, math, science questions, and multi-task problem\nsolving, and (3) the transferability of discovered agents to held-out domains and models (Section 4).\nOur experiments show that the discovered agents substantially outperform state-of-the-art hand-\ndesigned baselines. For instance, our agents improve F1 scores on reading comprehension tasks in\nDROP (Dua et al., 2019) by 13.6\/100 and accuracy rates on math tasks in MGSM (Shi et al., 2023) by\n14.4%. Additionally, they improve accuracy over baselines by 25.9%and13.2%on GSM8K (Cobbe\net al., 2021) and GSM-Hard (Gao et al., 2023) math tasks, respectively, after transferring across\ndomains. The promising performance of our algorithm over hand-designed solutions illustrates\nthe potential of ADAS in automating the design of agentic systems. Furthermore, the experiments\ndemonstrate that the discovered agents not only perform well when transferring across similar\ndomains but also exhibit strong performance when transferring across dissimilar domains, such as\nfrom mathematics to reading comprehension. This highlights the robustness and transferability of the\nag","chunk_id":"81c504ffbcc5ed882e234802135295ba","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"EXAMPLE AGENTS","type":"CONCEPT","description":"Example agents refer to the specific implementations or instances of agents discussed in the context of automated design and AI systems.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"CNNS","type":"TECHNOLOGY","description":"Convolutional Neural Networks (CNNs) are a class of deep learning algorithms primarily used for image processing and recognition tasks.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"AUTOML METHODS","type":"TECHNOLOGY","description":"AutoML methods are automated machine learning techniques that optimize the process of applying machine learning to real-world problems.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"AI-GENERATING ALGORITHMS","type":"TECHNOLOGY","description":"AI-Generating Algorithms (AI-GAs) are algorithms designed to create AI systems automatically, demonstrating superior performance compared to traditional methods.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"NEURAL ARCHITECTURE SEARCH","type":"TECHNIQUE","description":"Neural Architecture Search is a method for automating the design of neural networks, allowing for the discovery of optimal architectures.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"LLM ALIGNMENT","type":"CONCEPT","description":"LLM alignment refers to the process of ensuring that large language models (LLMs) behave in accordance with human intentions and values.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"LEARNED LOSS FUNCTIONS","type":"TECHNIQUE","description":"Learned loss functions are loss functions that are optimized through learning processes, often outperforming traditional hand-designed loss functions.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"DPO","type":"TECHNIQUE","description":"DPO (Differentiable Programming Optimization) is a hand-designed loss function used in training AI models.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"AI SCIENTIST","type":"CONCEPT","description":"The AI Scientist is an automated research pipeline that develops novel machine learning algorithms and conducts experiments.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"OMNI-EPIC","type":"TECHNOLOGY","description":"OMNI-EPIC is a framework for generating diverse robotics learning environments automatically, showcasing creativity and efficiency.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)","type":"RESEARCH AREA","description":"ADAS is a proposed research area focused on the automated invention of novel building blocks and design of powerful agentic systems.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"META AGENT SEARCH","type":"ALGORITHM","description":"Meta Agent Search is an algorithm within ADAS that enables the design of agents in code space through iterative creation and evaluation.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"F1 SCORES","type":"METRIC","description":"F1 scores are a measure of a model's accuracy, balancing precision and recall, particularly in classification tasks.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"DROP","type":"DATASET","description":"DROP is a dataset used for evaluating reading comprehension tasks, particularly in logic puzzles.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"MGSM","type":"DATASET","description":"MGSM is a dataset used for evaluating math tasks in AI systems.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"GSM8K","type":"DATASET","description":"GSM8K is a dataset used for assessing the performance of AI systems on math tasks.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"GSM-HARD","type":"DATASET","description":"GSM-HARD is a dataset that presents challenging math problems for AI systems to solve.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"CLUNE (2019)","type":"PERSON","description":"Clune (2019) is a reference to a study that highlights the advantages of AI-Generating Algorithms over traditional AI systems.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"ELSEN ET AL. (2019)","type":"PERSON","description":"Elsken et al. (2019) is a reference to a study that discusses the application of Neural Architecture Search in developing CNN models.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"SHEN ET AL. (2023)","type":"PERSON","description":"Shen et al. (2023) is a reference to a study that contributes to the understanding of Neural Architecture Search in CNNs.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"LU ET AL. (2024A)","type":"PERSON","description":"Lu et al. (2024a) is a reference to a study that compares learned loss functions with traditional loss functions in LLM alignment.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"RAFALIOV ET AL. (2024)","type":"PERSON","description":"Rafailov et al. (2024) is a reference to a study that discusses the DPO loss function in AI training.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"LU ET AL. (2024B)","type":"PERSON","description":"Lu et al. (2024b) is a reference to a study that presents the AI Scientist and its automated research capabilities.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"FALDOR ET AL. (2024)","type":"PERSON","description":"Faldor et al. (2024) is a reference to a study that discusses the OMNI-EPIC framework for generating robotics learning environments.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"FERNANDO ET AL. (2024)","type":"PERSON","description":"Fernando et al. (2024) is a reference to a study that explores existing methods in ADAS focused on designing prompts.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"YANG ET AL. (2024)","type":"PERSON","description":"Yang et al. (2024) is a reference to a study that examines limitations in current ADAS methods related to prompt design.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"BOYER & MOORE (1983)","type":"PERSON","description":"Boyer & Moore (1983) is a reference to a study discussing the Turing completeness of programming languages.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"LADHA (2024)","type":"PERSON","description":"Ladha (2024) is a reference to a study that discusses the implications of Turing completeness in programming languages.","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"HUTTER ET AL. (2019)","type":"","description":"","source_id":"81c504ffbcc5ed882e234802135295ba"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"EXAMPLE AGENTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Example agents refer to the specific implementations or instances of agents discussed in the context of automated design and AI systems.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"CNNS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Convolutional Neural Networks (CNNs) are a class of deep learning algorithms primarily used for image processing and recognition tasks.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"AUTOML METHODS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AutoML methods are automated machine learning techniques that optimize the process of applying machine learning to real-world problems.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"AI-GENERATING ALGORITHMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AI-Generating Algorithms (AI-GAs) are algorithms designed to create AI systems automatically, demonstrating superior performance compared to traditional methods.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Neural Architecture Search is a method for automating the design of neural networks, allowing for the discovery of optimal architectures.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"LLM ALIGNMENT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">LLM alignment refers to the process of ensuring that large language models (LLMs) behave in accordance with human intentions and values.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"LEARNED LOSS FUNCTIONS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Learned loss functions are loss functions that are optimized through learning processes, often outperforming traditional hand-designed loss functions.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"DPO\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">DPO (Differentiable Programming Optimization) is a hand-designed loss function used in training AI models.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"AI SCIENTIST\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The AI Scientist is an automated research pipeline that develops novel machine learning algorithms and conducts experiments.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"OMNI-EPIC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">OMNI-EPIC is a framework for generating diverse robotics learning environments automatically, showcasing creativity and efficiency.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\">      <data key=\"d0\">RESEARCH AREA<\/data>      <data key=\"d1\">ADAS is a proposed research area focused on the automated invention of novel building blocks and design of powerful agentic systems.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">Meta Agent Search is an algorithm within ADAS that enables the design of agents in code space through iterative creation and evaluation.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"F1 SCORES\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">F1 scores are a measure of a model's accuracy, balancing precision and recall, particularly in classification tasks.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">DROP is a dataset used for evaluating reading comprehension tasks, particularly in logic puzzles.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"MGSM\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MGSM is a dataset used for evaluating math tasks in AI systems.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">GSM8K is a dataset used for assessing the performance of AI systems on math tasks.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"GSM-HARD\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">GSM-HARD is a dataset that presents challenging math problems for AI systems to solve.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"CLUNE (2019)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clune (2019) is a reference to a study that highlights the advantages of AI-Generating Algorithms over traditional AI systems.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"ELSEN ET AL. (2019)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Elsken et al. (2019) is a reference to a study that discusses the application of Neural Architecture Search in developing CNN models.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"SHEN ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shen et al. (2023) is a reference to a study that contributes to the understanding of Neural Architecture Search in CNNs.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"LU ET AL. (2024A)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lu et al. (2024a) is a reference to a study that compares learned loss functions with traditional loss functions in LLM alignment.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"RAFALIOV ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rafailov et al. (2024) is a reference to a study that discusses the DPO loss function in AI training.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"LU ET AL. (2024B)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lu et al. (2024b) is a reference to a study that presents the AI Scientist and its automated research capabilities.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"FALDOR ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Faldor et al. (2024) is a reference to a study that discusses the OMNI-EPIC framework for generating robotics learning environments.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"FERNANDO ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fernando et al. (2024) is a reference to a study that explores existing methods in ADAS focused on designing prompts.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"YANG ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang et al. (2024) is a reference to a study that examines limitations in current ADAS methods related to prompt design.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"BOYER &amp; MOORE (1983)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Boyer &amp; Moore (1983) is a reference to a study discussing the Turing completeness of programming languages.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"LADHA (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ladha (2024) is a reference to a study that discusses the implications of Turing completeness in programming languages.<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"HUTTER ET AL. (2019)\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <edge source=\"EXAMPLE AGENTS\" target=\"META AGENT SEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search is responsible for creating new example agents through its iterative design process.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"CNNS\" target=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">CNNs are often optimized through Neural Architecture Search methods to achieve better performance.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"AUTOML METHODS\" target=\"AI-GENERATING ALGORITHMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AI-GAs are a subset of AutoML methods that focus on generating AI systems automatically.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"AUTOML METHODS\" target=\"HUTTER ET AL. (2019)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Hutter et al. (2019) provides foundational insights into the development and effectiveness of AutoML methods.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"AI-GENERATING ALGORITHMS\" target=\"CLUNE (2019)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Clune (2019) discusses the advantages of AI-Generating Algorithms, contributing to the understanding of their superiority over traditional methods.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"NEURAL ARCHITECTURE SEARCH\" target=\"ELSEN ET AL. (2019)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Elsken et al. (2019) provides insights into the application of Neural Architecture Search in optimizing CNN models.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"NEURAL ARCHITECTURE SEARCH\" target=\"SHEN ET AL. (2023)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Shen et al. (2023) contributes to the understanding of Neural Architecture Search techniques in CNN development.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"LEARNED LOSS FUNCTIONS\" target=\"DPO\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Learned loss functions are compared against DPO to demonstrate their superior performance in training AI models.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"LEARNED LOSS FUNCTIONS\" target=\"LU ET AL. (2024A)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Lu et al. (2024a) compares learned loss functions with traditional methods, highlighting their effectiveness in LLM alignment.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"DPO\" target=\"RAFALIOV ET AL. (2024)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Rafailov et al. (2024) discusses the DPO loss function, providing context for its use in AI training.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"AI SCIENTIST\" target=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The AI Scientist exemplifies the goals of ADAS by automating the development of machine learning algorithms.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"AI SCIENTIST\" target=\"LU ET AL. (2024B)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Lu et al. (2024b) presents the AI Scientist, illustrating its role in automated research and algorithm development.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"OMNI-EPIC\" target=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">OMNI-EPIC showcases the potential of ADAS by automatically generating diverse learning environments for robotics.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"OMNI-EPIC\" target=\"FALDOR ET AL. (2024)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Faldor et al. (2024) discusses the OMNI-EPIC framework, showcasing its capabilities in generating robotics learning environments.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"META AGENT SEARCH\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">ADAS encompasses the Meta Agent Search algorithm, which is designed to automate the creation of agentic systems.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"FERNANDO ET AL. (2024)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Fernando et al. (2024) explores existing methods in ADAS, focusing on prompt design limitations.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"YANG ET AL. (2024)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Yang et al. (2024) examines the constraints of current ADAS methods, particularly in designing prompts.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"BOYER &amp; MOORE (1983)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Boyer &amp; Moore (1983) discusses Turing completeness, relevant to the capabilities of the Meta Agent Search algorithm.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"LADHA (2024)\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Ladha (2024) provides insights into Turing completeness, which is significant for understanding the Meta Agent Search algorithm's potential.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"F1 SCORES\" target=\"DROP\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">F1 scores are used to evaluate the performance of agents on the DROP dataset for reading comprehension tasks.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"F1 SCORES\" target=\"MGSM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">F1 scores are also used to assess the performance of agents on the MGSM dataset for math tasks.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"F1 SCORES\" target=\"GSM8K\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">F1 scores are utilized to measure the effectiveness of agents on the GSM8K dataset for math tasks.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"F1 SCORES\" target=\"GSM-HARD\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">F1 scores are applied to evaluate agents on the GSM-HARD dataset, which presents challenging math problems.<\/data>      <data key=\"d5\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"4884e8429ca1e567dadf5e22b4b68274","chunk":" et al., 2023) math tasks, respectively, after transferring across\ndomains. The promising performance of our algorithm over hand-designed solutions illustrates\nthe potential of ADAS in automating the design of agentic systems. Furthermore, the experiments\ndemonstrate that the discovered agents not only perform well when transferring across similar\ndomains but also exhibit strong performance when transferring across dissimilar domains, such as\nfrom mathematics to reading comprehension. This highlights the robustness and transferability of the\nagentic systems discovered by Meta Agent Search. In conclusion, our work opens up many exciting\nresearch directions and encourages further studies (Section 6).\n2. New Research Area: Automated Design of Agentic Systems (ADAS)\nAt the time of writing, the community has not reached a consensus on the definitions or terminologies\nof agents. Here, by agents we refer to agentic systems that involve Foundation Models (FMs) as\nmodules in the control flow to solve tasks by planning, using tools, and carrying out multiple, iterative\nsteps of processing (Chase, 2024; Ng, 2024).\nIn this paper, we propose a new research area Automated Design of Agentic Systems (ADAS).\nSimilar to research areas in AI-GAs (Clune, 2019) and AutoML (Hutter et al., 2019), such as Neural\nArchitecture Search (Elsken et al., 2019), we formulate ADAS as an optimization process and identify\nthree key components of ADAS algorithms (Figure 2).\n3Automated Design of Agentic Systems\nSearch Space\nE.g. Agents defined by code\nSearch Algorithm\nE.g. LLM defines agents using code\nEvaluation Function\nE.g. Accuracy on the taskWhere is the capital of CanadaOttawa\n\u2705SampleNew AgentEvaluate the ObjectivesAgent\u2026\u20261 + 1 = ?\nFigure 2|The three key components of Automated Design of Agentic Systems (ADAS). The search\nspace determines which agentic systems can be represented in ADAS. The search algorithm specifies\nhow the ADAS method explores the search space. The evaluation function defines how to evaluate a\ncandidate agent on target objectives such as performance.\nFormulation\nAutomated Design of Agentic Systems (ADAS) involves using a search algorithm to discover\nagentic systems across a search space thatoptimize anevaluation function .\n\u2022Search Space : The search space defines which agentic systems can be represented and thus\ndiscovered in ADAS. For example, works like PromptBreeder (Fernando et al., 2024) mutate only\nthe text prompts of an agent, but their other components, such as control flow, remain the same.\nThus, in these search spaces, agents that have a different control flow than the predefined one can\nnot be represented. Existing works also explore search spaces such as graph structures (Zhuge\net al., 2024) and feed-forward networks (Liu et al., 2023).\n\u2022Search Algorithm : The search algorithm defines how ADAS algorithms explore the search space.\nSince the search space is often very large or even unbounded, the exploration-exploitation trade-\noff (Sutton & Barto, 2018) should be considered. Ideally, the algorithm can both quickly discover\nhigh-performance agentic systems and avoid remaining stuck in a local optimum. Existing ap-\nproachesincludeusingReinforcementLearning(Zhugeetal.,2024)oranFMiterativelygenerating\nnew solutions (Fernando et al., 2024) as search algorithms.\n\u2022Evaluation Function : Depending on the application of the ADAS algorithm, we may consider\ndifferentobjectivestooptimize,suchasperformance,cost,latency,orsafetyofagents. Anevaluation\nfunction defines how to evaluate a candidate agent on those objectives. For example, to assess\nthe agent\u2019s performance on unseen future data, a simple method is to calculate the accuracy rate\non the validation data for a task, which is commonly adopted in existing works (Fernando et al.,\n2024; Zhuge et al., 2024).\nAlthoughmanysearchspacedesignsarepossibleandsomehavealreadybeenexplored(Section5),\nthere is an unexplored yet promising approach where we can define the entire agentic system in\ncode and new agents can be automatically discovered by a meta agent programming even better\nones in code. Searching within a code space theoretically enables the ADAS algorithm to discover\nanypossible building blocks (e.g., prompts, tool use, control flow) and agentic systems that combine\nany of these building blocks in any way. This approach also offers better interpretability for agent\ndesign patterns since the program code is often readable, making debugging easier and enhancing AI\nsafety. Additionally, compared to search spaces using networks (Liu et al., 2023) or graphs (Zhuge\net al., 2024), searching in a code space allows us to more easily build on existing human efforts. For\nexample, it is possible to search within open-source agent frameworks like LangChain (LangChainAI,\n2022) and build upon all existing building blocks (e.g., RAG, search engine tools). Finally, since FMs\n4Automated Design of Agentic Systems\nare proficient in coding, utilizing a code search space allows us to leverage existing expertise from\nFMs during the search process. In contrast, search algorithms in custom search spaces, such as graphs,\nmay be much less efficient due to the absence of these priors. Therefore, we argue that the approach\nof using programming languages as the search space should be studied more in ADAS.\n3. Our Algorithm: Meta Agent Search\nIn this section, we present Meta Agent Search, a simple yet effective algorithm to demonstrate the\napproach of defining and searching for agents","chunk_id":"4884e8429ca1e567dadf5e22b4b68274","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)","type":"RESEARCH AREA","description":"ADAS is a proposed research area focused on the automated design of agentic systems that utilize Foundation Models (FMs) for task-solving through planning and iterative processing.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"FOUNDATION MODELS (FMS)","type":"TECHNOLOGY","description":"Foundation Models are large-scale models that serve as modules in the control flow of agentic systems, enabling them to perform complex tasks.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"META AGENT SEARCH","type":"ALGORITHM","description":"Meta Agent Search is an algorithm designed to define and discover agentic systems through a search process.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"SEARCH SPACE","type":"CONCEPT","description":"The search space defines the range of agentic systems that can be represented and discovered in ADAS, including various structures and components.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"SEARCH ALGORITHM","type":"CONCEPT","description":"The search algorithm refers to the method used to explore the search space in ADAS, aiming to discover high-performance agentic systems.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"EVALUATION FUNCTION","type":"CONCEPT","description":"The evaluation function is a criterion used to assess the performance of candidate agents based on specific objectives such as accuracy or cost.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"PROMPTBREEDER","type":"TECHNOLOGY","description":"PromptBreeder is a tool that mutates text prompts of agents while keeping other components, like control flow, unchanged.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"LANGCHAIN","type":"TECHNOLOGY","description":"LangChain is an open-source framework that facilitates the building of agentic systems by providing existing building blocks for development.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"ZHUKE ET AL. (2024)","type":"PERSON","description":"Zhuge et al. (2024) is a reference to a study discussing search algorithms and their application in exploring search spaces for agentic systems.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"LIU ET AL. (2023)","type":"PERSON","description":"Liu et al. (2023) is a reference to a study discussing search spaces and their designs in the context of agentic systems.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"CHASE (2024)","type":"PERSON","description":"Chase (2024) is a reference to a work that contributes to the understanding of agentic systems and their definitions.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"NG (2024)","type":"PERSON","description":"Ng (2024) is a reference to a work that contributes to the understanding of agentic systems and their definitions.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"OPTIMIZATION PROCESS","type":"CONCEPT","description":"An optimization process is a systematic approach to finding the best solution or outcome among various alternatives, often used in algorithm design.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"ITERATIVE PROCESSING","type":"CONCEPT","description":"Iterative processing involves repeatedly applying a set of operations or algorithms to refine results or improve performance over time.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"EXPLORATION-EXPLOITATION TRADE-OFF","type":"CONCEPT","description":"The exploration-exploitation trade-off is a dilemma in decision-making where one must balance between exploring new options and exploiting known ones for optimal outcomes.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"ACCURACY","type":"METRIC","description":"Accuracy is a metric used to evaluate the performance of models or agents, indicating the proportion of correct predictions made.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"COST","type":"METRIC","description":"Cost refers to the resources required to implement or operate a system, often considered in the evaluation of agentic systems.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"LATENCY","type":"METRIC","description":"Latency is the time delay between the initiation of a process and its completion, an important factor in assessing the performance of agentic systems.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"SAFETY","type":"METRIC","description":"Safety refers to the measures taken to ensure that systems operate without causing harm, a critical consideration in the design of agentic systems.","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"AGENTIC SYSTEMS","type":"","description":"","source_id":"4884e8429ca1e567dadf5e22b4b68274"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\">      <data key=\"d0\">RESEARCH AREA<\/data>      <data key=\"d1\">ADAS is a proposed research area focused on the automated design of agentic systems that utilize Foundation Models (FMs) for task-solving through planning and iterative processing.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"FOUNDATION MODELS (FMS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundation Models are large-scale models that serve as modules in the control flow of agentic systems, enabling them to perform complex tasks.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">Meta Agent Search is an algorithm designed to define and discover agentic systems through a search process.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"SEARCH SPACE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The search space defines the range of agentic systems that can be represented and discovered in ADAS, including various structures and components.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"SEARCH ALGORITHM\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The search algorithm refers to the method used to explore the search space in ADAS, aiming to discover high-performance agentic systems.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"EVALUATION FUNCTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The evaluation function is a criterion used to assess the performance of candidate agents based on specific objectives such as accuracy or cost.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"PROMPTBREEDER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">PromptBreeder is a tool that mutates text prompts of agents while keeping other components, like control flow, unchanged.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"LANGCHAIN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LangChain is an open-source framework that facilitates the building of agentic systems by providing existing building blocks for development.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"ZHUKE ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhuge et al. (2024) is a reference to a study discussing search algorithms and their application in exploring search spaces for agentic systems.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"LIU ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu et al. (2023) is a reference to a study discussing search spaces and their designs in the context of agentic systems.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"CHASE (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chase (2024) is a reference to a work that contributes to the understanding of agentic systems and their definitions.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"NG (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ng (2024) is a reference to a work that contributes to the understanding of agentic systems and their definitions.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"OPTIMIZATION PROCESS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">An optimization process is a systematic approach to finding the best solution or outcome among various alternatives, often used in algorithm design.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"ITERATIVE PROCESSING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Iterative processing involves repeatedly applying a set of operations or algorithms to refine results or improve performance over time.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"EXPLORATION-EXPLOITATION TRADE-OFF\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The exploration-exploitation trade-off is a dilemma in decision-making where one must balance between exploring new options and exploiting known ones for optimal outcomes.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"ACCURACY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Accuracy is a metric used to evaluate the performance of models or agents, indicating the proportion of correct predictions made.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"COST\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Cost refers to the resources required to implement or operate a system, often considered in the evaluation of agentic systems.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"LATENCY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Latency is the time delay between the initiation of a process and its completion, an important factor in assessing the performance of agentic systems.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"SAFETY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Safety refers to the measures taken to ensure that systems operate without causing harm, a critical consideration in the design of agentic systems.<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"AGENTIC SYSTEMS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"FOUNDATION MODELS (FMS)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">ADAS involves the use of Foundation Models as modules in the control flow for solving tasks.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"META AGENT SEARCH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search is an algorithm that exemplifies the principles of ADAS by discovering agentic systems.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"SEARCH SPACE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The search space is a fundamental component of ADAS, defining the agentic systems that can be represented.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"SEARCH ALGORITHM\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The search algorithm is crucial for exploring the search space in ADAS to find optimal agentic systems.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"EVALUATION FUNCTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The evaluation function is used in ADAS to assess the performance of candidate agents based on defined objectives.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"CHASE (2024)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Chase (2024) contributes to the foundational understanding of agentic systems within the context of ADAS.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"NG (2024)\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Ng (2024) contributes to the foundational understanding of agentic systems within the context of ADAS.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"AGENTIC SYSTEMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Agentic systems are the focus of the research area ADAS, which aims to automate their design and optimization.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"OPTIMIZATION PROCESS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The optimization process is a key component in the development of ADAS algorithms to enhance agentic systems.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"FOUNDATION MODELS (FMS)\" target=\"ITERATIVE PROCESSING\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Foundation Models often utilize iterative processing to refine their outputs and improve task performance.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"SEARCH SPACE\" target=\"PROMPTBREEDER\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">PromptBreeder operates within a specific search space by mutating text prompts while maintaining other components.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"SEARCH SPACE\" target=\"LANGCHAIN\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LangChain provides a framework that allows for the exploration of search spaces in building agentic systems.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"SEARCH SPACE\" target=\"LIU ET AL. (2023)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Liu et al. (2023) explores various designs of search spaces applicable to agentic systems.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"SEARCH ALGORITHM\" target=\"ZHUKE ET AL. (2024)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Zhuge et al. (2024) discusses search algorithms relevant to the exploration of search spaces in ADAS.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"SEARCH ALGORITHM\" target=\"EXPLORATION-EXPLOITATION TRADE-OFF\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The exploration-exploitation trade-off is a critical consideration in the design of search algorithms for ADAS.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"EVALUATION FUNCTION\" target=\"ACCURACY\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Accuracy is one of the key metrics used in the evaluation function to assess the performance of agents in ADAS.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"EVALUATION FUNCTION\" target=\"COST\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Cost is another important metric considered in the evaluation function for optimizing agentic systems.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"EVALUATION FUNCTION\" target=\"LATENCY\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Latency is a metric that can be included in the evaluation function to assess the efficiency of agentic systems.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"EVALUATION FUNCTION\" target=\"SAFETY\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Safety is a crucial metric in the evaluation function to ensure that agentic systems operate without causing harm.<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"24d7b89ae9522ae60d2317984951355b","chunk":" leverage existing expertise from\nFMs during the search process. In contrast, search algorithms in custom search spaces, such as graphs,\nmay be much less efficient due to the absence of these priors. Therefore, we argue that the approach\nof using programming languages as the search space should be studied more in ADAS.\n3. Our Algorithm: Meta Agent Search\nIn this section, we present Meta Agent Search, a simple yet effective algorithm to demonstrate the\napproach of defining and searching for agents in code. The core idea of Meta Agent Search is to adopt\nFMs as meta agents to iteratively program interestingly new agents based on an ever-growing archive\nof previous discoveries. Although any possible building blocks and agentic systems can theoretically\nbe programmed by the meta agent from scratch, it is inefficient in practice to avoid providing the\nmeta agent any basic functions such as FM query APIs or existing tools. Therefore, in this paper, we\ndefine a simple framework (within 100 lines of code) for the meta agent, providing it with a basic\nset of essential functions like querying FMs or formatting prompts. As a result, the meta agent only\nneeds to program a \u201cforward\u201d function to define a new agentic system, similar to the practice in\nFunSearch (Romera-Paredes et al., 2024). This function takes in the information of the task and\noutputs the agent\u2019s response to the task. Details of the framework codes and examples of the agents\ndefined with this framework can be found in Appendix B.\nAs shown in Figure 1, the core idea of Meta Agent Search is to have a meta agent iteratively\nprogram new agents in code. We show the main prompt for the meta agent to program new agents\nbelow, where variables in the prompts are highlighted. Similar to existing open-endedness algorithms\nthat leverage human notions of interestingness (Lu et al., 2024c; Zhang et al., 2024a), we encourage\nthe meta agent to explore interestingly new (e.g., novel or worthwhile) agents based on an ever-\ngrowing archive of previous discoveries. We also adopt self-reflection (Madaan et al., 2024; Shinn\net al., 2023) iterations in our meta agent, where it performs two iterations of refinement on the\nnovelty and correctness of the proposal and performs up to three refinements when errors occur\nwhile running the code. Full details of the prompt are presented in Appendix A.\nAfter a new agent is generated, we evaluate it using the validation data from the target domain.\nHere, we calculate the performance (e.g., success rate or F1 score) and 95% bootstrap confidence\ninterval as the metrics for the meta agent to maximize. The generated agent is then added to the\narchive with the evaluation metrics, and the iteration continues with the updated archive until the\nmaximum number of iterations is reached.\nMain prompt for the meta agent.\nYou are an expert machine learning researcher testing different agentic systems.\n[BriefDescriptionoftheDomain]\n[FrameworkCode]\n[OutputInstructionsandExamples]\n[DiscoveredAgentArchive] (initializedwithbaselines,updatedateveryiteration)\n# Your task\nYou are deeply familiar with prompting techniques and the agent works from the literature. Your goal is\nto maximize the performance by proposing interestingly new agents ......\nUse the knowledge from the archive and inspiration from academic literature to propose the next\ninteresting agentic system design.\n5Automated Design of Agentic Systems\n4. Experiments\nWe conduct extensive experiments on: (1) the challenging ARC logic puzzle task (Chollet, 2019)\n(Section 4.1), (2) four popular benchmarks assessing the agent\u2019s abilities on reading comprehension,\nmath, science questions, and multi-task problem solving (Section 4.2), (3) the transferability of\nthe discovered agents on ARC to three held-out models, and (4) the transferability of discovered\nagents on Math to four held-out math tasks and three tasks that are beyond math (Section 4.3).\nAcross all experiments, we find that the discovered agents substantially outperform baseline state-\nof-the-art hand-designed agents. Notably, our discovered agents improve over baselines on reading\ncomprehension tasks in DROP (Dua et al., 2019) by 13.6\/100 (F1 score) and on math tasks in\nMGSM (Shi et al., 2023) by 14.4%(accuracy rate). Additionally, our discovered agents improve over\nthe baseline on ARC tasks by 14%(accuracy rate) after transferring from GPT-3.5 to GPT-4, and by\n25.9%and13.2%(accuracy rate) after transferring from MGSM math tasks to held-out math tasks in\nGSM8K (Cobbe et al., 2021) and GSM-Hard (Gao et al., 2023) respectively. All code, prompts, and\nexperiment results are available at https:\/\/github.com\/ShengranHu\/ADAS .\n0 5 10 15 20 25\nIteration468101214Held-out T est Accuracy (%)\nInitially tested generating high-level strategies\nbefore implementing low-level details.An important strategy emerged: using multiple COT s\nto generate possible answers, refining them, and\nfinally ensembling the best answers.Introduced dynamic memory for doing more refinements.Scaled up the previous idea.Best agent: introduced multiple\ncritics for enhanced refinement.Meta-Agent Search on ARC\nChain-of-Thought\nSelf-Refine\nLLM DebateCOT-SC\nQuality-Diversity\nMeta-Agent Search\n(a)\nTask5 COTs\n5 Answers\nHuman -like \nCritic\nFeedbackEfficiency Expert\nReadability Expert\nSimplicity","chunk_id":"24d7b89ae9522ae60d2317984951355b","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"META AGENT SEARCH","type":"ALGORITHM","description":"Meta Agent Search is an algorithm designed to define and search for agents in code, utilizing foundational models (FMs) as meta agents to iteratively create new agents based on previous discoveries.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"FOUNDATIONAL MODELS (FMS)","type":"TECHNOLOGY","description":"Foundational models (FMs) are large-scale machine learning models that serve as the basis for creating new agents in the Meta Agent Search algorithm.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"AGENTIC SYSTEMS","type":"SYSTEM","description":"Agentic systems are computational systems designed to perform tasks autonomously, which can be programmed by the meta agent in the Meta Agent Search framework.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"ARCHIVE","type":"DATA STRUCTURE","description":"The archive is a collection of previously discovered agents and their evaluation metrics, used to inform the meta agent's future proposals.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"PROMPTING TECHNIQUES","type":"TECHNIQUE","description":"Prompting techniques are methods used to instruct the meta agent on how to generate new agents based on the information from the archive and academic literature.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"ARC LOGIC PUZZLE TASK","type":"TASK","description":"The ARC logic puzzle task is a benchmark used to evaluate the performance of agents in logical reasoning and problem-solving.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"DROP","type":"DATASET","description":"DROP is a dataset used for evaluating reading comprehension tasks, providing a benchmark for assessing the performance of agents.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"MGSM","type":"DATASET","description":"MGSM is a dataset used for evaluating math tasks, serving as a benchmark for assessing the performance of agents in mathematical problem-solving.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a version of the Generative Pre-trained Transformer model developed by OpenAI, used as a baseline for transferring knowledge to newer models.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is an advanced version of the Generative Pre-trained Transformer model developed by OpenAI, known for its improved capabilities over previous versions.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"GSM8K","type":"DATASET","description":"GSM8K is a dataset used for evaluating math tasks, providing a benchmark for assessing the performance of agents in mathematical problem-solving.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"GSM-HARD","type":"DATASET","description":"GSM-HARD is a dataset used for evaluating challenging math tasks, serving as a benchmark for assessing the performance of agents.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"SELF-REFLECTION","type":"TECHNIQUE","description":"Self-reflection is a technique used in the meta agent to iteratively refine the novelty and correctness of generated agents.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"ITERATIONS","type":"PROCESS","description":"Iterations refer to the repeated cycles in the Meta Agent Search process where the meta agent refines its proposals based on previous evaluations.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"EVALUATION METRICS","type":"DATA","description":"Evaluation metrics are quantitative measures, such as success rate or F1 score, used to assess the performance of the generated agents.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"VALIDATION DATA","type":"DATA","description":"Validation data is the dataset used to evaluate the performance of newly generated agents in the target domain.","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"FUNSEARCH","type":"","description":"","source_id":"24d7b89ae9522ae60d2317984951355b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">Meta Agent Search is an algorithm designed to define and search for agents in code, utilizing foundational models (FMs) as meta agents to iteratively create new agents based on previous discoveries.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"FOUNDATIONAL MODELS (FMS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundational models (FMs) are large-scale machine learning models that serve as the basis for creating new agents in the Meta Agent Search algorithm.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"AGENTIC SYSTEMS\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">Agentic systems are computational systems designed to perform tasks autonomously, which can be programmed by the meta agent in the Meta Agent Search framework.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"ARCHIVE\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">The archive is a collection of previously discovered agents and their evaluation metrics, used to inform the meta agent's future proposals.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"PROMPTING TECHNIQUES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Prompting techniques are methods used to instruct the meta agent on how to generate new agents based on the information from the archive and academic literature.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"ARC LOGIC PUZZLE TASK\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">The ARC logic puzzle task is a benchmark used to evaluate the performance of agents in logical reasoning and problem-solving.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">DROP is a dataset used for evaluating reading comprehension tasks, providing a benchmark for assessing the performance of agents.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"MGSM\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MGSM is a dataset used for evaluating math tasks, serving as a benchmark for assessing the performance of agents in mathematical problem-solving.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a version of the Generative Pre-trained Transformer model developed by OpenAI, used as a baseline for transferring knowledge to newer models.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is an advanced version of the Generative Pre-trained Transformer model developed by OpenAI, known for its improved capabilities over previous versions.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">GSM8K is a dataset used for evaluating math tasks, providing a benchmark for assessing the performance of agents in mathematical problem-solving.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"GSM-HARD\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">GSM-HARD is a dataset used for evaluating challenging math tasks, serving as a benchmark for assessing the performance of agents.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"SELF-REFLECTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-reflection is a technique used in the meta agent to iteratively refine the novelty and correctness of generated agents.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"ITERATIONS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Iterations refer to the repeated cycles in the Meta Agent Search process where the meta agent refines its proposals based on previous evaluations.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"EVALUATION METRICS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Evaluation metrics are quantitative measures, such as success rate or F1 score, used to assess the performance of the generated agents.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"VALIDATION DATA\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Validation data is the dataset used to evaluate the performance of newly generated agents in the target domain.<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"FUNSEARCH\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <edge source=\"META AGENT SEARCH\" target=\"FOUNDATIONAL MODELS (FMS)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search utilizes foundational models (FMs) as meta agents to create new agents based on previous discoveries.<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"AGENTIC SYSTEMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search is designed to define and search for agentic systems, programming them iteratively.<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ARCHIVE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The archive is used in Meta Agent Search to store previously discovered agents and their evaluation metrics, informing future proposals.<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"PROMPTING TECHNIQUES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Prompting techniques are employed in Meta Agent Search to guide the meta agent in generating new agents.<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ARC LOGIC PUZZLE TASK\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Meta Agent Search is evaluated using the ARC logic puzzle task to assess the performance of the generated agents.<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"DROP\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The performance of agents generated by Meta Agent Search is evaluated using the DROP dataset for reading comprehension tasks.<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MGSM\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The performance of agents generated by Meta Agent Search is evaluated using the MGSM dataset for math tasks.<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"FUNSEARCH\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Meta Agent Search references FunSearch as a framework for defining new agents based on tasks.<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SELF-REFLECTION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Self-reflection is integrated into the Meta Agent Search process to enhance the quality of generated agents through iterative refinement.<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ITERATIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Iterations are a fundamental part of the Meta Agent Search process, allowing for continuous improvement of agent proposals.<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"EVALUATION METRICS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Evaluation metrics are used in the Meta Agent Search to measure the performance of generated agents during the evaluation phase.<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"VALIDATION DATA\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Validation data is utilized in the Meta Agent Search to assess the effectiveness of newly generated agents in the target domain.<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"GPT-3.5\" target=\"GPT-4\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Knowledge and performance from GPT-3.5 are transferred to GPT-4, improving the capabilities of the agents generated by Meta Agent Search.<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"GSM8K\" target=\"GSM-HARD\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Both GSM8K and GSM-HARD are datasets used to evaluate the performance of agents in math tasks, providing benchmarks for comparison.<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"1a6353c9d196dc2debad7c27c902bcd7","chunk":" refining them, and\nfinally ensembling the best answers.Introduced dynamic memory for doing more refinements.Scaled up the previous idea.Best agent: introduced multiple\ncritics for enhanced refinement.Meta-Agent Search on ARC\nChain-of-Thought\nSelf-Refine\nLLM DebateCOT-SC\nQuality-Diversity\nMeta-Agent Search\n(a)\nTask5 COTs\n5 Answers\nHuman -like \nCritic\nFeedbackEfficiency Expert\nReadability Expert\nSimplicity ExpertExperts\nFeedback\nRefinement\n3 timesAll \nAnswers\nEvaluateTop-3 \nAnswersEnsembleFinal \nAnswer\nStructured Feedback and Ensemble AgentThe Best Discovered Agent on ARC (b)\nFigure 3|The results of Meta Agent Search on the ARC challenge. (a) Meta Agent Search progres-\nsively discovers high-performance agents based on an ever-growing archive of previous discoveries.\nWe report the median accuracy and the 95% bootstrap confidence interval on a held-out test set by\nevaluating agents five times. (b) The visualization of the best agent discovered by Meta Agent Search\non the ARC challenge. Detailed implementation of this agent is available in Appendix C.\n4.1. Case Study: ARC Challenge\nWefirstdemonstratehowMetaAgentSearchdiscoversnovelagenticsystemsandoutperformsexisting\nstate-of-the-art hand-designed agents in the Abstraction and Reasoning Corpus (ARC) challenge (Chol-\nlet, 2019). This challenge aims to evaluate the general intelligence of AI systems through their ability\nto efficiently acquire new skills. Questions in ARC include (1) showing multiple examples of visual\ninput-output grid patterns, (2) the AI system learning the transformation rule of grid patterns from\nexamples, and (3) predicting the output grid pattern given a test input grid pattern. Since each\nquestion in ARC has a unique transformation rule, it requires the AI system to learn efficiently with\n6Automated Design of Agentic Systems\nfew-shot examples, leveraging capabilities in number counting, geometry, and topology.\nSetup. Following common practice (Greenblatt, 2024), we require the agent to write code for the\ntransformation rule instead of answering directly. We provide tool functions in the framework that\nevaluate the generated transformation code. Given the significant challenge that ARC poses to current\nAI systems, we sample our data from questions with grid dimensions \u22645\u00d75in the \u201cPublic Training\nSet (Easy)\u201d. We sample a validation set and a test set with 20 and 60 questions, respectively, for\nsearching and testing. We calculate the validation and test accuracy of an agent by assessing it over\nthe validation and test sets five times to reduce the variance from the stochastic sampling of FMs. We\nevaluate all discovered agents on the held-out test set and report the test accuracy in Figure 3. Meta\nAgent Search runs for 25 iterations and the meta agent uses GPT-4 (OpenAI, 2024), while discovered\nagents and baselines are evaluated using GPT-3.5 (OpenAI, 2022) to reduce compute cost. More\nalgorithmic details and examples of ARC questions can be found in Appendix C.\nBaselines. We compared against five state-of-the-art hand-designed agents: (1) Chain-of-Thought\n(COT) (Wei et al., 2022), which instructs the agent to output the reasoning before answering to\nimprove complex problem-solving through intermediate steps; (2) Self-Consistency with Chain-of-\nThought (COT-SC) (Wang et al., 2023b), which ensembles multiple parallel answers from COT to\nproduce a more accurate answer; (3) Self-Refine (Madaan et al., 2024; Shinn et al., 2023), which\nallows iterative self-reflection to correct mistakes made in previous attempts; (4) LLM-Debate (Du\net al., 2023), which enables different LLMs to debate with each other, leveraging diverse perspectives\nto find better answers; (5) Quality-Diversity, a simplified version of Intelligent Go-Explore (Lu et al.,\n2024c), which produces and ensembles diverse answers to better explore potential solutions. We also\nuse all baselines as initial seeds in the archive for Meta Agent Search. More details about baselines\ncan be found in Appendix E.\nResults and Analysis. As shown in Figure 3a, Meta Agent Search effectively and progressively\ndiscovers agents that perform better than state-of-the-art hand-designed baselines. Important break-\nthroughs are highlighted in the text boxes. As is critical in prior works on open-endedness and AI-GAs\n(Faldor et al., 2024; Lehman & Stanley, 2011; Wang et al., 2019, 2020; Zhang et al., 2024a), Meta\nAgent Search innovates based on a growing archive of previous stepping stones. For example, an\nimportant design pattern emerged in iteration 3 where it uses multiple COTs to generate possible\nanswers, refines them, and finally ensembles the best answers. This became a crucial stepping\nstone that subsequent designs tended to utilize. Additionally, the best-discovered agent is shown\nin Figure 3b, where a complex feedback mechanism is adopted to refine answers more effectively.\nCareful observation of the search progress reveals that this sophisticated feedback mechanism did\nnot appear suddenly. Instead, the ideas of incorporating diverse feedback, evaluating for various\nspecific traits (via experts) such as efficiency and simplicity, and simulating human-like feedback\nemerged in iterations 5, 11, and 12, respectively. The final mechanism is an innovation based on\nthese three stepping stones. This illustrates that even though these stepping stones did not achieve","chunk_id":"1a6353c9d196dc2debad7c27c902bcd7","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"META AGENT SEARCH","type":"TECHNIQUE","description":"Meta Agent Search is a method for discovering high-performance agents through iterative evaluation and refinement based on previous discoveries in the ARC challenge.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"ARC CHALLENGE","type":"EVENT","description":"The ARC Challenge is a competition designed to evaluate the general intelligence of AI systems by testing their ability to learn transformation rules from visual input-output grid patterns.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a state-of-the-art language model developed by OpenAI, utilized in the Meta Agent Search for evaluating discovered agents.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a previous version of the language model developed by OpenAI, used for evaluating baselines in the Meta Agent Search.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"CHAIN-OF-THOUGHT (COT)","type":"TECHNIQUE","description":"Chain-of-Thought (COT) is a method that instructs agents to output reasoning steps before arriving at an answer, enhancing complex problem-solving.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"SELF-CONSISTENCY WITH COT (COT-SC)","type":"TECHNIQUE","description":"Self-Consistency with Chain-of-Thought (COT-SC) is a technique that ensembles multiple answers from COT to improve accuracy.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"SELF-REFINE","type":"TECHNIQUE","description":"Self-Refine is a method that allows agents to iteratively reflect on and correct mistakes from previous attempts.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"LLM DEBATE","type":"TECHNIQUE","description":"LLM Debate is a technique that enables different language models to debate, leveraging diverse perspectives to find better answers.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"QUALITY-DIVERSITY","type":"TECHNIQUE","description":"Quality-Diversity is a simplified method that produces and ensembles diverse answers to explore potential solutions more effectively.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"EXPERTS","type":"ROLE","description":"Experts are evaluators that assess the performance of agents based on specific traits such as efficiency, readability, and simplicity.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"FEEDBACK","type":"PROCESS","description":"Feedback is the information provided by experts to evaluate and enhance the performance of agents during the refinement process.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"HUMAN-LIKE CRITIC","type":"ROLE","description":"Human-like critics are evaluators that simulate human feedback to assess the quality of answers produced by agents.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"EFFICIENCY EXPERT","type":"ROLE","description":"Efficiency experts evaluate agents based on their ability to produce answers quickly and effectively.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"READABILITY EXPERT","type":"ROLE","description":"Readability experts assess the clarity and comprehensibility of the answers generated by agents.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"SIMPLICITY EXPERT","type":"ROLE","description":"Simplicity experts evaluate the straightforwardness and ease of understanding of the answers produced by agents.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"TOP-3 ANSWERS","type":"DATA FORMAT","description":"Top-3 answers refer to the best three responses generated by agents, selected for further evaluation and potential ensemble.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"FINAL ANSWER","type":"OUTPUT","description":"The final answer is the conclusive response produced after evaluating and refining the top answers through the ensemble process.","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"REFINEMENT","type":"","description":"","source_id":"1a6353c9d196dc2debad7c27c902bcd7"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Meta Agent Search is a method for discovering high-performance agents through iterative evaluation and refinement based on previous discoveries in the ARC challenge.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"ARC CHALLENGE\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The ARC Challenge is a competition designed to evaluate the general intelligence of AI systems by testing their ability to learn transformation rules from visual input-output grid patterns.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a state-of-the-art language model developed by OpenAI, utilized in the Meta Agent Search for evaluating discovered agents.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a previous version of the language model developed by OpenAI, used for evaluating baselines in the Meta Agent Search.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT (COT)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain-of-Thought (COT) is a method that instructs agents to output reasoning steps before arriving at an answer, enhancing complex problem-solving.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"SELF-CONSISTENCY WITH COT (COT-SC)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-Consistency with Chain-of-Thought (COT-SC) is a technique that ensembles multiple answers from COT to improve accuracy.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-Refine is a method that allows agents to iteratively reflect on and correct mistakes from previous attempts.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"LLM DEBATE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LLM Debate is a technique that enables different language models to debate, leveraging diverse perspectives to find better answers.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Quality-Diversity is a simplified method that produces and ensembles diverse answers to explore potential solutions more effectively.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"EXPERTS\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Experts are evaluators that assess the performance of agents based on specific traits such as efficiency, readability, and simplicity.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"FEEDBACK\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Feedback is the information provided by experts to evaluate and enhance the performance of agents during the refinement process.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"HUMAN-LIKE CRITIC\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Human-like critics are evaluators that simulate human feedback to assess the quality of answers produced by agents.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"EFFICIENCY EXPERT\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Efficiency experts evaluate agents based on their ability to produce answers quickly and effectively.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"READABILITY EXPERT\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Readability experts assess the clarity and comprehensibility of the answers generated by agents.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"SIMPLICITY EXPERT\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Simplicity experts evaluate the straightforwardness and ease of understanding of the answers produced by agents.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"TOP-3 ANSWERS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Top-3 answers refer to the best three responses generated by agents, selected for further evaluation and potential ensemble.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"FINAL ANSWER\">      <data key=\"d0\">OUTPUT<\/data>      <data key=\"d1\">The final answer is the conclusive response produced after evaluating and refining the top answers through the ensemble process.<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"REFINEMENT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <edge source=\"META AGENT SEARCH\" target=\"ARC CHALLENGE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search is applied to the ARC Challenge to discover novel agentic systems that outperform existing agents.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPT-4\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search utilizes GPT-4 for evaluating the performance of discovered agents.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPT-3.5\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search uses GPT-3.5 to evaluate baseline agents, allowing for a comparison of performance.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"CHAIN-OF-THOUGHT (COT)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search incorporates Chain-of-Thought as a baseline technique for agent evaluation.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SELF-CONSISTENCY WITH COT (COT-SC)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search compares its discovered agents against Self-Consistency with COT to assess improvements in accuracy.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SELF-REFINE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search evaluates agents that utilize Self-Refine to enhance their performance through iterative corrections.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"LLM DEBATE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search assesses agents that employ LLM Debate to leverage diverse perspectives for better answers.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"QUALITY-DIVERSITY\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search includes Quality-Diversity as a method for exploring diverse solutions in agent discovery.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"EXPERTS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Experts provide structured feedback to the agents discovered through Meta Agent Search, evaluating their performance on various traits.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"FEEDBACK\" target=\"REFINEMENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Refinement relies on feedback from experts to improve the quality of answers generated by agents.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"HUMAN-LIKE CRITIC\" target=\"REFINEMENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Human-like critics provide feedback during the refinement process to enhance the agents' performance.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"EFFICIENCY EXPERT\" target=\"REFINEMENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Efficiency experts contribute to the refinement process by evaluating the speed and effectiveness of agents' answers.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"READABILITY EXPERT\" target=\"REFINEMENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Readability experts play a role in the refinement process by assessing the clarity of the agents' answers.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"SIMPLICITY EXPERT\" target=\"REFINEMENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Simplicity experts assist in the refinement process by evaluating how straightforward the agents' answers are.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"TOP-3 ANSWERS\" target=\"FINAL ANSWER\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The final answer is derived from the evaluation of the top-3 answers generated by the agents during the refinement process.<\/data>      <data key=\"d5\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"bc26e68b0b2783ba912b9e5606d9eb0b","chunk":" is adopted to refine answers more effectively.\nCareful observation of the search progress reveals that this sophisticated feedback mechanism did\nnot appear suddenly. Instead, the ideas of incorporating diverse feedback, evaluating for various\nspecific traits (via experts) such as efficiency and simplicity, and simulating human-like feedback\nemerged in iterations 5, 11, and 12, respectively. The final mechanism is an innovation based on\nthese three stepping stones. This illustrates that even though these stepping stones did not achieve\nhigh performance immediately upon emergence, later discoveries benefited from these innovations\nby combining different stepping stones, resembling crossover in evolution via LLMs (Meyerson et al.,\n2023). Overall, the results showcase the potential of ADAS and the effectiveness of Meta Agent Search\nto progressively discover agents that outperform state-of-the-art hand-designed baselines and invent\nnovel design patterns through the innovation and combination of various stepping stones.\n4.2. Reasoning and Problem-Solving Domains\nSetup. Next,weinvestigatethepotentialofouralgorithmtoimprovethecapabilitiesofagentsacross\nmath, reading, and reasoning domains. We test Meta Agent Search on four popular benchmarks: (1)\nDROP (Dua et al., 2019) for evaluating Reading Comprehension ; (2) MGSM (Shi et al., 2023) for\n7Automated Design of Agentic Systems\nAgent NameF1 Score Accuracy (%)\nReading Comprehension Math Multi-task Science\nState-of-the-art Hand-designed Agents\nChain-of-Thought (Wei et al., 2022) 64.2\u00b10.9 28 .0\u00b13.1 65 .4\u00b13.3 29 .2\u00b13.1\nCOT-SC (Wang et al., 2023b) 64.4\u00b10.8 28 .2\u00b13.165.9\u00b13.230.5\u00b13.2\nSelf-Refine (Madaan et al., 2024) 59.2\u00b10.9 27 .5\u00b13.1 63 .5\u00b13.431.6\u00b13.2\nLLM Debate (Du et al., 2023) 60.6\u00b10.9 39.0\u00b13.465.6\u00b13.3 31 .4\u00b13.2\nStep-back Abstraction (Zheng et al., 2023) 60.4\u00b11.0 31 .1\u00b13.2 65 .1\u00b13.3 26 .9\u00b13.0\nQuality-Diversity (Lu et al., 2024c) 61.8\u00b10.9 23 .8\u00b13.0 65 .1\u00b13.3 30 .2\u00b13.1\nRole Assignment (Xu et al., 2023) 65.8\u00b10.9 30.1\u00b13.2 64 .5\u00b13.3 31 .1\u00b13.1\nAutomated Design of Agentic Systems on Different Domains\nBest Agents from Meta Agent Search 79.4\u00b10.8 53 .4\u00b13.5 69 .6\u00b13.2 34 .6\u00b13.2\nTable 1|Performance comparison between Meta Agent Search and state-of-the-art hand-\ndesigned agents across multiple domains. Meta Agent Search discovers superior agents compared\nto the baselines in every domain. We report the test accuracy and the 95% bootstrap confidence\ninterval on held-out test sets. The search is conducted independently for each domain.\nevaluating Mathcapability under a multi-lingual setting; (3) MMLU (Hendrycks et al., 2021) for\nevaluating Multi-task Problem Solving; and (4) GPQA (Rein et al., 2023) for evaluating the capability\nof solving hard (graduate-level) questions in Science. The search is conducted independently within\neach domain. Meta Agent Search runs for 30 iterations. The meta agent uses GPT-4 (OpenAI, 2024),\nwhile the discovered agents and baselines are evaluated using GPT-3.5 (OpenAI, 2022). More details\nabout datasets and experiment settings can be found in Appendix D.\nBaselines. We adopt all baselines introduced in Section 4.1. Additionally, since the above domains\nrequirestrongreasoningskills,weincludetwoadditionalbaselinesthatspecificallyfocusonenhancing\nthereasoningcapabilitiesofagentsforamorethoroughcomparison: (1)Step-backAbstraction(Zheng\net al., 2023), which instructs agents to first consider the principles involved in solving the task for\nbetter reasoning; (2) Role Assignment, which assigns different roles to FMs similar to Xu et al. (2023)\nto obtain better answers. More details about the baselines can be found in Appendix E.\nResults and Analysis. The results across multiple domains demonstrate that Meta Agent Search\ncan discover agents that outperform state-of-the-art hand-designed agents (Table 1). We want to\nhighlight the substantial gap between the learned agents and hand-designed agents in the Reading\nComprehension and Math domains, with improvements in F1 scores by 13.6\/100 and accuracy rates\nby14.4%, respectively. While Meta Agent Search also outperforms baselines in the Multi-task and\nScience domains, the gap is smaller. We hypothesize that for challenging questions in the Science\nand Multi-task domains, the knowledge in FMs is not sufficient to solve the questions, limiting the\nimprovement through optimizing agentic systems, which is a problem that will diminish as FMs\nimprove. In","chunk_id":"bc26e68b0b2783ba912b9e5606d9eb0b","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"META AGENT SEARCH","type":"TECHNIQUE","description":"Meta Agent Search is an algorithm designed to discover agents that outperform traditional hand-designed agents across various domains, including math, reading, and reasoning.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"ADAS","type":"CONCEPT","description":"ADAS refers to Advanced Driver Assistance Systems, which are technologies designed to enhance vehicle safety and facilitate driving.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"FEEDBACK MECHANISM","type":"TECHNIQUE","description":"The feedback mechanism is a sophisticated system that incorporates diverse feedback to refine answers and improve performance iteratively.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"STEP-BACK ABSTRACTION","type":"TECHNIQUE","description":"Step-back Abstraction is a baseline technique that instructs agents to consider the principles involved in solving tasks to enhance reasoning capabilities.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"ROLE ASSIGNMENT","type":"TECHNIQUE","description":"Role Assignment is a baseline technique that assigns different roles to function models (FMs) to improve the quality of answers.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a state-of-the-art language model developed by OpenAI, utilized in the Meta Agent Search for evaluating discovered agents.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a language model developed by OpenAI, used to evaluate the performance of discovered agents and baselines in the study.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"DROP","type":"BENCHMARK","description":"DROP is a benchmark for evaluating reading comprehension capabilities in agents, referenced in the context of Meta Agent Search.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"MGSM","type":"BENCHMARK","description":"MGSM is a benchmark for evaluating math capabilities in agents, referenced in the context of Meta Agent Search.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"MMLU","type":"BENCHMARK","description":"MMLU is a benchmark for evaluating multi-task problem-solving capabilities in agents, referenced in the context of Meta Agent Search.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"GPQA","type":"BENCHMARK","description":"GPQA is a benchmark for evaluating the capability of solving hard (graduate-level) questions in science, referenced in the context of Meta Agent Search.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"MEYERSON ET AL. (2023)","type":"PERSON","description":"Meyerson et al. (2023) is a reference to a study that discusses the evolution of feedback mechanisms in the context of agent performance.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"F1 SCORE","type":"METRIC","description":"F1 Score is a performance metric used to evaluate the accuracy of agents in tasks, particularly in reading comprehension and math.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"ACCURACY","type":"METRIC","description":"Accuracy is a performance metric that measures the correctness of agents' responses in various tasks, including reading comprehension and math.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"CHAIN-OF-THOUGHT","type":"TECHNIQUE","description":"Chain-of-Thought is a baseline technique that enhances reasoning by encouraging agents to articulate their thought processes while solving tasks.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"COT-SC","type":"TECHNIQUE","description":"COT-SC is a baseline technique that builds on Chain-of-Thought to improve reasoning and performance in various tasks.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"SELF-REFINE","type":"TECHNIQUE","description":"Self-Refine is a baseline technique that focuses on refining agents' responses through iterative feedback and adjustments.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"LLM DEBATE","type":"TECHNIQUE","description":"LLM Debate is a baseline technique that utilizes debate-style interactions among agents to enhance reasoning and answer quality.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"QUALITY-DIVERSITY","type":"TECHNIQUE","description":"Quality-Diversity is a baseline technique that aims to improve the diversity and quality of solutions generated by agents.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"AGENT NAME","type":"DATA FORMAT","description":"Agent Name refers to the identifier assigned to each agent evaluated in the performance comparison across various tasks.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"ITERATIONS","type":"PROCESS","description":"Iterations refer to the repeated cycles of evaluation and improvement that agents undergo during the Meta Agent Search process.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"HOLD-OUT TEST SETS","type":"DATA FORMAT","description":"Hold-out test sets are subsets of data reserved for evaluating the performance of agents after training, ensuring unbiased assessment.","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"STATE-OF-THE-ART HAND-DESIGNED AGENTS","type":"","description":"","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Meta Agent Search is an algorithm designed to discover agents that outperform traditional hand-designed agents across various domains, including math, reading, and reasoning.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"ADAS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">ADAS refers to Advanced Driver Assistance Systems, which are technologies designed to enhance vehicle safety and facilitate driving.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"FEEDBACK MECHANISM\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The feedback mechanism is a sophisticated system that incorporates diverse feedback to refine answers and improve performance iteratively.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"STEP-BACK ABSTRACTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Step-back Abstraction is a baseline technique that instructs agents to consider the principles involved in solving tasks to enhance reasoning capabilities.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"ROLE ASSIGNMENT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Role Assignment is a baseline technique that assigns different roles to function models (FMs) to improve the quality of answers.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a state-of-the-art language model developed by OpenAI, utilized in the Meta Agent Search for evaluating discovered agents.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model developed by OpenAI, used to evaluate the performance of discovered agents and baselines in the study.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">DROP is a benchmark for evaluating reading comprehension capabilities in agents, referenced in the context of Meta Agent Search.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"MGSM\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MGSM is a benchmark for evaluating math capabilities in agents, referenced in the context of Meta Agent Search.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MMLU is a benchmark for evaluating multi-task problem-solving capabilities in agents, referenced in the context of Meta Agent Search.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"GPQA\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">GPQA is a benchmark for evaluating the capability of solving hard (graduate-level) questions in science, referenced in the context of Meta Agent Search.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"MEYERSON ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Meyerson et al. (2023) is a reference to a study that discusses the evolution of feedback mechanisms in the context of agent performance.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"F1 SCORE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">F1 Score is a performance metric used to evaluate the accuracy of agents in tasks, particularly in reading comprehension and math.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"ACCURACY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Accuracy is a performance metric that measures the correctness of agents' responses in various tasks, including reading comprehension and math.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain-of-Thought is a baseline technique that enhances reasoning by encouraging agents to articulate their thought processes while solving tasks.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"COT-SC\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">COT-SC is a baseline technique that builds on Chain-of-Thought to improve reasoning and performance in various tasks.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-Refine is a baseline technique that focuses on refining agents' responses through iterative feedback and adjustments.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"LLM DEBATE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LLM Debate is a baseline technique that utilizes debate-style interactions among agents to enhance reasoning and answer quality.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Quality-Diversity is a baseline technique that aims to improve the diversity and quality of solutions generated by agents.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"AGENT NAME\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Agent Name refers to the identifier assigned to each agent evaluated in the performance comparison across various tasks.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"ITERATIONS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Iterations refer to the repeated cycles of evaluation and improvement that agents undergo during the Meta Agent Search process.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"HOLD-OUT TEST SETS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Hold-out test sets are subsets of data reserved for evaluating the performance of agents after training, ensuring unbiased assessment.<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"STATE-OF-THE-ART HAND-DESIGNED AGENTS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <edge source=\"META AGENT SEARCH\" target=\"ADAS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search showcases the potential of ADAS by discovering agents that outperform traditional systems through innovative design patterns.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"FEEDBACK MECHANISM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search utilizes a sophisticated feedback mechanism to refine agent performance iteratively.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"STEP-BACK ABSTRACTION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Step-back Abstraction is one of the baseline techniques compared against the performance of agents discovered by Meta Agent Search.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ROLE ASSIGNMENT\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Role Assignment is another baseline technique used for comparison with agents discovered by Meta Agent Search.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPT-4\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search employs GPT-4 for evaluating the performance of the discovered agents.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPT-3.5\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">GPT-3.5 is used to evaluate the performance of the discovered agents and baselines in the context of Meta Agent Search.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"DROP\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">DROP is one of the benchmarks used to evaluate the reading comprehension capabilities of agents discovered by Meta Agent Search.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MGSM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">MGSM is a benchmark used to evaluate the math capabilities of agents discovered by Meta Agent Search.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MMLU\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">MMLU is a benchmark used to evaluate the multi-task problem-solving capabilities of agents discovered by Meta Agent Search.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPQA\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">GPQA is a benchmark used to evaluate the capability of agents in solving hard science questions in the context of Meta Agent Search.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MEYERSON ET AL. (2023)\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Meyerson et al. (2023) provides insights relevant to the development and performance of the feedback mechanisms in Meta Agent Search.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"STATE-OF-THE-ART HAND-DESIGNED AGENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search is compared against state-of-the-art hand-designed agents to demonstrate its superior performance across various domains.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"AGENT NAME\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Agent Name is used to identify and evaluate the performance of agents discovered through the Meta Agent Search process.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ITERATIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Iterations refer to the cycles of evaluation that agents undergo during the Meta Agent Search process, impacting their performance outcomes.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"HOLD-OUT TEST SETS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Hold-out test sets are utilized in the Meta Agent Search to ensure unbiased evaluation of agent performance after training.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"F1 SCORE\" target=\"STATE-OF-THE-ART HAND-DESIGNED AGENTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">F1 Score is used to evaluate the performance of state-of-the-art hand-designed agents in reading comprehension and math tasks.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"ACCURACY\" target=\"STATE-OF-THE-ART HAND-DESIGNED AGENTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Accuracy is another metric used to assess the performance of state-of-the-art hand-designed agents in various tasks.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"STATE-OF-THE-ART HAND-DESIGNED AGENTS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Chain-of-Thought is a baseline technique that is compared against state-of-the-art hand-designed agents to evaluate reasoning improvements.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"COT-SC\" target=\"STATE-OF-THE-ART HAND-DESIGNED AGENTS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">COT-SC is a baseline technique that is evaluated alongside state-of-the-art hand-designed agents for performance comparison.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"STATE-OF-THE-ART HAND-DESIGNED AGENTS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Self-Refine is a baseline technique that is compared against state-of-the-art hand-designed agents to assess its effectiveness in improving responses.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"LLM DEBATE\" target=\"STATE-OF-THE-ART HAND-DESIGNED AGENTS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LLM Debate is a baseline technique evaluated against state-of-the-art hand-designed agents to measure enhancements in reasoning.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"QUALITY-DIVERSITY\" target=\"STATE-OF-THE-ART HAND-DESIGNED AGENTS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Quality-Diversity is a baseline technique that is compared with state-of-the-art hand-designed agents to evaluate its impact on solution quality.<\/data>      <data key=\"d5\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"2901d5e2711fa4f32d39cd8eea36cd71","chunk":"1 scores by 13.6\/100 and accuracy rates\nby14.4%, respectively. While Meta Agent Search also outperforms baselines in the Multi-task and\nScience domains, the gap is smaller. We hypothesize that for challenging questions in the Science\nand Multi-task domains, the knowledge in FMs is not sufficient to solve the questions, limiting the\nimprovement through optimizing agentic systems, which is a problem that will diminish as FMs\nimprove. In contrast, in the Reading Comprehension and Math domains, FMs possess adequate\nknowledge to solve the questions, and errors could mainly be hallucinations or calculation mistakes,\nwhich can be mitigated through well-designed agentic systems, like the ones discovered by Meta\nAgent Search. Overall, the results across various domains showcase the effectiveness of Meta Agent\nSearch in searching for agents tailored to specific domains. This could be increasingly useful for\nsaving human efforts and developing better task-specific agents as we continue to create agents for a\ndiverse set of applications (Wang et al., 2024).\n8Automated Design of Agentic Systems\n4.3. Generalization and transferability\nAgent NameAccuracy on ARC (%)\nGPT-3.5 Claude-Haiku GPT-4 Claude-Sonnet\nManually Designed Agents\nChain-of-Thought (Wei et al., 2022) 6.0\u00b12.7 4.3\u00b12.2 17 .7\u00b14.4 25 .3\u00b15.0\nCOT-SC (Wang et al., 2023b) 8.0\u00b13.2 5.3\u00b12.5 19 .7\u00b14.5 26 .3\u00b14.9\nLLM Debate (Du et al., 2023) 4.0\u00b12.2 1.7\u00b11.5 19 .0\u00b14.5 24 .7\u00b14.8\nSelf-Refine (Madaan et al., 2024) 6.7\u00b12.7 6.3\u00b12.8 23 .0\u00b15.2 39 .3\u00b15.5\nQuality-Diversity (Lu et al., 2024c) 7.0\u00b12.9 3.3\u00b12.2 23.0\u00b14.7 31.7\u00b15.3\nTop Agents Searched with GPT-3.5 Transferred to Other FMs\nStructured Feedback and Ensemble Agent 13.7\u00b13.9 5.0\u00b12.5 30 .0\u00b15.2 38 .7\u00b15.5\nHierarchical Committee Reinforcement Agent 13.3\u00b13.8 8.3\u00b13.2 32 .3\u00b18.9 39 .7\u00b15.5\nDynamic Memory and Refinement Agent\u202012.7\u00b13.9 9.7\u00b13.3 37 .0\u00b15.3 48 .3\u00b15.7\nTable 2|Performance on ARC when transferring top agents from GPT-3.5 to other FMs. Agents\ndiscovered by Meta Agent Search consistently outperform the baselines across different models. We\nreport the test accuracy and the 95% bootstrap confidence interval. The names of top agents are\ngenerated by Meta Agent Search.\u2020We manually changed this name because the original generated\nname was confusing.\nIn the previous sections, we illustrated that Meta Agent Search can find effective agents for\nindividual tasks. In this section, we further demonstrate the transferability and generalizability of the\ndiscovered agents. To show that the invented building blocks and design patterns are generalizable,\nwe conduct experiments on the transferability of the discovered agents.\nTransferability Across Foundation Models. We first transfer discovered agents from GPT-3.5 (Ope-\nnAI, 2022) to other FMs on ARC to test whether agents found when performing Meta Agent Search\nwith one FM generalize to others. We test the top 3 agents with the best test accuracy evaluated with\nGPT-3.5 on ARC and then transfer them to three popular models: Claude-Haiku (Anthropic, 2024a),\nGPT-4 (OpenAI, 2024), and Claude-Sonnet (Anthropic, 2024b). We adopt the same baselines as\nthose used in ARC (Section 4.1) and MGSM (Section 4.2). As shown in Table 2, we observe that the\nsearched agents consistently outperform the hand-designed agents with a substantial gap. Notably,\nwe found that Claude-Sonnet, the most powerful model from Anthropic, performs the best among all\ntested models, enabling our best agent to achieve nearly 50% accuracy on ARC.\nTransferability Across Domains. Next, we transfer the discovered agent from the MGSM (Math)\ndomain to other math domains to test whether the invented agents can generalize across different\ndomains. Similarly, we test the top 3 agents from MGSM and transfer them to (1) four popular math\ndomains: GSM8K (Cobbe et al., 2021), GSM-Hard (Gao et al., 2023), SVAMP (Patel et al., 2021),\nand ASDiv (Miao et al., 2020) and (2) three domains beyond math adopted in Section 4.2. As shown\nin Table 3, we observe a similar superiority in the performance of Meta Agent Search compared to\nbaselines. Notably, our agents improve accuracy by 25.9%and13.2%on GSM8K (Cobbe et al., 202","chunk_id":"2901d5e2711fa4f32d39cd8eea36cd71","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"META AGENT SEARCH","type":"TECHNIQUE","description":"Meta Agent Search is a method for discovering effective agents tailored to specific tasks across various domains, showcasing its effectiveness in improving performance metrics.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"FOUNDATION MODELS","type":"TECHNOLOGY","description":"Foundation models (FMs) are large-scale machine learning models that serve as the base for various applications, including those tested in the context of Meta Agent Search.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"GPT-3.5","type":"MODEL","description":"GPT-3.5 is a version of OpenAI's language model that serves as a foundation for testing and transferring discovered agents in the Meta Agent Search process.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"CLAUDE-HAIKU","type":"MODEL","description":"Claude-Haiku is a language model developed by Anthropic, used to evaluate the performance of agents discovered through Meta Agent Search.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is an advanced language model developed by OpenAI, utilized for testing the transferability of agents discovered by Meta Agent Search.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"CLAUDE-SONNET","type":"MODEL","description":"Claude-Sonnet is another language model from Anthropic, noted for its high performance in evaluations of agents discovered through Meta Agent Search.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"ARC","type":"EVALUATION METRIC","description":"ARC (AI Reading Comprehension) is a benchmark used to evaluate the accuracy of agents in reading comprehension tasks.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"MGSM","type":"EVALUATION METRIC","description":"MGSM (Math Generalization and Search Model) is a benchmark used to evaluate the performance of agents in math-related tasks.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"GSM8K","type":"DATASET","description":"GSM8K is a dataset used for evaluating the performance of agents in math tasks, specifically designed for challenging math problems.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"GSM-HARD","type":"DATASET","description":"GSM-HARD is a dataset that presents more difficult math problems for evaluating agent performance.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"SVAMP","type":"DATASET","description":"SVAMP is a dataset used for assessing the performance of agents in solving math problems.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"ASDIV","type":"DATASET","description":"ASDIV is a dataset used for evaluating the performance of agents in diverse math tasks.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"COT-SC","type":"TECHNIQUE","description":"COT-SC (Chain-of-Thought with Self-Consistency) is a technique that improves the performance of language models by generating multiple reasoning paths and selecting the best one.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"LLM DEBATE","type":"TECHNIQUE","description":"LLM Debate is a method where multiple language models engage in a debate format to enhance reasoning and decision-making capabilities.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"SELF-REFINE","type":"TECHNIQUE","description":"Self-Refine is a technique that allows language models to iteratively improve their responses by refining their outputs based on feedback.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"QUALITY-DIVERSITY","type":"TECHNIQUE","description":"Quality-Diversity is a method that focuses on generating a diverse set of high-quality solutions to a problem, enhancing the overall performance of language models.","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"CHAIN-OF-THOUGHT","type":"","description":"","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Meta Agent Search is a method for discovering effective agents tailored to specific tasks across various domains, showcasing its effectiveness in improving performance metrics.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"FOUNDATION MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundation models (FMs) are large-scale machine learning models that serve as the base for various applications, including those tested in the context of Meta Agent Search.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5 is a version of OpenAI's language model that serves as a foundation for testing and transferring discovered agents in the Meta Agent Search process.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"CLAUDE-HAIKU\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Claude-Haiku is a language model developed by Anthropic, used to evaluate the performance of agents discovered through Meta Agent Search.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is an advanced language model developed by OpenAI, utilized for testing the transferability of agents discovered by Meta Agent Search.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"CLAUDE-SONNET\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Claude-Sonnet is another language model from Anthropic, noted for its high performance in evaluations of agents discovered through Meta Agent Search.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">EVALUATION METRIC<\/data>      <data key=\"d1\">ARC (AI Reading Comprehension) is a benchmark used to evaluate the accuracy of agents in reading comprehension tasks.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"MGSM\">      <data key=\"d0\">EVALUATION METRIC<\/data>      <data key=\"d1\">MGSM (Math Generalization and Search Model) is a benchmark used to evaluate the performance of agents in math-related tasks.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">GSM8K is a dataset used for evaluating the performance of agents in math tasks, specifically designed for challenging math problems.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"GSM-HARD\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">GSM-HARD is a dataset that presents more difficult math problems for evaluating agent performance.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"SVAMP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">SVAMP is a dataset used for assessing the performance of agents in solving math problems.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"ASDIV\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">ASDIV is a dataset used for evaluating the performance of agents in diverse math tasks.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"COT-SC\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">COT-SC (Chain-of-Thought with Self-Consistency) is a technique that improves the performance of language models by generating multiple reasoning paths and selecting the best one.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"LLM DEBATE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LLM Debate is a method where multiple language models engage in a debate format to enhance reasoning and decision-making capabilities.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-Refine is a technique that allows language models to iteratively improve their responses by refining their outputs based on feedback.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Quality-Diversity is a method that focuses on generating a diverse set of high-quality solutions to a problem, enhancing the overall performance of language models.<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <edge source=\"META AGENT SEARCH\" target=\"FOUNDATION MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search discovers agents that can be transferred across different foundation models, demonstrating their generalizability.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPT-3.5\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">GPT-3.5 serves as a foundation model for testing the effectiveness of agents discovered through Meta Agent Search.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"CLAUDE-HAIKU\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Claude-Haiku is used to evaluate the performance of agents discovered by Meta Agent Search, contributing to the assessment of their effectiveness.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPT-4\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">GPT-4 is utilized to test the transferability of agents discovered through Meta Agent Search, showcasing their adaptability across models.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"CLAUDE-SONNET\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Claude-Sonnet is evaluated for its performance with agents discovered by Meta Agent Search, highlighting the effectiveness of these agents.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ARC\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">ARC is a benchmark used to evaluate the accuracy of agents discovered through Meta Agent Search, providing a measure of their performance in reading comprehension tasks.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MGSM\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">MGSM is a benchmark used to assess the performance of agents in math tasks discovered through Meta Agent Search, demonstrating their effectiveness in this domain.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"CHAIN-OF-THOUGHT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Chain-of-Thought is one of the techniques evaluated for its effectiveness in the context of Meta Agent Search, contributing to the discovery of effective agents.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"COT-SC\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">COT-SC is another technique assessed within the framework of Meta Agent Search, aimed at improving agent performance.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"LLM DEBATE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LLM Debate is evaluated as a technique within Meta Agent Search to enhance the reasoning capabilities of discovered agents.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SELF-REFINE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Self-Refine is tested in the context of Meta Agent Search to improve the outputs of agents through iterative refinement.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"QUALITY-DIVERSITY\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Quality-Diversity is assessed as a technique in Meta Agent Search to generate diverse and high-quality agents for various tasks.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"MGSM\" target=\"GSM8K\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">GSM8K is one of the datasets used in the MGSM benchmark to evaluate the performance of agents in solving math problems.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"MGSM\" target=\"GSM-HARD\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">GSM-HARD is another dataset used in the MGSM benchmark for evaluating agent performance in challenging math tasks.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"MGSM\" target=\"SVAMP\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">SVAMP is included in the MGSM benchmark to assess the performance of agents in solving various math problems.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"MGSM\" target=\"ASDIV\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">ASDIV is part of the MGSM benchmark used to evaluate the performance of agents in diverse math tasks.<\/data>      <data key=\"d5\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"0b6b4880e77d40e284702da16be4ef64","chunk":"3), SVAMP (Patel et al., 2021),\nand ASDiv (Miao et al., 2020) and (2) three domains beyond math adopted in Section 4.2. As shown\nin Table 3, we observe a similar superiority in the performance of Meta Agent Search compared to\nbaselines. Notably, our agents improve accuracy by 25.9%and13.2%on GSM8K (Cobbe et al., 2021)\nand GSM-Hard (Gao et al., 2023), respectively, compared to the baselines. More surprisingly, we\nobserve that agents discovered in the math domain can be transferred to non-math domains (Table 4).\nWhile the performance of agents originally searched in the math domain does not fully match that of\nagents specifically designed for the target domains, they still outperform (in Reading Comprehension\nand Multi-task) or match (in Science) the state-of-the-art hand-designed agent baselines. These\nresults illustrate that Meta Agent Search can discover generalizable design patterns and agentic\nsystems.\n9Automated Design of Agentic Systems\nAgent NameAccuracy (%)\nMGSM GSM8K GSM-Hard SVAMP ASDiv\nManually Designed Agents\nChain-of-Thought (Wei et al., 2022) 28.0\u00b13.134.9\u00b13.2 15 .0\u00b12.5 77 .8\u00b12.8 88 .9\u00b12.2\nCOT-SC (Wang et al., 2023b) 28.2\u00b13.137.8\u00b13.4 15 .5\u00b12.5 78 .2\u00b12.8 89 .0\u00b12.1\nSelf-Refine (Madaan et al., 2024) 27.5\u00b13.138.9\u00b13.4 15 .1\u00b12.478.5\u00b12.8 89 .2\u00b12.2\nLLM Debate (Du et al., 2023) 39.0\u00b13.443.6\u00b13.417.4\u00b12.6 76 .0\u00b13.0 88 .9\u00b12.2\nStep-back Abstraction (Zheng et al., 2023) 31.1\u00b13.231.5\u00b13.3 12 .2\u00b12.3 76 .1\u00b13.0 87 .8\u00b12.3\nQuality-Diversity (Lu et al., 2024c) 23.8\u00b13.028.0\u00b13.1 14 .1\u00b12.4 69 .8\u00b13.2 80 .1\u00b12.8\nRole Assignment (Xu et al., 2023) 30.1\u00b13.237.0\u00b13.418.0\u00b12.773.0\u00b13.0 83 .1\u00b12.6\nTop Agents Searched on MGSM (Math) Transferred to Other Math Domains\nDynamic Role-Playing Architecture 53.4\u00b13.569.5\u00b13.2 31 .2\u00b13.281.5\u00b12.691.8\u00b11.8\nStructured Multimodal Feedback Loop 50.2\u00b13.564.5\u00b13.4 30 .1\u00b13.282.6\u00b12.689.9\u00b12.1\nInteractive Multimodal Feedback Loop 47.4\u00b13.564.9\u00b13.3 27 .6\u00b13.2 80 .6\u00b12.8 89 .8\u00b12.1\nTable 3|Performance on different math domains when transferring top agents from MGSM to\nother math domains. Agents discovered by Meta Agent Search consistently outperform the baselines\nacross different math domains. We report the test accuracy and the 95% bootstrap confidence interval.\nThe names of top agents are generated by Meta Agent Search.\n5. Related Work\nAgentic Systems. Researchers develop various building blocks and design patterns for different\napplications. Important building blocks for agentic systems includes: prompting techniques (Chen\net al., 2023a; Schulhoff et al., 2024), chain-of-thought-based planning and reasoning methods (Hu\n& Clune, 2024; Wei et al., 2022; Yao et al., 2023), reflection (Madaan et al., 2024; Shinn et al.,\n2023), developing new skills for embodied agents in code (Vemprala et al., 2023; Wang et al., 2023a),\nexternal memory and RAG (Lewis et al., 2020; Zhang et al., 2024c), tool use (Nakano et al., 2021;\nQu et al., 2024; Schick et al., 2023), assigning FM modules in the agentic system with different\nroles and enabling them to collaborate (Hong et al., 2023; Qian et al., 2023, 2024; Wu et al., 2023;\nXu et al., 2023), and enabling the agent to instruct itself for the next action (Richards, 2023), etc.\nWhile the community has invested substantial effort in developing all the above important techniques,\nthis is only a partial list of the discovered building blocks, and many more remain to be uncovered.\nTherefore, in this paper, we propose a new research area, ADAS, which aims to invent novel building\nblocks and design powerful agentic systems in an automated manner.\nAI-Generating Algorithms and AutoML. Following the lessons learned from the history of machine\nlearning, research in AI-Generating Algorithms (AI-GAs","chunk_id":"0b6b4880e77d40e284702da16be4ef64","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"META AGENT SEARCH","type":"TECHNIQUE","description":"Meta Agent Search is a method that improves the performance of agents in various domains, particularly in math, by discovering generalizable design patterns and systems.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"GSM8K","type":"DATASET","description":"GSM8K is a benchmark dataset used to evaluate the performance of mathematical problem-solving agents, referenced in the context of accuracy improvements.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"GSM-HARD","type":"DATASET","description":"GSM-HARD is a challenging benchmark dataset for evaluating the performance of mathematical problem-solving agents, referenced in the context of accuracy improvements.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"SVAMP","type":"DATASET","description":"SVAMP is a dataset used for evaluating the performance of agents in mathematical problem-solving tasks, referenced in the context of accuracy improvements.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"ASDIV","type":"DATASET","description":"ASDiv is a dataset used for evaluating the performance of agents in mathematical problem-solving tasks, referenced in the context of accuracy improvements.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"CHAIN-OF-THOUGHT","type":"TECHNIQUE","description":"Chain-of-Thought is a reasoning method used in agentic systems to enhance problem-solving capabilities, referenced in the context of manually designed agents.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"COT-SC","type":"TECHNIQUE","description":"COT-SC is a specific technique used in agentic systems for problem-solving, referenced in the context of manually designed agents.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"SELF-REFINE","type":"TECHNIQUE","description":"Self-Refine is a technique used in agentic systems to improve performance, referenced in the context of manually designed agents.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"LLM DEBATE","type":"TECHNIQUE","description":"LLM Debate is a technique used in agentic systems for problem-solving, referenced in the context of manually designed agents.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"STEP-BACK ABSTRACTION","type":"TECHNIQUE","description":"Step-back Abstraction is a technique used in agentic systems for problem-solving, referenced in the context of manually designed agents.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"QUALITY-DIVERSITY","type":"TECHNIQUE","description":"Quality-Diversity is a technique used in agentic systems for problem-solving, referenced in the context of manually designed agents.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"ROLE ASSIGNMENT","type":"TECHNIQUE","description":"Role Assignment is a technique used in agentic systems to assign different roles to modules for collaboration, referenced in the context of manually designed agents.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"DYNAMIC ROLE-PLAYING ARCHITECTURE","type":"AGENT","description":"Dynamic Role-Playing Architecture is a top-performing agent discovered by Meta Agent Search, evaluated across various math domains.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"STRUCTURED MULTIMODAL FEEDBACK LOOP","type":"AGENT","description":"Structured Multimodal Feedback Loop is a top-performing agent discovered by Meta Agent Search, evaluated across various math domains.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"INTERACTIVE MULTIMODAL FEEDBACK LOOP","type":"AGENT","description":"Interactive Multimodal Feedback Loop is a top-performing agent discovered by Meta Agent Search, evaluated across various math domains.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"MANUALLY DESIGNED AGENTS","type":"","description":"","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"PROMPTING TECHNIQUES","type":"TECHNIQUE","description":"Prompting Techniques are methods used to guide the behavior of agents in agentic systems, enhancing their problem-solving capabilities.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"REFLECTION","type":"TECHNIQUE","description":"Reflection is a technique used in agentic systems to enable agents to evaluate their actions and improve future performance.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"EXTERNAL MEMORY","type":"TECHNIQUE","description":"External Memory is a technique used in agentic systems to store and retrieve information, enhancing the agents' capabilities.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"TOOL USE","type":"TECHNIQUE","description":"Tool Use refers to the ability of agents in agentic systems to utilize external tools to accomplish tasks.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"FM MODULES","type":"CONCEPT","description":"FM Modules are functional modules within agentic systems that can be assigned different roles to collaborate effectively.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"ADAS","type":"CONCEPT","description":"ADAS stands for Automated Design of Agentic Systems, a proposed research area focused on inventing novel building blocks for agentic systems.","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"AGENTIC SYSTEMS","type":"","description":"","source_id":"0b6b4880e77d40e284702da16be4ef64"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Meta Agent Search is a method that improves the performance of agents in various domains, particularly in math, by discovering generalizable design patterns and systems.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">GSM8K is a benchmark dataset used to evaluate the performance of mathematical problem-solving agents, referenced in the context of accuracy improvements.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"GSM-HARD\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">GSM-HARD is a challenging benchmark dataset for evaluating the performance of mathematical problem-solving agents, referenced in the context of accuracy improvements.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"SVAMP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">SVAMP is a dataset used for evaluating the performance of agents in mathematical problem-solving tasks, referenced in the context of accuracy improvements.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"ASDIV\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">ASDiv is a dataset used for evaluating the performance of agents in mathematical problem-solving tasks, referenced in the context of accuracy improvements.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain-of-Thought is a reasoning method used in agentic systems to enhance problem-solving capabilities, referenced in the context of manually designed agents.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"COT-SC\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">COT-SC is a specific technique used in agentic systems for problem-solving, referenced in the context of manually designed agents.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-Refine is a technique used in agentic systems to improve performance, referenced in the context of manually designed agents.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"LLM DEBATE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LLM Debate is a technique used in agentic systems for problem-solving, referenced in the context of manually designed agents.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"STEP-BACK ABSTRACTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Step-back Abstraction is a technique used in agentic systems for problem-solving, referenced in the context of manually designed agents.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Quality-Diversity is a technique used in agentic systems for problem-solving, referenced in the context of manually designed agents.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"ROLE ASSIGNMENT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Role Assignment is a technique used in agentic systems to assign different roles to modules for collaboration, referenced in the context of manually designed agents.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"DYNAMIC ROLE-PLAYING ARCHITECTURE\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Dynamic Role-Playing Architecture is a top-performing agent discovered by Meta Agent Search, evaluated across various math domains.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"STRUCTURED MULTIMODAL FEEDBACK LOOP\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Structured Multimodal Feedback Loop is a top-performing agent discovered by Meta Agent Search, evaluated across various math domains.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"INTERACTIVE MULTIMODAL FEEDBACK LOOP\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Interactive Multimodal Feedback Loop is a top-performing agent discovered by Meta Agent Search, evaluated across various math domains.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"MANUALLY DESIGNED AGENTS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"PROMPTING TECHNIQUES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Prompting Techniques are methods used to guide the behavior of agents in agentic systems, enhancing their problem-solving capabilities.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"REFLECTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Reflection is a technique used in agentic systems to enable agents to evaluate their actions and improve future performance.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"EXTERNAL MEMORY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">External Memory is a technique used in agentic systems to store and retrieve information, enhancing the agents' capabilities.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Tool Use refers to the ability of agents in agentic systems to utilize external tools to accomplish tasks.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"FM MODULES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">FM Modules are functional modules within agentic systems that can be assigned different roles to collaborate effectively.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"ADAS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">ADAS stands for Automated Design of Agentic Systems, a proposed research area focused on inventing novel building blocks for agentic systems.<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"AGENTIC SYSTEMS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <edge source=\"META AGENT SEARCH\" target=\"GSM8K\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search improves accuracy on the GSM8K dataset by discovering effective agents.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GSM-HARD\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search improves accuracy on the GSM-HARD dataset by discovering effective agents.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SVAMP\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search improves performance on the SVAMP dataset by discovering effective agents.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ASDIV\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search improves performance on the ASDiv dataset by discovering effective agents.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"DYNAMIC ROLE-PLAYING ARCHITECTURE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Dynamic Role-Playing Architecture is a top-performing agent discovered through Meta Agent Search.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"STRUCTURED MULTIMODAL FEEDBACK LOOP\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Structured Multimodal Feedback Loop is a top-performing agent discovered through Meta Agent Search.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"INTERACTIVE MULTIMODAL FEEDBACK LOOP\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Interactive Multimodal Feedback Loop is a top-performing agent discovered through Meta Agent Search.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"MANUALLY DESIGNED AGENTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Chain-of-Thought is a reasoning method used by manually designed agents to enhance their performance.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"COT-SC\" target=\"MANUALLY DESIGNED AGENTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">COT-SC is a technique used by manually designed agents to improve their performance.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"MANUALLY DESIGNED AGENTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Self-Refine is a technique used by manually designed agents to enhance their performance.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"LLM DEBATE\" target=\"MANUALLY DESIGNED AGENTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LLM Debate is a technique used by manually designed agents to improve their performance.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"STEP-BACK ABSTRACTION\" target=\"MANUALLY DESIGNED AGENTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Step-back Abstraction is a technique used by manually designed agents to enhance their performance.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"QUALITY-DIVERSITY\" target=\"MANUALLY DESIGNED AGENTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Quality-Diversity is a technique used by manually designed agents to improve their performance.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"ROLE ASSIGNMENT\" target=\"MANUALLY DESIGNED AGENTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Role Assignment is a technique used by manually designed agents to enable collaboration among modules.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"PROMPTING TECHNIQUES\" target=\"AGENTIC SYSTEMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Agentic Systems utilize Prompting Techniques to enhance their problem-solving capabilities.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"REFLECTION\" target=\"AGENTIC SYSTEMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Agentic Systems incorporate Reflection to evaluate actions and improve future performance.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"EXTERNAL MEMORY\" target=\"AGENTIC SYSTEMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Agentic Systems use External Memory to store and retrieve information, enhancing their capabilities.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"TOOL USE\" target=\"AGENTIC SYSTEMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Agentic Systems employ Tool Use to accomplish tasks effectively.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"FM MODULES\" target=\"AGENTIC SYSTEMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">FM Modules are integral to Agentic Systems, allowing for role assignment and collaboration among different components.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"AGENTIC SYSTEMS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">ADAS aims to develop new building blocks and design powerful Agentic Systems in an automated manner.<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"7c08d98f503d722d7de13be55375c8cb","chunk":"While the community has invested substantial effort in developing all the above important techniques,\nthis is only a partial list of the discovered building blocks, and many more remain to be uncovered.\nTherefore, in this paper, we propose a new research area, ADAS, which aims to invent novel building\nblocks and design powerful agentic systems in an automated manner.\nAI-Generating Algorithms and AutoML. Following the lessons learned from the history of machine\nlearning, research in AI-Generating Algorithms (AI-GAs) (Clune, 2019) and AutoML (Hutter et al.,\n2019) continually strives to learn more components in AI systems to replace handcrafted ones. There\nare mainly three pillars in this field: (1) meta-learning architectures, (2) meta-learning the learning\nalgorithms, and (3) generating effective learning environments and training data (Clune, 2019). For\nexample, Neural Architecture Search (Elsken et al., 2019; Hu et al., 2021; Lu et al., 2019) aims to\nautomate the design of neural network architectures like convolution, which falls under the first pillar.\nThe second pillar includes works like MAML (Finn et al., 2017) and Meta-RL (Duan et al., 2017;\nNorman & Clune, 2023; Wang et al., 2016; Zintgraf et al., 2021a,b), which allow \u201clearning to learn\u201d\nfor better sample efficiency, generalizability, and continuous learning of multiple tasks. Additionally,\nworks like POET (Dharna et al., 2020; Wang et al., 2019, 2020) and OMNI-EPIC (Faldor et al., 2024)\nunder the third pillar aim to generate learning environments in an open-ended manner. We believe\n10Automated Design of Agentic Systems\nAgent NameAccuracy (%) F1 Score Accuracy (%)\nMath Reading Comprehension Multi-task Science\nManually Designed Agents\nChain-of-Thought (Wei et al., 2022) 28.0\u00b13.1 64.2\u00b10.9 65 .4\u00b13.3 29 .2\u00b13.1\nCOT-SC (Wang et al., 2023b) 28.2\u00b13.1 64.4\u00b10.8 65.9\u00b13.230.5\u00b13.2\nSelf-Refine (Madaan et al., 2024) 27.5\u00b13.1 59.2\u00b10.9 63 .5\u00b13.431.6\u00b13.2\nLLM Debate (Du et al., 2023) 39.0\u00b13.4 60.6\u00b10.9 65 .6\u00b13.3 31 .4\u00b13.2\nStep-back Abstraction (Zheng et al., 2023) 31.1\u00b13.2 60.4\u00b11.0 65 .1\u00b13.3 26 .9\u00b13.0\nQuality-Diversity (Lu et al., 2024c) 23.8\u00b13.0 61.8\u00b10.9 65 .1\u00b13.1 30 .2\u00b13.1\nRole Assignment (Xu et al., 2023) 30.1\u00b13.2 65.8\u00b10.9 64.5\u00b13.3 31 .1\u00b13.1\nTop Agents Searched on Math (MGSM) Transferred beyond Math Domains\nDynamic Role-Playing Architecture 53.4\u00b13.5 70.4\u00b10.9 62 .4\u00b13.4 28 .6\u00b13.1\nStructured Multimodal Feedback Loop 50.2\u00b13.5 70.4\u00b10.9 67.0\u00b13.228.7\u00b13.1\nInteractive Multimodal Feedback Loop 47.4\u00b13.5 71.9\u00b10.8 64.8\u00b13.329.9\u00b13.2\nTable 4|Performance across multiple domains when transferring top agents from the Math\n(MGSM) domain to non-math domains. Agents discovered by Meta Agent Search in the math\ndomain can outperform or match the performance of baselines after being transferred to domains\nbeyond math. We report the test accuracy and the 95% bootstrap confidence interval.\nthat the proposed Automated Design of Agentic Systems belongs to both the first and second pillars:\nPillar one because ADAS meta-learns the architecture of agentic systems, but also Pillar two because\nagents are proficient in in-context learning, thus ADAS can also be considered as learning to learn, as\ndemonstrated in the ARC challenge (Section 4.1).\nAdditionally, recent AI-GA and AutoML works have incorporated Foundation Models (FMs) to\nwrite code. For example, in FunSearch (Romera-Paredes et al., 2024) and EoH (Liu et al., 2024),\nFMs write code to discover better optimization algorithms. In DiscoPOP (Lu et al., 2024a), FMs\nprogram the loss function for preference learning in FM alignment training (Rafailov et al., 2024).\nAdditionally, Eureka (Ma et al., 2023) and language-to-reward (Yu et al., 2023) enable FMs to write\nreward functions for reinforcement learning in robotics. Finally, OMNI-EPIC (Faldor et al., 2024)\nenables FMs to create robotics learning environments by programming in code","chunk_id":"7c08d98f503d722d7de13be55375c8cb","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"ADAS","type":"RESEARCH AREA","description":"ADAS is a proposed research area focused on inventing novel building blocks and designing powerful agentic systems in an automated manner.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"AI-GENERATING ALGORITHMS","type":"TECHNIQUE","description":"AI-Generating Algorithms (AI-GAs) are methods that aim to learn components in AI systems to replace handcrafted ones, enhancing automation in AI development.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"AUTOML","type":"TECHNIQUE","description":"AutoML refers to automated machine learning techniques that streamline the process of applying machine learning to real-world problems.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"META-LEARNING ARCHITECTURES","type":"CONCEPT","description":"Meta-learning architectures are frameworks that enable systems to learn how to learn, improving efficiency and adaptability in AI models.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"META-LEARNING ALGORITHMS","type":"CONCEPT","description":"Meta-learning algorithms are techniques that allow models to improve their learning processes over time, enhancing their performance on various tasks.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"LEARNING ENVIRONMENTS","type":"CONCEPT","description":"Learning environments are settings or frameworks designed to facilitate the training and development of AI models.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"NEURAL ARCHITECTURE SEARCH","type":"TECHNIQUE","description":"Neural Architecture Search is a method for automating the design of neural network architectures to optimize performance.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"MAML","type":"TECHNIQUE","description":"MAML (Model-Agnostic Meta-Learning) is a meta-learning algorithm that enables models to adapt quickly to new tasks with minimal data.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"META-RL","type":"TECHNIQUE","description":"Meta-RL (Meta-Reinforcement Learning) is a framework that focuses on improving the learning efficiency of reinforcement learning agents.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"POET","type":"TECHNIQUE","description":"POET (Population-Based Open-Ended Learning) is a method for generating learning environments in an open-ended manner.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"OMNI-EPIC","type":"TECHNIQUE","description":"OMNI-EPIC is a framework that enables the creation of robotics learning environments through automated programming.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"FOUNDATION MODELS","type":"CONCEPT","description":"Foundation Models are large-scale models that serve as a base for various AI applications, often used for generating code and optimizing algorithms.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"FUNSEARCH","type":"TECHNIQUE","description":"FunSearch is a method that utilizes Foundation Models to discover better optimization algorithms through automated coding.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"DISCOPOP","type":"TECHNIQUE","description":"DiscoPOP is a framework where Foundation Models program loss functions for preference learning in alignment training.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"EUREKA","type":"TECHNIQUE","description":"Eureka is a method that allows Foundation Models to write reward functions for reinforcement learning applications.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"LANGUAGE-TO-REWARD","type":"TECHNIQUE","description":"Language-to-reward is a technique that enables Foundation Models to create reward functions for reinforcement learning in robotics.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"COT-SC","type":"AGENT","description":"COT-SC is a manually designed agent that shows specific performance metrics in multi-task learning scenarios.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"SELF-REFINE","type":"AGENT","description":"Self-Refine is a manually designed agent that exhibits performance metrics in tasks such as reading comprehension and science.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"LLM DEBATE","type":"AGENT","description":"LLM Debate is a manually designed agent that performs across multiple tasks, including math and reading comprehension.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"STEP-BACK ABSTRACTION","type":"AGENT","description":"Step-back Abstraction is a manually designed agent that provides performance metrics in various learning tasks.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"QUALITY-DIVERSITY","type":"AGENT","description":"Quality-Diversity is a manually designed agent that demonstrates performance metrics across different tasks.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"ROLE ASSIGNMENT","type":"AGENT","description":"Role Assignment is a manually designed agent that shows performance metrics in multi-task learning scenarios.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"DYNAMIC ROLE-PLAYING ARCHITECTURE","type":"AGENT","description":"Dynamic Role-Playing Architecture is an agent that exhibits performance metrics when transferred from math to non-math domains.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"STRUCTURED MULTIMODAL FEEDBACK LOOP","type":"AGENT","description":"Structured Multimodal Feedback Loop is an agent that shows performance metrics across various tasks.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"INTERACTIVE MULTIMODAL FEEDBACK LOOP","type":"AGENT","description":"Interactive Multimodal Feedback Loop is an agent that provides performance metrics in multiple domains.","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"CHAIN-OF-THOUGHT","type":"","description":"","source_id":"7c08d98f503d722d7de13be55375c8cb"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ADAS\">      <data key=\"d0\">RESEARCH AREA<\/data>      <data key=\"d1\">ADAS is a proposed research area focused on inventing novel building blocks and designing powerful agentic systems in an automated manner.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"AI-GENERATING ALGORITHMS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">AI-Generating Algorithms (AI-GAs) are methods that aim to learn components in AI systems to replace handcrafted ones, enhancing automation in AI development.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"AUTOML\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">AutoML refers to automated machine learning techniques that streamline the process of applying machine learning to real-world problems.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"META-LEARNING ARCHITECTURES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Meta-learning architectures are frameworks that enable systems to learn how to learn, improving efficiency and adaptability in AI models.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"META-LEARNING ALGORITHMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Meta-learning algorithms are techniques that allow models to improve their learning processes over time, enhancing their performance on various tasks.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"LEARNING ENVIRONMENTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Learning environments are settings or frameworks designed to facilitate the training and development of AI models.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Neural Architecture Search is a method for automating the design of neural network architectures to optimize performance.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"MAML\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">MAML (Model-Agnostic Meta-Learning) is a meta-learning algorithm that enables models to adapt quickly to new tasks with minimal data.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"META-RL\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Meta-RL (Meta-Reinforcement Learning) is a framework that focuses on improving the learning efficiency of reinforcement learning agents.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"POET\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">POET (Population-Based Open-Ended Learning) is a method for generating learning environments in an open-ended manner.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"OMNI-EPIC\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">OMNI-EPIC is a framework that enables the creation of robotics learning environments through automated programming.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"FOUNDATION MODELS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Foundation Models are large-scale models that serve as a base for various AI applications, often used for generating code and optimizing algorithms.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"FUNSEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">FunSearch is a method that utilizes Foundation Models to discover better optimization algorithms through automated coding.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"DISCOPOP\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">DiscoPOP is a framework where Foundation Models program loss functions for preference learning in alignment training.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"EUREKA\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Eureka is a method that allows Foundation Models to write reward functions for reinforcement learning applications.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"LANGUAGE-TO-REWARD\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Language-to-reward is a technique that enables Foundation Models to create reward functions for reinforcement learning in robotics.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"COT-SC\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">COT-SC is a manually designed agent that shows specific performance metrics in multi-task learning scenarios.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Self-Refine is a manually designed agent that exhibits performance metrics in tasks such as reading comprehension and science.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"LLM DEBATE\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">LLM Debate is a manually designed agent that performs across multiple tasks, including math and reading comprehension.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"STEP-BACK ABSTRACTION\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Step-back Abstraction is a manually designed agent that provides performance metrics in various learning tasks.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Quality-Diversity is a manually designed agent that demonstrates performance metrics across different tasks.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"ROLE ASSIGNMENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Role Assignment is a manually designed agent that shows performance metrics in multi-task learning scenarios.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"DYNAMIC ROLE-PLAYING ARCHITECTURE\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Dynamic Role-Playing Architecture is an agent that exhibits performance metrics when transferred from math to non-math domains.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"STRUCTURED MULTIMODAL FEEDBACK LOOP\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Structured Multimodal Feedback Loop is an agent that shows performance metrics across various tasks.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"INTERACTIVE MULTIMODAL FEEDBACK LOOP\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Interactive Multimodal Feedback Loop is an agent that provides performance metrics in multiple domains.<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <edge source=\"ADAS\" target=\"AI-GENERATING ALGORITHMS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">ADAS aims to invent novel building blocks that can be developed using AI-Generating Algorithms for automated systems.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"CHAIN-OF-THOUGHT\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Chain-of-Thought is an example of a manually designed agent that can be improved through the principles of ADAS.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"COT-SC\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">COT-SC is an example of a manually designed agent that can benefit from the innovations proposed in ADAS.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"SELF-REFINE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Self-Refine is a manually designed agent that can be enhanced by the methodologies proposed in ADAS.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"LLM DEBATE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">LLM Debate is a manually designed agent that can be optimized through the principles of ADAS.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"STEP-BACK ABSTRACTION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Step-back Abstraction is a manually designed agent that can be improved by the innovations proposed in ADAS.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"QUALITY-DIVERSITY\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Quality-Diversity is a manually designed agent that can benefit from the advancements proposed in ADAS.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"ROLE ASSIGNMENT\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Role Assignment is a manually designed agent that can be enhanced through the methodologies proposed in ADAS.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"DYNAMIC ROLE-PLAYING ARCHITECTURE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Dynamic Role-Playing Architecture is an agent that can be optimized through the principles of ADAS.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"STRUCTURED MULTIMODAL FEEDBACK LOOP\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Structured Multimodal Feedback Loop is an agent that can be improved by the innovations proposed in ADAS.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"INTERACTIVE MULTIMODAL FEEDBACK LOOP\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Interactive Multimodal Feedback Loop is an agent that can benefit from the advancements proposed in ADAS.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AI-GENERATING ALGORITHMS\" target=\"AUTOML\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AI-Generating Algorithms and AutoML both focus on automating components in AI systems to enhance efficiency and performance.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AI-GENERATING ALGORITHMS\" target=\"META-LEARNING ARCHITECTURES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AI-Generating Algorithms include meta-learning architectures as a key component for improving AI system design.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AI-GENERATING ALGORITHMS\" target=\"META-LEARNING ALGORITHMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AI-Generating Algorithms utilize meta-learning algorithms to enhance the learning capabilities of AI systems.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AI-GENERATING ALGORITHMS\" target=\"LEARNING ENVIRONMENTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">AI-Generating Algorithms aim to create effective learning environments for training AI models.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"META-LEARNING ARCHITECTURES\" target=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Neural Architecture Search is a specific application of meta-learning architectures aimed at optimizing neural networks.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"META-LEARNING ALGORITHMS\" target=\"MAML\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">MAML is a prominent example of a meta-learning algorithm that enhances learning efficiency across tasks.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"META-LEARNING ALGORITHMS\" target=\"META-RL\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta-RL is another example of a meta-learning algorithm that focuses on reinforcement learning efficiency.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"LEARNING ENVIRONMENTS\" target=\"POET\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">POET generates learning environments in an open-ended manner, contributing to the development of effective training settings.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"OMNI-EPIC\" target=\"FOUNDATION MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">OMNI-EPIC utilizes Foundation Models to automate the creation of robotics learning environments.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"FOUNDATION MODELS\" target=\"FUNSEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">FunSearch employs Foundation Models to discover better optimization algorithms through automated coding.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"FOUNDATION MODELS\" target=\"DISCOPOP\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">DiscoPOP uses Foundation Models to program loss functions for preference learning in AI training.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"FOUNDATION MODELS\" target=\"EUREKA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Eureka enables Foundation Models to write reward functions for reinforcement learning applications.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"FOUNDATION MODELS\" target=\"LANGUAGE-TO-REWARD\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Language-to-reward allows Foundation Models to create reward functions for robotics reinforcement learning.<\/data>      <data key=\"d5\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"dc55f071b95dec721a9820d39cdb3ccd","chunk":"a), FMs\nprogram the loss function for preference learning in FM alignment training (Rafailov et al., 2024).\nAdditionally, Eureka (Ma et al., 2023) and language-to-reward (Yu et al., 2023) enable FMs to write\nreward functions for reinforcement learning in robotics. Finally, OMNI-EPIC (Faldor et al., 2024)\nenables FMs to create robotics learning environments by programming in code. Here, we adopt a\nsimilar idea that enables FMs to program new agents in code.\nExisting Attempts to ADAS. There are two categories of works that can be considered attempts at\nADAS in the literature: those that learn better prompts only, and those that learn more components\nin agents than just prompts. Most works fall into the first category: learning prompts only. Works like\nOPRO (Yang et al., 2024), PromptBreeder (Fernando et al., 2024), and Self-Discover (Zhou et al.,\n2024a) adopt FMs to automate prompt engineering for agents, primarily focusing on the phrasing of\ninstructions in the prompt to enhance the reasoning capability of agents. Thus, the learned prompts\nare domain-specific and difficult to generalize. Beyond instructions, works like EvoAgent (Yuan\net al., 2024) and AgentVerse (Chen et al., 2023b) optimize role definition in the prompt, as assigning\npersonas or roles to agents has been shown to be beneficial (Xu et al., 2023). Although tuning prompts\neffectively improves performance, other important components in agentic systems remain fixed and\nhand-designed, vastly limiting the space of agents that can be discovered.\nThere are far fewer attempts in the second category, which involves learning more components\nthan just prompts in agentic systems. Most represent agents as networks or graphs in the search\nspace. In these formulations, the FM with a certain prompt is considered a transformation function\nfor text on nodes, and the information flow of the text is considered as edges. DyLAN (Liu et al.,\n11Automated Design of Agentic Systems\n2023) starts with a fully connected feed-forward network and uses FMs to score the response quality\nof nodes in each layer to prune the connections. DSPy (Khattab et al., 2024) first generates a set\nof possible nodes and then optimizes across the Cartesian product of these nodes while optimizing\nthe few-shot examples for nodes. GPT-Swarm (Zhuge et al., 2024) represents an agentic system in\na graph with a predefined set of nodes and uses a Reinforcement Learning algorithm to optimize\nthe possible connections between nodes while optimizing the prompt for each node in a separate\nstage. Although these works allow the learning of control flow (optimizing edges in networks or\ngraphs), many other components, such as whether and which tools to learn or even how many nodes\nto have, are still not learned, greatly limiting the space of agents that can be discovered. Besides\nlearning prompts and control flow, AgentOptimizer (Zhang et al., 2024b) learns the tools used in\nagents, and Agent Symbolic Learning (Zhou et al., 2024b) learns prompts, tools, and control flow\ntogether. While Agent Symbolic Learning shares similar motivations to learn more components in\nagents, it manually designs the search space for each component separately, which may make it a\nharder search space for search algorithms. In addition, it mainly improves agents based on an existing\ncomplex agent, without showing the emergence of new design patterns or building blocks. In contrast,\nour work represents all possible components in code, allowing the search to be easier by leveraging\nhuman efforts in the existing codebase of agents and FMs\u2019 expertise in coding. We also demonstrate\nhow novel and diverse building blocks and design patterns emerge from a set of basic agent designs,\nillustrating the potential creativity that can emerge from ADAS.\n6. Discussion and Conclusion\nSafety Considerations. We strongly advise researchers to be aware of the safety concerns\nwhen executing untrusted model-generated code in Meta Agent Search and other research\ninvolvingcodegeneration. While it is highly unlikely that model-generated code will perform overtly\nmaliciousactionsinourcurrentsettingsandwiththeFoundationModels(FMs)weuse, suchcodemay\nstill act destructively due to limitations in model capability or alignment (Chen et al., 2021; Rokon\net al., 2020). Ideally, sandbox environments can be used to safely run untrusted model-generated\ncode (Chen et al., 2021; Yee et al., 2010).\nMore broadly, research on more powerful AI systems raises the question of whether we should\nbe conducting research to advance AI capabilities at all. That topic clearly includes the proposed\nAutomatedDesignofAgenticSystems(ADAS)asanewareainAI-GAresearch, whichcouldpotentially\ncontribute to an even faster way to create Artificial General Intelligence (AGI) than the current manual\napproach (Clune, 2019). The question of whether and why we should pursue AGI and AI-GA has\nbeen discussed in many papers (Bengio et al., 2024; Bostrom, 2002; Clune, 2019; Ecoffet et al., 2020;\nYudkowsky et al., 2008), and is beyond the scope of this paper. Specifically as regards ADAS, we\nbelieve it is net beneficial to publish this work. First, this work demonstrates that with the available\nAPI access to powerful FMs, it is easy to program powerful ADAS algorithms, and do so without any\nexp","chunk_id":"dc55f071b95dec721a9820d39cdb3ccd","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"FOUNDATION MODELS (FMS)","type":"TECHNOLOGY","description":"Foundation Models (FMs) are advanced AI models that can be utilized for various tasks, including programming and reinforcement learning in robotics.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"EUREKA","type":"TECHNOLOGY","description":"Eureka is a method that enables Foundation Models to write reward functions for reinforcement learning applications in robotics.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"LANGUAGE-TO-REWARD","type":"TECHNOLOGY","description":"Language-to-reward is a technique that allows Foundation Models to create reward functions for reinforcement learning.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"OMNI-EPIC","type":"TECHNOLOGY","description":"OMNI-EPIC is a framework that enables Foundation Models to create robotics learning environments through programming.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"ADAS","type":"CONCEPT","description":"Automated Design of Agentic Systems (ADAS) refers to a new area in AI research focused on automating the design and optimization of agentic systems.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"OPRO","type":"TECHNOLOGY","description":"OPRO is a method that utilizes Foundation Models to automate prompt engineering for agents, focusing on enhancing reasoning capabilities.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"PROMPTBREEDER","type":"TECHNOLOGY","description":"PromptBreeder is a technique that employs Foundation Models to improve prompt engineering for agents.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"SELF-DISCOVER","type":"TECHNOLOGY","description":"Self-Discover is a method that uses Foundation Models to automate the creation of effective prompts for agents.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"EVOAGENT","type":"TECHNOLOGY","description":"EvoAgent is a framework that optimizes role definitions in prompts to enhance agent performance.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"AGENTVERSE","type":"TECHNOLOGY","description":"AgentVerse is a system that optimizes agent roles and definitions to improve performance in agentic systems.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"DYLAN","type":"TECHNOLOGY","description":"DyLAN is a method that uses Foundation Models to score response quality in agent networks and prune connections.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"DSPY","type":"TECHNOLOGY","description":"DSPy is a framework that generates and optimizes nodes in agentic systems using Foundation Models.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"GPT-SWARM","type":"TECHNOLOGY","description":"GPT-Swarm is a system that represents agentic systems in a graph format and optimizes connections using reinforcement learning.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"AGENTOPTIMIZER","type":"TECHNOLOGY","description":"AgentOptimizer is a method that learns the tools used in agentic systems to enhance their functionality.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"AGENT SYMBOLIC LEARNING","type":"TECHNOLOGY","description":"Agent Symbolic Learning is a framework that learns prompts, tools, and control flow in agentic systems.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"SAFETY CONSIDERATIONS","type":"CONCEPT","description":"Safety considerations refer to the precautions and awareness needed when executing untrusted model-generated code in AI research.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"ARTIFICIAL GENERAL INTELLIGENCE (AGI)","type":"CONCEPT","description":"Artificial General Intelligence (AGI) is a theoretical form of AI that possesses the ability to understand, learn, and apply intelligence across a wide range of tasks.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"YU ET AL. (2023)","type":"PERSON","description":"Yu et al. (2023) is a reference to a study that discusses language-to-reward techniques for Foundation Models.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"FALDOR ET AL. (2024)","type":"PERSON","description":"Faldor et al. (2024) is a reference to a study on OMNI-EPIC, which enables Foundation Models to create robotics learning environments.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"YANG ET AL. (2024)","type":"PERSON","description":"Yang et al. (2024) is a reference to a study discussing OPRO, which automates prompt engineering for agents.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"FERNANDO ET AL. (2024)","type":"PERSON","description":"Fernando et al. (2024) is a reference to a study on PromptBreeder, which focuses on improving prompt engineering.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"ZHOU ET AL. (2024)","type":"PERSON","description":"Zhou et al. (2024) is a reference to a study discussing Self-Discover, which automates prompt creation for agents.\nZhou et al. (2024) is a reference to a study on Agent Symbolic Learning, which learns prompts, tools, and control flow.","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"XU ET AL. (2023)","type":"PERSON","description":"Xu et al. (2023) is a reference to a study that highlights the benefits of assigning personas or roles to agents.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"KHATTAB ET AL. (2024)","type":"PERSON","description":"Khattab et al. (2024) is a reference to a study discussing DSPy and its optimization of nodes in agentic systems.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"ZHUGE ET AL. (2024)","type":"PERSON","description":"Zhuge et al. (2024) is a reference to a study on GPT-Swarm, which optimizes connections in agentic systems.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"ZHANG ET AL. (2024)","type":"PERSON","description":"Zhang et al. (2024) is a reference to a study on AgentOptimizer, which learns tools used in agents.","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"RAFAILOV ET AL. (2024)","type":"","description":"","source_id":"dc55f071b95dec721a9820d39cdb3ccd"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"FOUNDATION MODELS (FMS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundation Models (FMs) are advanced AI models that can be utilized for various tasks, including programming and reinforcement learning in robotics.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"EUREKA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Eureka is a method that enables Foundation Models to write reward functions for reinforcement learning applications in robotics.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"LANGUAGE-TO-REWARD\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language-to-reward is a technique that allows Foundation Models to create reward functions for reinforcement learning.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"OMNI-EPIC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">OMNI-EPIC is a framework that enables Foundation Models to create robotics learning environments through programming.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"ADAS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Automated Design of Agentic Systems (ADAS) refers to a new area in AI research focused on automating the design and optimization of agentic systems.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"OPRO\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">OPRO is a method that utilizes Foundation Models to automate prompt engineering for agents, focusing on enhancing reasoning capabilities.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"PROMPTBREEDER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">PromptBreeder is a technique that employs Foundation Models to improve prompt engineering for agents.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"SELF-DISCOVER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-Discover is a method that uses Foundation Models to automate the creation of effective prompts for agents.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"EVOAGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">EvoAgent is a framework that optimizes role definitions in prompts to enhance agent performance.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"AGENTVERSE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AgentVerse is a system that optimizes agent roles and definitions to improve performance in agentic systems.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"DYLAN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">DyLAN is a method that uses Foundation Models to score response quality in agent networks and prune connections.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"DSPY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">DSPy is a framework that generates and optimizes nodes in agentic systems using Foundation Models.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"GPT-SWARM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-Swarm is a system that represents agentic systems in a graph format and optimizes connections using reinforcement learning.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"AGENTOPTIMIZER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AgentOptimizer is a method that learns the tools used in agentic systems to enhance their functionality.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"AGENT SYMBOLIC LEARNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Agent Symbolic Learning is a framework that learns prompts, tools, and control flow in agentic systems.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"SAFETY CONSIDERATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Safety considerations refer to the precautions and awareness needed when executing untrusted model-generated code in AI research.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"ARTIFICIAL GENERAL INTELLIGENCE (AGI)\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Artificial General Intelligence (AGI) is a theoretical form of AI that possesses the ability to understand, learn, and apply intelligence across a wide range of tasks.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"YU ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu et al. (2023) is a reference to a study that discusses language-to-reward techniques for Foundation Models.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"FALDOR ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Faldor et al. (2024) is a reference to a study on OMNI-EPIC, which enables Foundation Models to create robotics learning environments.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"YANG ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang et al. (2024) is a reference to a study discussing OPRO, which automates prompt engineering for agents.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"FERNANDO ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fernando et al. (2024) is a reference to a study on PromptBreeder, which focuses on improving prompt engineering.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"ZHOU ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhou et al. (2024) is a reference to a study discussing Self-Discover, which automates prompt creation for agents.Zhou et al. (2024) is a reference to a study on Agent Symbolic Learning, which learns prompts, tools, and control flow.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XU ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu et al. (2023) is a reference to a study that highlights the benefits of assigning personas or roles to agents.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"KHATTAB ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Khattab et al. (2024) is a reference to a study discussing DSPy and its optimization of nodes in agentic systems.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"ZHUGE ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhuge et al. (2024) is a reference to a study on GPT-Swarm, which optimizes connections in agentic systems.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"ZHANG ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang et al. (2024) is a reference to a study on AgentOptimizer, which learns tools used in agents.<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"RAFAILOV ET AL. (2024)\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <edge source=\"FOUNDATION MODELS (FMS)\" target=\"EUREKA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Foundation Models are utilized in the Eureka method to write reward functions for reinforcement learning in robotics.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"FOUNDATION MODELS (FMS)\" target=\"LANGUAGE-TO-REWARD\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Foundation Models enable the language-to-reward technique to create reward functions for reinforcement learning.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"FOUNDATION MODELS (FMS)\" target=\"OMNI-EPIC\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Foundation Models are used in OMNI-EPIC to create robotics learning environments through programming.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"FOUNDATION MODELS (FMS)\" target=\"RAFAILOV ET AL. (2024)\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Rafailov et al. (2024) discusses the application of Foundation Models in preference learning for FM alignment training.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"FOUNDATION MODELS (FMS)\" target=\"YU ET AL. (2023)\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Yu et al. (2023) provides insights into how Foundation Models can be used to create reward functions for reinforcement learning.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"FOUNDATION MODELS (FMS)\" target=\"FALDOR ET AL. (2024)\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Faldor et al. (2024) discusses the role of Foundation Models in creating robotics learning environments through OMNI-EPIC.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"OPRO\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">ADAS involves the use of OPRO to automate prompt engineering for agents, enhancing their reasoning capabilities.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"PROMPTBREEDER\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">ADAS includes the use of PromptBreeder to improve prompt engineering for agents.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"SELF-DISCOVER\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">ADAS incorporates Self-Discover to automate the creation of effective prompts for agents.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"EVOAGENT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">EvoAgent is part of the ADAS framework, optimizing role definitions in prompts for agents.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"AGENTVERSE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">AgentVerse contributes to ADAS by optimizing agent roles and definitions to improve performance.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"DYLAN\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">DyLAN is utilized in ADAS to score response quality in agent networks and prune connections.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"DSPY\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">DSPy is part of ADAS, generating and optimizing nodes in agentic systems.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"GPT-SWARM\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">GPT-Swarm represents agentic systems in a graph format, optimizing connections as part of ADAS.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"AGENTOPTIMIZER\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">AgentOptimizer is a component of ADAS that learns the tools used in agentic systems.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"AGENT SYMBOLIC LEARNING\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Agent Symbolic Learning is related to ADAS as it learns prompts, tools, and control flow in agentic systems.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"SAFETY CONSIDERATIONS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Safety considerations are crucial in the context of ADAS, especially when executing untrusted model-generated code.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"ARTIFICIAL GENERAL INTELLIGENCE (AGI)\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">ADAS is a new area of research that could contribute to the development of Artificial General Intelligence (AGI).<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"YANG ET AL. (2024)\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Yang et al. (2024) contributes to the ADAS framework by discussing OPRO for automating prompt engineering.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"FERNANDO ET AL. (2024)\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Fernando et al. (2024) contributes to ADAS by discussing PromptBreeder for improving prompt engineering.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"ZHOU ET AL. (2024)\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Zhou et al. (2024) contributes to ADAS by discussing Self-Discover for automating prompt creation.Zhou et al. (2024) contributes to ADAS by discussing Agent Symbolic Learning and its learning of prompts, tools, and control flow.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"KHATTAB ET AL. (2024)\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Khattab et al. (2024) provides insights relevant to ADAS through the study of DSPy and its optimization of nodes.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"ZHUGE ET AL. (2024)\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Zhuge et al. (2024) contributes to ADAS by discussing GPT-Swarm and its optimization of connections in agentic systems.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"ZHANG ET AL. (2024)\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Zhang et al. (2024) contributes to ADAS by discussing AgentOptimizer and its role in learning tools for agents.<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"6bdf681c0bd9e401ac72344a6a0ae479","chunk":"; Bostrom, 2002; Clune, 2019; Ecoffet et al., 2020;\nYudkowsky et al., 2008), and is beyond the scope of this paper. Specifically as regards ADAS, we\nbelieve it is net beneficial to publish this work. First, this work demonstrates that with the available\nAPI access to powerful FMs, it is easy to program powerful ADAS algorithms, and do so without any\nexpensive hardware like GPUs. We feel it is beneficial to let the community know such algorithms are\npowerful and easy to create, so they can be informed and account for them. Moreover, by sharing this\ninformation, we hope to motivate follow-up work into safe-ADAS, such as algorithms that conduct\nADAS safely during both search itself (e.g. not risking running any harmful code) and that refuse\nto create dishonest, unhelpful, and\/or harmful agents. Such an open-source research approach to\ncreate safe-ADAS could be a better way to create safer AI systems (Caldwell, 2011; Meta, 2024). One\ndirection we find particularly promising is to simply ask the Meta Agent Search algorithm to be safe\nduring training and only create helpful, harmless, honest agents, potentially incorporating ideas such\nas Constitutional AI (Bai et al., 2022).\n12Automated Design of Agentic Systems\nFuture Work. Our work also opens up many future research directions. For example:\n\u2022Higher-order ADAS. Since the meta agent used in ADAS to program new agents in code is also\nan agent, ADAS can become self-referential where the meta agent can be improved through ADAS\nas well. It would be an exciting direction to have a higher order of meta-learning to allow the\nlearning of the meta agent and even the meta-meta agent, etc. (Lu et al., 2023)\n\u2022Seeding ADAS with more existing building blocks. Although we can theoretically allow any\ncomponents in agentic systems to be programmed from scratch in the code space, it is not efficient\nin practice. Therefore, it would be interesting to explore ADAS by standing on the shoulders of\nexisting human efforts, such as search engine tools, RAG (Lewis et al., 2020), or functions from\nexisting agent frameworks like LangChain (LangChainAI, 2022). Additionally, it is interesting\nto support multi-modal capabilities (e.g. vision) in FMs or allow different FMs to be available in\nagentic systems. This will enable the meta agent to choose from different FMs flexibly according to\nthe difficulty of the instruction and whether data privacy is a priority.\n\u2022Multi-objective ADAS. We only consider one objective (i.e., performance) to optimize in this\npaper, but in practice, multiple objectives are often considered, such as cost, latency, and robustness\nof agentic systems (Hu et al., 2021; Huang et al., 2023). Thus, integrating multi-objective search\nalgorithms (Deb et al., 2002) in ADAS could be promising.\n\u2022Novelty search algorithms. In Meta Agent Search, the design of the search algorithm is rel-\natively simple, focusing solely on exploring interesting new designs. A more careful design of\nthe search algorithm can be a promising future direction. For example, one could incorporate\nmore sophisticated ideas from Quality-Diversity (Cully & Demiris, 2017; Mouret & Clune, 2015),\nAI-generating (Clune, 2019), and Open-ended Algorithms (Faldor et al., 2024; Stanley & Lehman,\n2015; Stanley et al., 2019; Zhang et al., 2024a). One could also include more classic approaches\nto balance exploration and exploitation (Liu et al., 2024; Sutton & Barto, 2018).\n\u2022More intelligent evaluation functions. In this work, we simply evaluate discovered agents on the\nevaluation set and use the numerical performance results. However, this approach is both expensive\nand misses a lot of information. A promising future direction is to enable the meta agent to analyze\ndetailed running logs during the evaluation, which contain rich information on the failure and\nsuccess modes for better debugging and improving agentic systems (Zhou et al., 2024b). Also,\nmany tasks involve subjective answer evaluations (Chiang et al., 2024; Lu et al., 2024b) that do\nnot have ground-truth answers. It is also important to design novel evaluation functions in ADAS\nto address these tasks. Finally, in this work, we targeted only one domain during the search. It\nwould be interesting to explore whether ADAS algorithms can design even better generalist agents\nwhen specifically searching for agents capable of performing well across multiple domains.\n\u2022More complex domains. Additionally, we only evaluate Meta Agent Search on single-step QA\ntasks in this paper. It would be interesting to extend the method to more complex domians, such\nas real-world applications involving multi-step interaction with complex environments.\n\u2022Understanding the emergence of complexity from human organizations. Beyond potentially\nsaving researchers\u2019 efforts and improving upon the manual design of agentic systems, the research\nin ADAS is also scientifically intriguing as it sheds light on the origins of complexity emerging from\nhuman organization and society. The agentic system is a machine learning system that operates\nprimarily over natural language\u2014a representation that is interpretable to humans and used by\nhumans in constructing our organization and society. Thus, there is a close connection between\nagentic systems and human organizations, as shown in works incorporating the organizational\nstructure for human companies in agents (Hong et al., 2023) or","chunk_id":"6bdf681c0bd9e401ac72344a6a0ae479","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"ADAS","type":"TECHNOLOGY","description":"ADAS (Automated Design of Agentic Systems) refers to algorithms designed to create intelligent agents that can operate autonomously, often utilizing powerful foundational models (FMs).","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"FOUNDATIONAL MODELS","type":"TECHNOLOGY","description":"Foundational models are large-scale machine learning models that serve as the basis for developing various AI applications, including ADAS algorithms.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"META AGENT SEARCH","type":"TECHNIQUE","description":"Meta Agent Search is a method used within ADAS to program new agents by leveraging existing algorithms and frameworks.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"CONSTITUTIONAL AI","type":"TECHNIQUE","description":"Constitutional AI is an approach that aims to ensure AI systems operate safely and ethically by incorporating predefined principles during their development.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"LANGCHAIN","type":"FRAMEWORK","description":"LangChain is a framework that provides tools and components for building applications with language models, which can be utilized in ADAS.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"QUALITY-DIVERSITY","type":"TECHNIQUE","description":"Quality-Diversity is a concept in evolutionary algorithms that focuses on generating a diverse set of high-quality solutions.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"MULTI-OBJECTIVE SEARCH ALGORITHMS","type":"TECHNIQUE","description":"Multi-objective search algorithms are optimization techniques that consider multiple criteria simultaneously, such as performance, cost, and robustness.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"EVALUATION FUNCTIONS","type":"TECHNIQUE","description":"Evaluation functions are metrics or criteria used to assess the performance and effectiveness of agents developed through ADAS.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"HUMAN ORGANIZATIONS","type":"SOCIETY","description":"Human organizations refer to structured groups of individuals working together, which can influence the design and complexity of agentic systems.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"COMPLEX DOMAINS","type":"CONCEPT","description":"Complex domains refer to intricate environments or tasks that require advanced interaction and problem-solving capabilities from AI agents.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"CLUNE","type":"PERSON","description":"Clune is a researcher whose work is cited in discussions about AI algorithms and their implications for safety and effectiveness.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"ECOFFET ET AL.","type":"PERSON","description":"Ecoffet et al. is a reference to a study or work that contributes to the understanding of ADAS and its applications.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"YUDKOWSKY","type":"PERSON","description":"Yudkowsky is a researcher known for his contributions to AI safety and ethics, referenced in the context of ADAS.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"CALDWELL","type":"PERSON","description":"Caldwell is a researcher referenced in discussions about the benefits of open-source approaches to AI safety.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"LU ET AL.","type":"PERSON","description":"Lu et al. is a reference to a study that discusses evaluation functions in the context of ADAS.)<|COMPLETE|>\nLu et al. is a reference to a study that discusses higher-order ADAS and the potential for self-referential learning in agents.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"LEWIS ET AL.","type":"PERSON","description":"Lewis et al. is a reference to a study that discusses RAG (Retrieval-Augmented Generation) in the context of ADAS.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"HU ET AL.","type":"PERSON","description":"Hu et al. is a reference to a study that discusses multi-objective optimization in agentic systems.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"HUANG ET AL.","type":"PERSON","description":"Huang et al. is a reference to a study that discusses considerations for optimizing agentic systems.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"DEB ET AL.","type":"PERSON","description":"Deb et al. is a reference to a study that discusses multi-objective search algorithms relevant to ADAS.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"CULLY & DEMIRIS","type":"PERSON","description":"Cully & Demiris is a reference to a study that discusses Quality-Diversity in the context of AI algorithms.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"MOURET & CLUNE","type":"PERSON","description":"Mouret & Clune is a reference to a study that discusses concepts related to diversity in AI solutions.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"FALDOR ET AL.","type":"PERSON","description":"Faldor et al. is a reference to a study that discusses open-ended algorithms in AI development.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"STANLEY ET AL.","type":"PERSON","description":"Stanley et al. is a reference to a study that discusses the evolution of algorithms in AI.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"ZHOU ET AL.","type":"PERSON","description":"Zhou et al. is a reference to a study that discusses evaluation methods for agentic systems.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"CHIANG ET AL.","type":"PERSON","description":"Chiang et al. is a reference to a study that discusses subjective evaluation tasks in AI.","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ADAS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ADAS (Automated Design of Agentic Systems) refers to algorithms designed to create intelligent agents that can operate autonomously, often utilizing powerful foundational models (FMs).<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"FOUNDATIONAL MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundational models are large-scale machine learning models that serve as the basis for developing various AI applications, including ADAS algorithms.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Meta Agent Search is a method used within ADAS to program new agents by leveraging existing algorithms and frameworks.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"CONSTITUTIONAL AI\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Constitutional AI is an approach that aims to ensure AI systems operate safely and ethically by incorporating predefined principles during their development.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"LANGCHAIN\">      <data key=\"d0\">FRAMEWORK<\/data>      <data key=\"d1\">LangChain is a framework that provides tools and components for building applications with language models, which can be utilized in ADAS.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Quality-Diversity is a concept in evolutionary algorithms that focuses on generating a diverse set of high-quality solutions.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"MULTI-OBJECTIVE SEARCH ALGORITHMS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Multi-objective search algorithms are optimization techniques that consider multiple criteria simultaneously, such as performance, cost, and robustness.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"EVALUATION FUNCTIONS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Evaluation functions are metrics or criteria used to assess the performance and effectiveness of agents developed through ADAS.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"HUMAN ORGANIZATIONS\">      <data key=\"d0\">SOCIETY<\/data>      <data key=\"d1\">Human organizations refer to structured groups of individuals working together, which can influence the design and complexity of agentic systems.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"COMPLEX DOMAINS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Complex domains refer to intricate environments or tasks that require advanced interaction and problem-solving capabilities from AI agents.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clune is a researcher whose work is cited in discussions about AI algorithms and their implications for safety and effectiveness.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"ECOFFET ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ecoffet et al. is a reference to a study or work that contributes to the understanding of ADAS and its applications.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"YUDKOWSKY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yudkowsky is a researcher known for his contributions to AI safety and ethics, referenced in the context of ADAS.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"CALDWELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Caldwell is a researcher referenced in discussions about the benefits of open-source approaches to AI safety.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"LU ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lu et al. is a reference to a study that discusses evaluation functions in the context of ADAS.)&lt;|COMPLETE|&gt;Lu et al. is a reference to a study that discusses higher-order ADAS and the potential for self-referential learning in agents.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEWIS ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lewis et al. is a reference to a study that discusses RAG (Retrieval-Augmented Generation) in the context of ADAS.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"HU ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hu et al. is a reference to a study that discusses multi-objective optimization in agentic systems.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"HUANG ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Huang et al. is a reference to a study that discusses considerations for optimizing agentic systems.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"DEB ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Deb et al. is a reference to a study that discusses multi-objective search algorithms relevant to ADAS.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"CULLY &amp; DEMIRIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cully &amp; Demiris is a reference to a study that discusses Quality-Diversity in the context of AI algorithms.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"MOURET &amp; CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mouret &amp; Clune is a reference to a study that discusses concepts related to diversity in AI solutions.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"FALDOR ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Faldor et al. is a reference to a study that discusses open-ended algorithms in AI development.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"STANLEY ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Stanley et al. is a reference to a study that discusses the evolution of algorithms in AI.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"ZHOU ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhou et al. is a reference to a study that discusses evaluation methods for agentic systems.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"CHIANG ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chiang et al. is a reference to a study that discusses subjective evaluation tasks in AI.<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <edge source=\"ADAS\" target=\"FOUNDATIONAL MODELS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">ADAS utilizes foundational models to create powerful algorithms for developing intelligent agents without the need for expensive hardware.<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"META AGENT SEARCH\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Meta Agent Search is a technique employed within ADAS to program new agents effectively by leveraging existing algorithms.<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"CONSTITUTIONAL AI\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">ADAS aims to incorporate Constitutional AI principles to ensure the safe and ethical operation of the agents it develops.<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"LANGCHAIN\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LangChain provides essential tools that can enhance the development of ADAS by facilitating the integration of language models.<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"MULTI-OBJECTIVE SEARCH ALGORITHMS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Integrating multi-objective search algorithms into ADAS can improve its ability to optimize for various performance criteria simultaneously.<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"EVALUATION FUNCTIONS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Evaluation functions are critical for assessing the performance of agents created through ADAS, guiding improvements and refinements.<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"HUMAN ORGANIZATIONS\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Research in ADAS provides insights into how complexity arises from human organizations, influencing the design of agentic systems.<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"COMPLEX DOMAINS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Exploring complex domains can enhance the capabilities of agents developed through ADAS, allowing them to perform in more intricate environments.<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"7de66b94cf868b37b1df51dc545c415f","chunk":"AS is also scientifically intriguing as it sheds light on the origins of complexity emerging from\nhuman organization and society. The agentic system is a machine learning system that operates\nprimarily over natural language\u2014a representation that is interpretable to humans and used by\nhumans in constructing our organization and society. Thus, there is a close connection between\nagentic systems and human organizations, as shown in works incorporating the organizational\nstructure for human companies in agents (Hong et al., 2023) or simulating a human town with\nagents (Park et al., 2023). Therefore, the study in ADAS may enable us to observe how to create\na simple set of conditions and have an algorithm to bootstrap itself from simplicity to produce\ncomplexity in a system akin to human society.\n13Automated Design of Agentic Systems\n\u2022Towards a Better Understanding of FMs. Works from Neural Architecture Search (Huang et al.,\n2023) show that by observing the emerged architecture, we could gain more insights into Neural\nNetworks. In this paper, we also gained insights about FMs from the results. For example, the\nbest agent with GPT-3.5 involves a complex feedback mechanism, but when we transfer to other\nadvanced models, the agent with a simpler feedback mechanism but more refinement becomes a\nbetter agent (Section 4.3). This shows that GPT-3.5 may have a worse capability in evaluating and\nrefining the answers, so it needs a complex feedback mechanism for better refinement, while other\nadvanced models benefit more from a simpler feedback mechanism.\nConclusion. Inthispaper, weproposeanewresearchproblem, AutomatedDesignofAgenticSystems\n(ADAS), which aims to automatically invent novel building blocks and design powerful agentic systems .\nWe demonstrated that a promising approach to ADAS is to define agents in code, allowing new agents\nto be automatically discovered by a \u201cmeta\u201d agent programming them in code. Following this idea,\nwe propose Meta Agent Search, where the meta agent iteratively builds on previous discoveries\nto program interesting new agents. The experiments show that Meta Agent Search consistently\noutperforms state-of-the-art hand-designed agents across an extensive number of domains, and the\ndiscovered agents transfer well across models and domains. Overall, our work illustrates the potential\nof an exciting new research direction toward full automation in developing powerful agentic systems\nfrom the bottom up.\nAcknowledgments\nThis work was supported by the Vector Institute, the Canada CIFAR AI Chairs program, grants from\nSchmidt Futures and Open Philanthropy, an NSERC Discovery Grant, and a generous donation from\nRafael Cosman. We thank Jenny Zhang, Rach Pradhan, Ruiyu Gou, Nicholas Ioannidis, and Eunjeong\nHwang for insightful discussions and feedback.\n14Automated Design of Agentic Systems\nReferences\nAnthropic. Introducing the next generation of claude. https:\/\/www.anthropic.com\/news\/\nclaude-3-family , March 2024a. Blog post.\nAnthropic. Introducing claude 3.5 sonnet. https:\/\/www.anthropic.com\/news\/\nclaude-3-5-sonnet , June 2024b. Blog post.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness\nfrom ai feedback. arXiv preprint arXiv:2212.08073 , 2022.\nYoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval Noah\nHarari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, et al. Managing extreme ai risks amid rapid\nprogress. Science, 384(6698):842\u2013845, 2024.\nN Bostrom. Existential Risks: analyzing human extinction scenarios and related hazards. Journal of\nEvolution and Technology , 9, 2002.\nRobert S Boyer and J Strother Moore. A mechanical proof of the Turing completeness of pure LISP .\nCiteseer, 1983.\nTracey Caldwell. Ethical hackers: putting on the white hat. Network Security , 2011(7):10\u201313, 2011.\nHarrison Chase. What is an agent? https:\/\/blog.langchain.dev\/what-is-an-agent\/ , June\n2024. Blog post.\nBanghao Chen, Zhaofeng Zhang, Nicolas Langren\u00e9, and Shengxin Zhu. Unleashing the poten-\ntial of prompt engineering in large language models: a comprehensive review. arXiv preprint\narXiv:2310.14735 , 2023a.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374 , 2021.\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu,\nYi-Hsin Hung, Chen Qian, et al. Agentverse: Facilitating multi-agent collaboration and exploring\nemergent behaviors. In The Twelfth International Conference on Learning Representations , 2023b.\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angel","chunk_id":"7de66b94cf868b37b1df51dc545c415f","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"AGENTIC SYSTEM","type":"TECHNOLOGY","description":"An agentic system is a machine learning system that operates primarily over natural language, which is interpretable to humans and used in constructing human organization and society.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"HUMAN ORGANIZATION","type":"SOCIETY","description":"Human organization refers to the structured ways in which humans interact and collaborate, forming societies and communities.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"ADAS","type":"RESEARCH PROBLEM","description":"Automated Design of Agentic Systems (ADAS) is a research problem focused on automatically inventing novel building blocks and designing powerful agentic systems.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"META AGENT SEARCH","type":"TECHNIQUE","description":"Meta Agent Search is an approach where a meta agent iteratively builds on previous discoveries to program new agents automatically.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a language model that requires a complex feedback mechanism for better refinement of answers compared to other advanced models.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"HONG ET AL. (2023)","type":"PERSON","description":"Hong et al. (2023) is a reference to a study that incorporates organizational structures for human companies in agents.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"PARK ET AL. (2023)","type":"PERSON","description":"Park et al. (2023) is a reference to a study that simulates a human town with agents.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"NEURAL ARCHITECTURE SEARCH","type":"TECHNIQUE","description":"Neural Architecture Search is a method that explores the architecture of neural networks to gain insights into their performance.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"ANTHROPIC","type":"ORGANIZATION","description":"Anthropic is an organization known for developing AI technologies, including the Claude series of models.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"CLAUDE 3","type":"TECHNOLOGY","description":"Claude 3 is a next-generation AI model developed by Anthropic.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"CLAUDE 3.5 SONNET","type":"TECHNOLOGY","description":"Claude 3.5 Sonnet is an iteration of the Claude model developed by Anthropic.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"YUNTAO BAI ET AL. (2022)","type":"PERSON","description":"Yuntao Bai et al. (2022) is a reference to a study discussing AI feedback for harmlessness.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"YOSHUA BENGIO ET AL. (2024)","type":"PERSON","description":"Yoshua Bengio et al. (2024) is a reference to a study on managing extreme AI risks.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"N BOSTROM","type":"PERSON","description":"N Bostrom is a researcher known for analyzing human extinction scenarios and related hazards.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"ROBERT S BOYER AND J STROTHER MOORE","type":"PERSON","description":"Robert S Boyer and J Strother Moore are known for their work on the Turing completeness of pure LISP.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"TRACEY CALDWELL","type":"PERSON","description":"Tracey Caldwell is a researcher discussing ethical hacking.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"HARRISON CHASE","type":"PERSON","description":"Harrison Chase is a researcher who explores the concept of agents in AI.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"BANGHAO CHEN ET AL. (2023)","type":"PERSON","description":"Banghao Chen et al. (2023) is a reference to a comprehensive review on prompt engineering in large language models.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"MARK CHEN ET AL. (2021)","type":"PERSON","description":"Mark Chen et al. (2021) is a reference to a study evaluating large language models trained on code.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"WEIZE CHEN ET AL. (2023)","type":"PERSON","description":"Weize Chen et al. (2023) is a reference to a study on facilitating multi-agent collaboration.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"FEEDBACK MECHANISM","type":"TECHNIQUE","description":"A feedback mechanism is a process that uses the outcomes of actions to adjust future actions, crucial for refining the performance of AI models.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"COMPLEXITY","type":"CONCEPT","description":"Complexity refers to the intricate and interconnected nature of systems, often emerging from simple rules or interactions in human organizations and societies.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"SOCIETY","type":"CONCEPT","description":"Society is a structured community of individuals who interact and form relationships, often studied in the context of human organization and behavior.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"AI RISKS","type":"CONCEPT","description":"AI risks refer to potential dangers and ethical concerns associated with the development and deployment of artificial intelligence technologies.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"STATE-OF-THE-ART AGENTS","type":"TECHNOLOGY","description":"State-of-the-art agents are advanced AI systems designed with the latest techniques and methodologies to perform optimally across various tasks.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"INSIGHTS","type":"CONCEPT","description":"Insights are valuable understandings or discoveries gained from research or analysis, often leading to advancements in technology or methodology.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"ORGANIZATIONAL STRUCTURE","type":"CONCEPT","description":"Organizational structure refers to the way in which the components of an organization are arranged, influencing how information flows and decisions are made.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"AUTOMATION","type":"CONCEPT","description":"Automation is the use of technology to perform tasks with minimal human intervention, often aimed at increasing efficiency and reducing errors.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"PROGRAMMING","type":"TECHNIQUE","description":"Programming is the process of designing and building executable computer software to accomplish specific tasks or solve problems.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"RESEARCH DIRECTION","type":"CONCEPT","description":"Research direction refers to the focus or path of inquiry in a particular field, guiding the development of new theories, technologies, or methodologies.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"VECTOR INSTITUTE","type":"ORGANIZATION","description":"The Vector Institute is a research organization focused on advancing artificial intelligence and machine learning through collaboration and innovation.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"CANADA CIFAR AI CHAIRS","type":"PROGRAM","description":"The Canada CIFAR AI Chairs program supports research in artificial intelligence by funding leading researchers in the field.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"SCHMIDT FUTURES","type":"ORGANIZATION","description":"Schmidt Futures is an organization that funds initiatives aimed at advancing science and technology for the public good.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"OPEN PHILANTHROPY","type":"ORGANIZATION","description":"Open Philanthropy is a philanthropic organization that aims to promote effective altruism and support high-impact charitable initiatives.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"NSERC DISCOVERY GRANT","type":"FUNDING","description":"The NSERC Discovery Grant is a funding program in Canada that supports research in natural sciences and engineering.","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"RAFAEL COSMAN","type":"PERSON","description":"Rafael Cosman is a donor who contributed to the research efforts discussed in the paper.)<|COMPLETE|>","source_id":"7de66b94cf868b37b1df51dc545c415f"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AGENTIC SYSTEM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">An agentic system is a machine learning system that operates primarily over natural language, which is interpretable to humans and used in constructing human organization and society.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"HUMAN ORGANIZATION\">      <data key=\"d0\">SOCIETY<\/data>      <data key=\"d1\">Human organization refers to the structured ways in which humans interact and collaborate, forming societies and communities.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"ADAS\">      <data key=\"d0\">RESEARCH PROBLEM<\/data>      <data key=\"d1\">Automated Design of Agentic Systems (ADAS) is a research problem focused on automatically inventing novel building blocks and designing powerful agentic systems.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Meta Agent Search is an approach where a meta agent iteratively builds on previous discoveries to program new agents automatically.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model that requires a complex feedback mechanism for better refinement of answers compared to other advanced models.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"HONG ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hong et al. (2023) is a reference to a study that incorporates organizational structures for human companies in agents.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"PARK ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Park et al. (2023) is a reference to a study that simulates a human town with agents.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Neural Architecture Search is a method that explores the architecture of neural networks to gain insights into their performance.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"ANTHROPIC\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Anthropic is an organization known for developing AI technologies, including the Claude series of models.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"CLAUDE 3\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Claude 3 is a next-generation AI model developed by Anthropic.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"CLAUDE 3.5 SONNET\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Claude 3.5 Sonnet is an iteration of the Claude model developed by Anthropic.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"YUNTAO BAI ET AL. (2022)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuntao Bai et al. (2022) is a reference to a study discussing AI feedback for harmlessness.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"YOSHUA BENGIO ET AL. (2024)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yoshua Bengio et al. (2024) is a reference to a study on managing extreme AI risks.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"N BOSTROM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">N Bostrom is a researcher known for analyzing human extinction scenarios and related hazards.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"ROBERT S BOYER AND J STROTHER MOORE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Robert S Boyer and J Strother Moore are known for their work on the Turing completeness of pure LISP.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"TRACEY CALDWELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tracey Caldwell is a researcher discussing ethical hacking.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"HARRISON CHASE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Harrison Chase is a researcher who explores the concept of agents in AI.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"BANGHAO CHEN ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Banghao Chen et al. (2023) is a reference to a comprehensive review on prompt engineering in large language models.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"MARK CHEN ET AL. (2021)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mark Chen et al. (2021) is a reference to a study evaluating large language models trained on code.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"WEIZE CHEN ET AL. (2023)\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weize Chen et al. (2023) is a reference to a study on facilitating multi-agent collaboration.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"FEEDBACK MECHANISM\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A feedback mechanism is a process that uses the outcomes of actions to adjust future actions, crucial for refining the performance of AI models.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"COMPLEXITY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Complexity refers to the intricate and interconnected nature of systems, often emerging from simple rules or interactions in human organizations and societies.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"SOCIETY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Society is a structured community of individuals who interact and form relationships, often studied in the context of human organization and behavior.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"AI RISKS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI risks refer to potential dangers and ethical concerns associated with the development and deployment of artificial intelligence technologies.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"STATE-OF-THE-ART AGENTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">State-of-the-art agents are advanced AI systems designed with the latest techniques and methodologies to perform optimally across various tasks.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"INSIGHTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Insights are valuable understandings or discoveries gained from research or analysis, often leading to advancements in technology or methodology.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"ORGANIZATIONAL STRUCTURE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Organizational structure refers to the way in which the components of an organization are arranged, influencing how information flows and decisions are made.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"AUTOMATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Automation is the use of technology to perform tasks with minimal human intervention, often aimed at increasing efficiency and reducing errors.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"PROGRAMMING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Programming is the process of designing and building executable computer software to accomplish specific tasks or solve problems.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"RESEARCH DIRECTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Research direction refers to the focus or path of inquiry in a particular field, guiding the development of new theories, technologies, or methodologies.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"VECTOR INSTITUTE\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">The Vector Institute is a research organization focused on advancing artificial intelligence and machine learning through collaboration and innovation.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"CANADA CIFAR AI CHAIRS\">      <data key=\"d0\">PROGRAM<\/data>      <data key=\"d1\">The Canada CIFAR AI Chairs program supports research in artificial intelligence by funding leading researchers in the field.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"SCHMIDT FUTURES\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Schmidt Futures is an organization that funds initiatives aimed at advancing science and technology for the public good.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"OPEN PHILANTHROPY\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Open Philanthropy is a philanthropic organization that aims to promote effective altruism and support high-impact charitable initiatives.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"NSERC DISCOVERY GRANT\">      <data key=\"d0\">FUNDING<\/data>      <data key=\"d1\">The NSERC Discovery Grant is a funding program in Canada that supports research in natural sciences and engineering.<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"RAFAEL COSMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rafael Cosman is a donor who contributed to the research efforts discussed in the paper.)&lt;|COMPLETE|&gt;<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <edge source=\"AGENTIC SYSTEM\" target=\"HUMAN ORGANIZATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Agentic systems are closely connected to human organizations as they simulate and model human interactions and structures.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"AGENTIC SYSTEM\" target=\"HONG ET AL. (2023)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Hong et al. (2023) discusses the incorporation of organizational structures in agentic systems, linking them to human organizations.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"AGENTIC SYSTEM\" target=\"PARK ET AL. (2023)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Park et al. (2023) simulates a human town with agents, illustrating the application of agentic systems in modeling human society.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"AGENTIC SYSTEM\" target=\"HARRISON CHASE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Harrison Chase's exploration of agents contributes to the understanding of agentic systems in AI.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"META AGENT SEARCH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">ADAS proposes the use of Meta Agent Search to automatically discover new agents through iterative programming.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"GPT-3.5\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The study of ADAS involves insights gained from the performance of GPT-3.5 in refining answers through feedback mechanisms.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Neural Architecture Search provides insights that can enhance the design and understanding of agentic systems in ADAS.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"YUNTAO BAI ET AL. (2022)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Yuntao Bai et al. (2022) provides insights relevant to the development of agentic systems in the context of AI feedback.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"YOSHUA BENGIO ET AL. (2024)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Yoshua Bengio et al. (2024) discusses risks that may inform the design of agentic systems in ADAS.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"N BOSTROM\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">N Bostrom's analysis of existential risks can provide a framework for understanding the implications of agentic systems in society.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"TRACEY CALDWELL\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Tracey Caldwell's work on ethical hacking may inform the ethical considerations in the design of agentic systems.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"BANGHAO CHEN ET AL. (2023)\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Banghao Chen et al. (2023) provides insights into prompt engineering that can enhance the design of agentic systems in ADAS.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"MARK CHEN ET AL. (2021)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Mark Chen et al. (2021) offers evaluations that may inform the development of agentic systems in ADAS.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"WEIZE CHEN ET AL. (2023)\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Weize Chen et al. (2023) discusses multi-agent collaboration, relevant to the design of agentic systems in ADAS.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ANTHROPIC\" target=\"CLAUDE 3\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Anthropic developed Claude 3, a significant advancement in AI technology.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ANTHROPIC\" target=\"CLAUDE 3.5 SONNET\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Anthropic developed Claude 3.5 Sonnet, an iteration of their AI model series.<\/data>      <data key=\"d5\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"022e7927d281e80e188f29ea343cc115","chunk":" Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu,\nYi-Hsin Hung, Chen Qian, et al. Agentverse: Facilitating multi-agent collaboration and exploring\nemergent behaviors. In The Twelfth International Conference on Learning Representations , 2023b.\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li,\nHao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An\nopen platform for evaluating llms by human preference, 2024.\nFran\u00e7ois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547 , 2019.\nJeff Clune. Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artificial\nintelligence. arXiv preprint arXiv:1905.10985 , 2019.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word\nproblems. arXiv preprint arXiv:2110.14168 , 2021.\nAntoineCullyandYiannisDemiris. Qualityanddiversityoptimization: Aunifyingmodularframework.\nIEEE Transactions on Evolutionary Computation , 22(2):245\u2013259, 2017.\n15Automated Design of Agentic Systems\nN. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In 2005 IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition (CVPR\u201905) , volume 1, pp. 886\u2013893\nvol. 1, 2005. doi: 10.1109\/CVPR.2005.177.\nKalyanmoyDeb, AmritPratap, SameerAgarwal, andTAMTMeyarivan. Afastandelitistmultiobjective\ngenetic algorithm: Nsga-ii. IEEE transactions on evolutionary computation , 6(2):182\u2013197, 2002.\nAaron Dharna, Julian Togelius, and Lisa B Soros. Co-generation of game levels and game-playing\nagents. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Enter-\ntainment , volume 16, pp. 203\u2013209, 2020.\nYilun Du, Shuang Li, Antonio Torralba, Joshua BTenenbaum, and Igor Mordatch. Improving factuality\nand reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325 ,\n2023.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.\nDROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Jill\nBurstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers) , pp. 2368\u20132378, Minneapolis, Minnesota, June 2019. Association\nfor Computational Linguistics. doi: 10.18653\/v1\/N19-1246.\nYan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. RL^2: Fast\nreinforcement learning via slow reinforcement learning. In International Conference on Learning\nRepresentations , 2017.\nAdrien Ecoffet, Jeff Clune, and Joel Lehman. Open questions in creating safe open-ended AI: Tensions\nbetween control and creativity. In Conference on Artificial Life , pp. 27\u201335. MIT Press, 2020.\nThomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. Journal\nof Machine Learning Research , 20(55):1\u201321, 2019.\nMaxence Faldor, Jenny Zhang, Antoine Cully, and Jeff Clune. Omni-epic: Open-endedness via\nmodels of human notions of interestingness with environments programmed in code. arXiv preprint\narXiv:2405.15568 , 2024.\nChrisantha Fernando, Dylan Sunil Banarse, Henryk Michalewski, Simon Osindero, and Tim Rock-\nt\u00e4schel. Promptbreeder: Self-referential self-improvement via prompt evolution, 2024.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. In International conference on machine learning , pp. 1126\u20131135. PMLR, 2017.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. In International Conference on Machine\nLearning , pp. 10764\u201310799. PMLR, 2023.\nRyan Greenblatt. Getting 50% sota on arc-agi with gpt-4. https:\/\/redwoodresearch.substack.\ncom\/p\/getting-50-sota-on-arc-agi-with-gpt , July 202","chunk_id":"022e7927d281e80e188f29ea343cc115","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"YUSHENG CHEN","type":"PERSON","description":"Yusheng Chen is a researcher involved in the study of multi-agent collaboration and emergent behaviors in AI systems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"SU JINGWEI","type":"PERSON","description":"Su Jingwei is a researcher contributing to the exploration of multi-agent collaboration in AI systems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"ZUO CHENG","type":"PERSON","description":"Zuo Cheng is a researcher focused on multi-agent collaboration and emergent behaviors in AI systems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"CHENFEI YUAN","type":"PERSON","description":"Chenfei Yuan is a researcher involved in the study of multi-agent collaboration in AI systems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"CHI-MIN CHAN","type":"PERSON","description":"Chi-Min Chan is a researcher contributing to the exploration of emergent behaviors in AI systems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"HEYANG YU","type":"PERSON","description":"Heyang Yu is a researcher focused on multi-agent collaboration in AI systems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YAXI LU","type":"PERSON","description":"Yaxi Lu is a researcher involved in the study of emergent behaviors in AI systems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YI-HSIN HUNG","type":"PERSON","description":"Yi-Hsin Hung is a researcher contributing to the exploration of multi-agent collaboration in AI systems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"CHEN QIAN","type":"PERSON","description":"Chen Qian is a researcher focused on multi-agent collaboration and emergent behaviors in AI systems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"AGENTVERSE","type":"RESEARCH","description":"Agentverse is a framework designed to facilitate multi-agent collaboration and explore emergent behaviors in AI systems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"THE TWELFTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS","type":"EVENT","description":"The Twelfth International Conference on Learning Representations is a conference where research on learning representations, including multi-agent systems, is presented.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"WEI-LIN CHIANG","type":"PERSON","description":"Wei-Lin Chiang is a researcher involved in creating an open platform for evaluating language models based on human preference.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"LIANMIN ZHENG","type":"PERSON","description":"Lianmin Zheng is a researcher contributing to the development of an open platform for evaluating language models.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YING SHENG","type":"PERSON","description":"Ying Sheng is a researcher focused on evaluating language models through human preference.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"ANASTASIOS NIKOLAS ANGELOPOULOS","type":"PERSON","description":"Anastasios Nikolas Angelopoulos is a researcher involved in the evaluation of language models.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"TIANLE LI","type":"PERSON","description":"Tianle Li is a researcher contributing to the development of an open platform for evaluating language models.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"DACHENG LI","type":"PERSON","description":"Dacheng Li is a researcher focused on evaluating language models through human preference.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"HAO ZHANG","type":"PERSON","description":"Hao Zhang is a researcher involved in the evaluation of language models.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"BANGHUA ZHU","type":"PERSON","description":"Banghua Zhu is a researcher contributing to the development of an open platform for evaluating language models.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"MICHAEL JORDAN","type":"PERSON","description":"Michael Jordan is a prominent researcher in machine learning and AI, contributing to the evaluation of language models.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JOSEPH E. GONZALEZ","type":"PERSON","description":"Joseph E. Gonzalez is a researcher involved in the evaluation of language models.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"ION STOICA","type":"PERSON","description":"Ion Stoica is a researcher contributing to the development of an open platform for evaluating language models.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"CHATBOT ARENA","type":"PLATFORM","description":"Chatbot Arena is an open platform designed for evaluating language models based on human preferences.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"FRAN\u00c7OIS CHOLLET","type":"PERSON","description":"Fran\u00e7ois Chollet is a researcher known for his work on the measure of intelligence in AI systems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is a researcher focused on AI-generating algorithms and their implications for general artificial intelligence.\nJeff Clune is a researcher focused on the challenges of creating safe open-ended AI.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"AI-GAS","type":"RESEARCH","description":"AI-GAS refers to AI-generating algorithms, a paradigm for producing general artificial intelligence.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"KARL COBBE","type":"PERSON","description":"Karl Cobbe is a researcher involved in training verifiers to solve math word problems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"VINEET KOSARAJU","type":"PERSON","description":"Vineet Kosaraju is a researcher contributing to the training of verifiers for solving math word problems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"MOHAMMAD BAVARIAN","type":"PERSON","description":"Mohammad Bavarian is a researcher focused on training verifiers for math word problems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"MARK CHEN","type":"PERSON","description":"Mark Chen is a researcher involved in the training of verifiers for solving math word problems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"HEEWOO JUN","type":"PERSON","description":"Heewoo Jun is a researcher contributing to the training of verifiers for math word problems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"LUKASZ KAISER","type":"PERSON","description":"Lukasz Kaiser is a researcher focused on training verifiers for solving math word problems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"MATTHIAS PLAPPERT","type":"PERSON","description":"Matthias Plappert is a researcher involved in the training of verifiers for math word problems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JERRY TWOREK","type":"PERSON","description":"Jerry Tworek is a researcher contributing to the training of verifiers for solving math word problems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JACOB HILTON","type":"PERSON","description":"Jacob Hilton is a researcher focused on training verifiers for math word problems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"REIICHIRO NAKANO","type":"PERSON","description":"Reiichiro Nakano is a researcher involved in the training of verifiers for solving math word problems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"KALYANMOY DEB","type":"PERSON","description":"Kalyanmoy Deb is a researcher known for his work on multi-objective genetic algorithms.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"AMRIT PRATAP","type":"PERSON","description":"Amrit Pratap is a researcher contributing to the development of multi-objective genetic algorithms.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"SAMEER AGARWAL","type":"PERSON","description":"Sameer Agarwal is a researcher focused on multi-objective genetic algorithms.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"TAMT MEYARIVAN","type":"PERSON","description":"Tamt Meyarivan is a researcher involved in the development of multi-objective genetic algorithms.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"NSGA-II","type":"ALGORITHM","description":"NSGA-II is a fast and elitist multi-objective genetic algorithm used for optimization problems.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"AARON DHARNA","type":"PERSON","description":"Aaron Dharna is a researcher involved in the co-generation of game levels and game-playing agents.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JULIAN TOGELIUS","type":"PERSON","description":"Julian Togelius is a researcher focused on the co-generation of game levels and agents.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"LISA B SOROS","type":"PERSON","description":"Lisa B Soros is a researcher contributing to the co-generation of game levels and game-playing agents.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YILUN DU","type":"PERSON","description":"Yilun Du is a researcher involved in improving factuality and reasoning in language models through multi-agent debate.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"SHUANG LI","type":"PERSON","description":"Shuang Li is a researcher focused on enhancing factuality and reasoning in language models.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"ANTONIO TORRALBA","type":"PERSON","description":"Antonio Torralba is a researcher contributing to the improvement of factuality in language models.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JOSHUA B TENENBAUM","type":"PERSON","description":"Joshua B Tenenbaum is a researcher involved in enhancing reasoning in language models.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"IGOR MORDATCH","type":"PERSON","description":"Igor Mordatch is a researcher focused on improving factuality and reasoning in language models.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"DHEERU DUA","type":"PERSON","description":"Dheeru Dua is a researcher involved in the development of a reading comprehension benchmark requiring discrete reasoning.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YIZHONG WANG","type":"PERSON","description":"Yizhong Wang is a researcher contributing to the development of a reading comprehension benchmark.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"PRADEEP DASIGI","type":"PERSON","description":"Pradeep Dasigi is a researcher focused on creating a reading comprehension benchmark.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"GABRIEL STANOVSKY","type":"PERSON","description":"Gabriel Stanovsky is a researcher involved in the development of a reading comprehension benchmark.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"SAMEER SINGH","type":"PERSON","description":"Sameer Singh is a researcher contributing to the creation of a reading comprehension benchmark.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"MATT GARDNER","type":"PERSON","description":"Matt Gardner is a researcher focused on developing a reading comprehension benchmark.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"DROP","type":"BENCHMARK","description":"DROP is a reading comprehension benchmark that requires discrete reasoning over paragraphs.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YAN DUAN","type":"PERSON","description":"Yan Duan is a researcher involved in fast reinforcement learning techniques.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JOHN SCHULMAN","type":"PERSON","description":"John Schulman is a researcher contributing to fast reinforcement learning methods.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"XI CHEN","type":"PERSON","description":"Xi Chen is a researcher focused on reinforcement learning techniques.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"PETER L BARTLETT","type":"PERSON","description":"Peter L Bartlett is a researcher involved in reinforcement learning research.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"ILYA SUTSKEVER","type":"PERSON","description":"Ilya Sutskever is a prominent researcher in the field of reinforcement learning.","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"PIETER ABBEEL","type":"PERSON","description":"Pieter Abbeel is a researcher focused on meta-learning techniques.\nPieter Abbeel is a researcher focused on reinforcement learning techniques.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"RL^2","type":"TECHNIQUE","description":"RL^2 is a method for fast reinforcement learning via slow reinforcement learning techniques.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"TECHNIQUE"},{"name":"ADRIEN ECOFFET","type":"PERSON","description":"Adrien Ecoffet is a researcher involved in creating safe open-ended AI systems.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"JOEL LEHMAN","type":"PERSON","description":"Joel Lehman is a researcher contributing to the study of safe open-ended AI systems.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"OPEN QUESTIONS IN CREATING SAFE OPEN-ENDED AI","type":"RESEARCH","description":"Open questions in creating safe open-ended AI address the tensions between control and creativity in AI systems.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"RESEARCH"},{"name":"THOMAS ELSKEN","type":"PERSON","description":"Thomas Elsken is a researcher known for his work on neural architecture search.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"JAN HENDRIK METZEN","type":"PERSON","description":"Jan Hendrik Metzen is a researcher contributing to neural architecture search.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"FRANK HUTTER","type":"PERSON","description":"Frank Hutter is a researcher focused on neural architecture search techniques.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"NEURAL ARCHITECTURE SEARCH","type":"TECHNIQUE","description":"Neural architecture search is a method for automating the design of neural networks.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"TECHNIQUE"},{"name":"MAXENCE FALDOR","type":"PERSON","description":"Maxence Faldor is a researcher involved in open-endedness in AI systems.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"JENNY ZHANG","type":"PERSON","description":"Jenny Zhang is a researcher contributing to the study of open-endedness in AI.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"ANTOINE CULLY","type":"PERSON","description":"Antoine Cully is a researcher focused on open-endedness in AI systems.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"OMNI-EPIC","type":"RESEARCH","description":"Omni-epic is a framework for achieving open-endedness in AI through human notions of interestingness.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"RESEARCH"},{"name":"CHRISANTHA FERNANDO","type":"PERSON","description":"Chrisantha Fernando is a researcher involved in self-referential self-improvement in AI.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"DYLAN SUNIL BANARSE","type":"PERSON","description":"Dylan Sunil Banarse is a researcher contributing to self-referential self-improvement in AI.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"HENRYK MICHALIEWSKI","type":"PERSON","description":"Henryk Michaliewski is a researcher focused on self-referential self-improvement in AI.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"SIMON OSINDERO","type":"PERSON","description":"Simon Osindero is a researcher involved in self-referential self-improvement in AI.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"TIM ROCKT\u00c4SCHEL","type":"PERSON","description":"Tim Rockt\u00e4schel is a researcher contributing to self-referential self-improvement in AI.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"CHELSEA FINN","type":"PERSON","description":"Chelsea Finn is a researcher known for her work on model-agnostic meta-learning.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"SERGEY LEVINE","type":"PERSON","description":"Sergey Levine is a researcher involved in model-agnostic meta-learning.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"MODEL-AGNOSTIC META-LEARNING","type":"TECHNIQUE","description":"Model-agnostic meta-learning is a method for fast adaptation of deep networks.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"TECHNIQUE"},{"name":"LUYU GAO","type":"PERSON","description":"Luyu Gao is a researcher contributing to program-aided language models.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"AMAN MADAAN","type":"PERSON","description":"Aman Madaan is a researcher focused on program-aided language models.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"SHUYAN ZHOU","type":"PERSON","description":"Shuyan Zhou is a researcher involved in program-aided language models.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"URI ALON","type":"PERSON","description":"Uri Alon is a researcher contributing to program-aided language models.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"PENGFEI LIU","type":"PERSON","description":"Pengfei Liu is a researcher focused on program-aided language models.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"YIMING YANG","type":"PERSON","description":"Yiming Yang is a researcher involved in program-aided language models.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"JAMIE CALLAN","type":"PERSON","description":"Jamie Callan is a researcher contributing to program-aided language models.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"GRAHAM NEUBIG","type":"PERSON","description":"Graham Neubig is a researcher focused on program-aided language models.","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"PERSON"},{"name":"GETTING 50% SOTA ON ARC-AGI WITH GPT-4","type":"RESEARCH","description":"Getting 50% SOTA on ARC-AGI with GPT-4 is a study focused on achieving state-of-the-art performance using the GPT-4 model.\nGetting 50% SOTA on ARC-AGI with GPT-4 is a study focused on achieving state-of-the-art performance using the GPT-4 model.)<|COMPLETE|>","source_id":"022e7927d281e80e188f29ea343cc115","entity_type":"RESEARCH"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"YUSHENG CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yusheng Chen is a researcher involved in the study of multi-agent collaboration and emergent behaviors in AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"SU JINGWEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Su Jingwei is a researcher contributing to the exploration of multi-agent collaboration in AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"ZUO CHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zuo Cheng is a researcher focused on multi-agent collaboration and emergent behaviors in AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"CHENFEI YUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chenfei Yuan is a researcher involved in the study of multi-agent collaboration in AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"CHI-MIN CHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chi-Min Chan is a researcher contributing to the exploration of emergent behaviors in AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"HEYANG YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heyang Yu is a researcher focused on multi-agent collaboration in AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YAXI LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yaxi Lu is a researcher involved in the study of emergent behaviors in AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YI-HSIN HUNG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi-Hsin Hung is a researcher contributing to the exploration of multi-agent collaboration in AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"CHEN QIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Qian is a researcher focused on multi-agent collaboration and emergent behaviors in AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"AGENTVERSE\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Agentverse is a framework designed to facilitate multi-agent collaboration and explore emergent behaviors in AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"THE TWELFTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The Twelfth International Conference on Learning Representations is a conference where research on learning representations, including multi-agent systems, is presented.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"WEI-LIN CHIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wei-Lin Chiang is a researcher involved in creating an open platform for evaluating language models based on human preference.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"LIANMIN ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lianmin Zheng is a researcher contributing to the development of an open platform for evaluating language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YING SHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ying Sheng is a researcher focused on evaluating language models through human preference.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"ANASTASIOS NIKOLAS ANGELOPOULOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anastasios Nikolas Angelopoulos is a researcher involved in the evaluation of language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"TIANLE LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tianle Li is a researcher contributing to the development of an open platform for evaluating language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"DACHENG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dacheng Li is a researcher focused on evaluating language models through human preference.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"HAO ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao Zhang is a researcher involved in the evaluation of language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"BANGHUA ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Banghua Zhu is a researcher contributing to the development of an open platform for evaluating language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"MICHAEL JORDAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Jordan is a prominent researcher in machine learning and AI, contributing to the evaluation of language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JOSEPH E. GONZALEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joseph E. Gonzalez is a researcher involved in the evaluation of language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"ION STOICA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ion Stoica is a researcher contributing to the development of an open platform for evaluating language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"CHATBOT ARENA\">      <data key=\"d0\">PLATFORM<\/data>      <data key=\"d1\">Chatbot Arena is an open platform designed for evaluating language models based on human preferences.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"FRAN&#199;OIS CHOLLET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fran&#231;ois Chollet is a researcher known for his work on the measure of intelligence in AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is a researcher focused on AI-generating algorithms and their implications for general artificial intelligence.Jeff Clune is a researcher focused on the challenges of creating safe open-ended AI.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AI-GAS\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">AI-GAS refers to AI-generating algorithms, a paradigm for producing general artificial intelligence.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"KARL COBBE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karl Cobbe is a researcher involved in training verifiers to solve math word problems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"VINEET KOSARAJU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vineet Kosaraju is a researcher contributing to the training of verifiers for solving math word problems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"MOHAMMAD BAVARIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mohammad Bavarian is a researcher focused on training verifiers for math word problems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"MARK CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mark Chen is a researcher involved in the training of verifiers for solving math word problems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"HEEWOO JUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heewoo Jun is a researcher contributing to the training of verifiers for math word problems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"LUKASZ KAISER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lukasz Kaiser is a researcher focused on training verifiers for solving math word problems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"MATTHIAS PLAPPERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthias Plappert is a researcher involved in the training of verifiers for math word problems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JERRY TWOREK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jerry Tworek is a researcher contributing to the training of verifiers for solving math word problems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JACOB HILTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Hilton is a researcher focused on training verifiers for math word problems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"REIICHIRO NAKANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Reiichiro Nakano is a researcher involved in the training of verifiers for solving math word problems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"KALYANMOY DEB\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kalyanmoy Deb is a researcher known for his work on multi-objective genetic algorithms.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"AMRIT PRATAP\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amrit Pratap is a researcher contributing to the development of multi-objective genetic algorithms.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"SAMEER AGARWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sameer Agarwal is a researcher focused on multi-objective genetic algorithms.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"TAMT MEYARIVAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tamt Meyarivan is a researcher involved in the development of multi-objective genetic algorithms.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"NSGA-II\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">NSGA-II is a fast and elitist multi-objective genetic algorithm used for optimization problems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"AARON DHARNA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aaron Dharna is a researcher involved in the co-generation of game levels and game-playing agents.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JULIAN TOGELIUS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Julian Togelius is a researcher focused on the co-generation of game levels and agents.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"LISA B SOROS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lisa B Soros is a researcher contributing to the co-generation of game levels and game-playing agents.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YILUN DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yilun Du is a researcher involved in improving factuality and reasoning in language models through multi-agent debate.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"SHUANG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shuang Li is a researcher focused on enhancing factuality and reasoning in language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"ANTONIO TORRALBA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Antonio Torralba is a researcher contributing to the improvement of factuality in language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JOSHUA B TENENBAUM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshua B Tenenbaum is a researcher involved in enhancing reasoning in language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"IGOR MORDATCH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Igor Mordatch is a researcher focused on improving factuality and reasoning in language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"DHEERU DUA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dheeru Dua is a researcher involved in the development of a reading comprehension benchmark requiring discrete reasoning.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YIZHONG WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yizhong Wang is a researcher contributing to the development of a reading comprehension benchmark.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PRADEEP DASIGI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pradeep Dasigi is a researcher focused on creating a reading comprehension benchmark.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GABRIEL STANOVSKY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gabriel Stanovsky is a researcher involved in the development of a reading comprehension benchmark.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SAMEER SINGH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sameer Singh is a researcher contributing to the creation of a reading comprehension benchmark.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MATT GARDNER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matt Gardner is a researcher focused on developing a reading comprehension benchmark.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">DROP is a reading comprehension benchmark that requires discrete reasoning over paragraphs.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YAN DUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yan Duan is a researcher involved in fast reinforcement learning techniques.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JOHN SCHULMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">John Schulman is a researcher contributing to fast reinforcement learning methods.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"XI CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xi Chen is a researcher focused on reinforcement learning techniques.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"PETER L BARTLETT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter L Bartlett is a researcher involved in reinforcement learning research.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"ILYA SUTSKEVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ilya Sutskever is a prominent researcher in the field of reinforcement learning.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"PIETER ABBEEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pieter Abbeel is a researcher focused on meta-learning techniques.Pieter Abbeel is a researcher focused on reinforcement learning techniques.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RL^2\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">RL^2 is a method for fast reinforcement learning via slow reinforcement learning techniques.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"ADRIEN ECOFFET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adrien Ecoffet is a researcher involved in creating safe open-ended AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOEL LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Lehman is a researcher contributing to the study of safe open-ended AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"OPEN QUESTIONS IN CREATING SAFE OPEN-ENDED AI\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Open questions in creating safe open-ended AI address the tensions between control and creativity in AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">RESEARCH<\/data>    <\/node>    <node id=\"THOMAS ELSKEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Elsken is a researcher known for his work on neural architecture search.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JAN HENDRIK METZEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jan Hendrik Metzen is a researcher contributing to neural architecture search.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FRANK HUTTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Frank Hutter is a researcher focused on neural architecture search techniques.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Neural architecture search is a method for automating the design of neural networks.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"MAXENCE FALDOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maxence Faldor is a researcher involved in open-endedness in AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JENNY ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jenny Zhang is a researcher contributing to the study of open-endedness in AI.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANTOINE CULLY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Antoine Cully is a researcher focused on open-endedness in AI systems.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"OMNI-EPIC\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Omni-epic is a framework for achieving open-endedness in AI through human notions of interestingness.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">RESEARCH<\/data>    <\/node>    <node id=\"CHRISANTHA FERNANDO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chrisantha Fernando is a researcher involved in self-referential self-improvement in AI.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DYLAN SUNIL BANARSE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dylan Sunil Banarse is a researcher contributing to self-referential self-improvement in AI.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HENRYK MICHALIEWSKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Henryk Michaliewski is a researcher focused on self-referential self-improvement in AI.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SIMON OSINDERO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Simon Osindero is a researcher involved in self-referential self-improvement in AI.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TIM ROCKT&#196;SCHEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tim Rockt&#228;schel is a researcher contributing to self-referential self-improvement in AI.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHELSEA FINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chelsea Finn is a researcher known for her work on model-agnostic meta-learning.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SERGEY LEVINE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sergey Levine is a researcher involved in model-agnostic meta-learning.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MODEL-AGNOSTIC META-LEARNING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Model-agnostic meta-learning is a method for fast adaptation of deep networks.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"LUYU GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luyu Gao is a researcher contributing to program-aided language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AMAN MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aman Madaan is a researcher focused on program-aided language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHUYAN ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shuyan Zhou is a researcher involved in program-aided language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"URI ALON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Uri Alon is a researcher contributing to program-aided language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PENGFEI LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pengfei Liu is a researcher focused on program-aided language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YIMING YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiming Yang is a researcher involved in program-aided language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JAMIE CALLAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jamie Callan is a researcher contributing to program-aided language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GRAHAM NEUBIG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Graham Neubig is a researcher focused on program-aided language models.<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GETTING 50% SOTA ON ARC-AGI WITH GPT-4\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Getting 50% SOTA on ARC-AGI with GPT-4 is a study focused on achieving state-of-the-art performance using the GPT-4 model.Getting 50% SOTA on ARC-AGI with GPT-4 is a study focused on achieving state-of-the-art performance using the GPT-4 model.)&lt;|COMPLETE|&gt;<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>      <data key=\"d3\">RESEARCH<\/data>    <\/node>    <edge source=\"YUSHENG CHEN\" target=\"AGENTVERSE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yusheng Chen is a researcher involved in the development of the Agentverse framework for multi-agent collaboration.<\/data>      <data key=\"d6\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"SU JINGWEI\" target=\"AGENTVERSE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Su Jingwei is a researcher contributing to the Agentverse framework for multi-agent collaboration.<\/data>      <data key=\"d6\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"ZUO CHENG\" target=\"AGENTVERSE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Zuo Cheng is a researcher focused on the Agentverse framework for multi-agent collaboration.<\/data>      <data key=\"d6\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"CHENFEI YUAN\" target=\"AGENTVERSE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Chenfei Yuan is a researcher involved in the Agentverse framework for multi-agent collaboration.<\/data>      <data key=\"d6\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"CHI-MIN CHAN\" target=\"AGENTVERSE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Chi-Min Chan is a researcher contributing to the Agentverse framework for multi-agent collaboration.<\/data>      <data key=\"d6\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"HEYANG YU\" target=\"AGENTVERSE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Heyang Yu is a researcher focused on the Agentverse framework for multi-agent collaboration.<\/data>      <data key=\"d6\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"YAXI LU\" target=\"AGENTVERSE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yaxi Lu is a researcher involved in the Agentverse framework for multi-agent collaboration.<\/data>      <data key=\"d6\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"YI-HSIN HUNG\" target=\"AGENTVERSE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yi-Hsin Hung is a researcher contributing to the Agentverse framework for multi-agent collaboration.<\/data>      <data key=\"d6\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"CHEN QIAN\" target=\"AGENTVERSE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Chen Qian is a researcher focused on the Agentverse framework for multi-agent collaboration.<\/data>      <data key=\"d6\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"WEI-LIN CHIANG\" target=\"CHATBOT ARENA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Wei-Lin Chiang is a researcher involved in the development of the Chatbot Arena platform for evaluating language models.<\/data>      <data key=\"d6\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"LIANMIN ZHENG\" target=\"CHATBOT ARENA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Lianmin Zheng is a researcher contributing to the Chatbot Arena platform for evaluating language models.<\/data>      <data key=\"d6\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"YING SHENG\" target=\"CHATBOT ARENA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ying Sheng is a researcher focused on the Chatbot Arena platform for evaluating language models.<\/data>      <data key=\"d6\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"ANASTASIOS NIKOLAS ANGELOPOULOS\" target=\"CHATBOT ARENA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Anastasios Nikolas Angelopoulos is a researcher involved in the Chatbot Arena platform for evaluating language models.<\/data>      <data key=\"d6\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"TIANLE LI\" target=\"CHATBOT ARENA\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Tianle Li is a researcher contributing to the Chatbot Arena platform for evaluating language models.<\/data>      <data key=\"d6\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"6109537356a2ce2339f77c827aa3668e","chunk":", Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. In International Conference on Machine\nLearning , pp. 10764\u201310799. PMLR, 2023.\nRyan Greenblatt. Getting 50% sota on arc-agi with gpt-4. https:\/\/redwoodresearch.substack.\ncom\/p\/getting-50-sota-on-arc-agi-with-gpt , July 2024. Technical Report.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference on\nLearning Representations , 2021.\n16Automated Design of Agentic Systems\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang,\nSteven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent\ncollaborative framework. arXiv preprint arXiv:2308.00352 , 2023.\nShengran Hu and Jeff Clune. Thought Cloning: Learning to think while acting by imitating human\nthinking. Advances in Neural Information Processing Systems , 36, 2024.\nShengran Hu, Ran Cheng, Cheng He, Zhichao Lu, Jing Wang, and Miao Zhang. Accelerating multi-\nobjective neural architecture search by random-weight evaluation. Complex & Intelligent Systems ,\npp. 1\u201310, 2021.\nShihua Huang, Zhichao Lu, Kalyanmoy Deb, and Vishnu Naresh Boddeti. Revisiting residual networks\nfor adversarial robustness. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition , pp. 8202\u20138211, 2023.\nFrank Hutter, Lars Kotthoff, and Joaquin Vanschoren. Automated machine learning: methods, systems,\nchallenges . Springer Nature, 2019.\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Saiful\nHaq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, Heather Miller, et al. Dspy: Compiling\ndeclarative language model calls into state-of-the-art pipelines. In The Twelfth International\nConference on Learning Representations , 2024.\nAlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. Imagenetclassificationwithdeepconvolutional\nneural networks. Advances in neural information processing systems , 25, 2012.\nAbrahim Ladha. Lecture 11: Turing-completeness. https:\/\/faculty.cc.gatech.edu\/~ladha\/\nS24\/4510\/L11.pdf , 2024. CS 4510 Automata and Complexity, February 21st, 2024, Scribed by\nRishabh Singhal.\nLangChainAI. Langchain: Build context-aware reasoning applications. https:\/\/github.com\/\nlangchain-ai\/langchain , 2022.\nJoel Lehman and Kenneth O Stanley. Abandoning objectives: Evolution through the search for novelty\nalone.Evolutionary computation , 19(2):189\u2013223, 2011.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation\nforknowledge-intensivenlptasks. AdvancesinNeuralInformationProcessingSystems ,33:9459\u20139474,\n2020.\nFei Liu, Tong Xialiang, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu\nZhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language\nmodel. In Forty-first International Conference on Machine Learning , 2024.\nZijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic llm-agent network: An llm-agent\ncollaboration framework with agent team optimization. arXiv preprint arXiv:2310.02170 , 2023.\nChris Lu, Sebastian Towers, and Jakob Foerster. Arbitrary order meta-learning with simple population-\nbased evolution. In ALIFE 2023: Ghost in the Machine: Proceedings of the 2023 Artificial Life\nConference . MIT Press, 2023.\nChris Lu, Samuel Holt, Claudio Fanconi, Alex J Chan, Jakob Foerster, Mihaela van der Schaar, and\nRobert Tjarko Lange. Discovering preference optimization algorithms with and for large language\nmodels. arXiv preprint arXiv:2406.08414 , 2024a.\n17Automated Design of Agentic Systems\nChris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI Scientist:\nTowards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292 , 2024b.\nCong Lu, Shengran Hu, and Jeff Clune. Intelligent go-explore: Standing on the shoulders of giant\nfoundation models. arXiv preprint arXiv:2405.15143 , 2024c.\nZhichao Lu, Ian Whalen, Vishnu Boddeti, Yashesh Dhe","chunk_id":"6109537356a2ce2339f77c827aa3668e","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"YIMING YANG","type":"PERSON","description":"Yiming Yang is a researcher who co-authored a paper on program-aided language models presented at the International Conference on Machine Learning in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"JAMIE CALLAN","type":"PERSON","description":"Jamie Callan is a researcher who co-authored a paper on program-aided language models presented at the International Conference on Machine Learning in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"GRAHAM NEUBIG","type":"PERSON","description":"Graham Neubig is a researcher who co-authored a paper on program-aided language models presented at the International Conference on Machine Learning in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"RYAN GREENBLATT","type":"PERSON","description":"Ryan Greenblatt is a researcher who authored a technical report on achieving state-of-the-art performance with GPT-4 in July 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"DAN HENDRYCKS","type":"PERSON","description":"Dan Hendrycks is a researcher who co-authored a paper on measuring massive multitask language understanding presented at the International Conference on Learning Representations in 2021.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"COLLIN BURNS","type":"PERSON","description":"Collin Burns is a researcher who co-authored a paper on measuring massive multitask language understanding presented at the International Conference on Learning Representations in 2021.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"STEVEN BASART","type":"PERSON","description":"Steven Basart is a researcher who co-authored a paper on measuring massive multitask language understanding presented at the International Conference on Learning Representations in 2021.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ANDY ZOU","type":"PERSON","description":"Andy Zou is a researcher who co-authored a paper on measuring massive multitask language understanding presented at the International Conference on Learning Representations in 2021.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"MANTAS MAZEIKA","type":"PERSON","description":"Mantas Mazeika is a researcher who co-authored a paper on measuring massive multitask language understanding presented at the International Conference on Learning Representations in 2021.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"DAWN SONG","type":"PERSON","description":"Dawn Song is a researcher who co-authored a paper on measuring massive multitask language understanding presented at the International Conference on Learning Representations in 2021.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"JACOB STEINHARDT","type":"PERSON","description":"Jacob Steinhardt is a researcher who co-authored a paper on measuring massive multitask language understanding presented at the International Conference on Learning Representations in 2021.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"SIRUI HONG","type":"PERSON","description":"Sirui Hong is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"XIAWU ZHENG","type":"PERSON","description":"Xiawu Zheng is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"JONATHAN CHEN","type":"PERSON","description":"Jonathan Chen is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"YUHENG CHENG","type":"PERSON","description":"Yuheng Cheng is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"JINLIN WANG","type":"PERSON","description":"Jinlin Wang is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"CEYAO ZHANG","type":"PERSON","description":"Ceyao Zhang is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ZILI WANG","type":"PERSON","description":"Zili Wang is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"STEVEN KA SHING YAU","type":"PERSON","description":"Steven Ka Shing Yau is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ZIJUAN LIN","type":"PERSON","description":"Zijuan Lin is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"LIYANG ZHOU","type":"PERSON","description":"Liyang Zhou is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"SHENGRAN HU","type":"PERSON","description":"Shengran Hu is a researcher who co-authored a paper on Thought Cloning, which focuses on learning to think while acting by imitating human thinking, in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is a researcher who co-authored a paper on Thought Cloning, which focuses on learning to think while acting by imitating human thinking, in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ZHICHAO LU","type":"PERSON","description":"Zhichao Lu is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.\nZhichao Lu is a researcher who co-authored a paper on accelerating multi-objective neural architecture search by random-weight evaluation, published in 2021.","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"KALYANMOY DEB","type":"PERSON","description":"Kalyanmoy Deb is a researcher who co-authored a paper on revisiting residual networks for adversarial robustness, presented at the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"VISHNU NARESH BODDETI","type":"PERSON","description":"Vishnu Naresh Boddeti is a researcher who co-authored a paper on revisiting residual networks for adversarial robustness, presented at the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"FRANK HUTTER","type":"PERSON","description":"Frank Hutter is a researcher who co-authored a book on automated machine learning, discussing methods, systems, and challenges, published in 2019.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"LARS KOTTHOFF","type":"PERSON","description":"Lars Kotthoff is a researcher who co-authored a book on automated machine learning, discussing methods, systems, and challenges, published in 2019.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"JOAQUIN VANSCHOREN","type":"PERSON","description":"Joaquin Vanschoren is a researcher who co-authored a book on automated machine learning, discussing methods, systems, and challenges, published in 2019.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"OMAR KHATTAB","type":"PERSON","description":"Omar Khattab is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ARNAV SINGHVI","type":"PERSON","description":"Arnav Singhvi is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"PARIDHI MAHESHWARI","type":"PERSON","description":"Paridhi Maheshwari is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ZHIZHUAN ZHANG","type":"PERSON","description":"Zhiyuan Zhang is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"KESHAV SANTHANAM","type":"PERSON","description":"Keshav Santhanam is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"SAIFUL HAQ","type":"PERSON","description":"Saiful Haq is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ASHUTOSH SHARMA","type":"PERSON","description":"Ashutosh Sharma is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"THOMAS T JOSHI","type":"PERSON","description":"Thomas T Joshi is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"HANNA MOAZAM","type":"PERSON","description":"Hanna Moazam is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"HEATHER MILLER","type":"PERSON","description":"Heather Miller is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ALEX KRIZHEVSKY","type":"PERSON","description":"Alex Krizhevsky is a researcher known for his work on ImageNet classification using deep convolutional neural networks, published in 2012.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ILYAS SUTSKEVER","type":"PERSON","description":"Ilya Sutskever is a researcher known for his work on ImageNet classification using deep convolutional neural networks, published in 2012.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"GEOFFREY HINTON","type":"PERSON","description":"Geoffrey Hinton is a researcher known for his work on ImageNet classification using deep convolutional neural networks, published in 2012.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ABRAHIM LADHA","type":"PERSON","description":"Abrahim Ladha is a researcher who authored a lecture on Turing-completeness, available online in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"LANGCHAINAI","type":"ORGANIZATION","description":"LangChainAI is an organization that focuses on building context-aware reasoning applications, with resources available on GitHub since 2022.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"JOEL LEHMAN","type":"PERSON","description":"Joel Lehman is a researcher who co-authored a paper on evolution through the search for novelty alone, published in 2011.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"KENNETH O STANLEY","type":"PERSON","description":"Kenneth O Stanley is a researcher who co-authored a paper on evolution through the search for novelty alone, published in 2011.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"PATRICK LEWIS","type":"PERSON","description":"Patrick Lewis is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ETHAN PEREZ","type":"PERSON","description":"Ethan Perez is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ALEKSANDRA PIKTUS","type":"PERSON","description":"Aleksandra Piktus is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"FABIO PETRONI","type":"PERSON","description":"Fabio Petroni is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"VLADIMIR KARPUKHIN","type":"PERSON","description":"Vladimir Karpukhin is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"NAMAN GOYAL","type":"PERSON","description":"Naman Goyal is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"HEINRICH K\u00dcTTLER","type":"PERSON","description":"Heinrich K\u00fcttler is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"MIKE LEWIS","type":"PERSON","description":"Mike Lewis is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"WEN-TAU YIH","type":"PERSON","description":"Wen-tau Yih is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"TIM ROCKT\u00c4SCHEL","type":"PERSON","description":"Tim Rockt\u00e4schel is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"FEI LIU","type":"PERSON","description":"Fei Liu is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"TONG XIALIANG","type":"PERSON","description":"Tong Xialiang is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"MINGXUAN YUAN","type":"PERSON","description":"Mingxuan Yuan is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"XI LIN","type":"PERSON","description":"Xi Lin is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"FU LUO","type":"PERSON","description":"Fu Luo is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ZHENKUN WANG","type":"PERSON","description":"Zhenkun Wang is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"QINGFU ZHANG","type":"PERSON","description":"Qingfu Zhang is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ZIJUN LIU","type":"PERSON","description":"Zijun Liu is a researcher who co-authored a paper on dynamic LLM-agent networks, which focuses on collaboration frameworks with agent team optimization, presented as an arXiv preprint in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"YANZHE ZHANG","type":"PERSON","description":"Yanzhe Zhang is a researcher who co-authored a paper on dynamic LLM-agent networks, which focuses on collaboration frameworks with agent team optimization, presented as an arXiv preprint in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"PENG LI","type":"PERSON","description":"Peng Li is a researcher who co-authored a paper on dynamic LLM-agent networks, which focuses on collaboration frameworks with agent team optimization, presented as an arXiv preprint in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"YANG LIU","type":"PERSON","description":"Yang Liu is a researcher who co-authored a paper on dynamic LLM-agent networks, which focuses on collaboration frameworks with agent team optimization, presented as an arXiv preprint in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"DIYI YANG","type":"PERSON","description":"Diyi Yang is a researcher who co-authored a paper on dynamic LLM-agent networks, which focuses on collaboration frameworks with agent team optimization, presented as an arXiv preprint in 2023.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"CHRIS LU","type":"PERSON","description":"Chris Lu is a researcher who co-authored a paper on arbitrary order meta-learning with simple population-based evolution, presented at the 2023 Artificial Life Conference.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"SEBASTIAN TOWERS","type":"PERSON","description":"Sebastian Towers is a researcher who co-authored a paper on arbitrary order meta-learning with simple population-based evolution, presented at the 2023 Artificial Life Conference.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"JAKOB FOERSTER","type":"PERSON","description":"Jakob Foerster is a researcher who co-authored a paper on arbitrary order meta-learning with simple population-based evolution, presented at the 2023 Artificial Life Conference.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"CLAUDIO FANCONI","type":"PERSON","description":"Claudio Fanconi is a researcher who co-authored a paper on discovering preference optimization algorithms with large language models, presented as an arXiv preprint in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ALEX J CHAN","type":"PERSON","description":"Alex J Chan is a researcher who co-authored a paper on discovering preference optimization(\"entity\"","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"META PROGRAMMING","type":"TECHNIQUE","description":"Meta programming refers to programming techniques that allow for the creation of programs that can manipulate other programs, as explored in the context of multi-agent collaboration in the MetaGPT paper.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"THOUGHT CLONING","type":"TECHNIQUE","description":"Thought Cloning is a method that involves learning to think while acting by imitating human cognitive processes, as discussed in a paper by Shengran Hu and Jeff Clune in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"AUTOMATED MACHINE LEARNING","type":"FIELD","description":"Automated machine learning encompasses methods and systems designed to automate the process of applying machine learning to real-world problems, as discussed in a book by Frank Hutter and others in 2019.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"RETRIEVAL-AUGMENTED GENERATION","type":"TECHNIQUE","description":"Retrieval-augmented generation is a technique that combines retrieval of information with generative models to enhance performance on knowledge-intensive NLP tasks, as discussed in a paper by Patrick Lewis and others in 2020.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"AGENTIC SYSTEMS","type":"FIELD","description":"Agentic systems are systems that can act autonomously and make decisions based on their environment, as explored in various research papers including those on MetaGPT and dynamic LLM-agent networks.\nAgentic systems refer to systems that can act autonomously and make decisions based on their environment, as explored in various research papers including those on MetaGPT and dynamic LLM-agent networks.","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"FIELD"},{"name":"NEURAL ARCHITECTURE SEARCH","type":"TECHNIQUE","description":"Neural architecture search is a method for automating the design of neural networks, often involving multi-objective optimization strategies, as discussed in the context of Shengran Hu's work in 2021.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"IMAGE CLASSIFICATION","type":"FIELD","description":"Image classification is a computer vision task that involves categorizing images into predefined classes, as exemplified by the work of Alex Krizhevsky and others in 2012.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"TURING-COMPLETENESS","type":"CONCEPT","description":"Turing-completeness is a concept in computer science that describes a system capable of performing any computation that can be described algorithmically, as discussed in Abrahim Ladha's lecture in 2024.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"LANGCHAIN","type":"TECHNOLOGY","description":"LangChain is a framework for building context-aware reasoning applications, enabling developers to create applications that can understand and utilize context effectively, as mentioned in resources from LangChainAI in 2022.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"EVOLUTIONARY COMPUTATION","type":"FIELD","description":"Evolutionary computation is a subset of artificial intelligence that involves algorithms inspired by natural selection, as discussed in the work of Joel Lehman and Kenneth O Stanley in 2011.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"MULTI-OBJECTIVE OPTIMIZATION","type":"TECHNIQUE","description":"Multi-objective optimization involves optimizing two or more conflicting objectives simultaneously, often used in neural architecture search and other complex decision-making scenarios.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"FOUNDATION MODELS","type":"TECHNOLOGY","description":"Foundation models are large-scale models trained on diverse data that can be fine-tuned for various downstream tasks, as referenced in the context of intelligent exploration and exploitation strategies.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"AGENT TEAM OPTIMIZATION","type":"TECHNIQUE","description":"Agent team optimization refers to strategies aimed at improving the performance and collaboration of multiple agents working together, as discussed in the context of dynamic LLM-agent networks.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"HUMAN-COMPUTER INTERACTION","type":"FIELD","description":"Human-computer interaction is a field of study focused on the design and use of computer technology, emphasizing the interfaces between people and computers, relevant to many of the discussed techniques.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"CROSS-DOMAIN LEARNING","type":"FIELD","description":"Cross-domain learning involves transferring knowledge from one domain to another, enhancing the performance of models in new, unseen domains, relevant to various research efforts in machine learning.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"AUTOMATED DESIGN","type":"FIELD","description":"Automated design refers to the use of algorithms and computational methods to automate the design process in various fields, including engineering and software development, as explored in the context of agentic systems.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"KNOWLEDGE-INTENSIVE TASKS","type":"FIELD","description":"Knowledge-intensive tasks are tasks that require a significant amount of domain-specific knowledge, often addressed through advanced NLP techniques like retrieval-augmented generation.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"COLLABORATIVE FRAMEWORK","type":"TECHNIQUE","description":"Collaborative frameworks are systems designed to facilitate cooperation among multiple agents or entities, enhancing their collective performance and decision-making capabilities.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"STATE-OF-THE-ART PIPELINES","type":"TECHNOLOGY","description":"State-of-the-art pipelines refer to the most advanced and effective processes or systems used in machine learning and data processing, as discussed in the context of DSpy.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"CROSS-VALIDATION","type":"TECHNIQUE","description":"Cross-validation is a statistical method used to estimate the skill of machine learning models, ensuring that they generalize well to unseen data.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"DATA AUGMENTATION","type":"TECHNIQUE","description":"Data augmentation involves creating new training examples by modifying existing data, enhancing the robustness and performance of machine learning models.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"DEEP LEARNING","type":"FIELD","description":"Deep learning is a subset of machine learning that uses neural networks with many layers to analyze various forms of data, widely applied in image and speech recognition.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"NATURAL LANGUAGE PROCESSING","type":"FIELD","description":"Natural language processing is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language, encompassing various techniques and applications.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"COMPUTER VISION","type":"FIELD","description":"Computer vision is a field of artificial intelligence that enables computers to interpret and make decisions based on visual data from the world, relevant to many discussed techniques.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"MULTI-AGENT SYSTEMS","type":"FIELD","description":"Multi-agent systems involve multiple agents interacting or working together to achieve a common goal, relevant to the discussions on MetaGPT and dynamic LLM-agent networks.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"AUTOMATED DECISION-MAKING","type":"FIELD","description":"Automated decision-making refers to the process of using algorithms and data to make decisions without human intervention, relevant to many of the discussed techniques.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"REINFORCEMENT LEARNING","type":"FIELD","description":"Reinforcement learning is a type of machine learning where agents learn to make decisions by receiving rewards or penalties based on their actions, relevant to the context of agentic systems.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"DATA SCIENCE","type":"FIELD","description":"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"MACHINE LEARNING","type":"FIELD","description":"Machine learning is a subset of artificial intelligence that focuses on the development of algorithms that allow computers to learn from and make predictions based on data.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"COMPUTATIONAL INTELLIGENCE","type":"FIELD","description":"Computational intelligence is a field of study that focuses on the design of intelligent agents and systems that can solve complex problems through adaptive learning and reasoning.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ALGORITHMIC DESIGN","type":"FIELD","description":"Algorithmic design refers to the process of creating algorithms to solve specific problems, often involving optimization and efficiency considerations.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"SYSTEMS ENGINEERING","type":"FIELD","description":"Systems engineering is an interdisciplinary field that focuses on the design, integration, and management of complex systems over their life cycles.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"DECISION SUPPORT SYSTEMS","type":"FIELD","description":"Decision support systems are computer-based information systems that support business or organizational decision-making activities.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"DATA MINING","type":"FIELD","description":"Data mining is the process of discovering patterns and knowledge from large amounts of data, often used in conjunction with machine learning techniques.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"BIG DATA","type":"FIELD","description":"Big data refers to large and complex data sets that traditional data processing applications cannot handle, often requiring advanced analytical techniques.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"CLOUD COMPUTING","type":"FIELD","description":"Cloud computing is the delivery of computing services over the internet, allowing for on-demand access to a shared pool of configurable resources.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"INFORMATION RETRIEVAL","type":"FIELD","description":"Information retrieval is the process of obtaining information system resources that are relevant to an information need from a collection of those resources.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"DATA ANALYSIS","type":"FIELD","description":"Data analysis is the process of inspecting, cleansing, transforming, and modeling data to discover useful information, inform conclusions, and support decision-making.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"PREDICTIVE ANALYTICS","type":"FIELD","description":"Predictive analytics involves using statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"STATISTICAL LEARNING","type":"FIELD","description":"Statistical learning is a framework for understanding and modeling the relationships between variables, often used in machine learning and data analysis.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"EXPLORATORY DATA ANALYSIS","type":"FIELD","description":"Exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often using visual methods.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"DATA VISUALIZATION","type":"FIELD","description":"Data visualization is the graphical representation of information and data, using visual elements like charts, graphs, and maps to communicate information clearly.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"INTERACTIVE SYSTEMS","type":"FIELD","description":"Interactive systems are systems designed to facilitate user interaction, often incorporating feedback mechanisms to enhance user experience.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"USER EXPERIENCE DESIGN","type":"FIELD","description":"User experience design is the process of enhancing user satisfaction by improving the usability, accessibility, and pleasure provided in the interaction with a product.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"SOFTWARE ENGINEERING","type":"FIELD","description":"Software engineering is the application of engineering principles to software development in a methodical way, encompassing the entire software development life cycle.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"DEVOPS","type":"FIELD","description":"DevOps is a set of practices that combines software development (Dev) and IT operations (Ops) to shorten the systems development life cycle and provide continuous delivery.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"BLOCKCHAIN","type":"FIELD","description":"Blockchain is a decentralized digital ledger technology that records transactions across many computers in a way that the registered transactions cannot be altered retroactively.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"QUANTUM COMPUTING","type":"FIELD","description":"Quantum computing is a type of computation that takes advantage of quantum mechanics to process information in fundamentally different ways than classical computers.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"NEUROMORPHIC COMPUTING","type":"FIELD","description":"Neuromorphic computing is a design approach that mimics the neural structure and functioning of the human brain to improve computational efficiency and performance.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ROBOTICS","type":"FIELD","description":"Robotics is the branch of technology that deals with the design, construction, operation, and use of robots, often involving AI and machine learning techniques.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"AUTONOMOUS SYSTEMS","type":"FIELD","description":"Autonomous systems are systems capable of performing tasks without human intervention, often utilizing AI and machine learning for decision-making.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"INTERNET OF THINGS","type":"FIELD","description":"The Internet of Things (IoT) refers to the interconnected network of physical devices that communicate and exchange data over the internet, often utilizing AI for data analysis and decision-making.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"VIRTUAL REALITY","type":"FIELD","description":"Virtual reality is a simulated experience that can be similar to or completely different from the real world, often used in gaming, training, and education.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"AUGMENTED REALITY","type":"FIELD","description":"Augmented reality is an interactive experience where real-world environments are enhanced by computer-generated perceptual information, often used in applications like gaming and education.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"MACHINE VISION","type":"FIELD","description":"Machine vision is the technology and methods used to provide imaging-based automatic inspection and analysis for process control and robot guidance.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"SENSOR NETWORKS","type":"FIELD","description":"Sensor networks are networks of spatially distributed sensors that monitor physical or environmental conditions, often used in IoT applications.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"CYBERSECURITY","type":"FIELD","description":"Cybersecurity is the practice of protecting systems, networks, and programs from digital attacks, often involving various technologies and strategies.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"DATA PRIVACY","type":"FIELD","description":"Data privacy refers to the proper handling, processing, storage, and usage of personal data, ensuring that individuals' privacy rights are respected.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ETHICAL AI","type":"FIELD","description":"Ethical AI refers to the development and implementation of artificial intelligence systems that adhere to ethical principles and guidelines, ensuring fairness, accountability, and transparency.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"AI GOVERNANCE","type":"FIELD","description":"AI governance involves the frameworks and policies that guide the development and use of AI technologies, ensuring they are used responsibly and ethically.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"SUSTAINABLE TECHNOLOGY","type":"FIELD","description":"Sustainable technology refers to technologies that are designed to have a minimal impact on the environment, promoting sustainability and reducing resource consumption.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"CIRCULAR ECONOMY","type":"FIELD","description":"Circular economy is an economic system aimed at eliminating waste and the continual use of resources, often involving innovative technologies and practices.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"GREEN TECHNOLOGY","type":"FIELD","description":"Green technology refers to technology that is environmentally friendly and sustainable, often focusing on renewable energy and resource efficiency.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"SMART CITIES","type":"FIELD","description":"Smart cities are urban areas that use various types of electronic data collection sensors to supply information used to manage assets and resources efficiently.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"DIGITAL TRANSFORMATION","type":"FIELD","description":"Digital transformation refers to the integration of digital technology into all areas of a business, fundamentally changing how it operates and delivers value to customers.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"INDUSTRY 4.0","type":"FIELD","description":"Industry 4.0 refers to the current trend of automation and data exchange in manufacturing technologies, including cyber-physical systems, IoT, and cloud computing.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"AGRICULTURAL TECHNOLOGY","type":"FIELD","description":"Agricultural technology refers to the use of technology in agriculture to improve yield, efficiency, and sustainability, often involving precision farming techniques.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"FINTECH","type":"FIELD","description":"Fintech refers to technology that aims to improve and automate the delivery and use of financial services, often involving innovative solutions for banking and payments.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"HEALTH TECH","type":"FIELD","description":"Health tech refers to technology that aims to improve health and healthcare delivery, often involving digital health solutions and medical devices.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"EDTECH","type":"FIELD","description":"Edtech refers to technology that enhances education and learning experiences, often involving online learning platforms and educational software.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"MOBILITY TECHNOLOGY","type":"FIELD","description":"Mobility technology refers to technology that enhances transportation and mobility solutions, often involving smart transportation systems and electric vehicles.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"SPACE TECHNOLOGY","type":"FIELD","description":"Space technology refers to the technology used in the exploration and utilization of outer space, often involving advanced engineering and scientific research.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"AEROSPACE ENGINEERING","type":"FIELD","description":"Aerospace engineering is the branch of engineering that deals with the design, development, and production of aircraft and spacecraft.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"ROBOTIC PROCESS AUTOMATION","type":"FIELD","description":"Robotic process automation refers to the use of software robots to automate repetitive tasks, improving efficiency and accuracy in business processes.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"QUANTUM MACHINE LEARNING","type":"FIELD","description":"Quantum machine learning is an interdisciplinary field that combines quantum computing and machine learning, exploring how quantum algorithms can enhance machine learning tasks.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"BIOINFORMATICS","type":"FIELD","description":"Bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data, particularly in genomics and molecular biology.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"NEUROSCIENCE","type":"FIELD","description":"Neuroscience is the scientific study of the nervous system, often intersecting with fields like artificial intelligence and cognitive science.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"COGNITIVE SCIENCE","type":"FIELD","description":"Cognitive science is the interdisciplinary study of the mind and its processes, including how people think, learn, and remember.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"SOCIAL ROBOTICS","type":"FIELD","description":"Social robotics is a field of study focused on the design and development of robots that can interact with humans in a social context.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"HUMAN-ROBOT INTERACTION","type":"FIELD","description":"Human-robot interaction is the study of how humans and robots communicate and collaborate, often focusing on improving user experience and effectiveness.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"VIRTUAL ASSISTANTS","type":"FIELD","description":"Virtual assistants are AI-powered software agents that can perform tasks or services for individuals based on commands or questions.","source_id":"6109537356a2ce2339f77c827aa3668e"},{"name":"CHATBOTS","type":"FIELD","description":"Chatbots are AI programs that simulate","source_id":"6109537356a2ce2339f77c827aa3668e"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"YIMING YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiming Yang is a researcher who co-authored a paper on program-aided language models presented at the International Conference on Machine Learning in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"JAMIE CALLAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jamie Callan is a researcher who co-authored a paper on program-aided language models presented at the International Conference on Machine Learning in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"GRAHAM NEUBIG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Graham Neubig is a researcher who co-authored a paper on program-aided language models presented at the International Conference on Machine Learning in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"RYAN GREENBLATT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ryan Greenblatt is a researcher who authored a technical report on achieving state-of-the-art performance with GPT-4 in July 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"DAN HENDRYCKS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dan Hendrycks is a researcher who co-authored a paper on measuring massive multitask language understanding presented at the International Conference on Learning Representations in 2021.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"COLLIN BURNS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Collin Burns is a researcher who co-authored a paper on measuring massive multitask language understanding presented at the International Conference on Learning Representations in 2021.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"STEVEN BASART\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Steven Basart is a researcher who co-authored a paper on measuring massive multitask language understanding presented at the International Conference on Learning Representations in 2021.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ANDY ZOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andy Zou is a researcher who co-authored a paper on measuring massive multitask language understanding presented at the International Conference on Learning Representations in 2021.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"MANTAS MAZEIKA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mantas Mazeika is a researcher who co-authored a paper on measuring massive multitask language understanding presented at the International Conference on Learning Representations in 2021.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"DAWN SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dawn Song is a researcher who co-authored a paper on measuring massive multitask language understanding presented at the International Conference on Learning Representations in 2021.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"JACOB STEINHARDT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Steinhardt is a researcher who co-authored a paper on measuring massive multitask language understanding presented at the International Conference on Learning Representations in 2021.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"SIRUI HONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sirui Hong is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"XIAWU ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiawu Zheng is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"JONATHAN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jonathan Chen is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"YUHENG CHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuheng Cheng is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"JINLIN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jinlin Wang is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"CEYAO ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ceyao Zhang is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ZILI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zili Wang is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"STEVEN KA SHING YAU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Steven Ka Shing Yau is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ZIJUAN LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zijuan Lin is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"LIYANG ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liyang Zhou is a researcher who co-authored a paper on MetaGPT, a meta programming framework for multi-agent collaboration, in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"SHENGRAN HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengran Hu is a researcher who co-authored a paper on Thought Cloning, which focuses on learning to think while acting by imitating human thinking, in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is a researcher who co-authored a paper on Thought Cloning, which focuses on learning to think while acting by imitating human thinking, in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ZHICHAO LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhichao Lu is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.Zhichao Lu is a researcher who co-authored a paper on accelerating multi-objective neural architecture search by random-weight evaluation, published in 2021.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KALYANMOY DEB\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kalyanmoy Deb is a researcher who co-authored a paper on revisiting residual networks for adversarial robustness, presented at the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"VISHNU NARESH BODDETI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vishnu Naresh Boddeti is a researcher who co-authored a paper on revisiting residual networks for adversarial robustness, presented at the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"FRANK HUTTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Frank Hutter is a researcher who co-authored a book on automated machine learning, discussing methods, systems, and challenges, published in 2019.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"LARS KOTTHOFF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lars Kotthoff is a researcher who co-authored a book on automated machine learning, discussing methods, systems, and challenges, published in 2019.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"JOAQUIN VANSCHOREN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joaquin Vanschoren is a researcher who co-authored a book on automated machine learning, discussing methods, systems, and challenges, published in 2019.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"OMAR KHATTAB\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Omar Khattab is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ARNAV SINGHVI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arnav Singhvi is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"PARIDHI MAHESHWARI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Paridhi Maheshwari is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ZHIZHUAN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyuan Zhang is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"KESHAV SANTHANAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Keshav Santhanam is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"SAIFUL HAQ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Saiful Haq is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ASHUTOSH SHARMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ashutosh Sharma is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"THOMAS T JOSHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas T Joshi is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"HANNA MOAZAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hanna Moazam is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"HEATHER MILLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heather Miller is a researcher who co-authored a paper on DSpy, which compiles declarative language model calls into state-of-the-art pipelines, presented at the International Conference on Learning Representations in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ALEX KRIZHEVSKY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex Krizhevsky is a researcher known for his work on ImageNet classification using deep convolutional neural networks, published in 2012.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ILYAS SUTSKEVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ilya Sutskever is a researcher known for his work on ImageNet classification using deep convolutional neural networks, published in 2012.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"GEOFFREY HINTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Geoffrey Hinton is a researcher known for his work on ImageNet classification using deep convolutional neural networks, published in 2012.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ABRAHIM LADHA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Abrahim Ladha is a researcher who authored a lecture on Turing-completeness, available online in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"LANGCHAINAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">LangChainAI is an organization that focuses on building context-aware reasoning applications, with resources available on GitHub since 2022.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"JOEL LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Lehman is a researcher who co-authored a paper on evolution through the search for novelty alone, published in 2011.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"KENNETH O STANLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kenneth O Stanley is a researcher who co-authored a paper on evolution through the search for novelty alone, published in 2011.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"PATRICK LEWIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Patrick Lewis is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ETHAN PEREZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ethan Perez is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ALEKSANDRA PIKTUS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aleksandra Piktus is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"FABIO PETRONI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fabio Petroni is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"VLADIMIR KARPUKHIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vladimir Karpukhin is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"NAMAN GOYAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Naman Goyal is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"HEINRICH K&#220;TTLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heinrich K&#252;ttler is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"MIKE LEWIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mike Lewis is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"WEN-TAU YIH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wen-tau Yih is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"TIM ROCKT&#196;SCHEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tim Rockt&#228;schel is a researcher who co-authored a paper on retrieval-augmented generation for knowledge-intensive NLP tasks, published in 2020.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"FEI LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fei Liu is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"TONG XIALIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tong Xialiang is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"MINGXUAN YUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mingxuan Yuan is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"XI LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xi Lin is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"FU LUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fu Luo is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ZHENKUN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhenkun Wang is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"QINGFU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qingfu Zhang is a researcher who co-authored a paper on the evolution of heuristics for efficient automatic algorithm design using large language models, presented at the International Conference on Machine Learning in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ZIJUN LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zijun Liu is a researcher who co-authored a paper on dynamic LLM-agent networks, which focuses on collaboration frameworks with agent team optimization, presented as an arXiv preprint in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"YANZHE ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yanzhe Zhang is a researcher who co-authored a paper on dynamic LLM-agent networks, which focuses on collaboration frameworks with agent team optimization, presented as an arXiv preprint in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"PENG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peng Li is a researcher who co-authored a paper on dynamic LLM-agent networks, which focuses on collaboration frameworks with agent team optimization, presented as an arXiv preprint in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"YANG LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang Liu is a researcher who co-authored a paper on dynamic LLM-agent networks, which focuses on collaboration frameworks with agent team optimization, presented as an arXiv preprint in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"DIYI YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Diyi Yang is a researcher who co-authored a paper on dynamic LLM-agent networks, which focuses on collaboration frameworks with agent team optimization, presented as an arXiv preprint in 2023.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"CHRIS LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chris Lu is a researcher who co-authored a paper on arbitrary order meta-learning with simple population-based evolution, presented at the 2023 Artificial Life Conference.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"SEBASTIAN TOWERS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sebastian Towers is a researcher who co-authored a paper on arbitrary order meta-learning with simple population-based evolution, presented at the 2023 Artificial Life Conference.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"JAKOB FOERSTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jakob Foerster is a researcher who co-authored a paper on arbitrary order meta-learning with simple population-based evolution, presented at the 2023 Artificial Life Conference.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"CLAUDIO FANCONI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Claudio Fanconi is a researcher who co-authored a paper on discovering preference optimization algorithms with large language models, presented as an arXiv preprint in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ALEX J CHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex J Chan is a researcher who co-authored a paper on discovering preference optimization(\"entity\"<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"META PROGRAMMING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Meta programming refers to programming techniques that allow for the creation of programs that can manipulate other programs, as explored in the context of multi-agent collaboration in the MetaGPT paper.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"THOUGHT CLONING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Thought Cloning is a method that involves learning to think while acting by imitating human cognitive processes, as discussed in a paper by Shengran Hu and Jeff Clune in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"AUTOMATED MACHINE LEARNING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Automated machine learning encompasses methods and systems designed to automate the process of applying machine learning to real-world problems, as discussed in a book by Frank Hutter and others in 2019.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED GENERATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Retrieval-augmented generation is a technique that combines retrieval of information with generative models to enhance performance on knowledge-intensive NLP tasks, as discussed in a paper by Patrick Lewis and others in 2020.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"AGENTIC SYSTEMS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Agentic systems are systems that can act autonomously and make decisions based on their environment, as explored in various research papers including those on MetaGPT and dynamic LLM-agent networks.Agentic systems refer to systems that can act autonomously and make decisions based on their environment, as explored in various research papers including those on MetaGPT and dynamic LLM-agent networks.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">FIELD<\/data>    <\/node>    <node id=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Neural architecture search is a method for automating the design of neural networks, often involving multi-objective optimization strategies, as discussed in the context of Shengran Hu's work in 2021.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"IMAGE CLASSIFICATION\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Image classification is a computer vision task that involves categorizing images into predefined classes, as exemplified by the work of Alex Krizhevsky and others in 2012.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"TURING-COMPLETENESS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Turing-completeness is a concept in computer science that describes a system capable of performing any computation that can be described algorithmically, as discussed in Abrahim Ladha's lecture in 2024.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"LANGCHAIN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LangChain is a framework for building context-aware reasoning applications, enabling developers to create applications that can understand and utilize context effectively, as mentioned in resources from LangChainAI in 2022.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"EVOLUTIONARY COMPUTATION\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Evolutionary computation is a subset of artificial intelligence that involves algorithms inspired by natural selection, as discussed in the work of Joel Lehman and Kenneth O Stanley in 2011.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"MULTI-OBJECTIVE OPTIMIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Multi-objective optimization involves optimizing two or more conflicting objectives simultaneously, often used in neural architecture search and other complex decision-making scenarios.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"FOUNDATION MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundation models are large-scale models trained on diverse data that can be fine-tuned for various downstream tasks, as referenced in the context of intelligent exploration and exploitation strategies.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"AGENT TEAM OPTIMIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Agent team optimization refers to strategies aimed at improving the performance and collaboration of multiple agents working together, as discussed in the context of dynamic LLM-agent networks.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"HUMAN-COMPUTER INTERACTION\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Human-computer interaction is a field of study focused on the design and use of computer technology, emphasizing the interfaces between people and computers, relevant to many of the discussed techniques.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"CROSS-DOMAIN LEARNING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Cross-domain learning involves transferring knowledge from one domain to another, enhancing the performance of models in new, unseen domains, relevant to various research efforts in machine learning.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"AUTOMATED DESIGN\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Automated design refers to the use of algorithms and computational methods to automate the design process in various fields, including engineering and software development, as explored in the context of agentic systems.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"KNOWLEDGE-INTENSIVE TASKS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Knowledge-intensive tasks are tasks that require a significant amount of domain-specific knowledge, often addressed through advanced NLP techniques like retrieval-augmented generation.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"COLLABORATIVE FRAMEWORK\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Collaborative frameworks are systems designed to facilitate cooperation among multiple agents or entities, enhancing their collective performance and decision-making capabilities.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"STATE-OF-THE-ART PIPELINES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">State-of-the-art pipelines refer to the most advanced and effective processes or systems used in machine learning and data processing, as discussed in the context of DSpy.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"CROSS-VALIDATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Cross-validation is a statistical method used to estimate the skill of machine learning models, ensuring that they generalize well to unseen data.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"DATA AUGMENTATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Data augmentation involves creating new training examples by modifying existing data, enhancing the robustness and performance of machine learning models.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"DEEP LEARNING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Deep learning is a subset of machine learning that uses neural networks with many layers to analyze various forms of data, widely applied in image and speech recognition.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"NATURAL LANGUAGE PROCESSING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Natural language processing is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language, encompassing various techniques and applications.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"COMPUTER VISION\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Computer vision is a field of artificial intelligence that enables computers to interpret and make decisions based on visual data from the world, relevant to many discussed techniques.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"MULTI-AGENT SYSTEMS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Multi-agent systems involve multiple agents interacting or working together to achieve a common goal, relevant to the discussions on MetaGPT and dynamic LLM-agent networks.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"AUTOMATED DECISION-MAKING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Automated decision-making refers to the process of using algorithms and data to make decisions without human intervention, relevant to many of the discussed techniques.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"REINFORCEMENT LEARNING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Reinforcement learning is a type of machine learning where agents learn to make decisions by receiving rewards or penalties based on their actions, relevant to the context of agentic systems.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"DATA SCIENCE\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"MACHINE LEARNING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Machine learning is a subset of artificial intelligence that focuses on the development of algorithms that allow computers to learn from and make predictions based on data.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"COMPUTATIONAL INTELLIGENCE\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Computational intelligence is a field of study that focuses on the design of intelligent agents and systems that can solve complex problems through adaptive learning and reasoning.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ALGORITHMIC DESIGN\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Algorithmic design refers to the process of creating algorithms to solve specific problems, often involving optimization and efficiency considerations.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"SYSTEMS ENGINEERING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Systems engineering is an interdisciplinary field that focuses on the design, integration, and management of complex systems over their life cycles.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"DECISION SUPPORT SYSTEMS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Decision support systems are computer-based information systems that support business or organizational decision-making activities.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"DATA MINING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Data mining is the process of discovering patterns and knowledge from large amounts of data, often used in conjunction with machine learning techniques.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"BIG DATA\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Big data refers to large and complex data sets that traditional data processing applications cannot handle, often requiring advanced analytical techniques.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"CLOUD COMPUTING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Cloud computing is the delivery of computing services over the internet, allowing for on-demand access to a shared pool of configurable resources.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"INFORMATION RETRIEVAL\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Information retrieval is the process of obtaining information system resources that are relevant to an information need from a collection of those resources.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"DATA ANALYSIS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Data analysis is the process of inspecting, cleansing, transforming, and modeling data to discover useful information, inform conclusions, and support decision-making.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"PREDICTIVE ANALYTICS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Predictive analytics involves using statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"STATISTICAL LEARNING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Statistical learning is a framework for understanding and modeling the relationships between variables, often used in machine learning and data analysis.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"EXPLORATORY DATA ANALYSIS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often using visual methods.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"DATA VISUALIZATION\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Data visualization is the graphical representation of information and data, using visual elements like charts, graphs, and maps to communicate information clearly.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"INTERACTIVE SYSTEMS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Interactive systems are systems designed to facilitate user interaction, often incorporating feedback mechanisms to enhance user experience.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"USER EXPERIENCE DESIGN\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">User experience design is the process of enhancing user satisfaction by improving the usability, accessibility, and pleasure provided in the interaction with a product.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"SOFTWARE ENGINEERING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Software engineering is the application of engineering principles to software development in a methodical way, encompassing the entire software development life cycle.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"DEVOPS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">DevOps is a set of practices that combines software development (Dev) and IT operations (Ops) to shorten the systems development life cycle and provide continuous delivery.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"BLOCKCHAIN\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Blockchain is a decentralized digital ledger technology that records transactions across many computers in a way that the registered transactions cannot be altered retroactively.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"QUANTUM COMPUTING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Quantum computing is a type of computation that takes advantage of quantum mechanics to process information in fundamentally different ways than classical computers.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"NEUROMORPHIC COMPUTING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Neuromorphic computing is a design approach that mimics the neural structure and functioning of the human brain to improve computational efficiency and performance.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ROBOTICS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Robotics is the branch of technology that deals with the design, construction, operation, and use of robots, often involving AI and machine learning techniques.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"AUTONOMOUS SYSTEMS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Autonomous systems are systems capable of performing tasks without human intervention, often utilizing AI and machine learning for decision-making.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"INTERNET OF THINGS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">The Internet of Things (IoT) refers to the interconnected network of physical devices that communicate and exchange data over the internet, often utilizing AI for data analysis and decision-making.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"VIRTUAL REALITY\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Virtual reality is a simulated experience that can be similar to or completely different from the real world, often used in gaming, training, and education.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"AUGMENTED REALITY\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Augmented reality is an interactive experience where real-world environments are enhanced by computer-generated perceptual information, often used in applications like gaming and education.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"MACHINE VISION\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Machine vision is the technology and methods used to provide imaging-based automatic inspection and analysis for process control and robot guidance.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"SENSOR NETWORKS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Sensor networks are networks of spatially distributed sensors that monitor physical or environmental conditions, often used in IoT applications.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"CYBERSECURITY\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Cybersecurity is the practice of protecting systems, networks, and programs from digital attacks, often involving various technologies and strategies.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"DATA PRIVACY\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Data privacy refers to the proper handling, processing, storage, and usage of personal data, ensuring that individuals' privacy rights are respected.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ETHICAL AI\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Ethical AI refers to the development and implementation of artificial intelligence systems that adhere to ethical principles and guidelines, ensuring fairness, accountability, and transparency.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"AI GOVERNANCE\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">AI governance involves the frameworks and policies that guide the development and use of AI technologies, ensuring they are used responsibly and ethically.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"SUSTAINABLE TECHNOLOGY\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Sustainable technology refers to technologies that are designed to have a minimal impact on the environment, promoting sustainability and reducing resource consumption.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"CIRCULAR ECONOMY\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Circular economy is an economic system aimed at eliminating waste and the continual use of resources, often involving innovative technologies and practices.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"GREEN TECHNOLOGY\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Green technology refers to technology that is environmentally friendly and sustainable, often focusing on renewable energy and resource efficiency.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"SMART CITIES\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Smart cities are urban areas that use various types of electronic data collection sensors to supply information used to manage assets and resources efficiently.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"DIGITAL TRANSFORMATION\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Digital transformation refers to the integration of digital technology into all areas of a business, fundamentally changing how it operates and delivers value to customers.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"INDUSTRY 4.0\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Industry 4.0 refers to the current trend of automation and data exchange in manufacturing technologies, including cyber-physical systems, IoT, and cloud computing.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"AGRICULTURAL TECHNOLOGY\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Agricultural technology refers to the use of technology in agriculture to improve yield, efficiency, and sustainability, often involving precision farming techniques.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"FINTECH\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Fintech refers to technology that aims to improve and automate the delivery and use of financial services, often involving innovative solutions for banking and payments.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"HEALTH TECH\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Health tech refers to technology that aims to improve health and healthcare delivery, often involving digital health solutions and medical devices.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"EDTECH\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Edtech refers to technology that enhances education and learning experiences, often involving online learning platforms and educational software.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"MOBILITY TECHNOLOGY\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Mobility technology refers to technology that enhances transportation and mobility solutions, often involving smart transportation systems and electric vehicles.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"SPACE TECHNOLOGY\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Space technology refers to the technology used in the exploration and utilization of outer space, often involving advanced engineering and scientific research.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"AEROSPACE ENGINEERING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Aerospace engineering is the branch of engineering that deals with the design, development, and production of aircraft and spacecraft.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"ROBOTIC PROCESS AUTOMATION\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Robotic process automation refers to the use of software robots to automate repetitive tasks, improving efficiency and accuracy in business processes.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"QUANTUM MACHINE LEARNING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Quantum machine learning is an interdisciplinary field that combines quantum computing and machine learning, exploring how quantum algorithms can enhance machine learning tasks.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"BIOINFORMATICS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data, particularly in genomics and molecular biology.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"NEUROSCIENCE\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Neuroscience is the scientific study of the nervous system, often intersecting with fields like artificial intelligence and cognitive science.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"COGNITIVE SCIENCE\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Cognitive science is the interdisciplinary study of the mind and its processes, including how people think, learn, and remember.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"SOCIAL ROBOTICS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Social robotics is a field of study focused on the design and development of robots that can interact with humans in a social context.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"HUMAN-ROBOT INTERACTION\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Human-robot interaction is the study of how humans and robots communicate and collaborate, often focusing on improving user experience and effectiveness.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"VIRTUAL ASSISTANTS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Virtual assistants are AI-powered software agents that can perform tasks or services for individuals based on commands or questions.<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>    <node id=\"CHATBOTS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Chatbots are AI programs that simulate<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"1b1399c76420a477c0c97893d258ae69","chunk":" The AI Scientist:\nTowards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292 , 2024b.\nCong Lu, Shengran Hu, and Jeff Clune. Intelligent go-explore: Standing on the shoulders of giant\nfoundation models. arXiv preprint arXiv:2405.15143 , 2024c.\nZhichao Lu, Ian Whalen, Vishnu Boddeti, Yashesh Dhebar, Kalyanmoy Deb, Erik Goodman, and\nWolfgang Banzhaf. Nsga-net: neural architecture search using multi-objective genetic algorithm.\nInProceedings of the genetic and evolutionary computation conference , pp. 419\u2013427, 2019.\nYecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman,\nYuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding\nlarge language models. In The Twelfth International Conference on Learning Representations , 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with\nself-feedback. Advances in Neural Information Processing Systems , 36, 2024.\nMeta. Open source ai is the path forward. https:\/\/about.fb.com\/news\/2024\/07\/\nopen-source-ai-is-the-path-forward\/ , July 2024. News article.\nElliot Meyerson, Mark J Nelson, Herbie Bradley, Adam Gaier, Arash Moradi, Amy K Hoover, and\nJoel Lehman. Language model crossover: Variation through few-shot prompting. arXiv preprint\narXiv:2302.12170 , 2023.\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing\nenglish math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics , pp. 975\u2013984, 2020.\nJean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. arXiv preprint\narXiv:1504.04909 , 2015.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint arXiv:2112.09332 , 2021.\nAndrew Ng. Issue 253. https:\/\/www.deeplearning.ai\/the-batch\/issue-253\/ , June 2024.\nNewsletter issue.\nBen Norman and Jeff Clune. First-explore, then exploit: Meta-learning intelligent exploration. arXiv\npreprint arXiv:2307.02276 , 2023.\nOpenAI. Introducing chatgpt. https:\/\/openai.com\/index\/chatgpt\/ , November 2022. Blog\npost.\nOpenAI. Simple evals, 2023. URL https:\/\/github.com\/openai\/simple-evals . Accessed:\n2024-08-10.\nOpenAI. Gpt-4 technical report, 2024.\nJoon Sung Park, Joseph O\u2019Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S\nBernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th\nannual acm symposium on user interface software and technology , pp. 1\u201322, 2023.\n18Automated Design of Agentic Systems\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies , pp. 2080\u20132094, Online,\nJune 2021. Association for Computational Linguistics. doi: 10.18653\/v1\/2021.naacl-main.168.\nChen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong\nSun. Communicative agents for software development. arXiv preprint arXiv:2307.07924 , 2023.\nChen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang,\nZhiyuan Liu, and Maosong Sun. Scaling large-language-model-based multi-agent collaboration.\narXiv preprint arXiv:2406.07155 , 2024.\nChangle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong\nWen. Tool learning with large language models: A survey. arXiv preprint arXiv:2405.17935 , 2024.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. Advances in\nNeural Information Processing Systems , ","chunk_id":"1b1399c76420a477c0c97893d258ae69","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"AI SCIENTIST","type":"RESEARCH","description":"The AI Scientist is a concept focused on fully automated open-ended scientific discovery, as discussed in a preprint document.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"CONG LU","type":"PERSON","description":"Cong Lu is a researcher who co-authored a paper on intelligent exploration using foundation models.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"SHENGRAN HU","type":"PERSON","description":"Shengran Hu is a researcher who co-authored a paper on intelligent exploration using foundation models.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is a researcher involved in multiple studies related to AI and intelligent exploration.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"ZHICHAO LU","type":"PERSON","description":"Zhichao Lu is a researcher who co-authored a paper on neural architecture search using genetic algorithms.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"IAN WHALEN","type":"PERSON","description":"Ian Whalen is a researcher who co-authored a paper on neural architecture search using genetic algorithms.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"VISHNU BODDETI","type":"PERSON","description":"Vishnu Boddeti is a researcher who co-authored a paper on neural architecture search using genetic algorithms.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"YASHESH DHEBAR","type":"PERSON","description":"Yashesh Dhebar is a researcher who co-authored a paper on neural architecture search using genetic algorithms.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"KALYANMOY DEB","type":"PERSON","description":"Kalyanmoy Deb is a researcher who co-authored a paper on neural architecture search using genetic algorithms.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"ERIK GOODMAN","type":"PERSON","description":"Erik Goodman is a researcher who co-authored a paper on neural architecture search using genetic algorithms.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"WOLFGANG BANZHAF","type":"PERSON","description":"Wolfgang Banzhaf is a researcher who co-authored a paper on neural architecture search using genetic algorithms.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"YECHENG JASON MA","type":"PERSON","description":"Yecheng Jason Ma is a researcher who co-authored a paper on human-level reward design using large language models.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"WILLIAM LIANG","type":"PERSON","description":"William Liang is a researcher who co-authored a paper on human-level reward design using large language models.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"GUANZHI WANG","type":"PERSON","description":"Guanzhi Wang is a researcher who co-authored a paper on human-level reward design using large language models.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"DE-AN HUANG","type":"PERSON","description":"De-An Huang is a researcher who co-authored a paper on human-level reward design using large language models.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"OSBERT BASTANI","type":"PERSON","description":"Osbert Bastani is a researcher who co-authored a paper on human-level reward design using large language models.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"DINESH JAYARAMAN","type":"PERSON","description":"Dinesh Jayaraman is a researcher who co-authored a paper on human-level reward design using large language models.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"YUKE ZHU","type":"PERSON","description":"Yuke Zhu is a researcher who co-authored a paper on human-level reward design using large language models.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"LINXI FAN","type":"PERSON","description":"Linxi Fan is a researcher who co-authored a paper on human-level reward design using large language models.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"ANIMA ANANDKUMAR","type":"PERSON","description":"Anima Anandkumar is a researcher who co-authored a paper on human-level reward design using large language models.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AMAN MAADAAN","type":"PERSON","description":"Aman Madaan is a researcher who co-authored a paper on iterative refinement with self-feedback.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"NIKET TANDON","type":"PERSON","description":"Niket Tandon is a researcher who co-authored a paper on iterative refinement with self-feedback.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"PRAKHAR GUPTA","type":"PERSON","description":"Prakhar Gupta is a researcher who co-authored a paper on iterative refinement with self-feedback.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"SKYLER HALLINAN","type":"PERSON","description":"Skyler Hallinan is a researcher who co-authored a paper on iterative refinement with self-feedback.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"LUYU GAO","type":"PERSON","description":"Luyu Gao is a researcher who co-authored a paper on iterative refinement with self-feedback.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"SARAH WIEGREFFE","type":"PERSON","description":"Sarah Wiegreffe is a researcher who co-authored a paper on iterative refinement with self-feedback.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"URI ALON","type":"PERSON","description":"Uri Alon is a researcher who co-authored a paper on iterative refinement with self-feedback.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"NOUHA DZIRI","type":"PERSON","description":"Nouha Dziri is a researcher who co-authored a paper on iterative refinement with self-feedback.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"SHRIMAI PRABHUMOYE","type":"PERSON","description":"Shrimai Prabhumoye is a researcher who co-authored a paper on iterative refinement with self-feedback.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"YIMING YANG","type":"PERSON","description":"Yiming Yang is a researcher who co-authored a paper on iterative refinement with self-feedback.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"ELLIOT MEYERSON","type":"PERSON","description":"Elliot Meyerson is a researcher who co-authored a paper on language model crossover.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"MARK J NELSON","type":"PERSON","description":"Mark J Nelson is a researcher who co-authored a paper on language model crossover.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"HERBIE BRADLEY","type":"PERSON","description":"Herbie Bradley is a researcher who co-authored a paper on language model crossover.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"ADAM GAIER","type":"PERSON","description":"Adam Gaier is a researcher who co-authored a paper on language model crossover.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"ARASH MORADI","type":"PERSON","description":"Arash Moradi is a researcher who co-authored a paper on language model crossover.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AMY K HOOVER","type":"PERSON","description":"Amy K Hoover is a researcher who co-authored a paper on language model crossover.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"JOEL LEHMAN","type":"PERSON","description":"Joel Lehman is a researcher who co-authored a paper on language model crossover.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"JEAN-BAPTISTE MOURET","type":"PERSON","description":"Jean-Baptiste Mouret is a researcher who co-authored a paper on mapping elites in search spaces.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"REIICHIRO NAKANO","type":"PERSON","description":"Reiichiro Nakano is a researcher who co-authored a paper on browser-assisted question-answering.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"JACOB HILTON","type":"PERSON","description":"Jacob Hilton is a researcher who co-authored a paper on browser-assisted question-answering.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"SUCHIR BALAJI","type":"PERSON","description":"Suchir Balaji is a researcher who co-authored a paper on browser-assisted question-answering.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"JEFF WU","type":"PERSON","description":"Jeff Wu is a researcher who co-authored a paper on browser-assisted question-answering.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"LONG OUYANG","type":"PERSON","description":"Long Ouyang is a researcher who co-authored a paper on browser-assisted question-answering.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"CHRISTINA KIM","type":"PERSON","description":"Christina Kim is a researcher who co-authored a paper on browser-assisted question-answering.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"CHRISTOPHER HESSE","type":"PERSON","description":"Christopher Hesse is a researcher who co-authored a paper on browser-assisted question-answering.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"SHANTANU JAIN","type":"PERSON","description":"Shantanu Jain is a researcher who co-authored a paper on browser-assisted question-answering.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"VINEET KOSARAJU","type":"PERSON","description":"Vineet Kosaraju is a researcher who co-authored a paper on browser-assisted question-answering.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"WILLIAM SAUNDERS","type":"PERSON","description":"William Saunders is a researcher who co-authored a paper on browser-assisted question-answering.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"OPENAI","type":"ORGANIZATION","description":"OpenAI is an artificial intelligence research organization known for developing models like ChatGPT and GPT-4.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"CHATGPT","type":"TECHNOLOGY","description":"ChatGPT is a conversational AI model developed by OpenAI, designed to generate human-like text responses.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a state-of-the-art language model developed by OpenAI, known for its advanced capabilities in natural language processing.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"GENERIC AGENTS","type":"RESEARCH","description":"Generative agents are interactive models that simulate human behavior, as discussed in a paper by Joon Sung Park and others.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"JOON SUNG PARK","type":"PERSON","description":"Joon Sung Park is a researcher who co-authored a paper on generative agents.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"JOSEPH O\u2019BRIEN","type":"PERSON","description":"Joseph O\u2019Brien is a researcher who co-authored a paper on generative agents.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"CARRIE JUN CAI","type":"PERSON","description":"Carrie Jun Cai is a researcher who co-authored a paper on generative agents.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"MEREDITH RINGEL MORRIS","type":"PERSON","description":"Meredith Ringel Morris is a researcher who co-authored a paper on generative agents.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"PERCY LIANG","type":"PERSON","description":"Percy Liang is a researcher who co-authored a paper on generative agents.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"MICHAEL S BERNSTEIN","type":"PERSON","description":"Michael S Bernstein is a researcher who co-authored a paper on generative agents.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"LANGUAGE MODEL CROSSOVER","type":"","description":"\nLanguage model crossover is a technique that explores variations in language models through few-shot prompting, as discussed in a paper by Elliot Meyerson and others.","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"RESEARCH"},{"name":"ILLUMINATING SEARCH SPACES","type":"","description":"","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"BROWSER-ASSISTED QUESTION-ANSWERING","type":"","description":"\nBrowser-assisted question-answering is a technique that combines web browsing capabilities with question-answering systems to enhance information retrieval, as discussed in the context of WebGPT.","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"TECHNIQUE"},{"name":"SELF-REFINE","type":"TECHNIQUE","description":"Self-refine is a technique involving iterative refinement with self-feedback to improve model performance, as discussed in a paper by Aman Madaan and others.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"EUREKA","type":"RESEARCH","description":"Eureka is a project focused on human-level reward design via coding large language models, as discussed in a paper by Yecheng Jason Ma and others.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"NSGA-NET","type":"RESEARCH","description":"NSGA-Net is a method for neural architecture search using a multi-objective genetic algorithm, as discussed in a paper by Zhichao Lu and others.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"TOOL LEARNING","type":"RESEARCH","description":"Tool learning with large language models is a survey discussing how LLMs can learn to use tools effectively, as discussed in a paper by Changle Qu and others.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"COMMUNICATIVE AGENTS","type":"RESEARCH","description":"Communicative agents for software development are models designed to assist in software development through communication, as discussed in a paper by Chen Qian and others.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"SCALING MULTI-AGENT COLLABORATION","type":"RESEARCH","description":"Scaling large-language-model-based multi-agent collaboration refers to research on enhancing collaboration among multiple agents using large language models, as discussed in a paper by Chen Qian and others.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"DIRECT PREFERENCE OPTIMIZATION","type":"RESEARCH","description":"Direct preference optimization is a method that treats language models as reward models to improve their performance, as discussed in a paper by Rafael Rafailov and others.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE","type":"CONFERENCE","description":"The Genetic and Evolutionary Computation Conference is an academic conference where research related to genetic algorithms and evolutionary computation is presented, including work by Zhichao Lu and others.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"TWELFTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS","type":"CONFERENCE","description":"The Twelfth International Conference on Learning Representations is an academic conference focused on machine learning and representation learning, where research by Yecheng Jason Ma and others was presented.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS","type":"JOURNAL","description":"Advances in Neural Information Processing Systems is a prominent journal that publishes research in the field of neural information processing, including work by Aman Madaan and others.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"ASSOCIATION FOR COMPUTATIONAL LINGUISTICS","type":"ORGANIZATION","description":"The Association for Computational Linguistics is a professional organization that promotes research and education in computational linguistics, associated with various conferences and publications.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY","type":"CONFERENCE","description":"The ACM Symposium on User Interface Software and Technology is a conference focused on user interface software and technology, where research by Joon Sung Park and others was presented.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"DEEP LEARNING","type":"FIELD","description":"Deep learning is a subset of machine learning that uses neural networks with many layers to analyze various forms of data, relevant to many of the discussed papers.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"FOUNDATION MODELS","type":"CONCEPT","description":"Foundation models are large-scale models trained on diverse data that can be fine-tuned for various tasks, as referenced in the context of intelligent exploration.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"MULTI-OBJECTIVE OPTIMIZATION","type":"TECHNIQUE","description":"Multi-objective optimization is a technique used in various algorithms, including genetic algorithms, to optimize multiple conflicting objectives simultaneously, as discussed in the context of NSGA-Net.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"FEW-SHOT PROMPTING","type":"TECHNIQUE","description":"Few-shot prompting is a technique in which a model is given a few examples to learn from in order to perform a task, as discussed in the context of language model crossover.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"ITERATIVE REFINEMENT","type":"TECHNIQUE","description":"Iterative refinement is a process of gradually improving a model's performance through repeated adjustments, as discussed in the context of self-refine.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"HUMAN FEEDBACK","type":"CONCEPT","description":"Human feedback refers to the input provided by users to improve the performance of AI systems, particularly in the context of training models like WebGPT.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"META","type":"ORGANIZATION","description":"Meta is a technology company known for its work in social media and artificial intelligence, including initiatives related to open-source AI.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"GENETIC ALGORITHMS","type":"TECHNIQUE","description":"Genetic algorithms are optimization algorithms inspired by the process of natural selection, used in various research contexts including NSGA-Net.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"LARGE LANGUAGE MODELS","type":"TECHNOLOGY","description":"Large language models are AI models trained on vast amounts of text data to understand and generate human-like text, relevant to many discussed papers.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"MULTI-AGENT SYSTEMS","type":"CONCEPT","description":"Multi-agent systems refer to systems composed of multiple interacting agents, often used in AI research to study collaboration and communication.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"REWARD DESIGN","type":"CONCEPT","description":"Reward design involves creating reward structures for training AI models, particularly in reinforcement learning contexts, as discussed in the context of Eureka.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"MATH WORD PROBLEMS","type":"CONCEPT","description":"Math word problems are problems that require mathematical reasoning to solve, relevant to the research on evaluating and developing solvers.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"LANGUAGE MODELING","type":"FIELD","description":"Language modeling is a field of study focused on predicting the next word in a sequence, foundational to many AI applications including chatbots and text generation.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"INTERACTIVE SIMULACRA","type":"CONCEPT","description":"Interactive simulacra refer to models that simulate human behavior in an interactive manner, as discussed in the context of generative agents.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"HUMAN-LEVEL AI","type":"CONCEPT","description":"Human-level AI refers to artificial intelligence systems that can perform tasks at a level comparable to human intelligence, a goal of many research efforts.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AUTOMATED DESIGN","type":"CONCEPT","description":"Automated design refers to the use of algorithms and AI to create designs or solutions without human intervention, relevant to the context of agentic systems.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AGENTIC SYSTEMS","type":"CONCEPT","description":"Agentic systems are systems that can act autonomously and make decisions based on their environment, relevant to discussions on AI and automation.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"REINFORCEMENT LEARNING","type":"FIELD","description":"Reinforcement learning is a type of machine learning where agents learn to make decisions by receiving rewards or penalties, relevant to many discussed papers.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"NEURAL NETWORKS","type":"TECHNOLOGY","description":"Neural networks are computational models inspired by the human brain, widely used in AI for tasks such as image and speech recognition.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"NLP MODELS","type":"TECHNOLOGY","description":"NLP models are models designed for natural language processing tasks, relevant to many discussions in the context of AI research.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"MULTI-OBJECTIVE GENETIC ALGORITHM","type":"TECHNIQUE","description":"Multi-objective genetic algorithms are optimization techniques that evolve solutions to optimize multiple objectives simultaneously, as discussed in the context of NSGA-Net.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"HUMAN-CENTERED AI","type":"CONCEPT","description":"Human-centered AI refers to the design and implementation of AI systems that prioritize human needs and values, relevant to discussions on AI ethics and usability.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI ETHICS","type":"FIELD","description":"AI ethics is the field of study that examines the moral implications and societal impacts of artificial intelligence technologies.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"DATA SCIENCE","type":"FIELD","description":"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"COMPUTATIONAL LINGUISTICS","type":"FIELD","description":"Computational linguistics is the study of using computational methods to process and analyze human language, relevant to many discussed papers.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AUTOMATED SCIENTIFIC DISCOVERY","type":"CONCEPT","description":"Automated scientific discovery refers to the use of AI and algorithms to conduct scientific research and make discoveries without human intervention.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"RESEARCH COLLABORATION","type":"CONCEPT","description":"Research collaboration refers to the joint effort of researchers to achieve common goals, often seen in the co-authorship of papers.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI RESEARCH","type":"FIELD","description":"AI research encompasses the study and development of artificial intelligence technologies and methodologies.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"TECHNICAL REPORT","type":"DOCUMENT","description":"A technical report is a document that describes the process, progress, or results of technical or scientific research, often published by organizations like OpenAI.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"PREPRINT","type":"DOCUMENT","description":"A preprint is a version of a scholarly paper that precedes formal peer review and publication in a scientific journal, often shared on platforms like arXiv.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"PAPER","type":"DOCUMENT","description":"A paper is a written work that presents original research findings, often published in academic journals or presented at conferences.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"RESEARCH PAPER","type":"DOCUMENT","description":"A research paper is a detailed document that presents the results of original research, typically peer-reviewed and published in academic journals.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"TECHNOLOGY","type":"CONCEPT","description":"Technology refers to the application of scientific knowledge for practical purposes, especially in industry and research.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI TOOLS","type":"CONCEPT","description":"AI tools are software applications that utilize artificial intelligence to perform tasks, often enhancing productivity and efficiency in various fields.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"DATA ANALYSIS","type":"FIELD","description":"Data analysis is the process of inspecting, cleansing, transforming, and modeling data to discover useful information, inform conclusions, and support decision-making.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"MACHINE LEARNING","type":"FIELD","description":"Machine learning is a subset of AI that focuses on the development of algorithms that allow computers to learn from and make predictions based on data.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AUTOMATED SYSTEMS","type":"CONCEPT","description":"Automated systems are systems that operate automatically with minimal human intervention, often used in various applications including manufacturing and AI.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI MODELS","type":"TECHNOLOGY","description":"AI models are algorithms or mathematical representations that simulate human-like intelligence to perform specific tasks, relevant to many discussed papers.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"RESEARCH FINDINGS","type":"CONCEPT","description":"Research findings are the results or conclusions drawn from scientific studies, often published in academic papers.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI APPLICATIONS","type":"CONCEPT","description":"AI applications refer to the practical uses of artificial intelligence technologies in various fields, including healthcare, finance, and education.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"DATASETS","type":"CONCEPT","description":"Datasets are collections of data used for training and testing AI models, crucial for the development of machine learning algorithms.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI SYSTEMS","type":"CONCEPT","description":"AI systems are integrated systems that utilize artificial intelligence technologies to perform tasks or solve problems.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"RESEARCH INITIATIVES","type":"CONCEPT","description":"Research initiatives are organized efforts to conduct research in specific areas, often involving collaboration among multiple researchers or institutions.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI TECHNOLOGIES","type":"CONCEPT","description":"AI technologies encompass a wide range of tools and methods used to create intelligent systems, including machine learning, natural language processing, and robotics.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"COLLABORATIVE RESEARCH","type":"CONCEPT","description":"Collaborative research refers to research conducted by multiple parties working together towards a common goal, often resulting in co-authored publications.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI SOLUTIONS","type":"CONCEPT","description":"AI solutions are specific applications or systems designed to address particular problems using artificial intelligence technologies.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"RESEARCH NETWORKS","type":"CONCEPT","description":"Research networks are collaborative groups of researchers and institutions that work together to advance knowledge in specific fields.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI INNOVATIONS","type":"CONCEPT","description":"AI innovations refer to new and improved methods, technologies, or applications in the field of artificial intelligence.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI STRATEGIES","type":"CONCEPT","description":"AI strategies are plans or approaches developed to effectively implement artificial intelligence technologies in various contexts.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI TRENDS","type":"CONCEPT","description":"AI trends refer to the current directions and developments in the field of artificial intelligence, influencing research and applications.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI CHALLENGES","type":"CONCEPT","description":"AI challenges are obstacles or issues faced in the development and implementation of artificial intelligence technologies.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI FUTURE","type":"CONCEPT","description":"The future of AI refers to the anticipated developments and impacts of artificial intelligence technologies on society and various industries.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI RESEARCH COMMUNITY","type":"CONCEPT","description":"The AI research community encompasses researchers, practitioners, and organizations involved in the study and development of artificial intelligence.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI EDUCATION","type":"FIELD","description":"AI education refers to the teaching and learning of artificial intelligence concepts and technologies, often integrated into academic curricula.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI POLICY","type":"CONCEPT","description":"AI policy refers to the guidelines and regulations governing the development and use of artificial intelligence technologies.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI REGULATIONS","type":"CONCEPT","description":"AI regulations are laws and guidelines established to ensure the ethical and responsible use of artificial intelligence technologies.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI ETHICS COMMITTEES","type":"ORGANIZATION","description":"AI ethics committees are groups formed to address ethical considerations in the development and deployment of AI technologies.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI STANDARDS","type":"CONCEPT","description":"AI standards are established criteria and benchmarks for the development and evaluation of artificial intelligence technologies.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI FRAMEWORKS","type":"CONCEPT","description":"AI frameworks are structured approaches or methodologies for developing and implementing artificial intelligence systems.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI PLATFORMS","type":"CONCEPT","description":"AI platforms are software environments that provide tools and services for developing and deploying AI applications.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI RESEARCH FUNDING","type":"CONCEPT","description":"AI research funding refers to financial support provided for research projects in the field of artificial intelligence.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI STARTUPS","type":"CONCEPT","description":"AI startups are new companies focused on developing innovative artificial intelligence technologies and applications.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI INVESTMENT","type":"CONCEPT","description":"AI investment refers to the allocation of financial resources towards the development and commercialization of artificial intelligence technologies.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI CONSULTING","type":"CONCEPT","description":"AI consulting involves providing expert advice and services related to the implementation and use of artificial intelligence technologies.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI RESEARCH LABS","type":"ORGANIZATION","description":"AI research labs are specialized facilities dedicated to conducting research and development in the field of artificial intelligence.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI CONFERENCES","type":"EVENT","description":"AI conferences are events where researchers and practitioners gather to share knowledge and advancements in artificial intelligence.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI SYMPOSIUMS","type":"EVENT","description":"AI symposiums are formal meetings or conferences focused on discussing specific topics in artificial intelligence.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI WORKSHOPS","type":"EVENT","description":"AI workshops are interactive sessions where participants engage in hands-on learning and discussions about artificial intelligence topics.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI SEMINARS","type":"EVENT","description":"AI seminars are educational sessions focused on specific aspects of artificial intelligence, often featuring expert speakers.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI WEBINARS","type":"EVENT","description":"AI webinars are online seminars that provide information and discussions on various artificial intelligence topics.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI PODCASTS","type":"MEDIA","description":"AI podcasts are audio programs that discuss topics related to artificial intelligence, featuring interviews and expert insights.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI BLOGS","type":"MEDIA","description":"AI blogs are online platforms where articles and discussions about artificial intelligence are published.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI NEWSLETTERS","type":"MEDIA","description":"AI newsletters are periodic publications that provide updates and insights on developments in the field of artificial intelligence.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI REPORTS","type":"DOCUMENT","description":"AI reports are comprehensive documents that analyze trends, challenges, and advancements in artificial intelligence.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI WHITE PAPERS","type":"DOCUMENT","description":"AI white papers are authoritative reports that provide detailed information on specific topics in artificial intelligence.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI CASE STUDIES","type":"DOCUMENT","description":"AI case studies are detailed analyses of specific applications or implementations of artificial intelligence technologies.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI SURVEYS","type":"DOCUMENT","description":"AI surveys are research studies that gather data and insights on various aspects of artificial intelligence.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI ANALYTICS","type":"FIELD","description":"AI analytics refers to the use of artificial intelligence techniques to analyze data and extract insights.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI METRICS","type":"CONCEPT","description":"AI metrics are standards used to measure the performance and effectiveness of artificial intelligence systems.","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"AI BENCHMARKS","type":"CONCEPT","description":"AI benchmarks are reference points used","source_id":"1b1399c76420a477c0c97893d258ae69"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AI SCIENTIST\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">The AI Scientist is a concept focused on fully automated open-ended scientific discovery, as discussed in a preprint document.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"CONG LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cong Lu is a researcher who co-authored a paper on intelligent exploration using foundation models.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"SHENGRAN HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengran Hu is a researcher who co-authored a paper on intelligent exploration using foundation models.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is a researcher involved in multiple studies related to AI and intelligent exploration.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"ZHICHAO LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhichao Lu is a researcher who co-authored a paper on neural architecture search using genetic algorithms.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"IAN WHALEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ian Whalen is a researcher who co-authored a paper on neural architecture search using genetic algorithms.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"VISHNU BODDETI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vishnu Boddeti is a researcher who co-authored a paper on neural architecture search using genetic algorithms.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"YASHESH DHEBAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yashesh Dhebar is a researcher who co-authored a paper on neural architecture search using genetic algorithms.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"KALYANMOY DEB\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kalyanmoy Deb is a researcher who co-authored a paper on neural architecture search using genetic algorithms.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"ERIK GOODMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Erik Goodman is a researcher who co-authored a paper on neural architecture search using genetic algorithms.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"WOLFGANG BANZHAF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wolfgang Banzhaf is a researcher who co-authored a paper on neural architecture search using genetic algorithms.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"YECHENG JASON MA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yecheng Jason Ma is a researcher who co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"WILLIAM LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">William Liang is a researcher who co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"GUANZHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guanzhi Wang is a researcher who co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"DE-AN HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">De-An Huang is a researcher who co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"OSBERT BASTANI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Osbert Bastani is a researcher who co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"DINESH JAYARAMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dinesh Jayaraman is a researcher who co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"YUKE ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuke Zhu is a researcher who co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"LINXI FAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linxi Fan is a researcher who co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"ANIMA ANANDKUMAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anima Anandkumar is a researcher who co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AMAN MAADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aman Madaan is a researcher who co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"NIKET TANDON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Niket Tandon is a researcher who co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"PRAKHAR GUPTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Prakhar Gupta is a researcher who co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"SKYLER HALLINAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Skyler Hallinan is a researcher who co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"LUYU GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luyu Gao is a researcher who co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"SARAH WIEGREFFE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sarah Wiegreffe is a researcher who co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"URI ALON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Uri Alon is a researcher who co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"NOUHA DZIRI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nouha Dziri is a researcher who co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"SHRIMAI PRABHUMOYE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shrimai Prabhumoye is a researcher who co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"YIMING YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiming Yang is a researcher who co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"ELLIOT MEYERSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Elliot Meyerson is a researcher who co-authored a paper on language model crossover.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"MARK J NELSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mark J Nelson is a researcher who co-authored a paper on language model crossover.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"HERBIE BRADLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Herbie Bradley is a researcher who co-authored a paper on language model crossover.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"ADAM GAIER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adam Gaier is a researcher who co-authored a paper on language model crossover.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"ARASH MORADI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arash Moradi is a researcher who co-authored a paper on language model crossover.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AMY K HOOVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amy K Hoover is a researcher who co-authored a paper on language model crossover.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"JOEL LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Lehman is a researcher who co-authored a paper on language model crossover.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"JEAN-BAPTISTE MOURET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jean-Baptiste Mouret is a researcher who co-authored a paper on mapping elites in search spaces.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"REIICHIRO NAKANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Reiichiro Nakano is a researcher who co-authored a paper on browser-assisted question-answering.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"JACOB HILTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Hilton is a researcher who co-authored a paper on browser-assisted question-answering.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"SUCHIR BALAJI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Suchir Balaji is a researcher who co-authored a paper on browser-assisted question-answering.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"JEFF WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Wu is a researcher who co-authored a paper on browser-assisted question-answering.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"LONG OUYANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Long Ouyang is a researcher who co-authored a paper on browser-assisted question-answering.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"CHRISTINA KIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Christina Kim is a researcher who co-authored a paper on browser-assisted question-answering.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"CHRISTOPHER HESSE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Christopher Hesse is a researcher who co-authored a paper on browser-assisted question-answering.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"SHANTANU JAIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shantanu Jain is a researcher who co-authored a paper on browser-assisted question-answering.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"VINEET KOSARAJU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vineet Kosaraju is a researcher who co-authored a paper on browser-assisted question-answering.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"WILLIAM SAUNDERS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">William Saunders is a researcher who co-authored a paper on browser-assisted question-answering.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"OPENAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">OpenAI is an artificial intelligence research organization known for developing models like ChatGPT and GPT-4.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"CHATGPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ChatGPT is a conversational AI model developed by OpenAI, designed to generate human-like text responses.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a state-of-the-art language model developed by OpenAI, known for its advanced capabilities in natural language processing.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"GENERIC AGENTS\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Generative agents are interactive models that simulate human behavior, as discussed in a paper by Joon Sung Park and others.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"JOON SUNG PARK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joon Sung Park is a researcher who co-authored a paper on generative agents.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"JOSEPH O&#8217;BRIEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joseph O&#8217;Brien is a researcher who co-authored a paper on generative agents.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"CARRIE JUN CAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carrie Jun Cai is a researcher who co-authored a paper on generative agents.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"MEREDITH RINGEL MORRIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Meredith Ringel Morris is a researcher who co-authored a paper on generative agents.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"PERCY LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Percy Liang is a researcher who co-authored a paper on generative agents.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"MICHAEL S BERNSTEIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael S Bernstein is a researcher who co-authored a paper on generative agents.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"LANGUAGE MODEL CROSSOVER\">      <data key=\"d0\" \/>      <data key=\"d1\">Language model crossover is a technique that explores variations in language models through few-shot prompting, as discussed in a paper by Elliot Meyerson and others.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">RESEARCH<\/data>    <\/node>    <node id=\"ILLUMINATING SEARCH SPACES\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"BROWSER-ASSISTED QUESTION-ANSWERING\">      <data key=\"d0\" \/>      <data key=\"d1\">Browser-assisted question-answering is a technique that combines web browsing capabilities with question-answering systems to enhance information retrieval, as discussed in the context of WebGPT.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-refine is a technique involving iterative refinement with self-feedback to improve model performance, as discussed in a paper by Aman Madaan and others.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"EUREKA\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Eureka is a project focused on human-level reward design via coding large language models, as discussed in a paper by Yecheng Jason Ma and others.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"NSGA-NET\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">NSGA-Net is a method for neural architecture search using a multi-objective genetic algorithm, as discussed in a paper by Zhichao Lu and others.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"TOOL LEARNING\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Tool learning with large language models is a survey discussing how LLMs can learn to use tools effectively, as discussed in a paper by Changle Qu and others.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"COMMUNICATIVE AGENTS\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Communicative agents for software development are models designed to assist in software development through communication, as discussed in a paper by Chen Qian and others.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"SCALING MULTI-AGENT COLLABORATION\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Scaling large-language-model-based multi-agent collaboration refers to research on enhancing collaboration among multiple agents using large language models, as discussed in a paper by Chen Qian and others.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"DIRECT PREFERENCE OPTIMIZATION\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Direct preference optimization is a method that treats language models as reward models to improve their performance, as discussed in a paper by Rafael Rafailov and others.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE\">      <data key=\"d0\">CONFERENCE<\/data>      <data key=\"d1\">The Genetic and Evolutionary Computation Conference is an academic conference where research related to genetic algorithms and evolutionary computation is presented, including work by Zhichao Lu and others.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"TWELFTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS\">      <data key=\"d0\">CONFERENCE<\/data>      <data key=\"d1\">The Twelfth International Conference on Learning Representations is an academic conference focused on machine learning and representation learning, where research by Yecheng Jason Ma and others was presented.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">Advances in Neural Information Processing Systems is a prominent journal that publishes research in the field of neural information processing, including work by Aman Madaan and others.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"ASSOCIATION FOR COMPUTATIONAL LINGUISTICS\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">The Association for Computational Linguistics is a professional organization that promotes research and education in computational linguistics, associated with various conferences and publications.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY\">      <data key=\"d0\">CONFERENCE<\/data>      <data key=\"d1\">The ACM Symposium on User Interface Software and Technology is a conference focused on user interface software and technology, where research by Joon Sung Park and others was presented.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"DEEP LEARNING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Deep learning is a subset of machine learning that uses neural networks with many layers to analyze various forms of data, relevant to many of the discussed papers.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"FOUNDATION MODELS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Foundation models are large-scale models trained on diverse data that can be fine-tuned for various tasks, as referenced in the context of intelligent exploration.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"MULTI-OBJECTIVE OPTIMIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Multi-objective optimization is a technique used in various algorithms, including genetic algorithms, to optimize multiple conflicting objectives simultaneously, as discussed in the context of NSGA-Net.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"FEW-SHOT PROMPTING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Few-shot prompting is a technique in which a model is given a few examples to learn from in order to perform a task, as discussed in the context of language model crossover.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"ITERATIVE REFINEMENT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Iterative refinement is a process of gradually improving a model's performance through repeated adjustments, as discussed in the context of self-refine.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"HUMAN FEEDBACK\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Human feedback refers to the input provided by users to improve the performance of AI systems, particularly in the context of training models like WebGPT.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"META\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Meta is a technology company known for its work in social media and artificial intelligence, including initiatives related to open-source AI.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"GENETIC ALGORITHMS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Genetic algorithms are optimization algorithms inspired by the process of natural selection, used in various research contexts including NSGA-Net.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"LARGE LANGUAGE MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Large language models are AI models trained on vast amounts of text data to understand and generate human-like text, relevant to many discussed papers.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"MULTI-AGENT SYSTEMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Multi-agent systems refer to systems composed of multiple interacting agents, often used in AI research to study collaboration and communication.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"REWARD DESIGN\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Reward design involves creating reward structures for training AI models, particularly in reinforcement learning contexts, as discussed in the context of Eureka.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"MATH WORD PROBLEMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Math word problems are problems that require mathematical reasoning to solve, relevant to the research on evaluating and developing solvers.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"LANGUAGE MODELING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Language modeling is a field of study focused on predicting the next word in a sequence, foundational to many AI applications including chatbots and text generation.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"INTERACTIVE SIMULACRA\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Interactive simulacra refer to models that simulate human behavior in an interactive manner, as discussed in the context of generative agents.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"HUMAN-LEVEL AI\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Human-level AI refers to artificial intelligence systems that can perform tasks at a level comparable to human intelligence, a goal of many research efforts.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AUTOMATED DESIGN\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Automated design refers to the use of algorithms and AI to create designs or solutions without human intervention, relevant to the context of agentic systems.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AGENTIC SYSTEMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Agentic systems are systems that can act autonomously and make decisions based on their environment, relevant to discussions on AI and automation.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"REINFORCEMENT LEARNING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Reinforcement learning is a type of machine learning where agents learn to make decisions by receiving rewards or penalties, relevant to many discussed papers.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"NEURAL NETWORKS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Neural networks are computational models inspired by the human brain, widely used in AI for tasks such as image and speech recognition.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"NLP MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">NLP models are models designed for natural language processing tasks, relevant to many discussions in the context of AI research.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"MULTI-OBJECTIVE GENETIC ALGORITHM\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Multi-objective genetic algorithms are optimization techniques that evolve solutions to optimize multiple objectives simultaneously, as discussed in the context of NSGA-Net.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"HUMAN-CENTERED AI\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Human-centered AI refers to the design and implementation of AI systems that prioritize human needs and values, relevant to discussions on AI ethics and usability.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI ETHICS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">AI ethics is the field of study that examines the moral implications and societal impacts of artificial intelligence technologies.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"DATA SCIENCE\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"COMPUTATIONAL LINGUISTICS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Computational linguistics is the study of using computational methods to process and analyze human language, relevant to many discussed papers.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AUTOMATED SCIENTIFIC DISCOVERY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Automated scientific discovery refers to the use of AI and algorithms to conduct scientific research and make discoveries without human intervention.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"RESEARCH COLLABORATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Research collaboration refers to the joint effort of researchers to achieve common goals, often seen in the co-authorship of papers.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI RESEARCH\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">AI research encompasses the study and development of artificial intelligence technologies and methodologies.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"TECHNICAL REPORT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">A technical report is a document that describes the process, progress, or results of technical or scientific research, often published by organizations like OpenAI.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"PREPRINT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">A preprint is a version of a scholarly paper that precedes formal peer review and publication in a scientific journal, often shared on platforms like arXiv.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"PAPER\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">A paper is a written work that presents original research findings, often published in academic journals or presented at conferences.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"RESEARCH PAPER\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">A research paper is a detailed document that presents the results of original research, typically peer-reviewed and published in academic journals.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"TECHNOLOGY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Technology refers to the application of scientific knowledge for practical purposes, especially in industry and research.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI TOOLS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI tools are software applications that utilize artificial intelligence to perform tasks, often enhancing productivity and efficiency in various fields.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"DATA ANALYSIS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Data analysis is the process of inspecting, cleansing, transforming, and modeling data to discover useful information, inform conclusions, and support decision-making.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"MACHINE LEARNING\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">Machine learning is a subset of AI that focuses on the development of algorithms that allow computers to learn from and make predictions based on data.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AUTOMATED SYSTEMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Automated systems are systems that operate automatically with minimal human intervention, often used in various applications including manufacturing and AI.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AI models are algorithms or mathematical representations that simulate human-like intelligence to perform specific tasks, relevant to many discussed papers.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"RESEARCH FINDINGS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Research findings are the results or conclusions drawn from scientific studies, often published in academic papers.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI APPLICATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI applications refer to the practical uses of artificial intelligence technologies in various fields, including healthcare, finance, and education.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"DATASETS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Datasets are collections of data used for training and testing AI models, crucial for the development of machine learning algorithms.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI SYSTEMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI systems are integrated systems that utilize artificial intelligence technologies to perform tasks or solve problems.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"RESEARCH INITIATIVES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Research initiatives are organized efforts to conduct research in specific areas, often involving collaboration among multiple researchers or institutions.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI TECHNOLOGIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI technologies encompass a wide range of tools and methods used to create intelligent systems, including machine learning, natural language processing, and robotics.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"COLLABORATIVE RESEARCH\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Collaborative research refers to research conducted by multiple parties working together towards a common goal, often resulting in co-authored publications.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI SOLUTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI solutions are specific applications or systems designed to address particular problems using artificial intelligence technologies.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"RESEARCH NETWORKS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Research networks are collaborative groups of researchers and institutions that work together to advance knowledge in specific fields.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI INNOVATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI innovations refer to new and improved methods, technologies, or applications in the field of artificial intelligence.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI STRATEGIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI strategies are plans or approaches developed to effectively implement artificial intelligence technologies in various contexts.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI TRENDS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI trends refer to the current directions and developments in the field of artificial intelligence, influencing research and applications.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI CHALLENGES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI challenges are obstacles or issues faced in the development and implementation of artificial intelligence technologies.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI FUTURE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The future of AI refers to the anticipated developments and impacts of artificial intelligence technologies on society and various industries.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI RESEARCH COMMUNITY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The AI research community encompasses researchers, practitioners, and organizations involved in the study and development of artificial intelligence.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI EDUCATION\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">AI education refers to the teaching and learning of artificial intelligence concepts and technologies, often integrated into academic curricula.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI POLICY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI policy refers to the guidelines and regulations governing the development and use of artificial intelligence technologies.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI REGULATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI regulations are laws and guidelines established to ensure the ethical and responsible use of artificial intelligence technologies.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI ETHICS COMMITTEES\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">AI ethics committees are groups formed to address ethical considerations in the development and deployment of AI technologies.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI STANDARDS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI standards are established criteria and benchmarks for the development and evaluation of artificial intelligence technologies.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI FRAMEWORKS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI frameworks are structured approaches or methodologies for developing and implementing artificial intelligence systems.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI PLATFORMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI platforms are software environments that provide tools and services for developing and deploying AI applications.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI RESEARCH FUNDING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI research funding refers to financial support provided for research projects in the field of artificial intelligence.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI STARTUPS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI startups are new companies focused on developing innovative artificial intelligence technologies and applications.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI INVESTMENT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI investment refers to the allocation of financial resources towards the development and commercialization of artificial intelligence technologies.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI CONSULTING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI consulting involves providing expert advice and services related to the implementation and use of artificial intelligence technologies.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI RESEARCH LABS\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">AI research labs are specialized facilities dedicated to conducting research and development in the field of artificial intelligence.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI CONFERENCES\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">AI conferences are events where researchers and practitioners gather to share knowledge and advancements in artificial intelligence.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI SYMPOSIUMS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">AI symposiums are formal meetings or conferences focused on discussing specific topics in artificial intelligence.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI WORKSHOPS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">AI workshops are interactive sessions where participants engage in hands-on learning and discussions about artificial intelligence topics.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI SEMINARS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">AI seminars are educational sessions focused on specific aspects of artificial intelligence, often featuring expert speakers.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI WEBINARS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">AI webinars are online seminars that provide information and discussions on various artificial intelligence topics.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI PODCASTS\">      <data key=\"d0\">MEDIA<\/data>      <data key=\"d1\">AI podcasts are audio programs that discuss topics related to artificial intelligence, featuring interviews and expert insights.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI BLOGS\">      <data key=\"d0\">MEDIA<\/data>      <data key=\"d1\">AI blogs are online platforms where articles and discussions about artificial intelligence are published.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI NEWSLETTERS\">      <data key=\"d0\">MEDIA<\/data>      <data key=\"d1\">AI newsletters are periodic publications that provide updates and insights on developments in the field of artificial intelligence.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI REPORTS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">AI reports are comprehensive documents that analyze trends, challenges, and advancements in artificial intelligence.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI WHITE PAPERS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">AI white papers are authoritative reports that provide detailed information on specific topics in artificial intelligence.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI CASE STUDIES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">AI case studies are detailed analyses of specific applications or implementations of artificial intelligence technologies.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI SURVEYS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">AI surveys are research studies that gather data and insights on various aspects of artificial intelligence.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI ANALYTICS\">      <data key=\"d0\">FIELD<\/data>      <data key=\"d1\">AI analytics refers to the use of artificial intelligence techniques to analyze data and extract insights.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI METRICS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI metrics are standards used to measure the performance and effectiveness of artificial intelligence systems.<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"AI BENCHMARKS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI benchmarks are reference points used<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <edge source=\"AI SCIENTIST\" target=\"JEFF CLUNE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Jeff Clune is involved in research related to the AI Scientist concept, contributing to automated scientific discovery.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"CONG LU\" target=\"JEFF CLUNE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Cong Lu and Jeff Clune co-authored a paper on intelligent exploration using foundation models.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"SHENGRAN HU\" target=\"JEFF CLUNE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Shengran Hu and Jeff Clune co-authored a paper on intelligent exploration using foundation models.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"JEFF CLUNE\" target=\"ZHICHAO LU\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Zhichao Lu and Jeff Clune co-authored a paper on neural architecture search using genetic algorithms.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"ZHICHAO LU\" target=\"IAN WHALEN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ian Whalen and Zhichao Lu co-authored a paper on neural architecture search using genetic algorithms.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"ZHICHAO LU\" target=\"VISHNU BODDETI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Vishnu Boddeti and Zhichao Lu co-authored a paper on neural architecture search using genetic algorithms.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"ZHICHAO LU\" target=\"YASHESH DHEBAR\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yashesh Dhebar and Zhichao Lu co-authored a paper on neural architecture search using genetic algorithms.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"ZHICHAO LU\" target=\"KALYANMOY DEB\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Kalyanmoy Deb and Zhichao Lu co-authored a paper on neural architecture search using genetic algorithms.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"ZHICHAO LU\" target=\"ERIK GOODMAN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Erik Goodman and Zhichao Lu co-authored a paper on neural architecture search using genetic algorithms.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"ZHICHAO LU\" target=\"WOLFGANG BANZHAF\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Wolfgang Banzhaf and Zhichao Lu co-authored a paper on neural architecture search using genetic algorithms.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"YECHENG JASON MA\" target=\"ANIMA ANANDKUMAR\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yecheng Jason Ma and Anima Anandkumar co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"YECHENG JASON MA\" target=\"WILLIAM LIANG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">William Liang and Yecheng Jason Ma co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"YECHENG JASON MA\" target=\"GUANZHI WANG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Guanzhi Wang and Yecheng Jason Ma co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"YECHENG JASON MA\" target=\"DE-AN HUANG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">De-An Huang and Yecheng Jason Ma co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"YECHENG JASON MA\" target=\"OSBERT BASTANI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Osbert Bastani and Yecheng Jason Ma co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"YECHENG JASON MA\" target=\"DINESH JAYARAMAN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Dinesh Jayaraman and Yecheng Jason Ma co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"YECHENG JASON MA\" target=\"YUKE ZHU\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yuke Zhu and Yecheng Jason Ma co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"YECHENG JASON MA\" target=\"LINXI FAN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Linxi Fan and Yecheng Jason Ma co-authored a paper on human-level reward design using large language models.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"AMAN MAADAAN\" target=\"NIKET TANDON\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Aman Madaan and Niket Tandon co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"AMAN MAADAAN\" target=\"PRAKHAR GUPTA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Prakhar Gupta and Aman Madaan co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"AMAN MAADAAN\" target=\"SKYLER HALLINAN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Skyler Hallinan and Aman Madaan co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"AMAN MAADAAN\" target=\"LUYU GAO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Luyu Gao and Aman Madaan co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"AMAN MAADAAN\" target=\"SARAH WIEGREFFE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Sarah Wiegreffe and Aman Madaan co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"AMAN MAADAAN\" target=\"URI ALON\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Uri Alon and Aman Madaan co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"AMAN MAADAAN\" target=\"NOUHA DZIRI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Nouha Dziri and Aman Madaan co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"AMAN MAADAAN\" target=\"SHRIMAI PRABHUMOYE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Shrimai Prabhumoye and Aman Madaan co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"AMAN MAADAAN\" target=\"YIMING YANG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yiming Yang and Aman Madaan co-authored a paper on iterative refinement with self-feedback.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"ELLIOT MEYERSON\" target=\"LANGUAGE MODEL CROSSOVER\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Elliot Meyerson is a co-author of a paper discussing language model crossover techniques.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"MARK J NELSON\" target=\"LANGUAGE MODEL CROSSOVER\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mark J Nelson is a co-author of a paper discussing language model crossover techniques.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"HERBIE BRADLEY\" target=\"LANGUAGE MODEL CROSSOVER\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Herbie Bradley is a co-author of a paper discussing language model crossover techniques.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"ADAM GAIER\" target=\"LANGUAGE MODEL CROSSOVER\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Adam Gaier is a co-author of a paper discussing language model crossover techniques.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"ARASH MORADI\" target=\"LANGUAGE MODEL CROSSOVER\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Arash Moradi is a co-author of a paper discussing language model crossover techniques.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"AMY K HOOVER\" target=\"LANGUAGE MODEL CROSSOVER\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Amy K Hoover is a co-author of a paper discussing language model crossover techniques.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"JOEL LEHMAN\" target=\"LANGUAGE MODEL CROSSOVER\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Joel Lehman is a co-author of a paper discussing language model crossover techniques.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"JEAN-BAPTISTE MOURET\" target=\"ILLUMINATING SEARCH SPACES\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jean-Baptiste Mouret co-authored a paper on mapping elites to illuminate search spaces.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"REIICHIRO NAKANO\" target=\"BROWSER-ASSISTED QUESTION-ANSWERING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Reiichiro Nakano is a co-author of a paper discussing browser-assisted question-answering techniques.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>    <edge source=\"JACOB HILTON\" target=\"BROWSER-ASSISTED QUESTION-ANSWERING\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Jacob Hilton is a co-author of a paper discussing browser-assisted question-answering techniques.<\/data>      <data key=\"d6\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"34d0bb2211fc795fe1096442e086a2b3","chunk":"qiang Wang, Dawei Yin, Jun Xu, and Ji-Rong\nWen. Tool learning with large language models: A survey. arXiv preprint arXiv:2405.17935 , 2024.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. Advances in\nNeural Information Processing Systems , 36, 2024.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani,\nJulian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark,\n2023.\nToran Bruce Richards. Autogpt. https:\/\/github.com\/Significant-Gravitas\/AutoGPT ,\n2023. GitHub repository.\nTim Rockt\u00e4schel. Artificial Intelligence: 10 Things You Should Know . Seven Dials, September 2024.\nISBN 978-1399626521.\nMd Omar Faruk Rokon, Risul Islam, Ahmad Darki, Evangelos E Papalexakis, and Michalis Faloutsos.\n{SourceFinder}: Finding malware {Source-Code}from publicly available repositories in {GitHub}.\nIn23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020) , pp.\n149\u2013163, 2020.\nBernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan\nKumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al.\nMathematical discoveries from program search with large language models. Nature, 625(7995):\n468\u2013475, 2024.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke\nZettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems ,\n2023. URL https:\/\/openreview.net\/forum?id=Yacmpz84TH .\nSander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si,\nYinheng Li, Aayush Gupta, HyoJung Han, Sevien Schulhoff, et al. The prompt report: A systematic\nsurvey of prompting techniques. arXiv preprint arXiv:2406.06608 , 2024.\nXuan Shen, Yaohua Wang, Ming Lin, Yilun Huang, Hao Tang, Xiuyu Sun, and Yanzhi Wang. Deepmad:\nMathematical architecture design for deep convolutional neural network. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition , pp. 6163\u20136173, 2023.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won\nChung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models\n19Automated Design of Agentic Systems\nare multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning\nRepresentations , 2023.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:\nLanguage agents with verbal reinforcement learning. Advances in Neural Information Processing\nSystems, 36, 2023.\nKenneth O Stanley and Joel Lehman. Why greatness cannot be planned: The myth of the objective .\nSpringer, 2015.\nKenneth O Stanley, Jeff Clune, Joel Lehman, and Risto Miikkulainen. Designing neural networks\nthrough neuroevolution. Nature Machine Intelligence , 1(1):24\u201335, 2019.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.\nSai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. Chatgpt for robotics:\nDesign principles and model abilities. Technical Report MSR-TR-2023-8, Microsoft,\nFebruary 2023. URL https:\/\/www.microsoft.com\/en-us\/research\/publication\/\nchatgpt-for-robotics-design-principles-and-model-abilities\/ .\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv\npreprint arXiv: Arxiv-2305.16291 , 2023a.\nJane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles\nBlundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint\narXiv:1611.05763 , 2016.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\nFrontiers of Computer Science ,","chunk_id":"34d0bb2211fc795fe1096442e086a2b3","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"QIANG WANG","type":"PERSON","description":"Qiang Wang is a researcher who co-authored a survey on tool learning with large language models.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DAWEI YIN","type":"PERSON","description":"Dawei Yin is a researcher who co-authored a survey on tool learning with large language models.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JUN XU","type":"PERSON","description":"Jun Xu is a researcher who co-authored a survey on tool learning with large language models.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JI-RONG WEN","type":"PERSON","description":"Ji-Rong Wen is a researcher who co-authored a survey on tool learning with large language models.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"RAFAEL RAFAILOV","type":"PERSON","description":"Rafael Rafailov is a researcher who co-authored a paper on direct preference optimization.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ARCHIT SHARMA","type":"PERSON","description":"Archit Sharma is a researcher who co-authored a paper on direct preference optimization.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ERIC MITCHELL","type":"PERSON","description":"Eric Mitchell is a researcher who co-authored a paper on direct preference optimization.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"CHRISTOPHER D MANNING","type":"PERSON","description":"Christopher D Manning is a researcher who co-authored a paper on direct preference optimization.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"STEFANO ERMON","type":"PERSON","description":"Stefano Ermon is a researcher who co-authored a paper on direct preference optimization.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"CHELSEA FINN","type":"PERSON","description":"Chelsea Finn is a researcher who co-authored a paper on direct preference optimization.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DAVID REIN","type":"PERSON","description":"David Rein is a researcher who co-authored a paper on the GPQA benchmark.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"BETTY LI HOU","type":"PERSON","description":"Betty Li Hou is a researcher who co-authored a paper on the GPQA benchmark.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ASA COOPER STICKLAND","type":"PERSON","description":"Asa Cooper Stickland is a researcher who co-authored a paper on the GPQA benchmark.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JACKSON PETTY","type":"PERSON","description":"Jackson Petty is a researcher who co-authored a paper on the GPQA benchmark.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"RICHARD YUANZHE PANG","type":"PERSON","description":"Richard Yuanzhe Pang is a researcher who co-authored a paper on the GPQA benchmark.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JULIEN DIRANI","type":"PERSON","description":"Julien Dirani is a researcher who co-authored a paper on the GPQA benchmark.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JULIAN MICHAEL","type":"PERSON","description":"Julian Michael is a researcher who co-authored a paper on the GPQA benchmark.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SAMUEL R BOWMAN","type":"PERSON","description":"Samuel R. Bowman is a researcher who co-authored a paper on the GPQA benchmark.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"TORAN BRUCE RICHARDS","type":"PERSON","description":"Toran Bruce Richards is a researcher who developed AutoGPT, a GitHub repository.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"TIM ROCKT\u00c4SCHEL","type":"PERSON","description":"Tim Rockt\u00e4schel is an author of a book on artificial intelligence.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MD OMAR FARUK ROKON","type":"PERSON","description":"Md Omar Faruk Rokon is a researcher who co-authored a paper on malware source code detection.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"RISUL ISLAM","type":"PERSON","description":"Risul Islam is a researcher who co-authored a paper on malware source code detection.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"AHMAD DARKI","type":"PERSON","description":"Ahmad Darki is a researcher who co-authored a paper on malware source code detection.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"EVANGELOS E PAPALEXAKIS","type":"PERSON","description":"Evangelos E Papalexakis is a researcher who co-authored a paper on malware source code detection.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MICHAELIS FALOUTSOS","type":"PERSON","description":"Michalis Faloutsos is a researcher who co-authored a paper on malware source code detection.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"BERNARDINO ROMERA-PAREDES","type":"PERSON","description":"Bernardino Romera-Paredes is a researcher who co-authored a paper on mathematical discoveries from program search.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MOHAMMADAMIN BAREKATAIN","type":"PERSON","description":"Mohammadamin Barekatain is a researcher who co-authored a paper on mathematical discoveries from program search.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ALEXANDER NOVIKOV","type":"PERSON","description":"Alexander Novikov is a researcher who co-authored a paper on mathematical discoveries from program search.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MATEJ BALOG","type":"PERSON","description":"Matej Balog is a researcher who co-authored a paper on mathematical discoveries from program search.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"M PAWAN KUMAR","type":"PERSON","description":"M Pawan Kumar is a researcher who co-authored a paper on mathematical discoveries from program search.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"EMILIEN DUPONT","type":"PERSON","description":"Emilien Dupont is a researcher who co-authored a paper on mathematical discoveries from program search.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"FRANCISCO JR RUIZ","type":"PERSON","description":"Francisco JR Ruiz is a researcher who co-authored a paper on mathematical discoveries from program search.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JORDAN S ELLENBERG","type":"PERSON","description":"Jordan S Ellenberg is a researcher who co-authored a paper on mathematical discoveries from program search.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"PENGMING WANG","type":"PERSON","description":"Pengming Wang is a researcher who co-authored a paper on mathematical discoveries from program search.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"OMAR FAWZI","type":"PERSON","description":"Omar Fawzi is a researcher who co-authored a paper on mathematical discoveries from program search.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"TIMO SCHICK","type":"PERSON","description":"Timo Schick is a researcher who co-authored a paper on Toolformer.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JANE DWIVEDI-YU","type":"PERSON","description":"Jane Dwivedi-Yu is a researcher who co-authored a paper on Toolformer.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ROBERTO DESSI","type":"PERSON","description":"Roberto Dessi is a researcher who co-authored a paper on Toolformer.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ROBERTA RAILEANU","type":"PERSON","description":"Roberta Raileanu is a researcher who co-authored a paper on Toolformer.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MARIA LOMELI","type":"PERSON","description":"Maria Lomeli is a researcher who co-authored a paper on Toolformer.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ERIC HAMBRO","type":"PERSON","description":"Eric Hambro is a researcher who co-authored a paper on Toolformer.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"LUKE ZETTLEMOYER","type":"PERSON","description":"Luke Zettlemoyer is a researcher who co-authored a paper on Toolformer.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"NICOLA CANCEDDA","type":"PERSON","description":"Nicola Cancedda is a researcher who co-authored a paper on Toolformer.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"THOMAS SCIALOM","type":"PERSON","description":"Thomas Scialom is a researcher who co-authored a paper on Toolformer.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SANDER SCHULHOFF","type":"PERSON","description":"Sander Schulhoff is a researcher who co-authored a systematic survey on prompting techniques.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MICHAEL ILIE","type":"PERSON","description":"Michael Ilie is a researcher who co-authored a systematic survey on prompting techniques.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"NISHANT BALEPUR","type":"PERSON","description":"Nishant Balepur is a researcher who co-authored a systematic survey on prompting techniques.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"KONSTANTINE KAHADZE","type":"PERSON","description":"Konstantine Kahadze is a researcher who co-authored a systematic survey on prompting techniques.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"AMANDA LIU","type":"PERSON","description":"Amanda Liu is a researcher who co-authored a systematic survey on prompting techniques.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"CHENGLEI SI","type":"PERSON","description":"Chenglei Si is a researcher who co-authored a systematic survey on prompting techniques.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"YINHENG LI","type":"PERSON","description":"Yinheng Li is a researcher who co-authored a systematic survey on prompting techniques.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"AAYUSH GUPTA","type":"PERSON","description":"Aayush Gupta is a researcher who co-authored a systematic survey on prompting techniques.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"HYOJUNG HAN","type":"PERSON","description":"HyoJung Han is a researcher who co-authored a systematic survey on prompting techniques.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"FREDA SHI","type":"PERSON","description":"Freda Shi is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MIRAC SUZGUN","type":"PERSON","description":"Mirac Suzgun is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MARKUS FREITAG","type":"PERSON","description":"Markus Freitag is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"XUEZHI WANG","type":"PERSON","description":"Xuezhi Wang is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SURAJ SRIVATS","type":"PERSON","description":"Suraj Srivats is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SOROUSH VOSOUGHI","type":"PERSON","description":"Soroush Vosoughi is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"HYUNG WON CHUNG","type":"PERSON","description":"Hyung Won Chung is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"YI TAY","type":"PERSON","description":"Yi Tay is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SEBASTIAN RUDER","type":"PERSON","description":"Sebastian Ruder is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DENNY ZHOU","type":"PERSON","description":"Denny Zhou is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DIPANJAN DAS","type":"PERSON","description":"Dipanjan Das is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JASON WEI","type":"PERSON","description":"Jason Wei is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"NOAH SHINN","type":"PERSON","description":"Noah Shinn is a researcher who co-authored a paper on language agents with verbal reinforcement learning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"FEDERICO CASSANO","type":"PERSON","description":"Federico Cassano is a researcher who co-authored a paper on language agents with verbal reinforcement learning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ASHWIN GOPINATH","type":"PERSON","description":"Ashwin Gopinath is a researcher who co-authored a paper on language agents with verbal reinforcement learning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"KARTHIK NARASIMHAN","type":"PERSON","description":"Karthik Narasimhan is a researcher who co-authored a paper on language agents with verbal reinforcement learning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SHUNYU YAO","type":"PERSON","description":"Shunyu Yao is a researcher who co-authored a paper on language agents with verbal reinforcement learning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"KENNETH O STANLEY","type":"PERSON","description":"Kenneth O Stanley is a researcher who authored a book on the myth of the objective in planning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JOEL LEHMAN","type":"PERSON","description":"Joel Lehman is a researcher who co-authored a book on the myth of the objective in planning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is a researcher who co-authored a paper on designing neural networks through neuroevolution.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"RISTO MIIKKULAINEN","type":"PERSON","description":"Risto Miikkulainen is a researcher who co-authored a paper on designing neural networks through neuroevolution.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"RICHARD S SUTTON","type":"PERSON","description":"Richard S Sutton is a researcher who co-authored a book on reinforcement learning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ANDREW G BARTO","type":"PERSON","description":"Andrew G Barto is a researcher who co-authored a book on reinforcement learning.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SAI VEMPRALA","type":"PERSON","description":"Sai Vemprala is a researcher who authored a technical report on ChatGPT for robotics.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ROGERIO BONATTI","type":"PERSON","description":"Rogerio Bonatti is a researcher who co-authored a technical report on ChatGPT for robotics.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ARTHUR BUCKER","type":"PERSON","description":"Arthur Bucker is a researcher who co-authored a technical report on ChatGPT for robotics.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ASHISH KAPOOR","type":"PERSON","description":"Ashish Kapoor is a researcher who co-authored a technical report on ChatGPT for robotics.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"GUANZHI WANG","type":"PERSON","description":"Guanzhi Wang is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"YUQI XIE","type":"PERSON","description":"Yuqi Xie is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"YUNFAN JIANG","type":"PERSON","description":"Yunfan Jiang is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"AJAY MANDLEKAR","type":"PERSON","description":"Ajay Mandlekar is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"CHAOWEI XIAO","type":"PERSON","description":"Chaowei Xiao is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"YUKE ZHU","type":"PERSON","description":"Yuke Zhu is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"LINXI FAN","type":"PERSON","description":"Linxi Fan is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ANIMA ANANDKUMAR","type":"PERSON","description":"Anima Anandkumar is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JANE X WANG","type":"PERSON","description":"Jane X Wang is a researcher who co-authored a paper on learning to reinforcement learn.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ZEB KURTH-NELSON","type":"PERSON","description":"Zeb Kurth-Nelson is a researcher who co-authored a paper on learning to reinforcement learn.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DHRUVA TIRUMALA","type":"PERSON","description":"Dhruva Tirumala is a researcher who co-authored a paper on learning to reinforcement learn.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"HUBERT SOYER","type":"PERSON","description":"Hubert Soyer is a researcher who co-authored a paper on learning to reinforcement learn.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JOEL Z LEIBO","type":"PERSON","description":"Joel Z Leibo is a researcher who co-authored a paper on learning to reinforcement learn.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"REMI MUNOS","type":"PERSON","description":"Remi Munos is a researcher who co-authored a paper on learning to reinforcement learn.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"CHARLES BLUNDELL","type":"PERSON","description":"Charles Blundell is a researcher who co-authored a paper on learning to reinforcement learn.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DHARSHAN KUMARAN","type":"PERSON","description":"Dharshan Kumaran is a researcher who co-authored a paper on learning to reinforcement learn.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MATT BOTVINICK","type":"PERSON","description":"Matt Botvinick is a researcher who co-authored a paper on learning to reinforcement learn.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"LEI WANG","type":"PERSON","description":"Lei Wang is a researcher who co-authored a survey on large language model-based autonomous agents.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"CHEN MA","type":"PERSON","description":"Chen Ma is a researcher who co-authored a survey on large language model-based autonomous agents.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ZEYU ZHANG","type":"PERSON","description":"Zeyu Zhang is a researcher who co-authored a survey on large language model-based autonomous agents.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"HAO YANG","type":"PERSON","description":"Hao Yang is a researcher who co-authored a survey on large language model-based autonomous agents.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JINGSEN ZHANG","type":"PERSON","description":"Jingsen Zhang is a researcher who co-authored a survey on large language model-based autonomous agents.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ZHIYUAN CHEN","type":"PERSON","description":"Zhiyuan Chen is a researcher who co-authored a survey on large language model-based autonomous agents.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"(\"ENTITY\"","type":"TOOL LEARNING WITH LARGE LANGUAGE MODELS","description":"RESEARCH","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DIRECT PREFERENCE OPTIMIZATION","type":"RESEARCH","description":"Direct preference optimization is a method that treats language models as reward models to optimize preferences in outputs.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"GPQA","type":"RESEARCH","description":"GPQA is a graduate-level benchmark designed to evaluate question and answer systems, ensuring they are robust against Google searches.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"AUTOGPT","type":"TECHNOLOGY","description":"AutoGPT is a tool developed to automate tasks using large language models, available as a GitHub repository.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MATHEMATICAL DISCOVERIES FROM PROGRAM SEARCH","type":"RESEARCH","description":"This research discusses the mathematical insights gained from using large language models in program search.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"TOOLFORMER","type":"TECHNOLOGY","description":"Toolformer is a framework that allows language models to learn how to use tools effectively.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"PROMPT REPORT","type":"RESEARCH","description":"The prompt report is a systematic survey that analyzes various prompting techniques used in language models.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DEEPMAD","type":"RESEARCH","description":"Deepmad is a study focused on the mathematical architecture design for deep convolutional neural networks.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"LANGUAGE AGENTS WITH VERBAL REINFORCEMENT LEARNING","type":"RESEARCH","description":"This research explores the development of language agents that utilize verbal reinforcement learning techniques.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"LEARNING TO REINFORCEMENT LEARN","type":"RESEARCH","description":"Learning to reinforcement learn is a study that investigates methods for improving reinforcement learning algorithms.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"LARGE LANGUAGE MODEL BASED AUTONOMOUS AGENTS","type":"RESEARCH","description":"This survey focuses on the development and capabilities of autonomous agents that utilize large language models.","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"TOOL LEARNING WITH LARGE LANGUAGE MODELS","type":"","description":"","source_id":"34d0bb2211fc795fe1096442e086a2b3"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"QIANG WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qiang Wang is a researcher who co-authored a survey on tool learning with large language models.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DAWEI YIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dawei Yin is a researcher who co-authored a survey on tool learning with large language models.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JUN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jun Xu is a researcher who co-authored a survey on tool learning with large language models.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JI-RONG WEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ji-Rong Wen is a researcher who co-authored a survey on tool learning with large language models.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"RAFAEL RAFAILOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rafael Rafailov is a researcher who co-authored a paper on direct preference optimization.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ARCHIT SHARMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Archit Sharma is a researcher who co-authored a paper on direct preference optimization.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ERIC MITCHELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Mitchell is a researcher who co-authored a paper on direct preference optimization.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"CHRISTOPHER D MANNING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Christopher D Manning is a researcher who co-authored a paper on direct preference optimization.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"STEFANO ERMON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Stefano Ermon is a researcher who co-authored a paper on direct preference optimization.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"CHELSEA FINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chelsea Finn is a researcher who co-authored a paper on direct preference optimization.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DAVID REIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David Rein is a researcher who co-authored a paper on the GPQA benchmark.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"BETTY LI HOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Betty Li Hou is a researcher who co-authored a paper on the GPQA benchmark.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ASA COOPER STICKLAND\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Asa Cooper Stickland is a researcher who co-authored a paper on the GPQA benchmark.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JACKSON PETTY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jackson Petty is a researcher who co-authored a paper on the GPQA benchmark.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"RICHARD YUANZHE PANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Richard Yuanzhe Pang is a researcher who co-authored a paper on the GPQA benchmark.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JULIEN DIRANI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Julien Dirani is a researcher who co-authored a paper on the GPQA benchmark.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JULIAN MICHAEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Julian Michael is a researcher who co-authored a paper on the GPQA benchmark.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SAMUEL R BOWMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Samuel R. Bowman is a researcher who co-authored a paper on the GPQA benchmark.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"TORAN BRUCE RICHARDS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Toran Bruce Richards is a researcher who developed AutoGPT, a GitHub repository.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"TIM ROCKT&#196;SCHEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tim Rockt&#228;schel is an author of a book on artificial intelligence.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MD OMAR FARUK ROKON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Md Omar Faruk Rokon is a researcher who co-authored a paper on malware source code detection.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"RISUL ISLAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Risul Islam is a researcher who co-authored a paper on malware source code detection.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"AHMAD DARKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ahmad Darki is a researcher who co-authored a paper on malware source code detection.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"EVANGELOS E PAPALEXAKIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Evangelos E Papalexakis is a researcher who co-authored a paper on malware source code detection.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MICHAELIS FALOUTSOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michalis Faloutsos is a researcher who co-authored a paper on malware source code detection.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"BERNARDINO ROMERA-PAREDES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bernardino Romera-Paredes is a researcher who co-authored a paper on mathematical discoveries from program search.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MOHAMMADAMIN BAREKATAIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mohammadamin Barekatain is a researcher who co-authored a paper on mathematical discoveries from program search.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ALEXANDER NOVIKOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alexander Novikov is a researcher who co-authored a paper on mathematical discoveries from program search.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MATEJ BALOG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matej Balog is a researcher who co-authored a paper on mathematical discoveries from program search.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"M PAWAN KUMAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M Pawan Kumar is a researcher who co-authored a paper on mathematical discoveries from program search.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"EMILIEN DUPONT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Emilien Dupont is a researcher who co-authored a paper on mathematical discoveries from program search.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"FRANCISCO JR RUIZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Francisco JR Ruiz is a researcher who co-authored a paper on mathematical discoveries from program search.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JORDAN S ELLENBERG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jordan S Ellenberg is a researcher who co-authored a paper on mathematical discoveries from program search.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"PENGMING WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pengming Wang is a researcher who co-authored a paper on mathematical discoveries from program search.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"OMAR FAWZI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Omar Fawzi is a researcher who co-authored a paper on mathematical discoveries from program search.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"TIMO SCHICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Timo Schick is a researcher who co-authored a paper on Toolformer.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JANE DWIVEDI-YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jane Dwivedi-Yu is a researcher who co-authored a paper on Toolformer.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ROBERTO DESSI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Roberto Dessi is a researcher who co-authored a paper on Toolformer.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ROBERTA RAILEANU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Roberta Raileanu is a researcher who co-authored a paper on Toolformer.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MARIA LOMELI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maria Lomeli is a researcher who co-authored a paper on Toolformer.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ERIC HAMBRO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Hambro is a researcher who co-authored a paper on Toolformer.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"LUKE ZETTLEMOYER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luke Zettlemoyer is a researcher who co-authored a paper on Toolformer.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"NICOLA CANCEDDA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicola Cancedda is a researcher who co-authored a paper on Toolformer.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"THOMAS SCIALOM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Scialom is a researcher who co-authored a paper on Toolformer.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SANDER SCHULHOFF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sander Schulhoff is a researcher who co-authored a systematic survey on prompting techniques.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MICHAEL ILIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Ilie is a researcher who co-authored a systematic survey on prompting techniques.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"NISHANT BALEPUR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nishant Balepur is a researcher who co-authored a systematic survey on prompting techniques.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"KONSTANTINE KAHADZE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Konstantine Kahadze is a researcher who co-authored a systematic survey on prompting techniques.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"AMANDA LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amanda Liu is a researcher who co-authored a systematic survey on prompting techniques.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"CHENGLEI SI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chenglei Si is a researcher who co-authored a systematic survey on prompting techniques.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"YINHENG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yinheng Li is a researcher who co-authored a systematic survey on prompting techniques.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"AAYUSH GUPTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aayush Gupta is a researcher who co-authored a systematic survey on prompting techniques.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"HYOJUNG HAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">HyoJung Han is a researcher who co-authored a systematic survey on prompting techniques.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"FREDA SHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Freda Shi is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MIRAC SUZGUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mirac Suzgun is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MARKUS FREITAG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Markus Freitag is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"XUEZHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuezhi Wang is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SURAJ SRIVATS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Suraj Srivats is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SOROUSH VOSOUGHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Soroush Vosoughi is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"HYUNG WON CHUNG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hyung Won Chung is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"YI TAY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi Tay is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SEBASTIAN RUDER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sebastian Ruder is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DENNY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Denny Zhou is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DIPANJAN DAS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dipanjan Das is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JASON WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jason Wei is a researcher who co-authored a paper on multilingual chain-of-thought reasoning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"NOAH SHINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Noah Shinn is a researcher who co-authored a paper on language agents with verbal reinforcement learning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"FEDERICO CASSANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Federico Cassano is a researcher who co-authored a paper on language agents with verbal reinforcement learning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ASHWIN GOPINATH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ashwin Gopinath is a researcher who co-authored a paper on language agents with verbal reinforcement learning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"KARTHIK NARASIMHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karthik Narasimhan is a researcher who co-authored a paper on language agents with verbal reinforcement learning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SHUNYU YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shunyu Yao is a researcher who co-authored a paper on language agents with verbal reinforcement learning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"KENNETH O STANLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kenneth O Stanley is a researcher who authored a book on the myth of the objective in planning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JOEL LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Lehman is a researcher who co-authored a book on the myth of the objective in planning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is a researcher who co-authored a paper on designing neural networks through neuroevolution.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"RISTO MIIKKULAINEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Risto Miikkulainen is a researcher who co-authored a paper on designing neural networks through neuroevolution.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"RICHARD S SUTTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Richard S Sutton is a researcher who co-authored a book on reinforcement learning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ANDREW G BARTO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrew G Barto is a researcher who co-authored a book on reinforcement learning.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SAI VEMPRALA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sai Vemprala is a researcher who authored a technical report on ChatGPT for robotics.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ROGERIO BONATTI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rogerio Bonatti is a researcher who co-authored a technical report on ChatGPT for robotics.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ARTHUR BUCKER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arthur Bucker is a researcher who co-authored a technical report on ChatGPT for robotics.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ASHISH KAPOOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ashish Kapoor is a researcher who co-authored a technical report on ChatGPT for robotics.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"GUANZHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guanzhi Wang is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"YUQI XIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuqi Xie is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"YUNFAN JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yunfan Jiang is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"AJAY MANDLEKAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ajay Mandlekar is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"CHAOWEI XIAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chaowei Xiao is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"YUKE ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuke Zhu is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"LINXI FAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linxi Fan is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ANIMA ANANDKUMAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anima Anandkumar is a researcher who co-authored a paper on Voyager, an open-ended embodied agent.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JANE X WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jane X Wang is a researcher who co-authored a paper on learning to reinforcement learn.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ZEB KURTH-NELSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zeb Kurth-Nelson is a researcher who co-authored a paper on learning to reinforcement learn.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DHRUVA TIRUMALA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dhruva Tirumala is a researcher who co-authored a paper on learning to reinforcement learn.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"HUBERT SOYER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hubert Soyer is a researcher who co-authored a paper on learning to reinforcement learn.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JOEL Z LEIBO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Z Leibo is a researcher who co-authored a paper on learning to reinforcement learn.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"REMI MUNOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Remi Munos is a researcher who co-authored a paper on learning to reinforcement learn.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"CHARLES BLUNDELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Charles Blundell is a researcher who co-authored a paper on learning to reinforcement learn.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DHARSHAN KUMARAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dharshan Kumaran is a researcher who co-authored a paper on learning to reinforcement learn.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MATT BOTVINICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matt Botvinick is a researcher who co-authored a paper on learning to reinforcement learn.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"LEI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lei Wang is a researcher who co-authored a survey on large language model-based autonomous agents.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"CHEN MA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Ma is a researcher who co-authored a survey on large language model-based autonomous agents.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ZEYU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zeyu Zhang is a researcher who co-authored a survey on large language model-based autonomous agents.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"HAO YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao Yang is a researcher who co-authored a survey on large language model-based autonomous agents.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JINGSEN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jingsen Zhang is a researcher who co-authored a survey on large language model-based autonomous agents.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ZHIYUAN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyuan Chen is a researcher who co-authored a survey on large language model-based autonomous agents.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"(&quot;ENTITY&quot;\">      <data key=\"d0\">TOOL LEARNING WITH LARGE LANGUAGE MODELS<\/data>      <data key=\"d1\">RESEARCH<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DIRECT PREFERENCE OPTIMIZATION\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Direct preference optimization is a method that treats language models as reward models to optimize preferences in outputs.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"GPQA\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">GPQA is a graduate-level benchmark designed to evaluate question and answer systems, ensuring they are robust against Google searches.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"AUTOGPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AutoGPT is a tool developed to automate tasks using large language models, available as a GitHub repository.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MATHEMATICAL DISCOVERIES FROM PROGRAM SEARCH\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">This research discusses the mathematical insights gained from using large language models in program search.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"TOOLFORMER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Toolformer is a framework that allows language models to learn how to use tools effectively.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"PROMPT REPORT\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">The prompt report is a systematic survey that analyzes various prompting techniques used in language models.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DEEPMAD\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Deepmad is a study focused on the mathematical architecture design for deep convolutional neural networks.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"LANGUAGE AGENTS WITH VERBAL REINFORCEMENT LEARNING\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">This research explores the development of language agents that utilize verbal reinforcement learning techniques.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"LEARNING TO REINFORCEMENT LEARN\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Learning to reinforcement learn is a study that investigates methods for improving reinforcement learning algorithms.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"LARGE LANGUAGE MODEL BASED AUTONOMOUS AGENTS\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">This survey focuses on the development and capabilities of autonomous agents that utilize large language models.<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"TOOL LEARNING WITH LARGE LANGUAGE MODELS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <edge source=\"QIANG WANG\" target=\"TOOL LEARNING WITH LARGE LANGUAGE MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Qiang Wang co-authored a survey on tool learning with large language models, contributing to the research in this area.<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"RAFAEL RAFAILOV\" target=\"DIRECT PREFERENCE OPTIMIZATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Rafael Rafailov co-authored a paper on direct preference optimization, contributing to the understanding of reward models in language processing.<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"DAVID REIN\" target=\"GPQA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">David Rein co-authored a paper on GPQA, contributing to the development of a benchmark for question and answer systems.<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"TORAN BRUCE RICHARDS\" target=\"AUTOGPT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Toran Bruce Richards developed AutoGPT, a tool for automating tasks using language models.<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"BERNARDINO ROMERA-PAREDES\" target=\"MATHEMATICAL DISCOVERIES FROM PROGRAM SEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Bernardino Romera-Paredes co-authored a paper discussing mathematical discoveries from program search with large language models.<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"TIMO SCHICK\" target=\"TOOLFORMER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Timo Schick co-authored a paper on Toolformer, which focuses on how language models can learn to use tools.<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"SANDER SCHULHOFF\" target=\"PROMPT REPORT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Sander Schulhoff co-authored a systematic survey on prompting techniques, contributing to the understanding of effective prompts in language models.<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"FREDA SHI\" target=\"LANGUAGE AGENTS WITH VERBAL REINFORCEMENT LEARNING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Freda Shi co-authored a paper on language agents that utilize verbal reinforcement learning techniques.<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"JANE X WANG\" target=\"LEARNING TO REINFORCEMENT LEARN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Jane X Wang co-authored a paper on learning to reinforcement learn, contributing to advancements in reinforcement learning methodologies.<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"LEI WANG\" target=\"LARGE LANGUAGE MODEL BASED AUTONOMOUS AGENTS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Lei Wang co-authored a survey on large language model-based autonomous agents, contributing to the field of autonomous systems.<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"2600a1ed94ad2d3675ea80575c39cbd1","chunk":"shan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint\narXiv:1611.05763 , 2016.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\nFrontiers of Computer Science , 18(6):186345, 2024.\nRui Wang, Joel Lehman, Jeff Clune, and Kenneth O. Stanley. Poet: open-ended coevolution of\nenvironments and their optimized solutions. In Proceedings of the Genetic and Evolutionary Compu-\ntation Conference , GECCO \u201919, pp. 142\u2013151, New York, NY, USA, 2019. Association for Computing\nMachinery. ISBN 9781450361118. doi: 10.1145\/3321707.3321799.\nRui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeffrey Clune, and Kenneth Stanley.\nEnhanced poet: Open-ended reinforcement learning through unbounded invention of learning\nchallenges and their solutions. In International conference on machine learning , pp. 9940\u20139951.\nPMLR, 2020.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations , 2023b.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural\ninformation processing systems , 35:24824\u201324837, 2022.\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang,\nXiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent\nconversation framework. arXiv preprint arXiv:2308.08155 , 2023.\n20Automated Design of Agentic Systems\nBenfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong Mao.\nExpertprompting: Instructing large language models to be distinguished experts. arXiv preprint\narXiv:2305.14688 , 2023.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen.\nLarge language models as optimizers. In The Twelfth International Conference on Learning Represen-\ntations, 2024. URL https:\/\/openreview.net\/forum?id=Bb4VGOWELI .\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan\nCao. React: Synergizing reasoning and acting in language models. In The Eleventh International\nConference on Learning Representations , 2023. URL https:\/\/openreview.net\/forum?id=WE_\nvluYUL-X .\nBennet Yee, David Sehr, Gregory Dardyk, J Bradley Chen, Robert Muth, Tavis Ormandy, Shiki Okasaka,\nNeha Narula, and Nicholas Fullagar. Native client: A sandbox for portable, untrusted x86 native\ncode.Communications of the ACM , 53(1):91\u201399, 2010.\nWenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montserrat Gonzalez\nArenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al. Language to\nrewards for robotic skill synthesis. In Conference on Robot Learning , pp. 374\u2013404. PMLR, 2023.\nSiyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, and Deqing Yang. Evoagent: Towards\nautomatic multi-agent generation via evolutionary algorithms. arXiv preprint arXiv:2406.14228 ,\n2024.\nEliezer Yudkowsky et al. Artificial Intelligence as a positive and negative factor in global risk. Global\ncatastrophic risks , 1(303):184, 2008.\nMatei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather Miller, Chris Potts,\nJames Zou, Michael Carbin, Jonathan Frankle, Naveen Rao, and Ali Ghodsi. The shift\nfrom models to compound ai systems. https:\/\/bair.berkeley.edu\/blog\/2024\/02\/18\/\ncompound-ai-systems\/ , 2024.\nJennyZhang, JoelLehman, KennethStanley, andJeffClune. OMNI:Open-endednessviamodelsofhu-\nman notions of interestingness. In The Twelfth International Conference on Learning Representations ,\n2024a. URL https:\/\/openreview.net\/forum?id=AgM3MzT99c .\nShaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang","chunk_id":"2600a1ed94ad2d3675ea80575c39cbd1","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"SHAN KUMARAN","type":"PERSON","description":"Shan Kumaran is a researcher known for contributions in the field of reinforcement learning.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"MATT BOTVINICK","type":"PERSON","description":"Matt Botvinick is a researcher recognized for his work in reinforcement learning and cognitive science.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"LEI WANG","type":"PERSON","description":"Lei Wang is a researcher who has contributed to the study of large language model-based autonomous agents.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"CHEN MA","type":"PERSON","description":"Chen Ma is a researcher involved in the survey of large language model-based autonomous agents.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"XUEYANG FENG","type":"PERSON","description":"Xueyang Feng is a researcher who has worked on large language model-based autonomous agents.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"ZEYU ZHANG","type":"PERSON","description":"Zeyu Zhang is a researcher contributing to the field of large language model-based autonomous agents.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"HAO YANG","type":"PERSON","description":"Hao Yang is a researcher involved in the study of large language model-based autonomous agents.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"JINGSEN ZHANG","type":"PERSON","description":"Jingsen Zhang is a researcher contributing to the field of large language model-based autonomous agents.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"ZHIYUAN CHEN","type":"PERSON","description":"Zhiyuan Chen is a researcher involved in the survey of large language model-based autonomous agents.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"JIAKAI TANG","type":"PERSON","description":"Jiakai Tang is a researcher contributing to the field of large language model-based autonomous agents.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"XU CHEN","type":"PERSON","description":"Xu Chen is a researcher involved in the study of large language model-based autonomous agents.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"YANKAI LIN","type":"PERSON","description":"Yankai Lin is a researcher contributing to the field of large language model-based autonomous agents.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"RUI WANG","type":"PERSON","description":"Rui Wang is a researcher known for work on open-ended coevolution in reinforcement learning.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"JOEL LEHMAN","type":"PERSON","description":"Joel Lehman is a researcher involved in the development of open-ended reinforcement learning systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is a researcher recognized for contributions to evolutionary computation and reinforcement learning.\nJeff Clune is a researcher recognized for contributions to evolutionary computation and reinforcement learning, also mentioned in multiple papers.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"KENNETH O. STANLEY","type":"PERSON","description":"Kenneth O. Stanley is a researcher known for his work in evolutionary algorithms and reinforcement learning.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"XUEZHI WANG","type":"PERSON","description":"Xuezhi Wang is a researcher who has contributed to reasoning in language models.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"JASON WEI","type":"PERSON","description":"Jason Wei is a researcher involved in prompting techniques for large language models.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"DALE SCHUURMANS","type":"PERSON","description":"Dale Schuurmans is a researcher contributing to advancements in language model reasoning.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"QUOC V LE","type":"PERSON","description":"Quoc V Le is a researcher known for his work in machine learning and language models.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"ED H. CHI","type":"PERSON","description":"Ed H. Chi is a researcher involved in the study of language models and their applications.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"SHARAN NARANG","type":"PERSON","description":"Sharan Narang is a researcher contributing to advancements in language models.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"AAKANKSHA CHOWDHERY","type":"PERSON","description":"Aakanksha Chowdhery is a researcher involved in language model applications.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"DENNY ZHOU","type":"PERSON","description":"Denny Zhou is a researcher known for contributions to language models and reasoning techniques.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"QINGYUN WU","type":"PERSON","description":"Qingyun Wu is a researcher involved in multi-agent conversation frameworks.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"GAGAN BANSAL","type":"PERSON","description":"Gagan Bansal is a researcher contributing to advancements in language model applications.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"JIEYU ZHANG","type":"PERSON","description":"Jieyu Zhang is a researcher involved in robotic skill synthesis using language models.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"YIRAN WU","type":"PERSON","description":"Yiran Wu is a researcher contributing to advancements in language model applications.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"SHAOKUN ZHANG","type":"PERSON","description":"Shaokun Zhang is a researcher involved in multi-agent generation via evolutionary algorithms.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"ELIEZER YUDKOWSKY","type":"PERSON","description":"Eliezer Yudkowsky is a researcher known for his work on AI and global risks.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"MATEI ZAHARIA","type":"PERSON","description":"Matei Zaharia is a researcher involved in the development of compound AI systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"OMNI","type":"TECHNOLOGY","description":"OMNI is a system designed to explore open-endedness through models of human notions of interestingness.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"AUTOGEN","type":"TECHNOLOGY","description":"Autogen is a framework enabling next-generation applications of large language models through multi-agent conversations.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"REACT","type":"TECHNOLOGY","description":"React is a system that synergizes reasoning and acting in language models.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"JIALE ZHI","type":"PERSON","description":"Jiale Zhi is a researcher involved in the study of enhanced open-ended reinforcement learning.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"YULUN LI","type":"PERSON","description":"Yulun Li is a researcher contributing to advancements in open-ended reinforcement learning.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"MAARTEN BOSMA","type":"PERSON","description":"Maarten Bosma is a researcher involved in prompting techniques for large language models.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"FEI XIA","type":"PERSON","description":"Fei Xia is a researcher contributing to advancements in language models and their applications.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"NIMROD GILEADI","type":"PERSON","description":"Nimrod Gileadi is a researcher involved in the development of language model applications.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"CHUYUAN FU","type":"PERSON","description":"Chuyuan Fu is a researcher contributing to advancements in language models.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"SEAN KIRMANI","type":"PERSON","description":"Sean Kirmani is a researcher involved in the study of language models and their applications.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"KUANG-HUEI LEE","type":"PERSON","description":"Kuang-Huei Lee is a researcher contributing to advancements in language models.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"MONTSERRAT GONZALEZ ARENAS","type":"PERSON","description":"Montserrat Gonzalez Arenas is a researcher involved in language model applications.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"HAO-TIEN LEWIS CHIANG","type":"PERSON","description":"Hao-Tien Lewis Chiang is a researcher contributing to advancements in language models.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"TOM EREZ","type":"PERSON","description":"Tom Erez is a researcher involved in the study of language models and their applications.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"LEONARD HASENCLEVER","type":"PERSON","description":"Leonard Hasenclever is a researcher contributing to advancements in language models.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"JAN HUMPLIK","type":"PERSON","description":"Jan Humplik is a researcher involved in the development of language model applications.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"BENFENG XU","type":"PERSON","description":"Benfeng Xu is a researcher contributing to advancements in agentic systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"AN YANG","type":"PERSON","description":"An Yang is a researcher involved in the study of agentic systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"JUNYANG LIN","type":"PERSON","description":"Junyang Lin is a researcher contributing to advancements in agentic systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"QUAN WANG","type":"PERSON","description":"Quan Wang is a researcher involved in the study of agentic systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"CHANG ZHOU","type":"PERSON","description":"Chang Zhou is a researcher contributing to advancements in agentic systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"YONGDONG ZHANG","type":"PERSON","description":"Yongdong Zhang is a researcher involved in the study of agentic systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"ZHENDONG MAO","type":"PERSON","description":"Zhendong Mao is a researcher contributing to advancements in agentic systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"JENNY ZHANG","type":"PERSON","description":"Jenny Zhang is a researcher involved in the study of open-endedness in AI systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"NICHOLAS FULLAGAR","type":"PERSON","description":"Nicholas Fullagar is a researcher contributing to advancements in AI systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"GREGORY DARDYK","type":"PERSON","description":"Gregory Dardyk is a researcher involved in the study of AI systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"J BRADLEY CHEN","type":"PERSON","description":"J Bradley Chen is a researcher contributing to advancements in AI systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"ROBERT MUTH","type":"PERSON","description":"Robert Muth is a researcher involved in the study of AI systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"TAVIS ORMANDY","type":"PERSON","description":"Tavis Ormandy is a researcher contributing to advancements in AI systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"SHIKI OKASAKA","type":"PERSON","description":"Shiki Okasaka is a researcher involved in the study of AI systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"NEHA NARULA","type":"PERSON","description":"Neha Narula is a researcher contributing to advancements in AI systems.","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"},{"name":"ADITYA RAWAL","type":"","description":"","source_id":"2600a1ed94ad2d3675ea80575c39cbd1"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"SHAN KUMARAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shan Kumaran is a researcher known for contributions in the field of reinforcement learning.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"MATT BOTVINICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matt Botvinick is a researcher recognized for his work in reinforcement learning and cognitive science.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"LEI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lei Wang is a researcher who has contributed to the study of large language model-based autonomous agents.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"CHEN MA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Ma is a researcher involved in the survey of large language model-based autonomous agents.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"XUEYANG FENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xueyang Feng is a researcher who has worked on large language model-based autonomous agents.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"ZEYU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zeyu Zhang is a researcher contributing to the field of large language model-based autonomous agents.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"HAO YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao Yang is a researcher involved in the study of large language model-based autonomous agents.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"JINGSEN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jingsen Zhang is a researcher contributing to the field of large language model-based autonomous agents.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"ZHIYUAN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyuan Chen is a researcher involved in the survey of large language model-based autonomous agents.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"JIAKAI TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiakai Tang is a researcher contributing to the field of large language model-based autonomous agents.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"XU CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu Chen is a researcher involved in the study of large language model-based autonomous agents.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"YANKAI LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yankai Lin is a researcher contributing to the field of large language model-based autonomous agents.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"RUI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rui Wang is a researcher known for work on open-ended coevolution in reinforcement learning.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"JOEL LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Lehman is a researcher involved in the development of open-ended reinforcement learning systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is a researcher recognized for contributions to evolutionary computation and reinforcement learning.Jeff Clune is a researcher recognized for contributions to evolutionary computation and reinforcement learning, also mentioned in multiple papers.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KENNETH O. STANLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kenneth O. Stanley is a researcher known for his work in evolutionary algorithms and reinforcement learning.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"XUEZHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuezhi Wang is a researcher who has contributed to reasoning in language models.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"JASON WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jason Wei is a researcher involved in prompting techniques for large language models.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"DALE SCHUURMANS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dale Schuurmans is a researcher contributing to advancements in language model reasoning.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"QUOC V LE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quoc V Le is a researcher known for his work in machine learning and language models.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"ED H. CHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ed H. Chi is a researcher involved in the study of language models and their applications.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"SHARAN NARANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sharan Narang is a researcher contributing to advancements in language models.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"AAKANKSHA CHOWDHERY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aakanksha Chowdhery is a researcher involved in language model applications.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"DENNY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Denny Zhou is a researcher known for contributions to language models and reasoning techniques.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"QINGYUN WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qingyun Wu is a researcher involved in multi-agent conversation frameworks.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"GAGAN BANSAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gagan Bansal is a researcher contributing to advancements in language model applications.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"JIEYU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jieyu Zhang is a researcher involved in robotic skill synthesis using language models.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"YIRAN WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiran Wu is a researcher contributing to advancements in language model applications.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"SHAOKUN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shaokun Zhang is a researcher involved in multi-agent generation via evolutionary algorithms.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"ELIEZER YUDKOWSKY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eliezer Yudkowsky is a researcher known for his work on AI and global risks.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"MATEI ZAHARIA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matei Zaharia is a researcher involved in the development of compound AI systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"OMNI\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">OMNI is a system designed to explore open-endedness through models of human notions of interestingness.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"AUTOGEN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Autogen is a framework enabling next-generation applications of large language models through multi-agent conversations.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">React is a system that synergizes reasoning and acting in language models.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"JIALE ZHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiale Zhi is a researcher involved in the study of enhanced open-ended reinforcement learning.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"YULUN LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yulun Li is a researcher contributing to advancements in open-ended reinforcement learning.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"MAARTEN BOSMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maarten Bosma is a researcher involved in prompting techniques for large language models.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"FEI XIA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fei Xia is a researcher contributing to advancements in language models and their applications.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"NIMROD GILEADI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nimrod Gileadi is a researcher involved in the development of language model applications.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"CHUYUAN FU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chuyuan Fu is a researcher contributing to advancements in language models.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"SEAN KIRMANI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sean Kirmani is a researcher involved in the study of language models and their applications.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"KUANG-HUEI LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kuang-Huei Lee is a researcher contributing to advancements in language models.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"MONTSERRAT GONZALEZ ARENAS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Montserrat Gonzalez Arenas is a researcher involved in language model applications.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"HAO-TIEN LEWIS CHIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao-Tien Lewis Chiang is a researcher contributing to advancements in language models.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"TOM EREZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tom Erez is a researcher involved in the study of language models and their applications.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"LEONARD HASENCLEVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Leonard Hasenclever is a researcher contributing to advancements in language models.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"JAN HUMPLIK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jan Humplik is a researcher involved in the development of language model applications.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"BENFENG XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Benfeng Xu is a researcher contributing to advancements in agentic systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"AN YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An Yang is a researcher involved in the study of agentic systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"JUNYANG LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Junyang Lin is a researcher contributing to advancements in agentic systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"QUAN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quan Wang is a researcher involved in the study of agentic systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"CHANG ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chang Zhou is a researcher contributing to advancements in agentic systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"YONGDONG ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yongdong Zhang is a researcher involved in the study of agentic systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"ZHENDONG MAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhendong Mao is a researcher contributing to advancements in agentic systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"JENNY ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jenny Zhang is a researcher involved in the study of open-endedness in AI systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"NICHOLAS FULLAGAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicholas Fullagar is a researcher contributing to advancements in AI systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"GREGORY DARDYK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gregory Dardyk is a researcher involved in the study of AI systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"J BRADLEY CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J Bradley Chen is a researcher contributing to advancements in AI systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"ROBERT MUTH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Robert Muth is a researcher involved in the study of AI systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"TAVIS ORMANDY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tavis Ormandy is a researcher contributing to advancements in AI systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"SHIKI OKASAKA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shiki Okasaka is a researcher involved in the study of AI systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"NEHA NARULA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Neha Narula is a researcher contributing to advancements in AI systems.<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <node id=\"ADITYA RAWAL\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/node>    <edge source=\"SHAN KUMARAN\" target=\"MATT BOTVINICK\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Shan Kumaran and Matt Botvinick are co-authors of a paper on reinforcement learning.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"LEI WANG\" target=\"CHEN MA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Lei Wang and Chen Ma are co-authors of a survey on large language model-based autonomous agents.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"RUI WANG\" target=\"JOEL LEHMAN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Rui Wang and Joel Lehman co-authored a paper on open-ended coevolution in reinforcement learning.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"RUI WANG\" target=\"ADITYA RAWAL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Aditya Rawal and Rui Wang co-authored a paper on enhanced open-ended reinforcement learning techniques.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"RUI WANG\" target=\"JIALE ZHI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jiale Zhi and Rui Wang are co-authors on research related to enhanced open-ended reinforcement learning.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"RUI WANG\" target=\"YULUN LI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yulun Li and Rui Wang are co-authors on research related to open-ended reinforcement learning.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"RUI WANG\" target=\"JEFF CLUNE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jeff Clune and Rui Wang are co-authors on multiple papers related to open-ended reinforcement learning.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"JEFF CLUNE\" target=\"KENNETH O. STANLEY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jeff Clune and Kenneth O. Stanley are co-authors on multiple papers related to evolutionary algorithms and reinforcement learning.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"XUEZHI WANG\" target=\"JASON WEI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Xuezhi Wang and Jason Wei are co-authors on research related to reasoning in language models.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"JASON WEI\" target=\"MAARTEN BOSMA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Maarten Bosma and Jason Wei are co-authors on research related to prompting techniques for large language models.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"QINGYUN WU\" target=\"GAGAN BANSAL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Qingyun Wu and Gagan Bansal are co-authors on advancements in multi-agent conversation frameworks.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"QINGYUN WU\" target=\"NIMROD GILEADI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Nimrod Gileadi and Qingyun Wu are co-authors on advancements in multi-agent conversation frameworks.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"JIEYU ZHANG\" target=\"SHAOKUN ZHANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Shaokun Zhang and Jieyu Zhang are co-authors on research related to robotic skill synthesis using language models.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"ELIEZER YUDKOWSKY\" target=\"MATEI ZAHARIA\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Eliezer Yudkowsky and Matei Zaharia are both involved in discussions about AI and its implications for global risks.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"MATEI ZAHARIA\" target=\"JENNY ZHANG\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Jenny Zhang and Matei Zaharia are both involved in discussions about open-endedness in AI systems.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"OMNI\" target=\"REACT\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">OMNI and React are both systems designed to enhance the capabilities of language models in different ways.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"BENFENG XU\" target=\"AN YANG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Benfeng Xu and An Yang are co-authors on advancements in agentic systems.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"NICHOLAS FULLAGAR\" target=\"GREGORY DARDYK\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Nicholas Fullagar and Gregory Dardyk are co-authors on advancements in AI systems.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"NICHOLAS FULLAGAR\" target=\"NEHA NARULA\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Neha Narula and Nicholas Fullagar are co-authors on advancements in AI systems.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"J BRADLEY CHEN\" target=\"ROBERT MUTH\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">J Bradley Chen and Robert Muth are co-authors on advancements in AI systems.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>    <edge source=\"TAVIS ORMANDY\" target=\"SHIKI OKASAKA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Tavis Ormandy and Shiki Okasaka are co-authors on advancements in AI systems.<\/data>      <data key=\"d6\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"cc802d9b841fde55e9c0c2ba0ef7869d","chunk":"ai-systems\/ , 2024.\nJennyZhang, JoelLehman, KennethStanley, andJeffClune. OMNI:Open-endednessviamodelsofhu-\nman notions of interestingness. In The Twelfth International Conference on Learning Representations ,\n2024a. URL https:\/\/openreview.net\/forum?id=AgM3MzT99c .\nShaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun\nWu. Offline training of language model agents with functions as learnable weights. In Forty-first\nInternational Conference on Machine Learning , 2024b.\nZeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and\nJi-Rong Wen. A survey on the memory mechanism of large language model based agents. arXiv\npreprint arXiv:2404.13501 , 2024c.\nHuaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le, and\nDenny Zhou. Take a step back: Evoking reasoning via abstraction in large language models. arXiv\npreprint arXiv:2310.06117 , 2023.\nPei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V Le, Ed H Chi, Denny Zhou,\nSwaroop Mishra, and Huaixiu Steven Zheng. Self-discover: Large language models self-compose\nreasoning structures. arXiv preprint arXiv:2402.03620 , 2024a.\n21Automated Design of Agentic Systems\nWangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen,\nShuai Wang, Xiaohua Xu, Ningyu Zhang, et al. Symbolic learning enables self-evolving agents.\narXiv preprint arXiv:2406.18532 , 2024b.\nMingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and J\u00fcrgen\nSchmidhuber. Gptswarm: Language agents as optimizable graphs. In Forty-first International\nConference on Machine Learning , 2024.\nLuisaZintgraf,SebastianSchulze,CongLu,LeoFeng,MaximilianIgl,KyriacosShiarlis,YarinGal,Katja\nHofmann, and Shimon Whiteson. Varibad: Variational bayes-adaptive deep rl via meta-learning.\nJournal of Machine Learning Research , 22(289):1\u201339, 2021a.\nLuisa M Zintgraf, Leo Feng, Cong Lu, Maximilian Igl, Kristian Hartikainen, Katja Hofmann, and\nShimon Whiteson. Exploration in approximate hyper-state space for meta reinforcement learning.\nInInternational Conference on Machine Learning , pp. 12991\u201313001. PMLR, 2021b.\n22Automated Design of Agentic Systems\nSupplementary Material\nTable of Contents\nA Prompts 24\nB Framework Code 26\nC Experiment Details for ARC Challenge 30\nD Experiment Details for Reasoning and Problem-Solving Domains 33\nE Baselines 35\nF Example Agents 36\nG Cost of Experiments 39\n23Automated Design of Agentic Systems\nA. Prompts\nWe use the following prompts for the meta agent in Meta Agent Search. Variables in the prompts\nthat vary depending on domains and iterations are highlighted. All detailed prompts are available at\nhttps:\/\/github.com\/ShengranHu\/ADAS .\nWe use the following system prompt for every query in the meta agent.\nSystem prompt for the meta agent.\nYou are a helpful assistant. Make sure to return in a WELL-FORMED JSON object.\nWe use the following prompt for the meta agent to design the new agent based on the archive of\npreviously discovered agents.\nMain prompt for the meta agent.\nYou are an expert machine learning researcher testing various agentic systems. Your objective is to design\nbuilding blocks such as prompts and control flows within these systems to solve complex tasks. Your aim\nis to design an optimal agent performing well on [BriefDescriptionoftheDomain].\n[FrameworkCode]\n[OutputInstructionsandExamples]\n[DiscoveredAgentArchive] (initializedwithbaselines,updatedateveryiteration)\n# Your task\nYou are deeply familiar with prompting techniques and the agent works from the literature. Your goal is\nto maximize the specified performance metrics by proposing interestingly new agents.\nObserve the discovered agents carefully and think about what insights, lessons, or stepping stones can be\nlearned from them.\nBe creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration\nfrom related agent papers or academic papers from other research areas.\nUse the knowledge from the archive and inspiration from academic literature to propose the next\ninteresting agentic system design.\nTHINK OUTSIDE THE BOX.\nThe domain descriptions are available in Appendices C and D and the framework code is available\nin Appendix B. We use the following prompt to instruct and format the output of the meta agent.\nHere, we collect and present some common mistakes that the meta agent may make in the prompt.\nWe found it effective in improving the quality of the generated code.\nOutput Instruction and Example.\n# Output Instruction and Example:\nThe first key should be (\u201cthought\u201d), and it should capture your thought process for designing the\nnext function. In","chunk_id":"cc802d9b841fde55e9c0c2ba0ef7869d","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"JENNY ZHANG","type":"PERSON","description":"Jenny Zhang is a researcher who co-authored a paper on open-endedness in models of human notions of interestingness.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"JOEL LEHMAN","type":"PERSON","description":"Joel Lehman is a researcher who co-authored a paper on open-endedness in models of human notions of interestingness.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"KENNETH STANLEY","type":"PERSON","description":"Kenneth Stanley is a researcher who co-authored a paper on open-endedness in models of human notions of interestingness.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is a researcher who co-authored a paper on open-endedness in models of human notions of interestingness.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"OMNI","type":"RESEARCH","description":"OMNI is a research project focused on open-endedness via models of human notions of interestingness.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"SHAOKUN ZHANG","type":"PERSON","description":"Shaokun Zhang is a researcher who co-authored a paper on offline training of language model agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"JIEYU ZHANG","type":"PERSON","description":"Jieyu Zhang is a researcher who co-authored a paper on offline training of language model agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"JIALE LIU","type":"PERSON","description":"Jiale Liu is a researcher who co-authored a paper on offline training of language model agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"LINXIN SONG","type":"PERSON","description":"Linxin Song is a researcher who co-authored a paper on offline training of language model agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"CHI WANG","type":"PERSON","description":"Chi Wang is a researcher who co-authored a paper on offline training of language model agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"RANJAY KRISHNA","type":"PERSON","description":"Ranjay Krishna is a researcher who co-authored a paper on offline training of language model agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"QINGYUN WU","type":"PERSON","description":"Qingyun Wu is a researcher who co-authored a paper on offline training of language model agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"MEMORY MECHANISM","type":"RESEARCH","description":"The memory mechanism refers to the methods and structures used in large language model-based agents to manage and utilize memory.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"ZEU ZHANG","type":"PERSON","description":"Zeyu Zhang is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"XIAOHE BO","type":"PERSON","description":"Xiaohe Bo is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"CHEN MA","type":"PERSON","description":"Chen Ma is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"RUI LI","type":"PERSON","description":"Rui Li is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"XU CHEN","type":"PERSON","description":"Xu Chen is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"QUANYU DAI","type":"PERSON","description":"Quanyu Dai is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"JIEMING ZHU","type":"PERSON","description":"Jieming Zhu is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"ZHENHUA DONG","type":"PERSON","description":"Zhenhua Dong is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"JI-RONG WEN","type":"PERSON","description":"Ji-Rong Wen is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"HUAIXIU STEVEN ZHENG","type":"PERSON","description":"Huaixiu Steven Zheng is a researcher who co-authored a paper on reasoning via abstraction in large language models.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"SWAROOP MISHRA","type":"PERSON","description":"Swaroop Mishra is a researcher who co-authored a paper on reasoning via abstraction in large language models.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"XINYUN CHEN","type":"PERSON","description":"Xinyun Chen is a researcher who co-authored a paper on self-discovery in large language models.\nXinyun Chen is a researcher who co-authored a paper on reasoning via abstraction in large language models.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"HENG-TZE CHENG","type":"PERSON","description":"Heng-Tze Cheng is a researcher who co-authored a paper on self-discovery in large language models.\nHeng-Tze Cheng is a researcher who co-authored a paper on reasoning via abstraction in large language models.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"ED H CHI","type":"PERSON","description":"Ed H Chi is a researcher who co-authored a paper on reasoning via abstraction in large language models.\nEd H Chi is a researcher who co-authored a paper on self-discovery in large language models.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"QUOC V LE","type":"PERSON","description":"Quoc V Le is a researcher who co-authored a paper on self-discovery in large language models.\nQuoc V Le is a researcher who co-authored a paper on reasoning via abstraction in large language models.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"DENNY ZHOU","type":"PERSON","description":"Denny Zhou is a researcher who co-authored a paper on reasoning via abstraction in large language models.\nDenny Zhou is a researcher who co-authored a paper on self-discovery in large language models.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"PEI ZHOU","type":"PERSON","description":"Pei Zhou is a researcher who co-authored a paper on self-discovery in large language models.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"JAY PUJARA","type":"PERSON","description":"Jay Pujara is a researcher who co-authored a paper on self-discovery in large language models.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"XIANG REN","type":"PERSON","description":"Xiang Ren is a researcher who co-authored a paper on self-discovery in large language models.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"WANGCHUNSHU ZHOU","type":"PERSON","description":"Wangchunshu Zhou is a researcher who co-authored a paper on symbolic learning for self-evolving agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"YIXIN OU","type":"PERSON","description":"Yixin Ou is a researcher who co-authored a paper on symbolic learning for self-evolving agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"SHENGWEI DING","type":"PERSON","description":"Shengwei Ding is a researcher who co-authored a paper on symbolic learning for self-evolving agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"LONG LI","type":"PERSON","description":"Long Li is a researcher who co-authored a paper on symbolic learning for self-evolving agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"JIALONG WU","type":"PERSON","description":"Jialong Wu is a researcher who co-authored a paper on symbolic learning for self-evolving agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"TIANNAN WANG","type":"PERSON","description":"Tiannan Wang is a researcher who co-authored a paper on symbolic learning for self-evolving agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"JIAMIN CHEN","type":"PERSON","description":"Jiamin Chen is a researcher who co-authored a paper on symbolic learning for self-evolving agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"SHUAI WANG","type":"PERSON","description":"Shuai Wang is a researcher who co-authored a paper on symbolic learning for self-evolving agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"XIAOHUA XU","type":"PERSON","description":"Xiaohua Xu is a researcher who co-authored a paper on symbolic learning for self-evolving agents.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"MINGCHEN ZHUGE","type":"PERSON","description":"Mingchen Zhuge is a researcher who co-authored a paper on language agents as optimizable graphs.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"WENYI WANG","type":"PERSON","description":"Wenyi Wang is a researcher who co-authored a paper on language agents as optimizable graphs.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"LOUIS KIRSCH","type":"PERSON","description":"Louis Kirsch is a researcher who co-authored a paper on language agents as optimizable graphs.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"FRANCESCO FACCIO","type":"PERSON","description":"Francesco Faccio is a researcher who co-authored a paper on language agents as optimizable graphs.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"DMITRII KHIZBULLIN","type":"PERSON","description":"Dmitrii Khizbullin is a researcher who co-authored a paper on language agents as optimizable graphs.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"J\u00dcRGEN SCHMIDHUBER","type":"PERSON","description":"J\u00fcrgen Schmidhuber is a researcher who co-authored a paper on language agents as optimizable graphs.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"VARIBAD","type":"RESEARCH","description":"Varibad is a research project focused on variational Bayes-adaptive deep reinforcement learning via meta-learning.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"RESEARCH"},{"name":"LUISA ZINTGRAF","type":"PERSON","description":"Luisa Zintgraf is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"SEBASTIAN SCHULZE","type":"PERSON","description":"Sebastian Schulze is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"CONG LU","type":"PERSON","description":"Cong Lu is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"LEO FENG","type":"PERSON","description":"Leo Feng is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"MAXIMILIAN IGL","type":"PERSON","description":"Maximilian Igl is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"KYRIACOS SHIARLIS","type":"PERSON","description":"Kyriacos Shiarlis is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"YARIN GAL","type":"PERSON","description":"Yarin Gal is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"KATJA HOFMANN","type":"PERSON","description":"Katja Hofmann is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"SHIMON WHITESON","type":"PERSON","description":"Shimon Whiteson is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"OFFLINE TRAINING","type":"","description":"","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"REASONING","type":"","description":"","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"SELF-DISCOVERY","type":"","description":"","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"SELF(\"ENTITY\"","type":"","description":"","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"LANGUAGE AGENTS","type":"","description":"","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"JENNY ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jenny Zhang is a researcher who co-authored a paper on open-endedness in models of human notions of interestingness.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"JOEL LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Lehman is a researcher who co-authored a paper on open-endedness in models of human notions of interestingness.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"KENNETH STANLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kenneth Stanley is a researcher who co-authored a paper on open-endedness in models of human notions of interestingness.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is a researcher who co-authored a paper on open-endedness in models of human notions of interestingness.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"OMNI\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">OMNI is a research project focused on open-endedness via models of human notions of interestingness.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"SHAOKUN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shaokun Zhang is a researcher who co-authored a paper on offline training of language model agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"JIEYU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jieyu Zhang is a researcher who co-authored a paper on offline training of language model agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"JIALE LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiale Liu is a researcher who co-authored a paper on offline training of language model agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"LINXIN SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linxin Song is a researcher who co-authored a paper on offline training of language model agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"CHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chi Wang is a researcher who co-authored a paper on offline training of language model agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"RANJAY KRISHNA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ranjay Krishna is a researcher who co-authored a paper on offline training of language model agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"QINGYUN WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qingyun Wu is a researcher who co-authored a paper on offline training of language model agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"MEMORY MECHANISM\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">The memory mechanism refers to the methods and structures used in large language model-based agents to manage and utilize memory.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"ZEU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zeyu Zhang is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"XIAOHE BO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiaohe Bo is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"CHEN MA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Ma is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"RUI LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rui Li is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"XU CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu Chen is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"QUANYU DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quanyu Dai is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"JIEMING ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jieming Zhu is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"ZHENHUA DONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhenhua Dong is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"JI-RONG WEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ji-Rong Wen is a researcher who co-authored a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"HUAIXIU STEVEN ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Huaixiu Steven Zheng is a researcher who co-authored a paper on reasoning via abstraction in large language models.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"SWAROOP MISHRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Swaroop Mishra is a researcher who co-authored a paper on reasoning via abstraction in large language models.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"XINYUN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xinyun Chen is a researcher who co-authored a paper on self-discovery in large language models.Xinyun Chen is a researcher who co-authored a paper on reasoning via abstraction in large language models.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HENG-TZE CHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heng-Tze Cheng is a researcher who co-authored a paper on self-discovery in large language models.Heng-Tze Cheng is a researcher who co-authored a paper on reasoning via abstraction in large language models.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ED H CHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ed H Chi is a researcher who co-authored a paper on reasoning via abstraction in large language models.Ed H Chi is a researcher who co-authored a paper on self-discovery in large language models.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QUOC V LE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quoc V Le is a researcher who co-authored a paper on self-discovery in large language models.Quoc V Le is a researcher who co-authored a paper on reasoning via abstraction in large language models.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DENNY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Denny Zhou is a researcher who co-authored a paper on reasoning via abstraction in large language models.Denny Zhou is a researcher who co-authored a paper on self-discovery in large language models.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PEI ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pei Zhou is a researcher who co-authored a paper on self-discovery in large language models.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"JAY PUJARA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jay Pujara is a researcher who co-authored a paper on self-discovery in large language models.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"XIANG REN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiang Ren is a researcher who co-authored a paper on self-discovery in large language models.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"WANGCHUNSHU ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wangchunshu Zhou is a researcher who co-authored a paper on symbolic learning for self-evolving agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"YIXIN OU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yixin Ou is a researcher who co-authored a paper on symbolic learning for self-evolving agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"SHENGWEI DING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengwei Ding is a researcher who co-authored a paper on symbolic learning for self-evolving agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"LONG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Long Li is a researcher who co-authored a paper on symbolic learning for self-evolving agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"JIALONG WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jialong Wu is a researcher who co-authored a paper on symbolic learning for self-evolving agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"TIANNAN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tiannan Wang is a researcher who co-authored a paper on symbolic learning for self-evolving agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"JIAMIN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiamin Chen is a researcher who co-authored a paper on symbolic learning for self-evolving agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"SHUAI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shuai Wang is a researcher who co-authored a paper on symbolic learning for self-evolving agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"XIAOHUA XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiaohua Xu is a researcher who co-authored a paper on symbolic learning for self-evolving agents.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"MINGCHEN ZHUGE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mingchen Zhuge is a researcher who co-authored a paper on language agents as optimizable graphs.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"WENYI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wenyi Wang is a researcher who co-authored a paper on language agents as optimizable graphs.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LOUIS KIRSCH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Louis Kirsch is a researcher who co-authored a paper on language agents as optimizable graphs.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FRANCESCO FACCIO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Francesco Faccio is a researcher who co-authored a paper on language agents as optimizable graphs.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DMITRII KHIZBULLIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dmitrii Khizbullin is a researcher who co-authored a paper on language agents as optimizable graphs.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"J&#220;RGEN SCHMIDHUBER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J&#252;rgen Schmidhuber is a researcher who co-authored a paper on language agents as optimizable graphs.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"VARIBAD\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Varibad is a research project focused on variational Bayes-adaptive deep reinforcement learning via meta-learning.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">RESEARCH<\/data>    <\/node>    <node id=\"LUISA ZINTGRAF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luisa Zintgraf is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SEBASTIAN SCHULZE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sebastian Schulze is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CONG LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cong Lu is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEO FENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Leo Feng is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MAXIMILIAN IGL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maximilian Igl is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KYRIACOS SHIARLIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kyriacos Shiarlis is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YARIN GAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yarin Gal is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KATJA HOFMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Katja Hofmann is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHIMON WHITESON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shimon Whiteson is a researcher who co-authored a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"OFFLINE TRAINING\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"REASONING\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"SELF-DISCOVERY\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"SELF(&quot;ENTITY&quot;\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"LANGUAGE AGENTS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <edge source=\"JENNY ZHANG\" target=\"OMNI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jenny Zhang is a co-author of the OMNI research project focused on open-endedness in models of human notions of interestingness.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"JOEL LEHMAN\" target=\"OMNI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Joel Lehman is a co-author of the OMNI research project focused on open-endedness in models of human notions of interestingness.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"KENNETH STANLEY\" target=\"OMNI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Kenneth Stanley is a co-author of the OMNI research project focused on open-endedness in models of human notions of interestingness.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"JEFF CLUNE\" target=\"OMNI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jeff Clune is a co-author of the OMNI research project focused on open-endedness in models of human notions of interestingness.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"SHAOKUN ZHANG\" target=\"OFFLINE TRAINING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Shaokun Zhang is a co-author of a paper on offline training of language model agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"JIEYU ZHANG\" target=\"OFFLINE TRAINING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jieyu Zhang is a co-author of a paper on offline training of language model agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"JIALE LIU\" target=\"OFFLINE TRAINING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jiale Liu is a co-author of a paper on offline training of language model agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"LINXIN SONG\" target=\"OFFLINE TRAINING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Linxin Song is a co-author of a paper on offline training of language model agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"CHI WANG\" target=\"OFFLINE TRAINING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Chi Wang is a co-author of a paper on offline training of language model agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"RANJAY KRISHNA\" target=\"OFFLINE TRAINING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ranjay Krishna is a co-author of a paper on offline training of language model agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"QINGYUN WU\" target=\"OFFLINE TRAINING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Qingyun Wu is a co-author of a paper on offline training of language model agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"MEMORY MECHANISM\" target=\"ZEU ZHANG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Zeyu Zhang is a co-author of a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"MEMORY MECHANISM\" target=\"XIAOHE BO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Xiaohe Bo is a co-author of a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"MEMORY MECHANISM\" target=\"CHEN MA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Chen Ma is a co-author of a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"MEMORY MECHANISM\" target=\"RUI LI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Rui Li is a co-author of a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"MEMORY MECHANISM\" target=\"XU CHEN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Xu Chen is a co-author of a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"MEMORY MECHANISM\" target=\"QUANYU DAI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Quanyu Dai is a co-author of a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"MEMORY MECHANISM\" target=\"JIEMING ZHU\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jieming Zhu is a co-author of a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"MEMORY MECHANISM\" target=\"ZHENHUA DONG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Zhenhua Dong is a co-author of a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"MEMORY MECHANISM\" target=\"JI-RONG WEN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ji-Rong Wen is a co-author of a survey on the memory mechanism of large language model-based agents.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"HUAIXIU STEVEN ZHENG\" target=\"REASONING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Huaixiu Steven Zheng is a co-author of a paper on reasoning via abstraction in large language models.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"SWAROOP MISHRA\" target=\"REASONING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Swaroop Mishra is a co-author of a paper on reasoning via abstraction in large language models.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"XINYUN CHEN\" target=\"REASONING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Xinyun Chen is a co-author of a paper on reasoning via abstraction in large language models.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"XINYUN CHEN\" target=\"SELF-DISCOVERY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Xinyun Chen is a co-author of a paper on self-discovery in large language models.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"HENG-TZE CHENG\" target=\"REASONING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Heng-Tze Cheng is a co-author of a paper on reasoning via abstraction in large language models.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"HENG-TZE CHENG\" target=\"SELF-DISCOVERY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Heng-Tze Cheng is a co-author of a paper on self-discovery in large language models.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"ED H CHI\" target=\"REASONING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ed H Chi is a co-author of a paper on reasoning via abstraction in large language models.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"QUOC V LE\" target=\"REASONING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Quoc V Le is a co-author of a paper on reasoning via abstraction in large language models.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"QUOC V LE\" target=\"SELF(&quot;ENTITY&quot;\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">MINGCHEN ZHUGE<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"DENNY ZHOU\" target=\"REASONING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Denny Zhou is a co-author of a paper on reasoning via abstraction in large language models.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"PEI ZHOU\" target=\"SELF-DISCOVERY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Pei Zhou is a co-author of a paper on self-discovery in large language models.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"JAY PUJARA\" target=\"SELF-DISCOVERY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jay Pujara is a co-author of a paper on self-discovery in large language models.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"XIANG REN\" target=\"SELF-DISCOVERY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Xiang Ren is a co-author of a paper on self-discovery in large language models.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"MINGCHEN ZHUGE\" target=\"LANGUAGE AGENTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mingchen Zhuge is a co-author of a paper on language agents as optimizable graphs.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"WENYI WANG\" target=\"LANGUAGE AGENTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Wenyi Wang is a co-author of a paper on language agents as optimizable graphs.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"LOUIS KIRSCH\" target=\"LANGUAGE AGENTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Louis Kirsch is a co-author of a paper on language agents as optimizable graphs.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"FRANCESCO FACCIO\" target=\"LANGUAGE AGENTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Francesco Faccio is a co-author of a paper on language agents as optimizable graphs.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"DMITRII KHIZBULLIN\" target=\"LANGUAGE AGENTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Dmitrii Khizbullin is a co-author of a paper on language agents as optimizable graphs.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"J&#220;RGEN SCHMIDHUBER\" target=\"LANGUAGE AGENTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">J&#252;rgen Schmidhuber is a co-author of a paper on language agents as optimizable graphs.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"VARIBAD\" target=\"LUISA ZINTGRAF\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Luisa Zintgraf is a co-author of a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"VARIBAD\" target=\"SEBASTIAN SCHULZE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Sebastian Schulze is a co-author of a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"VARIBAD\" target=\"CONG LU\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Cong Lu is a co-author of a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"VARIBAD\" target=\"LEO FENG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Leo Feng is a co-author of a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"VARIBAD\" target=\"MAXIMILIAN IGL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Maximilian Igl is a co-author of a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"VARIBAD\" target=\"KYRIACOS SHIARLIS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Kyriacos Shiarlis is a co-author of a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"VARIBAD\" target=\"YARIN GAL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yarin Gal is a co-author of a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"VARIBAD\" target=\"KATJA HOFMANN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Katja Hofmann is a co-author of a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"VARIBAD\" target=\"SHIMON WHITESON\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Shimon Whiteson is a co-author of a paper on variational Bayes-adaptive deep reinforcement learning.<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"282313a8340c6792e8c35f53ed157cd0","chunk":" available in Appendices C and D and the framework code is available\nin Appendix B. We use the following prompt to instruct and format the output of the meta agent.\nHere, we collect and present some common mistakes that the meta agent may make in the prompt.\nWe found it effective in improving the quality of the generated code.\nOutput Instruction and Example.\n# Output Instruction and Example:\nThe first key should be (\u201cthought\u201d), and it should capture your thought process for designing the\nnext function. In the \u201cthought\u201d section, first reason about what the next interesting agent to try\nshould be, then describe your reasoning and the overall concept behind the agent design, and\nfinally detail the implementation steps. The second key (\u201cname\u201d) corresponds to the name of\nyour next agent architecture. Finally, the last key (\u201ccode\u201d) corresponds to the exact \u201cforward()\u201d\nfunction in Python code that you would like to try. You must write COMPLETE CODE in \u201ccode\u201d:\nYourcodewillbepartoftheentireproject, sopleaseimplementcomplete, reliable, reusablecodesnippets.\n24Automated Design of Agentic Systems\nHere is an example of the output format for the next agent:\n{\u201cthought\u201d: \u201c**Insights:** Your insights on what should be the next interesting agent. **Overall Idea:**\nyour reasoning and the overall concept behind the agent design. **Implementation:** describe the\nimplementation step by step.\u201d,\n\u201cname\u201d: \u201cName of your proposed agent\u201d,\n\u201ccode\u201d: \u201cdef forward(self, taskInfo): # Your code here\u201d}\n## WRONG Implementation examples:\n[Examplesofpotentialmistakesthemetaagentmaymakeinimplementation]\nAfter the first response from the meta agent, we perform two rounds of self-reflection to make the\ngenerated agent novel and error-free (Madaan et al., 2024; Shinn et al., 2023).\nPrompt for self-reflection round 1.\n[GeneratedAgentfromPreviousIteration]\nCarefully review the proposed new architecture and reflect on the following points:\n1. **Interestingness**: Assess whether your proposed architecture is interesting or innovative compared\nto existing methods in the archive. If you determine that the proposed architecture is not interesting,\nsuggest a new architecture that addresses these shortcomings.\n- Make sure to check the difference between the proposed architecture and previous attempts.\n- Compare the proposal and the architectures in the archive CAREFULLY, including their actual differences\nin the implementation.\n- Decide whether the current architecture is innovative.\n- USE CRITICAL THINKING!\n2. **Implementation Mistakes**: Identify any mistakes you may have made in the implementation.\nReview the code carefully, debug any issues you find, and provide a corrected version. REMEMBER\nchecking \"## WRONG Implementation examples\" in the prompt.\n3. **Improvement**: Based on the proposed architecture, suggest improvements in the detailed\nimplementation that could increase its performance or effectiveness. In this step, focus on refining and\noptimizing the existing implementation without altering the overall design framework, except if you\nwant to propose a different architecture if the current is not interesting.\n- Observe carefully about whether the implementation is actually doing what it is supposed to do.\n- Check if there is redundant code or unnecessary steps in the implementation. Replace them with\neffective implementation.\n- Try to avoid the implementation being too similar to the previous agent.\nAnd then, you need to improve or revise the implementation, or implement the new proposed architecture\nbased on the reflection.\nYour response should be organized as follows:\n\"reflection\": Provide your thoughts on the interestingness of the architecture, identify any mistakes in the\nimplementation, and suggest improvements.\n\"thought\": Revise your previous proposal or propose a new architecture if necessary, using the same\nformat as the example response.\n\"name\": Provide a name for the revised or new architecture. (Don\u2019t put words like \"new\" or \"improved\"\nin the name.)\n\"code\": Provide the corrected code or an improved implementation. Make sure you actually implement\nyour fix and improvement in this code.\n25Automated Design of Agentic Systems\nPrompt for self-reflection round 2.\nUsing the tips in \u201c## WRONG Implementation examples\u201d section, further revise the code.\nYour response should be organized as follows:\nInclude your updated reflections in the \u201creflection\u201d. Repeat the previous \u201cthought\u201d and \u201cname\u201d. Update\nthe corrected version of the code in the \u201ccode\u201d section.\nWhen an error is encountered during the execution of the generated code, we conduct a reflection\nand re-run the code. This process is repeated up to five times if errors persist. Here is the prompt we\nuse to self-reflect any runtime error:\nPrompt for self-reflection when a runtime error occurs.\nError during evaluation:\n[Runtimeerrors]\nCarefully consider where you went wrong in your latest implementation. Using insights from previous\nattempts, try to debug the current code to implement the same thought. Repeat your previous thought in\n\u201cthought\u201d, and put your thinking for debugging in \u201cdebug_thought\u201d.\nB. Framework Code\nIn this paper, we provide the meta agent with a simple framework to implement basic functions,\nsuch as querying Foundation Models (FMs) and formatting prompts. The framework consists of\nfewer than 100 lines of code (excluding comments). In this framework, we encapsulate every\npiece of information into a namedtuple Info object, making it easy to combine different types of\ninformation (e.g., FM responses, results from tool function calls, task descriptions) and facilitate\ncommunicationbetweendifferentmodules. Additionally,intheFMmodule,weautomaticallyconstruct\nthe prompt by concatenating all input Info objects into a structured format, with each Info titled by\nits metadata (e.g., name, author). Throughout the appendix, we renamed some variables in the\ncode to match the terminologies used in the main text. The full framework code is available at","chunk_id":"282313a8340c6792e8c35f53ed157cd0","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"META AGENT","type":"TECHNOLOGY","description":"The meta agent is a system designed to generate code and improve its quality through self-reflection and iterative design processes.","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"OUTPUT INSTRUCTION","type":"PROCESS","description":"Output instruction refers to the guidelines provided for formatting the output of the meta agent, including keys like \"thought,\" \"name,\" and \"code.\"","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"AGENT ARCHITECTURE","type":"CONCEPT","description":"Agent architecture is the design framework for creating agents, detailing their structure and functionality.","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"FORWARD FUNCTION","type":"FUNCTION","description":"The forward function is a specific implementation in Python that defines how the agent processes input and generates output.","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"SELF-REFLECTION","type":"PROCESS","description":"Self-reflection is a process where the meta agent reviews its previous outputs to identify mistakes and suggest improvements.","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"IMPLEMENTATION MISTAKES","type":"ERROR TYPE","description":"Implementation mistakes refer to errors made during the coding process that need to be identified and corrected.","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"IMPROVEMENT SUGGESTIONS","type":"PROCESS","description":"Improvement suggestions are recommendations made to enhance the performance or effectiveness of the agent's implementation.","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"RUNTIME ERROR","type":"ERROR TYPE","description":"A runtime error is an error that occurs during the execution of the code, prompting the need for debugging and reflection.","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"FRAMEWORK CODE","type":"CODE","description":"Framework code is the foundational code provided to the meta agent, enabling it to perform basic functions like querying and formatting prompts.","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"NAMEDTUPLE INFO OBJECT","type":"DATA STRUCTURE","description":"The namedtuple Info object is a data structure used to encapsulate various types of information for easy communication between modules.","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"PROMPT","type":"INSTRUCTION","description":"A prompt is a specific instruction given to the meta agent to guide its output and reasoning process.","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"EXAMPLES OF POTENTIAL MISTAKES","type":"ERROR TYPE","description":"Examples of potential mistakes are instances of incorrect implementations that the meta agent may encounter during its coding process.","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"DEBUG_THOUGHT","type":"PROCESS","description":"Debug_thought is a reflective process where the meta agent analyzes its code to identify and fix errors during execution.","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"APPENDICES","type":"","description":"","source_id":"282313a8340c6792e8c35f53ed157cd0"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"META AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The meta agent is a system designed to generate code and improve its quality through self-reflection and iterative design processes.<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"OUTPUT INSTRUCTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Output instruction refers to the guidelines provided for formatting the output of the meta agent, including keys like \"thought,\" \"name,\" and \"code.\"<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"AGENT ARCHITECTURE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Agent architecture is the design framework for creating agents, detailing their structure and functionality.<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"FORWARD FUNCTION\">      <data key=\"d0\">FUNCTION<\/data>      <data key=\"d1\">The forward function is a specific implementation in Python that defines how the agent processes input and generates output.<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"SELF-REFLECTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Self-reflection is a process where the meta agent reviews its previous outputs to identify mistakes and suggest improvements.<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"IMPLEMENTATION MISTAKES\">      <data key=\"d0\">ERROR TYPE<\/data>      <data key=\"d1\">Implementation mistakes refer to errors made during the coding process that need to be identified and corrected.<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"IMPROVEMENT SUGGESTIONS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Improvement suggestions are recommendations made to enhance the performance or effectiveness of the agent's implementation.<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"RUNTIME ERROR\">      <data key=\"d0\">ERROR TYPE<\/data>      <data key=\"d1\">A runtime error is an error that occurs during the execution of the code, prompting the need for debugging and reflection.<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"FRAMEWORK CODE\">      <data key=\"d0\">CODE<\/data>      <data key=\"d1\">Framework code is the foundational code provided to the meta agent, enabling it to perform basic functions like querying and formatting prompts.<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"NAMEDTUPLE INFO OBJECT\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">The namedtuple Info object is a data structure used to encapsulate various types of information for easy communication between modules.<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"PROMPT\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">A prompt is a specific instruction given to the meta agent to guide its output and reasoning process.<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"EXAMPLES OF POTENTIAL MISTAKES\">      <data key=\"d0\">ERROR TYPE<\/data>      <data key=\"d1\">Examples of potential mistakes are instances of incorrect implementations that the meta agent may encounter during its coding process.<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"DEBUG_THOUGHT\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Debug_thought is a reflective process where the meta agent analyzes its code to identify and fix errors during execution.<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"APPENDICES\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <edge source=\"META AGENT\" target=\"OUTPUT INSTRUCTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The meta agent follows output instructions to format its responses correctly, ensuring clarity and structure.<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"SELF-REFLECTION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The meta agent engages in self-reflection to improve its code generation and overall performance.<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"AGENT ARCHITECTURE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The meta agent operates based on a specific agent architecture that defines its design and functionality.<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"PROMPT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The meta agent uses prompts to guide its output and reasoning, ensuring it follows the intended instructions.<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"AGENT ARCHITECTURE\" target=\"FORWARD FUNCTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The forward function is a critical component of the agent architecture, defining how the agent processes tasks.<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"IMPLEMENTATION MISTAKES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">During self-reflection, the meta agent identifies implementation mistakes to correct them in future iterations.<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"IMPROVEMENT SUGGESTIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Self-reflection leads to improvement suggestions that enhance the agent's functionality and effectiveness.<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"RUNTIME ERROR\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">When a runtime error occurs, the meta agent uses self-reflection to debug and correct the issue.<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"EXAMPLES OF POTENTIAL MISTAKES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Self-reflection includes reviewing examples of potential mistakes to avoid similar errors in future implementations.<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"RUNTIME ERROR\" target=\"DEBUG_THOUGHT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">When a runtime error occurs, the meta agent engages in debug_thought to analyze and resolve the issue.<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"FRAMEWORK CODE\" target=\"NAMEDTUPLE INFO OBJECT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The framework code utilizes the namedtuple Info object to facilitate communication and organization of information.<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"FRAMEWORK CODE\" target=\"APPENDICES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The framework code is detailed in the appendices, providing additional context and examples for implementation.<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"d66dc9ce4a9545b44f7486ea057b5937","chunk":"\ninformation (e.g., FM responses, results from tool function calls, task descriptions) and facilitate\ncommunicationbetweendifferentmodules. Additionally,intheFMmodule,weautomaticallyconstruct\nthe prompt by concatenating all input Info objects into a structured format, with each Info titled by\nits metadata (e.g., name, author). Throughout the appendix, we renamed some variables in the\ncode to match the terminologies used in the main text. The full framework code is available at\nhttps:\/\/github.com\/ShengranHu\/ADAS .\nCode 1|The simple framework used in Meta-Agent Search.\n1# Named tuple for holding task information\n2Info = namedtuple (\u2019Info \u2019, [\u2019name \u2019, \u2019author \u2019, \u2019content \u2019, \u2019\niteration_idx \u2019])\n3\n4# Format instructions for FM response\n5FORMAT_INST = lambda request_keys : f\" Reply EXACTLY with the\nfollowing JSON format .\\n{str( request_keys )}\\ nDO NOT MISS ANY\nFIELDS AND MAKE SURE THE JSON FORMAT IS CORRECT !\\n\"\n6\n7# Description of the role of the FM Module\n8ROLE_DESC = lambda role : f\"You are a { role }.\"\n9\n10@backoff . on_exception ( backoff .expo , openai . RateLimitError )\n11def get_json_response_from_gpt (msg , model , system_message ,\ntemperature ):\n12 \\\"\"\"\n13 Function to get JSON response from GPT model .\n14\n15 Args :\n16 - msg (str ): The user message .\n26Automated Design of Agentic Systems\n17 - model (str ): The model to use .\n18 - system_message (str ): The system message .\n19 - temperature ( float ): Sampling temperature .\n20\n21 Returns :\n22 - dict : The JSON response .\n23 \\\"\"\"\n24 ...\n25 return json_dict\n26\n27class FM_Module :\n28 \\\"\"\"\n29 Base class for an FM module .\n30\n31 Attributes :\n32 - output_fields ( list ): Fields expected in the output .\n33 - name (str ): Name of the FM module .\n34 - role (str ): Role description for the FM module .\n35 - model (str ): Model to be used .\n36 - temperature ( float ): Sampling temperature .\n37 - id (str ): Unique identifier for the FM module instance .\n38 \\\"\"\"\n39\n40 def __init__ (self , output_fields : list , name : str , role =\u2019helpful\nassistant \u2019, model =\u2019gpt -3.5 - turbo -0125 \u2019, temperature =0.5) ->\nNone :\n41 ...\n42\n43 def generate_prompt (self , input_infos , instruction ) -> str:\n44 \\\"\"\"\n45 Generates a prompt for the FM.\n46\n47 Args :\n48 - input_infos ( list ): List of input information .\n49 - instruction (str ): Instruction for the task .\n50\n51 Returns :\n52 - tuple : System prompt and user prompt .\n53\n54 An example of generated prompt :\n55 \"\"\n56 You are a helpful assistant .\n57\n58 # Output Format :\n59 Reply EXACTLY with the following JSON format .\n60 ...\n61\n62 # Your Task :\n63 You will given some number of paired example inputs and\noutputs . The outputs ...\n64\n65 ### thinking #1 by Chain -of - Thought hkFo ( yourself ):\n66 ...\n67\n68 # Instruction :\n69 Please think step by step and then solve the task by writing\n27Automated Design of Agentic Systems\nthe code .\n70 \"\"\n71 \\\"\"\"\n72 ...\n73 return system_prompt , prompt\n74\n75 def query (self , input_infos : list , instruction , iteration_idx\n= -1) -> list [ Info ]:\n76 \\\"\"\"\n77 Queries the FM with provided input information and\ninstruction .\n78\n79 Args :\n80 - input_infos ( list ): List of input information .\n81 - instruction (str ): Instruction for the task .\n82 - iteration_idx (int ): Iteration index for the task .\n83\n84 Returns :\n85 - output_infos ( list [ Info ]): Output information .\n86 \\\"\"\"\n87 ...\n88 return output_infos\n89\n90 def __repr__ ( self ):\n91 return f\"{ self . agent_name } { self .id}\"\n92\n93 def __call__ (self , input_infos : list , instruction , iteration_idx\n= -1):\n94 return self . query ( input_infos , instruction , iteration_idx =\niteration_idx )\n95\n96class AgentSystem :\n97 def forward (self , taskInfo ) -> Union [Info , str ]:\n98 \\\"\"\"\n99 Placeholder method for processing task information .\n100\n101 Args :\n102 - taskInfo ( Info ): Task information .\n103\n104 Returns :\n105 - Answer ( Union [Info , str ]): Your FINAL Answer . Return\neither a namedtuple Info or a string for the answer .\n106 \\\"\"\"\n107 pass\nWith the provided framework, an agent can be easily defined with a \u201cforward\u201d function. Here we\nshow an example of implementing self-reflection using the framework.\nCode 2|Self-Reflection implementation example\n1def forward (self , taskInfo ):\n2 # Instruction for initial reasoning\n3 cot_initial_instruction = \" Please think step by step and then\nsolve the task .\"\n4\n5 # Instruction for reflecting on previous attempts and feedback\n28Automated Design of Agentic Systems\nto improve\n6 cot_reflect_instruction = \" Given previous attempts and feedback ,\ncarefully consider where you could go wrong in your latest\nattempt . Using insights from previous attempts , try to solve\nthe task better .\"\n7 cot_module = FM_Module ([ \u2019thinking \u2019, \u2019answer \u2019], \u2019Chain -of - Thought\n\u2019)\n8\n9 # Instruction for providing feedback and correcting the answer\n10 critic_instruction = \" Please review the answer above and\ncriticize","chunk_id":"d66dc9ce4a9545b44f7486ea057b5937","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"FM MODULE","type":"MODULE","description":"The FM Module is a base class designed for managing the functionality of a framework module, including attributes like output fields, name, role, model, temperature, and a unique identifier.","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"INFO","type":"DATA STRUCTURE","description":"Info is a named tuple that holds task information, including attributes such as name, author, content, and iteration index.","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"AGENT SYSTEM","type":"SYSTEM","description":"The Agent System is a framework component that processes task information and is designed to implement functionalities like self-reflection.","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"JSON RESPONSE","type":"DATA FORMAT","description":"The JSON response is a structured output format returned by the FM module after processing a user message through a model.","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"GPT MODEL","type":"TECHNOLOGY","description":"The GPT model is a generative pre-trained transformer model used for generating text responses based on input messages.","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"BACKOFF","type":"TECHNIQUE","description":"Backoff is a technique used to handle exceptions, specifically for managing rate limit errors when querying the GPT model.","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"COT INSTRUCTION","type":"INSTRUCTION","description":"Chain-of-Thought (COT) instruction is a directive for the model to think step by step before solving a task.","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"OUTPUT FIELDS","type":"DATA FORMAT","description":"Output fields are the expected fields in the output generated by the FM Module, defining the structure of the response.","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"SYSTEM MESSAGE","type":"DATA FORMAT","description":"The system message is a directive provided to the GPT model that sets the context for generating responses.","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"ITERATION INDEX","type":"DATA FORMAT","description":"The iteration index is a numerical identifier used to track the iteration of a task within the FM Module.","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"RESPONSE","type":"DATA FORMAT","description":"Response refers to the output generated by the FM Module or Agent System after processing input information and instructions.","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"TASK INFORMATION","type":"","description":"","source_id":"d66dc9ce4a9545b44f7486ea057b5937"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"FM MODULE\">      <data key=\"d0\">MODULE<\/data>      <data key=\"d1\">The FM Module is a base class designed for managing the functionality of a framework module, including attributes like output fields, name, role, model, temperature, and a unique identifier.<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"INFO\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Info is a named tuple that holds task information, including attributes such as name, author, content, and iteration index.<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"AGENT SYSTEM\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">The Agent System is a framework component that processes task information and is designed to implement functionalities like self-reflection.<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"JSON RESPONSE\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">The JSON response is a structured output format returned by the FM module after processing a user message through a model.<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"GPT MODEL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The GPT model is a generative pre-trained transformer model used for generating text responses based on input messages.<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"BACKOFF\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Backoff is a technique used to handle exceptions, specifically for managing rate limit errors when querying the GPT model.<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"COT INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">Chain-of-Thought (COT) instruction is a directive for the model to think step by step before solving a task.<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"OUTPUT FIELDS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Output fields are the expected fields in the output generated by the FM Module, defining the structure of the response.<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"SYSTEM MESSAGE\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">The system message is a directive provided to the GPT model that sets the context for generating responses.<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"ITERATION INDEX\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">The iteration index is a numerical identifier used to track the iteration of a task within the FM Module.<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"RESPONSE\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Response refers to the output generated by the FM Module or Agent System after processing input information and instructions.<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"TASK INFORMATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <edge source=\"FM MODULE\" target=\"INFO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The FM Module utilizes the Info data structure to hold and manage task-related information for processing.<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"AGENT SYSTEM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Agent System interacts with the FM Module to process task information and implement functionalities like self-reflection.<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"JSON RESPONSE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The FM Module generates a JSON response as output after processing user messages through the GPT model.<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"GPT MODEL\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The FM Module queries the GPT model to obtain responses based on user input and instructions.<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"OUTPUT FIELDS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The FM Module defines the output fields that are expected in the responses it generates for tasks.<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"SYSTEM MESSAGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The FM Module utilizes a system message to provide context for the GPT model when generating responses.<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"ITERATION INDEX\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The FM Module uses the iteration index to manage and track the progress of tasks across multiple iterations.<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"AGENT SYSTEM\" target=\"COT INSTRUCTION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The Agent System uses Chain-of-Thought instructions to guide the reasoning process for solving tasks.<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"AGENT SYSTEM\" target=\"TASK INFORMATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Agent System processes task information to derive answers or outputs based on the defined instructions.<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"AGENT SYSTEM\" target=\"RESPONSE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The Agent System generates a response based on the processed task information and instructions provided to it.<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"GPT MODEL\" target=\"BACKOFF\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Backoff technique is applied when querying the GPT model to manage rate limit errors effectively.<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"4b43decac6833d1515992f8869ecada7","chunk":"entic Systems\nto improve\n6 cot_reflect_instruction = \" Given previous attempts and feedback ,\ncarefully consider where you could go wrong in your latest\nattempt . Using insights from previous attempts , try to solve\nthe task better .\"\n7 cot_module = FM_Module ([ \u2019thinking \u2019, \u2019answer \u2019], \u2019Chain -of - Thought\n\u2019)\n8\n9 # Instruction for providing feedback and correcting the answer\n10 critic_instruction = \" Please review the answer above and\ncriticize on where might be wrong . If you are absolutely sure\nit is correct , output \u2019True \u2019 in \u2019correct \u2019.\"\n11 critic_module = FM_Module ([ \u2019feedback \u2019, \u2019correct \u2019], \u2019Critic \u2019)\n12\n13 N_max = 5 # Maximum number of attempts\n14\n15 # Initial attempt\n16 cot_inputs = [ taskInfo ]\n17 thinking , answer = cot_module ( cot_inputs ,\ncot_initial_instruction , 0)\n18\n19 for i in range ( N_max ):\n20 # Get feedback and correct status from the critic\n21 feedback , correct = critic_module ([ taskInfo , thinking ,\nanswer ], critic_instruction , i)\n22 if correct . content == \u2019True \u2019:\n23 break\n24\n25 # Add feedback to the inputs for the next iteration\n26 cot_inputs . extend ([ thinking , answer , feedback ])\n27\n28 # Reflect on previous attemps and refine the answer\n29 thinking , answer = cot_module ( cot_inputs ,\ncot_reflect_instruction , i + 1)\n30 return answer\n29Automated Design of Agentic Systems\nExample Input -output grid #1\nExample Input -output grid #2\nTest grid\nAnswer\nFigure 4|An example task from the ARC challenge (Chollet, 2019). Given the input-output grid\nexamples, the AI system is asked to learn the transformation rules and then apply these learned rules\nto the test grid to predict the final answer.\nC. Experiment Details for ARC Challenge\nAn example task from the ARC challenge is shown in Figure 4. In the ARC challenge experiments\n(Section 4.1), we represent the grids as strings of 2-D arrays, where each color is represented by an\ninteger. Weinstructthemetaagenttodesignagentsthatgeneratecodeassolutionsratherthandirectly\noutputting answers. Additionally, we provide two tool functions within the framework: (1) to test\nwhetherthegeneratedcodecansolvetheexamplegridsand(2)toobtainthetask\u2019sanswerbyapplying\nthe generated code to the test grid. The accuracy rate is calculated by the Exact Match between the\nreference solution and the predicted answer. The meta agent uses \u201cgpt-4o-2024-05-13\u201d (OpenAI,\n2024), while discovered agents and baselines are evaluated using \u201cgpt-3.5-turbo-0125\u201d (OpenAI,\n2022) to reduce compute cost.\nThe domain description of ARC for the meta agent is shown below:\nDescription of ARC for the meta agent.\nYour aim is to find an optimal agent performing well on the ARC (Abstraction and Reasoning Corpus)\nchallenge.\nIn this challenge, each task consists of three demonstration examples, and one test example. Each\nExample consists of an \u201cinput grid\u201d and an \u201coutput grid\u201d. Test-takers need to use the transformation rule\nlearned from the examples to predict the output grid for the test example.\n30Automated Design of Agentic Systems\n# An example task from ARC challenge:\n## Task Overview:\nYou will be given some number of paired example inputs and outputs grids. The outputs were produced\nby applying a transformation rule to the input grids. In addition to the paired example inputs and\noutputs, there is also one test input without a known output.\nThe inputs and outputs are each \u201cgrids\u201d. A grid is a rectangular matrix of integers between 0 and 9\n(inclusive). Each number corresponds to a color. 0 is black.\nYour task is to determine the transformation rule from examples and find out the answer, involving\ndetermining the size of the output grid for the test and correctly filling each cell of the grid with the\nappropriate color or number.\nThe transformation only needs to be unambiguous and applicable to the example inputs and the test\ninput. It doesn\u2019t need to work for all possible inputs. Observe the examples carefully, imagine the grid\nvisually, and try to find the pattern.\n## Examples:\n### Example 0:\ninput = [[0,0,0,0,5,0,0,0,0], [0,0,0,0,5,0,0,0,0], [0,0,0,4,5,0,0,0,0], [0,0,0,4,5,4,4,0,0],\n[0,0,3,3,5,0,0,0,0], [0,0,0,3,5,0,0,0,0], [0,0,0,3,5,3,3,3,0], [0,0,0,3,5,0,0,0,0], [0,0,0,0,5,0,0,0,0],\n[0,0,0,0,5,0,0,0,0]]\noutput = [[0,0,0,0], [0,0,0,0], [0,0,0,4], [0,0,4,4], [0,0,3,3], [0,0,0,3], [0,3,3,3], [0,0,0,3], [0,0","chunk_id":"4b43decac6833d1515992f8869ecada7","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"COT_REFLECT_INSTRUCTION","type":"INSTRUCTION","description":"The cot_reflect_instruction is a directive for the system to consider previous attempts and feedback to improve the current task solution.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"COT_MODULE","type":"MODULE","description":"The cot_module is a functional module that processes inputs related to thinking and answering, utilizing a chain-of-thought approach.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"CRITIC_INSTRUCTION","type":"INSTRUCTION","description":"The critic_instruction is a directive for the system to review and critique the answer provided, determining its correctness.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"CRITIC_MODULE","type":"MODULE","description":"The critic_module is a functional module that provides feedback and correctness status on the answers generated by the system.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"N_MAX","type":"PARAMETER","description":"N_max is a parameter that defines the maximum number of attempts the system can make to solve a task.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"COT_INPUTS","type":"DATA FORMAT","description":"COT_inputs are the inputs provided to the cot_module for processing, including task information and previous feedback.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"ARC_CHALLENGE","type":"CHALLENGE","description":"The ARC (Abstraction and Reasoning Corpus) challenge is a task that involves predicting outputs based on transformation rules applied to input grids.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"EXAMPLE_INPUT_OUTPUT_GRID","type":"DATA FORMAT","description":"Example input-output grids are paired examples used in the ARC challenge to demonstrate the transformation rules.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"TEST_GRID","type":"DATA FORMAT","description":"The test grid is an input grid provided in the ARC challenge for which the output is unknown and needs to be predicted.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"GTP-4O-2024-05-13","type":"MODEL","description":"GPT-4O-2024-05-13 is a version of the GPT model used by the meta agent in the ARC challenge for generating solutions.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"GPT-3.5-TURBO-0125","type":"MODEL","description":"GPT-3.5-Turbo-0125 is a version of the GPT model used for evaluating discovered agents and baselines in the ARC challenge.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"FEEDBACK","type":"DATA FORMAT","description":"Feedback is the information provided by the critic_module regarding the correctness and quality of the answers generated.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"CORRECT","type":"STATUS","description":"Correct is a status indicating whether the answer generated by the system is accurate, as determined by the critic_module.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"INPUT_GRID","type":"DATA FORMAT","description":"Input_grid is a rectangular matrix of integers representing the initial state in the ARC challenge, from which transformation rules are derived.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"OUTPUT_GRID","type":"DATA FORMAT","description":"Output_grid is a rectangular matrix of integers representing the expected result after applying transformation rules to the input grid in the ARC challenge.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"TRANSFORMATION_RULE","type":"RULE","description":"Transformation_rule refers to the logic or method applied to the input grid to derive the output grid in the ARC challenge.","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"TASK_INFO","type":"","description":"","source_id":"4b43decac6833d1515992f8869ecada7"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"COT_REFLECT_INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">The cot_reflect_instruction is a directive for the system to consider previous attempts and feedback to improve the current task solution.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"COT_MODULE\">      <data key=\"d0\">MODULE<\/data>      <data key=\"d1\">The cot_module is a functional module that processes inputs related to thinking and answering, utilizing a chain-of-thought approach.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"CRITIC_INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">The critic_instruction is a directive for the system to review and critique the answer provided, determining its correctness.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"CRITIC_MODULE\">      <data key=\"d0\">MODULE<\/data>      <data key=\"d1\">The critic_module is a functional module that provides feedback and correctness status on the answers generated by the system.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"N_MAX\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">N_max is a parameter that defines the maximum number of attempts the system can make to solve a task.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"COT_INPUTS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">COT_inputs are the inputs provided to the cot_module for processing, including task information and previous feedback.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"ARC_CHALLENGE\">      <data key=\"d0\">CHALLENGE<\/data>      <data key=\"d1\">The ARC (Abstraction and Reasoning Corpus) challenge is a task that involves predicting outputs based on transformation rules applied to input grids.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"EXAMPLE_INPUT_OUTPUT_GRID\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Example input-output grids are paired examples used in the ARC challenge to demonstrate the transformation rules.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"TEST_GRID\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">The test grid is an input grid provided in the ARC challenge for which the output is unknown and needs to be predicted.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"GTP-4O-2024-05-13\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4O-2024-05-13 is a version of the GPT model used by the meta agent in the ARC challenge for generating solutions.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO-0125\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5-Turbo-0125 is a version of the GPT model used for evaluating discovered agents and baselines in the ARC challenge.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"FEEDBACK\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Feedback is the information provided by the critic_module regarding the correctness and quality of the answers generated.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"CORRECT\">      <data key=\"d0\">STATUS<\/data>      <data key=\"d1\">Correct is a status indicating whether the answer generated by the system is accurate, as determined by the critic_module.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"INPUT_GRID\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Input_grid is a rectangular matrix of integers representing the initial state in the ARC challenge, from which transformation rules are derived.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"OUTPUT_GRID\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Output_grid is a rectangular matrix of integers representing the expected result after applying transformation rules to the input grid in the ARC challenge.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"TRANSFORMATION_RULE\">      <data key=\"d0\">RULE<\/data>      <data key=\"d1\">Transformation_rule refers to the logic or method applied to the input grid to derive the output grid in the ARC challenge.<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"TASK_INFO\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <edge source=\"COT_REFLECT_INSTRUCTION\" target=\"COT_MODULE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The cot_reflect_instruction guides the cot_module to refine answers based on previous attempts and feedback.<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"COT_MODULE\" target=\"COT_INPUTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The cot_module processes the cot_inputs to generate thinking and answers based on the provided task information.<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"CRITIC_MODULE\" target=\"COT_INPUTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The critic_module uses cot_inputs to provide feedback and correctness status on the answers generated by the cot_module.<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"CRITIC_MODULE\" target=\"FEEDBACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The critic_module generates feedback based on the answers provided by the cot_module, assessing their correctness.<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"CRITIC_MODULE\" target=\"CORRECT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The critic_module determines the correct status of the answer generated by the cot_module, indicating its accuracy.<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"COT_INPUTS\" target=\"TASK_INFO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">COT_inputs include task_info as part of the data processed by the cot_module to generate answers.<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC_CHALLENGE\" target=\"EXAMPLE_INPUT_OUTPUT_GRID\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The ARC challenge utilizes example input-output grids to teach the transformation rules necessary for predicting outputs.<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC_CHALLENGE\" target=\"TEST_GRID\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The test grid is part of the ARC challenge, where the system must apply learned transformation rules to predict the output.<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC_CHALLENGE\" target=\"GTP-4O-2024-05-13\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4O-2024-05-13 is used by the meta agent to generate solutions for tasks in the ARC challenge.<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC_CHALLENGE\" target=\"GPT-3.5-TURBO-0125\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">GPT-3.5-Turbo-0125 is used to evaluate the performance of agents and baselines in the ARC challenge.<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"INPUT_GRID\" target=\"OUTPUT_GRID\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The output_grid is derived from the input_grid by applying the transformation_rule in the ARC challenge.<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"INPUT_GRID\" target=\"TRANSFORMATION_RULE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The transformation_rule is applied to the input_grid to predict the output_grid in the ARC challenge.<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"OUTPUT_GRID\" target=\"TRANSFORMATION_RULE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The transformation_rule defines how the input_grid is transformed into the output_grid in the ARC challenge.<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"449db721e37968e073e3579b59e023b2","chunk":",0],\n[0,0,0,0,5,0,0,0,0]]\noutput = [[0,0,0,0], [0,0,0,0], [0,0,0,4], [0,0,4,4], [0,0,3,3], [0,0,0,3], [0,3,3,3], [0,0,0,3], [0,0,0,0],\n[0,0,0,0]]\n### Example 1:\ninput = [[0,0,0,0,5,0,0,0,0], [0,0,0,2,5,0,0,0,0], [0,0,0,2,5,2,6,0,0], [0,0,0,2,5,0,0,0,0],\n[0,0,0,2,5,2,2,2,0], [0,0,6,6,5,6,0,0,0], [0,0,0,2,5,0,0,0,0], [0,2,2,0,5,2,0,0,0], [0,0,0,2,5,0,0,0,0],\n[0,0,0,0,5,0,0,0,0]]\noutput = [[0,0,0,0], [0,0,0,2], [0,0,6,2], [0,0,0,2], [0,2,2,2], [0,0,6,6], [0,0,0,2], [0,2,2,2], [0,0,0,2],\n[0,0,0,0]]\n### Example 2:\ninput = [[0,0,0,0,5,0,0,0,0], [0,0,0,0,5,7,0,0,0], [0,0,0,8,5,0,0,0,0], [0,0,0,8,5,0,0,0,0],\n[0,7,8,8,5,0,0,0,0], [0,0,0,0,5,8,8,0,0], [0,0,0,8,5,0,0,0,0], [0,0,0,8,5,0,0,0,0], [0,0,0,0,5,8,7,0,0],\n[0,0,0,0,5,0,0,0,0]]\noutput= [[0,0,0,0], [0,0,0,7], [0,0,0,8], [0,0,0,8], [0,7,8,8], [0,0,8,8], [0,0,0,8], [0,0,0,8], [0,0,7,8],\n[0,0,0,0]]\n### Test Problem:\ninput = [[0,0,0,0,5,0,0,0,0], [0,0,0,1,5,0,0,0,0], [0,0,0,1,5,1,0,0,0], [0,1,1,1,5,1,1,1,6],\n[0,0,0,6,5,6,6,0,0], [0,0,0,0,5,1,1,1,0], [0,0,0,1,5,0,0,0,0], [0,0,0,1,5,1,6,0,0], [0,0,0,0,5,6,0,0,0],\n[0,0,0,0,5,0,0,0,0]]\nAnalyze the transformation rules based on the provided Examples and determine what the output should\nbe for the Test Problem.\nHere we present the best agent on ARC discovered by Meta Agent Search. All agents from the\nexperiment can be found at https:\/\/github.com\/ShengranHu\/ADAS .\nCode 3|The best agent on ARC discovered by Meta Agent Search\n1# Structured Feedback and Ensemble Agent\n2def forward (self , taskInfo ):\n3 # Step 1: Generate initial candidate solutions using multiple FM\nModules\n31Automated Design of Agentic Systems\n4 initial_instruction = \u2019Please think step by step and then solve\nthe task by writing the code .\u2019\n5 num_candidates = 5 # Number of initial candidates\n6 initial_module = [ FM_Module ([ \u2019thinking \u2019, \u2019code \u2019], \u2019Initial\nSolution \u2019, temperature =0.8) for _ in range ( num_candidates )]\n7\n8 initial_solutions = []\n9 for i in range ( num_candidates ):\n10 thoughts = initial_module [i ]([ taskInfo ], initial_instruction\n)\n11 thinking , code = thoughts [0] , thoughts [1]\n12 feedback , correct_examples , wrong_examples = self .\nrun_examples_and_get_feedback ( code )\n13 if len ( correct_examples ) > 0: # Only consider solutions\nthat passed at least one example\n14 initial_solutions . append ({ \u2019thinking \u2019: thinking , \u2019code \u2019:\ncode , \u2019feedback \u2019: feedback , \u2019 correct_count \u2019: len (\ncorrect_examples )","chunk_id":"449db721e37968e073e3579b59e023b2","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"META AGENT SEARCH","type":"RESEARCH INITIATIVE","description":"Meta Agent Search is a research initiative focused on discovering optimal agents for various tasks, including those evaluated on the ARC benchmark.","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"ARC","type":"BENCHMARK","description":"ARC (AI Research Challenge) is a benchmark designed to evaluate the performance of AI agents on a variety of tasks.","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"STRUCTURED FEEDBACK AND ENSEMBLE AGENT","type":"AGENT","description":"The Structured Feedback and Ensemble Agent is a specific AI agent developed to generate solutions through structured feedback mechanisms.","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"FM_MODULE","type":"MODULE","description":"FM_Module refers to a functional module used within the agent to generate candidate solutions based on given instructions.","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"TASKINFO","type":"DATA FORMAT","description":"TaskInfo is the input data structure that contains information about the task to be solved by the agent.","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"THOUGHTS","type":"DATA FORMAT","description":"Thoughts refer to the outputs generated by the FM_Module, which include the agent's reasoning and the corresponding code for the task.","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"FEEDBACK","type":"DATA FORMAT","description":"Feedback is the evaluation provided to the generated solutions, indicating their correctness and areas for improvement.","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"CORRECT EXAMPLES","type":"DATA FORMAT","description":"Correct examples are instances where the generated solutions successfully meet the task requirements, used to assess the performance of the agent.","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"WRONG EXAMPLES","type":"DATA FORMAT","description":"Wrong examples are instances where the generated solutions fail to meet the task requirements, providing insights into the agent's shortcomings.","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"INITIAL CANDIDATE SOLUTIONS","type":"","description":"","source_id":"449db721e37968e073e3579b59e023b2"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">RESEARCH INITIATIVE<\/data>      <data key=\"d1\">Meta Agent Search is a research initiative focused on discovering optimal agents for various tasks, including those evaluated on the ARC benchmark.<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">ARC (AI Research Challenge) is a benchmark designed to evaluate the performance of AI agents on a variety of tasks.<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"STRUCTURED FEEDBACK AND ENSEMBLE AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Structured Feedback and Ensemble Agent is a specific AI agent developed to generate solutions through structured feedback mechanisms.<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"FM_MODULE\">      <data key=\"d0\">MODULE<\/data>      <data key=\"d1\">FM_Module refers to a functional module used within the agent to generate candidate solutions based on given instructions.<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"TASKINFO\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">TaskInfo is the input data structure that contains information about the task to be solved by the agent.<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"THOUGHTS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Thoughts refer to the outputs generated by the FM_Module, which include the agent's reasoning and the corresponding code for the task.<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"FEEDBACK\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Feedback is the evaluation provided to the generated solutions, indicating their correctness and areas for improvement.<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"CORRECT EXAMPLES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Correct examples are instances where the generated solutions successfully meet the task requirements, used to assess the performance of the agent.<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"WRONG EXAMPLES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Wrong examples are instances where the generated solutions fail to meet the task requirements, providing insights into the agent's shortcomings.<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"INITIAL CANDIDATE SOLUTIONS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <edge source=\"META AGENT SEARCH\" target=\"ARC\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search aims to discover agents that perform well on the ARC benchmark, contributing to advancements in AI evaluation.<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"STRUCTURED FEEDBACK AND ENSEMBLE AGENT\" target=\"FM_MODULE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Structured Feedback and Ensemble Agent utilizes FM_Module to generate initial candidate solutions for tasks.<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"STRUCTURED FEEDBACK AND ENSEMBLE AGENT\" target=\"TASKINFO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Structured Feedback and Ensemble Agent processes TaskInfo to generate solutions based on the provided task details.<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"STRUCTURED FEEDBACK AND ENSEMBLE AGENT\" target=\"INITIAL CANDIDATE SOLUTIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Structured Feedback and Ensemble Agent generates initial candidate solutions as part of its problem-solving process.<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"STRUCTURED FEEDBACK AND ENSEMBLE AGENT\" target=\"FEEDBACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Structured Feedback and Ensemble Agent uses feedback to refine its generated solutions and improve performance.<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"STRUCTURED FEEDBACK AND ENSEMBLE AGENT\" target=\"CORRECT EXAMPLES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Structured Feedback and Ensemble Agent considers correct examples to evaluate the success of its generated solutions.<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"STRUCTURED FEEDBACK AND ENSEMBLE AGENT\" target=\"WRONG EXAMPLES\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The Structured Feedback and Ensemble Agent analyzes wrong examples to identify areas for improvement in its solutions.<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"TASKINFO\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">FM_Module uses TaskInfo as input to generate candidate solutions for the agent's tasks.<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"THOUGHTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">FM_Module produces thoughts that include reasoning and code as outputs for the task at hand.<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"84317ae35cc75d612287186d93461447","chunk":" ]([ taskInfo ], initial_instruction\n)\n11 thinking , code = thoughts [0] , thoughts [1]\n12 feedback , correct_examples , wrong_examples = self .\nrun_examples_and_get_feedback ( code )\n13 if len ( correct_examples ) > 0: # Only consider solutions\nthat passed at least one example\n14 initial_solutions . append ({ \u2019thinking \u2019: thinking , \u2019code \u2019:\ncode , \u2019feedback \u2019: feedback , \u2019 correct_count \u2019: len (\ncorrect_examples )})\n15\n16 # Step 2: Simulate human - like feedback for each candidate\nsolution\n17 human_like_feedback_module = FM_Module ([ \u2019thinking \u2019, \u2019feedback \u2019],\n\u2019Human - like Feedback \u2019, temperature =0.5)\n18 human_feedback_instruction = \u2019Please provide human - like feedback\nfor the code , focusing on common mistakes , heuristic\ncorrections , and best practices .\u2019\n19\n20 for sol in initial_solutions :\n21 thoughts = human_like_feedback_module ([ taskInfo , sol[\u2019\nthinking \u2019], sol[\u2019code \u2019]], human_feedback_instruction )\n22 human_thinking , human_feedback = thoughts [0] , thoughts [1]\n23 sol [\u2019 human_feedback \u2019] = human_feedback\n24\n25 # Step 3: Assign expert advisors to evaluate and provide\ntargeted feedback\n26 expert_roles = [\u2019Efficiency Expert \u2019, \u2019 Readability Expert \u2019, \u2019\nSimplicity Expert \u2019]\n27 expert_advisors = [ FM_Module ([ \u2019thinking \u2019, \u2019feedback \u2019], role ,\ntemperature =0.6) for role in expert_roles ]\n28 expert_instruction = \u2019Please evaluate the given code and provide\ntargeted feedback for improvement .\u2019\n29\n30 for sol in initial_solutions :\n31 sol_feedback = {}\n32 for advisor in expert_advisors :\n33 thoughts = advisor ([ taskInfo , sol[\u2019thinking \u2019], sol[\u2019code\n\u2019]], expert_instruction )\n34 thinking , feedback = thoughts [0] , thoughts [1]\n35 sol_feedback [ advisor . role ] = feedback\n36 sol [\u2019 expert_feedback \u2019] = sol_feedback\n37\n38 # Step 4: Parse and structure the feedback to avoid redundancy\nand refine the solutions iteratively\n39 max_refinement_iterations = 3\n40 refinement_module = FM_Module ([ \u2019thinking \u2019, \u2019code \u2019], \u2019Refinement\nModule \u2019, temperature =0.5)\n32Automated Design of Agentic Systems\n41 refined_solutions = []\n42\n43 for sol in initial_solutions :\n44 for i in range ( max_refinement_iterations ):\n45 combined_feedback = sol[\u2019feedback \u2019]. content + sol [\u2019\nhuman_feedback \u2019]. content + \u2019\u2019. join ([ fb. content for fb\nin sol [\u2019 expert_feedback \u2019]. values () ])\n46 structured_feedback = \u2019 \u2019. join (set( combined_feedback .\nsplit ())) # Avoid redundancy\n47 refinement_instruction = \u2019Using the structured feedback ,\nrefine the solution to improve its performance .\u2019\n48 thoughts = refinement_module ([ taskInfo , sol[\u2019thinking \u2019],\nsol[\u2019code \u2019], Info (\u2019feedback \u2019, \u2019Structured Feedback \u2019,\nstructured_feedback , i)], refinement_instruction , i)\n49 refinement_thinking , refined_code = thoughts [0] ,\nthoughts [1]\n50 feedback , correct_examples , wrong_examples = self .\nrun_examples_and_get_feedback ( refined_code )\n51 if len ( correct_examples ) > 0:\n52 sol. update ({ \u2019thinking \u2019: refinement_thinking , \u2019code \u2019:\nrefined_code , \u2019feedback \u2019: feedback , \u2019\ncorrect_count \u2019: len( correct_examples )})\n53 refined_solutions . append ( sol)\n54\n55 # Step 5: Select the best - performing solutions and make a final\ndecision using an ensemble approach\n56 sorted_solutions = sorted ( refined_solutions , key= lambda x: x[\u2019\ncorrect_count \u2019], reverse = True )\n57 top_solutions = sorted_solutions [:3] # Select the top 3\nsolutions\n58\n59 final_decision_instruction = \u2019Given all the above solutions ,\nreason over them carefully and provide a final answer by\nwriting the code .\u2019\n60 final_decision_module = refinement_module ([ \u2019thinking \u2019, \u2019code \u2019],\n\u2019Final Decision Module \u2019, temperature =0.1)\n61 final_inputs = [ taskInfo ] + [ item for solution in top_solutions\nfor item in [ solution [\u2019thinking \u2019], solution [\u2019code \u2019], solution\n[\u2019feedback \u2019]]]\n62 final_thoughts = final_decision_module ( final_inputs ,\nfinal_decision_instruction )\n63 final_thinking , final_code = final_thoughts [0] , final_thoughts\n[1]\n64 answer = self . get_test_output_from_code ( final_code )\n65 return answer\nD. Experiment Details for Reasoning and Problem-Solving Domains\nTo reduce costs during search and evaluation, we sample subsets of data from each domain. For GPQA\n(Science), the validation set consists of 32 questions, while the remaining 166 questions form the\ntest set. For the other domains, the validation and test sets are sampled with 128 and 800 questions,\nrespectively. We evaluate agents five times for GPQA and once for the other domains to maintain a\nconsistent total number of evaluations. Each domain uses zero-shot style questions, except DROP\n(Reading Comprehension), which uses one-shot style questions following the practice in (OpenAI,\n33Automated Design of Agentic Systems\n2023). The meta agent uses \u201cgpt-4o-2024-05-13\u201d (OpenAI, 2024), while discovered agents and\nbaselines are evaluated using \u201cgpt-3.5-turbo-0125\u201d (OpenAI, 2022) to reduce compute cost","chunk_id":"84317ae35cc75d612287186d93461447","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"HUMAN-LIKE FEEDBACK MODULE","type":"TECHNIQUE","description":"The Human-Like Feedback Module is a component designed to simulate human feedback for code solutions, focusing on common mistakes and best practices.","source_id":"84317ae35cc75d612287186d93461447"},{"name":"FM_MODULE","type":"TECHNIQUE","description":"FM Module refers to a framework or module used for processing thoughts and feedback in various roles, such as human-like feedback and expert evaluation.","source_id":"84317ae35cc75d612287186d93461447"},{"name":"INITIAL_SOLUTIONS","type":"DATA STRUCTURE","description":"Initial Solutions is a collection of candidate solutions that have been generated and are awaiting evaluation and feedback.","source_id":"84317ae35cc75d612287186d93461447"},{"name":"EXPERT ADVISORS","type":"ROLE","description":"Expert Advisors are specialized roles assigned to evaluate code solutions and provide targeted feedback for improvement.","source_id":"84317ae35cc75d612287186d93461447"},{"name":"REFINEMENT MODULE","type":"TECHNIQUE","description":"The Refinement Module is a component that iteratively refines solutions based on structured feedback to enhance performance.","source_id":"84317ae35cc75d612287186d93461447"},{"name":"FINAL DECISION MODULE","type":"TECHNIQUE","description":"The Final Decision Module is a component that synthesizes the best-performing solutions to make a final decision on the code.","source_id":"84317ae35cc75d612287186d93461447"},{"name":"GPQA","type":"DATA SET","description":"GPQA (Generalized Problem Solving Questions and Answers) is a dataset used for evaluating reasoning and problem-solving capabilities in various domains.","source_id":"84317ae35cc75d612287186d93461447"},{"name":"DROP","type":"DATA SET","description":"DROP (Reading Comprehension) is a dataset that employs one-shot style questions for evaluating reading comprehension abilities.","source_id":"84317ae35cc75d612287186d93461447"},{"name":"GTP-4O-2024-05-13","type":"TECHNOLOGY","description":"GPT-4O-2024-05-13 is a version of the GPT model used for evaluations in the meta agent context.","source_id":"84317ae35cc75d612287186d93461447"},{"name":"GPT-3.5-TURBO-0125","type":"TECHNOLOGY","description":"GPT-3.5-Turbo-0125 is a version of the GPT model used for evaluating discovered agents and baselines.","source_id":"84317ae35cc75d612287186d93461447"},{"name":"FEEDBACK","type":"DATA STRUCTURE","description":"Feedback is the information provided regarding the performance of the code solutions, including corrections and suggestions for improvement.","source_id":"84317ae35cc75d612287186d93461447"},{"name":"CORRECT_EXAMPLES","type":"DATA STRUCTURE","description":"Correct Examples are instances of solutions that have successfully passed evaluation criteria, indicating their correctness.","source_id":"84317ae35cc75d612287186d93461447"},{"name":"WRONG_EXAMPLES","type":"DATA STRUCTURE","description":"Wrong Examples are instances of solutions that failed to meet evaluation criteria, highlighting areas for improvement.","source_id":"84317ae35cc75d612287186d93461447"},{"name":"TASKINFO","type":"DATA STRUCTURE","description":"TaskInfo is the contextual information or parameters related to the specific task being evaluated or solved.","source_id":"84317ae35cc75d612287186d93461447"},{"name":"THOUGHTS","type":"","description":"","source_id":"84317ae35cc75d612287186d93461447"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"HUMAN-LIKE FEEDBACK MODULE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Human-Like Feedback Module is a component designed to simulate human feedback for code solutions, focusing on common mistakes and best practices.<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"FM_MODULE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">FM Module refers to a framework or module used for processing thoughts and feedback in various roles, such as human-like feedback and expert evaluation.<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"INITIAL_SOLUTIONS\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Initial Solutions is a collection of candidate solutions that have been generated and are awaiting evaluation and feedback.<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"EXPERT ADVISORS\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Expert Advisors are specialized roles assigned to evaluate code solutions and provide targeted feedback for improvement.<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"REFINEMENT MODULE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Refinement Module is a component that iteratively refines solutions based on structured feedback to enhance performance.<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"FINAL DECISION MODULE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Final Decision Module is a component that synthesizes the best-performing solutions to make a final decision on the code.<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"GPQA\">      <data key=\"d0\">DATA SET<\/data>      <data key=\"d1\">GPQA (Generalized Problem Solving Questions and Answers) is a dataset used for evaluating reasoning and problem-solving capabilities in various domains.<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">DATA SET<\/data>      <data key=\"d1\">DROP (Reading Comprehension) is a dataset that employs one-shot style questions for evaluating reading comprehension abilities.<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"GTP-4O-2024-05-13\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4O-2024-05-13 is a version of the GPT model used for evaluations in the meta agent context.<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO-0125\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5-Turbo-0125 is a version of the GPT model used for evaluating discovered agents and baselines.<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"FEEDBACK\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Feedback is the information provided regarding the performance of the code solutions, including corrections and suggestions for improvement.<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"CORRECT_EXAMPLES\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Correct Examples are instances of solutions that have successfully passed evaluation criteria, indicating their correctness.<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"WRONG_EXAMPLES\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Wrong Examples are instances of solutions that failed to meet evaluation criteria, highlighting areas for improvement.<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"TASKINFO\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">TaskInfo is the contextual information or parameters related to the specific task being evaluated or solved.<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"THOUGHTS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <edge source=\"HUMAN-LIKE FEEDBACK MODULE\" target=\"INITIAL_SOLUTIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Human-Like Feedback Module processes the initial solutions to provide simulated human feedback on their quality.<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"INITIAL_SOLUTIONS\" target=\"EXPERT ADVISORS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Expert Advisors evaluate the initial solutions and provide targeted feedback for improvement.<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"INITIAL_SOLUTIONS\" target=\"REFINEMENT MODULE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Refinement Module iteratively refines the initial solutions based on structured feedback received from various sources.<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"INITIAL_SOLUTIONS\" target=\"GPQA\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">GPQA is a dataset used to generate and evaluate initial solutions in the context of reasoning and problem-solving.<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"INITIAL_SOLUTIONS\" target=\"DROP\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">DROP is a dataset that influences the style of questions used in evaluating initial solutions, particularly in reading comprehension.<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"INITIAL_SOLUTIONS\" target=\"GTP-4O-2024-05-13\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">GPT-4O-2024-05-13 is utilized in the evaluation of the initial solutions generated by the meta agent.<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"INITIAL_SOLUTIONS\" target=\"GPT-3.5-TURBO-0125\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">GPT-3.5-Turbo-125 is used for evaluating discovered agents and baselines related to the initial solutions.<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"INITIAL_SOLUTIONS\" target=\"THOUGHTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Thoughts are generated as part of the evaluation process for each initial solution, reflecting the cognitive analysis performed.<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"INITIAL_SOLUTIONS\" target=\"FEEDBACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Feedback is derived from the evaluation of initial solutions, providing insights into their performance and areas for improvement.<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"INITIAL_SOLUTIONS\" target=\"CORRECT_EXAMPLES\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Correct Examples are identified from the initial solutions that have met the evaluation criteria successfully.<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"INITIAL_SOLUTIONS\" target=\"WRONG_EXAMPLES\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Wrong Examples are identified from the initial solutions that have not met the evaluation criteria, indicating failures.<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"INITIAL_SOLUTIONS\" target=\"TASKINFO\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">TaskInfo provides the necessary context and parameters for evaluating the initial solutions effectively.<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"REFINEMENT MODULE\" target=\"FINAL DECISION MODULE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Final Decision Module uses the refined solutions to make a final decision on the best-performing code.<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"10fda605f670bcfccfc13c2ca0dde959","chunk":" style questions, except DROP\n(Reading Comprehension), which uses one-shot style questions following the practice in (OpenAI,\n33Automated Design of Agentic Systems\n2023). The meta agent uses \u201cgpt-4o-2024-05-13\u201d (OpenAI, 2024), while discovered agents and\nbaselines are evaluated using \u201cgpt-3.5-turbo-0125\u201d (OpenAI, 2022) to reduce compute cost.\nWe present the description of each domain we provide to the meta agent.\nDescription of DROP (Reading Comprehension).\nYour aim is to find an optimal agent performing well on the Reading Comprehension Benchmark\nRequiring Discrete Reasoning Over Paragraphs (DROP), which assesses the ability to perform discrete\nreasoning and comprehend detailed information across multiple paragraphs.\n## An example question from DROP:\nYou will be asked to read a passage and answer a question.\nPassage:\nNon-nationals make up more than half of the population of Bahrain, with immigrants making up\nabout 55% of the overall population. Of those, the vast majority come from South and Southeast Asia:\naccording to various media reports and government statistics dated between 2005-2009 roughly 290,000\nIndians, 125,000 Bangladeshis, 45,000 Pakistanis, 45,000 Filipinos, and 8,000 Indonesians.\nQuestion: What two nationalities had the same number of people living in Bahrain between\n2005-2009?\nAnswer [Not Given]: Pakistanis and Filipinos\nDescription of GPQA (Science) for the meta agent.\nYour aim is to find an optimal agent performing well on the GPQA (Graduate-Level Google-Proof Q&A\nBenchmark). This benchmark consists of challenging multiple-choice questions across the domains of\nbiology, physics, and chemistry, designed by domain experts to ensure high quality and difficulty.\n## An example question from GPQA:\nTwo quantum states with energies E1 and E2 have a lifetime of 10\u22129sec and 10\u22128sec, respectively. We\nwant to clearly distinguish these two energy levels. Which one of the following options could be their\nenergy difference so that they be clearly resolved?\nAnswer choices:\n10\u22129eV\n10\u22128eV\n10\u22127eV\n10\u22126eV\nCorrect answer [Not provided]:\n10\u22127eV\nExplanation [Not provided]:\nAccording to the uncertainty principle, Delta E* Delta t=hbar\/2. Delta t is the lifetime and Delta E is the\nwidth of the energy level. With Delta t= 10\u22129s== >Delta E1= 3.3 10\u22127ev. And Delta t= 10\u221211s gives\nDelta E2= 3.310\u22128eV. Therefore, the energy difference between the two states must be significantly\ngreater than 10\u22127ev. So the answer is 10\u22124ev.\n34Automated Design of Agentic Systems\nDescription of MGSM (Math) for the meta agent.\nYour aim is to find an optimal agent performing well on the Multilingual Grade School Math Benchmark\n(MGSM) which evaluates mathematical problem-solving abilities across various languages to ensure\nbroad and effective multilingual performance.\n## An example question from MGSM:\n**Question**:\u3053\u306e\u6570\u5b66\u306e\u554f\u984c\u3092\u89e3\u3044\u3066\u304f\u3060\u3055\u3044 \u3002\n\u8fd1\u6240\u3067\u306f\u3001\u30da\u30c3\u30c8\u306e\u30a6\u30b5\u30ae\u306e\u6570\u304c\u30da\u30c3\u30c8\u306e\u72ac\u3068\u732b\u3092\u5408\u308f\u305b\u305f\u6570\u3088\u308a\u308212\u5339\u5c11\u306a\u3044\u3002\u72ac1\u5339\u3042\u305f\u308a2\u5339\n\u306e\u732b\u304c\u304a\u308a\u3001\u72ac\u306e\u6570\u306f60\u5339\u3060\u3068\u3059\u308b\u3068 \u3001\u5168\u90e8\u3067\u8fd1\u6240\u306b\u306f\u4f55\u5339\u306e\u30da\u30c3\u30c8\u304c\u3044\u307e\u3059\u304b \uff1f\n**Answer (Not Given)**: 348\nDescription of MMLU (Mult-task) for the meta agent.\nYour aim is to find an optimal agent performing well on the MMLU (Massive Multitask Language\nUnderstanding) benchmark, a challenging evaluation that assesses a model\u2019s ability to answer questions\nacross a wide range of subjects and difficulty levels. It includes subjects from STEM, social sciences,\nhumanities, and more.\n## An example question from MMLU:\nAnswer the following multiple-choice question.\nThe constellation ... is a bright W-shaped constellation in the northern sky.\n(A) Centaurus\n(B) Cygnus\n(C) Cassiopeia\n(D) Cepheus\nE. Baselines\nIn this paper, we implement five state-of-the-art hand-designed agent baselines for experiments\non ARC (Section 4.1): (1) Chain-of-Thought (COT) (Wei et al., 2022), (2) Self-Consistency with\nChain-of-Thought (COT-SC)(Wang et al., 2023b), (3) Self-Refine (Madaan et al., 2024; Shinn et al.,\n2023), (4) LLM-Debate (Du et al., 2023), and (5) Quality-Diversity, a simplified version of Intelligent\nGo-Explore (Lu et al., 2024c).\nIn addition to these baselines, we implement two more for experiments on Reasoning and\nProblem-Solving domains (Section 4.2): (6) Step-back Abstraction (Zheng et al., 2023) and (7)\nRole Assignment (Xu et al., 2023). An example implementation of Self-Refine with our simple\nframework is shown in Appendix B. Detailed implementations of all baselines can be found at\nhttps:\/\/github.com\/ShengranHu\/AD","chunk_id":"10fda605f670bcfccfc13c2ca0dde959","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"DROP","type":"BENCHMARK","description":"DROP (Reading Comprehension) is a benchmark that assesses the ability to perform discrete reasoning and comprehend detailed information across multiple paragraphs.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"GPQA","type":"BENCHMARK","description":"GPQA (Graduate-Level Google-Proof Q&A Benchmark) consists of challenging multiple-choice questions across the domains of biology, physics, and chemistry.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"MGSM","type":"BENCHMARK","description":"MGSM (Multilingual Grade School Math Benchmark) evaluates mathematical problem-solving abilities across various languages.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"MMLU","type":"BENCHMARK","description":"MMLU (Massive Multitask Language Understanding) benchmark assesses a model\u2019s ability to answer questions across a wide range of subjects and difficulty levels.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"GTP-4O-2024-05-13","type":"TECHNOLOGY","description":"GPT-4O-2024-05-13 is a version of the GPT-4 model used by the meta agent for processing.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"GPT-3.5-TURBO-0125","type":"TECHNOLOGY","description":"GPT-3.5-Turbo-0125 is a version of the GPT-3.5 model used for evaluating discovered agents and baselines.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"CHAIN-OF-THOUGHT (COT)","type":"TECHNIQUE","description":"Chain-of-Thought (COT) is a hand-designed agent baseline for reasoning tasks.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"SELF-CONSISTENCY WITH COT (COT-SC)","type":"TECHNIQUE","description":"Self-Consistency with Chain-of-Thought (COT-SC) is a hand-designed agent baseline that builds on the COT technique.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"SELF-REFINE","type":"TECHNIQUE","description":"Self-Refine is a hand-designed agent baseline for reasoning and problem-solving tasks.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"LLM-DEBATE","type":"TECHNIQUE","description":"LLM-Debate is a hand-designed agent baseline for reasoning tasks.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"QUALITY-DIVERSITY","type":"TECHNIQUE","description":"Quality-Diversity is a simplified version of Intelligent Go-Explore, used as a hand-designed agent baseline.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"STEP-BACK ABSTRACTION","type":"TECHNIQUE","description":"Step-back Abstraction is a hand-designed agent baseline for reasoning and problem-solving domains.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"ROLE ASSIGNMENT","type":"TECHNIQUE","description":"Role Assignment is a hand-designed agent baseline for reasoning and problem-solving domains.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"INDIANS","type":"NATIONALITY","description":"Indians are a nationality represented in Bahrain, with a population of approximately 290,000 according to reports from 2005-2009.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"BANGLADESHIS","type":"NATIONALITY","description":"Bangladeshis are a nationality represented in Bahrain, with a population of approximately 125,000 according to reports from 2005-2009.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"PAKISTANIS","type":"NATIONALITY","description":"Pakistanis are a nationality represented in Bahrain, with a population of approximately 45,000 according to reports from 2005-2009.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"FILIPINOS","type":"NATIONALITY","description":"Filipinos are a nationality represented in Bahrain, with a population of approximately 45,000 according to reports from 2005-2009.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"INDONESIANS","type":"NATIONALITY","description":"Indonesians are a nationality represented in Bahrain, with a population of approximately 8,000 according to reports from 2005-2009.","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"BAHRAIN","type":"","description":"","source_id":"10fda605f670bcfccfc13c2ca0dde959"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"DROP\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">DROP (Reading Comprehension) is a benchmark that assesses the ability to perform discrete reasoning and comprehend detailed information across multiple paragraphs.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"GPQA\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">GPQA (Graduate-Level Google-Proof Q&amp;A Benchmark) consists of challenging multiple-choice questions across the domains of biology, physics, and chemistry.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"MGSM\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MGSM (Multilingual Grade School Math Benchmark) evaluates mathematical problem-solving abilities across various languages.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MMLU (Massive Multitask Language Understanding) benchmark assesses a model&#8217;s ability to answer questions across a wide range of subjects and difficulty levels.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"GTP-4O-2024-05-13\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4O-2024-05-13 is a version of the GPT-4 model used by the meta agent for processing.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO-0125\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5-Turbo-0125 is a version of the GPT-3.5 model used for evaluating discovered agents and baselines.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT (COT)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain-of-Thought (COT) is a hand-designed agent baseline for reasoning tasks.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"SELF-CONSISTENCY WITH COT (COT-SC)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-Consistency with Chain-of-Thought (COT-SC) is a hand-designed agent baseline that builds on the COT technique.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-Refine is a hand-designed agent baseline for reasoning and problem-solving tasks.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"LLM-DEBATE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LLM-Debate is a hand-designed agent baseline for reasoning tasks.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Quality-Diversity is a simplified version of Intelligent Go-Explore, used as a hand-designed agent baseline.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"STEP-BACK ABSTRACTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Step-back Abstraction is a hand-designed agent baseline for reasoning and problem-solving domains.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"ROLE ASSIGNMENT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Role Assignment is a hand-designed agent baseline for reasoning and problem-solving domains.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"INDIANS\">      <data key=\"d0\">NATIONALITY<\/data>      <data key=\"d1\">Indians are a nationality represented in Bahrain, with a population of approximately 290,000 according to reports from 2005-2009.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"BANGLADESHIS\">      <data key=\"d0\">NATIONALITY<\/data>      <data key=\"d1\">Bangladeshis are a nationality represented in Bahrain, with a population of approximately 125,000 according to reports from 2005-2009.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"PAKISTANIS\">      <data key=\"d0\">NATIONALITY<\/data>      <data key=\"d1\">Pakistanis are a nationality represented in Bahrain, with a population of approximately 45,000 according to reports from 2005-2009.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"FILIPINOS\">      <data key=\"d0\">NATIONALITY<\/data>      <data key=\"d1\">Filipinos are a nationality represented in Bahrain, with a population of approximately 45,000 according to reports from 2005-2009.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"INDONESIANS\">      <data key=\"d0\">NATIONALITY<\/data>      <data key=\"d1\">Indonesians are a nationality represented in Bahrain, with a population of approximately 8,000 according to reports from 2005-2009.<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"BAHRAIN\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <edge source=\"DROP\" target=\"GTP-4O-2024-05-13\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The meta agent uses GPT-4O-2024-05-13 to perform well on the DROP benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"DROP\" target=\"GPT-3.5-TURBO-0125\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Discovered agents and baselines are evaluated using GPT-3.5-Turbo-0125 for the DROP benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"DROP\" target=\"CHAIN-OF-THOUGHT (COT)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Chain-of-Thought (COT) is one of the hand-designed agent baselines implemented for experiments on the DROP benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"DROP\" target=\"SELF-CONSISTENCY WITH COT (COT-SC)\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Self-Consistency with Chain-of-Thought (COT-SC) is one of the hand-designed agent baselines implemented for experiments on the DROP benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"DROP\" target=\"SELF-REFINE\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Self-Refine is one of the hand-designed agent baselines implemented for experiments on the DROP benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"DROP\" target=\"LLM-DEBATE\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">LLM-Debate is one of the hand-designed agent baselines implemented for experiments on the DROP benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"DROP\" target=\"QUALITY-DIVERSITY\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Quality-Diversity is one of the hand-designed agent baselines implemented for experiments on the DROP benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"DROP\" target=\"STEP-BACK ABSTRACTION\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Step-back Abstraction is one of the hand-designed agent baselines implemented for experiments on the DROP benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"DROP\" target=\"ROLE ASSIGNMENT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Role Assignment is one of the hand-designed agent baselines implemented for experiments on the DROP benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"GPQA\" target=\"GTP-4O-2024-05-13\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The meta agent uses GPT-4O-2024-05-13 to perform well on the GPQA benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"GPQA\" target=\"GPT-3.5-TURBO-0125\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Discovered agents and baselines are evaluated using GPT-3.5-Turbo-0125 for the GPQA benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"MGSM\" target=\"GTP-4O-2024-05-13\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The meta agent uses GPT-4O-2024-05-13 to perform well on the MGSM benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"MGSM\" target=\"GPT-3.5-TURBO-0125\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Discovered agents and baselines are evaluated using GPT-3.5-Turbo-0125 for the MGSM benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"GTP-4O-2024-05-13\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The meta agent uses GPT-4O-2024-05-13 to perform well on the MMLU benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"GPT-3.5-TURBO-0125\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Discovered agents and baselines are evaluated using GPT-3.5-Turbo-0125 for the MMLU benchmark.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"INDIANS\" target=\"BAHRAIN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Indians make up a significant portion of the non-national population in Bahrain, with approximately 290,000 residing there.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"BANGLADESHIS\" target=\"BAHRAIN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Bangladeshis make up a significant portion of the non-national population in Bahrain, with approximately 125,000 residing there.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"PAKISTANIS\" target=\"BAHRAIN\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Pakistanis make up a portion of the non-national population in Bahrain, with approximately 45,000 residing there.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"FILIPINOS\" target=\"BAHRAIN\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Filipinos make up a portion of the non-national population in Bahrain, with approximately 45,000 residing there.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"INDONESIANS\" target=\"BAHRAIN\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Indonesians make up a portion of the non-national population in Bahrain, with approximately 8,000 residing there.<\/data>      <data key=\"d5\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"97457e990eb6e3c88c11c862f9e3265b","chunk":" to these baselines, we implement two more for experiments on Reasoning and\nProblem-Solving domains (Section 4.2): (6) Step-back Abstraction (Zheng et al., 2023) and (7)\nRole Assignment (Xu et al., 2023). An example implementation of Self-Refine with our simple\nframework is shown in Appendix B. Detailed implementations of all baselines can be found at\nhttps:\/\/github.com\/ShengranHu\/ADAS .\nIn COT, we prompt the FM to think step by step before answering the question. In COT-SC, we\nsample \ud835\udc41=5answers and then perform an ensemble using either majority voting or an FM query.\nIn Self-Refine, we allow up to five refinement iterations, with an early stop if the critic deems the\nanswer correct. In LLM-Debate, each debate module is assigned a unique role, such as Physics Expert\nor Chemistry Expert, and the debate lasts for two rounds. In Quality-Diversity, we conduct three\n35Automated Design of Agentic Systems\niterations to collect diverse answers based on previously proposed ones. In Role Assignment, we use\nan FM query to first choose a role from a predefined set, and then use another FM query to answer\nthe question by acting within the chosen role.\nF. Example Agents\nIn this section, we present the detailed implementation of three example discovered agents by Meta\nAgent Search shown in Figure 1. The \u201cMulti-Step Peer Review Agent\u201d and \u201cDivide and Conquer Agent\u201d\nwere discovered during the search in the Reading Comprehension domain (GPQA) (Rein et al., 2023),\nwhile the \u201cVerified Multimodal Agent\u201d was discovered during the search in the Math domain (MGSM)\n(Shietal.,2023). Alldiscoveredagentscanbefoundat https:\/\/github.com\/ShengranHu\/ADAS .\nCode 4|Example discovered agent: Multi-Step Peer Review Agent\n1def forward (self , taskInfo ):\n2 initial_instruction = \" Please think step by step and then solve\nthe task .\"\n3 critique_instruction = \" Please review the answer above and\nprovide feedback on where it might be wrong . If you are\nabsolutely sure it is correct , output \u2019True \u2019 in \u2019correct \u2019.\"\n4 refine_instruction = \" Given previous attempts and feedback ,\ncarefully consider where you could go wrong in your latest\nattempt . Using insights from previous attempts , try to solve\nthe task better .\"\n5 final_decision_instruction = \" Given all the above thinking and\nanswers , reason over them carefully and provide a final\nanswer .\"\n6\n7 FM_modules = [ FM_module ([ \u2019thinking \u2019, \u2019answer \u2019], \u2019FM Module \u2019,\nrole = role ) for role in [\u2019Physics Expert \u2019, \u2019Chemistry Expert \u2019,\n\u2019Biology Expert \u2019, \u2019Science Generalist \u2019]]\n8 critic_modules = [ FM_module ([ \u2019feedback \u2019, \u2019correct \u2019], \u2019Critic \u2019,\nrole = role ) for role in [\u2019Physics Critic \u2019, \u2019Chemistry Critic \u2019,\n\u2019Biology Critic \u2019, \u2019General Critic \u2019]]\n9 final_decision_module = FM_module ([ \u2019thinking \u2019, \u2019answer \u2019], \u2019Final\nDecision \u2019, temperature =0.1)\n10\n11 all_thinking = [[] for _ in range (len( FM_modules ))]\n12 all_answer = [[] for _ in range (len( FM_modules ))]\n13 all_feedback = [[] for _ in range (len( FM_modules ))]\n14\n15 for i in range (len( FM_modules )):\n16 thinking , answer = FM_modules [i]([ taskInfo ],\ninitial_instruction )\n17 all_thinking [i]. append ( thinking )\n18 all_answer [i]. append ( answer )\n19\n20 for i in range (len( FM_modules )):\n21 for j in range (len( FM_modules )):\n22 if i != j:\n23 feedback , correct = critic_modules [j]([ taskInfo ,\nall_thinking [i][0] , all_answer [i][0]] ,\ncritique_instruction )\n24 all_feedback [i]. append ( feedback )\n25\n36Automated Design of Agentic Systems\n26 for i in range (len( FM_modules )):\n27 refine_inputs = [ taskInfo , all_thinking [i][0] , all_answer [i\n][0]] + all_feedback [i]\n28 thinking , answer = FM_modules [i]( refine_inputs ,\nrefine_instruction )\n29 all_thinking [i]. append ( thinking )\n30 all_answer [i]. append ( answer )\n31\n32 final_inputs = [ taskInfo ] + [ all_thinking [i ][1] for i in range (\nlen( FM_modules ))] + [ all_answer [i ][1] for i in range (len(\nFM_modules ))]\n33 thinking , answer = final_decision_module ( final_inputs ,\nfinal_decision_instruction )\n34\n35 return answer\nCode 5|Example discovered agent: Divide and Conquer Agent\n1def forward (self , taskInfo ):\n2 # Step 1: Decompose the problem into sub - problems\n3 decomposition_instruction = \" Please decompose the problem into\nsmaller , manageable sub - problems . List each sub - problem\nclearly .\"\n4 decomposition_module = FM_Module ([ \u2019thinking \u2019, \u2019 sub_problems \u2019], \u2019\nDecomposition Module \u2019)\n5\n6 # Step 2: Assign each sub - problem to a specialized expert\n7 sub_problem_instruction = \" Please think step by step and then\nsolve the sub - problem .\"\n8 specialized_experts = [ FM_Module ([ \u2019thinking \u2019, \u2019 sub_solution \u2019], \u2019\nSpecialized Expert \u2019, role = role ) for role in [\u2019Physics Expert","chunk_id":"97457e990eb6e3c88c11c862f9e3265b","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"STEP-BACK ABSTRACTION","type":"TECHNIQUE","description":"Step-back Abstraction is a method used in reasoning and problem-solving experiments, as referenced in the text.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"ROLE ASSIGNMENT","type":"TECHNIQUE","description":"Role Assignment is a technique that involves assigning specific roles to debate modules in problem-solving contexts, as mentioned in the text.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"SELF-REFINE","type":"TECHNIQUE","description":"Self-Refine is a method that allows for iterative refinement of answers based on feedback, as described in the text.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"COT","type":"TECHNIQUE","description":"COT (Chain of Thought) is a prompting technique that encourages the model to think step by step before providing an answer.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"COT-SC","type":"TECHNIQUE","description":"COT-SC is a variation of COT that involves sampling multiple answers and using ensemble methods for final decision-making.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"LLM-DEBATE","type":"TECHNIQUE","description":"LLM-Debate is a method where debate modules are assigned roles and engage in discussions to arrive at a solution.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"QUALITY-DIVERSITY","type":"TECHNIQUE","description":"Quality-Diversity is a technique that aims to collect diverse answers through multiple iterations based on previous responses.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"MULTI-STEP PEER REVIEW AGENT","type":"AGENT","description":"The Multi-Step Peer Review Agent is an example of an agent designed to review and refine answers through a structured process.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"DIVIDE AND CONQUER AGENT","type":"AGENT","description":"The Divide and Conquer Agent is an example of an agent that decomposes problems into smaller sub-problems for specialized solutions.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"VERIFIED MULTIMODAL AGENT","type":"AGENT","description":"The Verified Multimodal Agent is another example of an agent discovered in the Math domain, focusing on multimodal problem-solving.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"FM MODULE","type":"TECHNOLOGY","description":"FM Module refers to a functional module used in the agents for processing tasks and providing solutions.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"CRITIC MODULE","type":"TECHNOLOGY","description":"Critic Module is a component used in the agents to provide feedback and evaluate the correctness of answers.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"TASK INFO","type":"DATA FORMAT","description":"Task Info represents the information or context provided to the agents for processing and problem-solving.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"FEEDBACK","type":"DATA FORMAT","description":"Feedback is the information provided by the Critic Module regarding the correctness of answers and suggestions for improvement.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"FINAL DECISION MODULE","type":"TECHNOLOGY","description":"Final Decision Module is a component that synthesizes the outputs from various agents to arrive at a final answer.","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"FM QUERY","type":"","description":"","source_id":"97457e990eb6e3c88c11c862f9e3265b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"STEP-BACK ABSTRACTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Step-back Abstraction is a method used in reasoning and problem-solving experiments, as referenced in the text.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"ROLE ASSIGNMENT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Role Assignment is a technique that involves assigning specific roles to debate modules in problem-solving contexts, as mentioned in the text.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-Refine is a method that allows for iterative refinement of answers based on feedback, as described in the text.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"COT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">COT (Chain of Thought) is a prompting technique that encourages the model to think step by step before providing an answer.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"COT-SC\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">COT-SC is a variation of COT that involves sampling multiple answers and using ensemble methods for final decision-making.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"LLM-DEBATE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LLM-Debate is a method where debate modules are assigned roles and engage in discussions to arrive at a solution.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Quality-Diversity is a technique that aims to collect diverse answers through multiple iterations based on previous responses.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"MULTI-STEP PEER REVIEW AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Multi-Step Peer Review Agent is an example of an agent designed to review and refine answers through a structured process.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"DIVIDE AND CONQUER AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Divide and Conquer Agent is an example of an agent that decomposes problems into smaller sub-problems for specialized solutions.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"VERIFIED MULTIMODAL AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Verified Multimodal Agent is another example of an agent discovered in the Math domain, focusing on multimodal problem-solving.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"FM MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">FM Module refers to a functional module used in the agents for processing tasks and providing solutions.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"CRITIC MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Critic Module is a component used in the agents to provide feedback and evaluate the correctness of answers.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"TASK INFO\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Task Info represents the information or context provided to the agents for processing and problem-solving.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"FEEDBACK\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Feedback is the information provided by the Critic Module regarding the correctness of answers and suggestions for improvement.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"FINAL DECISION MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Final Decision Module is a component that synthesizes the outputs from various agents to arrive at a final answer.<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"FM QUERY\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <edge source=\"STEP-BACK ABSTRACTION\" target=\"SELF-REFINE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Step-back Abstraction is implemented as part of the Self-Refine process in reasoning and problem-solving experiments.<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"ROLE ASSIGNMENT\" target=\"LLM-DEBATE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Role Assignment is a key component of the LLM-Debate method, where specific roles are assigned to debate modules.<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"ROLE ASSIGNMENT\" target=\"FM QUERY\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">FM Query is utilized in the Role Assignment technique to select roles for agents based on predefined criteria.<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"MULTI-STEP PEER REVIEW AGENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Multi-Step Peer Review Agent utilizes the Self-Refine method to enhance the quality of answers through iterative feedback.<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"FEEDBACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Feedback is a crucial element in the Self-Refine method, guiding the iterative improvement of answers.<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"COT\" target=\"COT-SC\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">COT-SC builds upon the COT technique by incorporating sampling and ensemble methods for decision-making.<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"COT-SC\" target=\"FINAL DECISION MODULE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The Final Decision Module is used in the COT-SC technique to consolidate multiple sampled answers into a final response.<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"MULTI-STEP PEER REVIEW AGENT\" target=\"DIVIDE AND CONQUER AGENT\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Both agents are examples of discovered agents that utilize structured methodologies for problem-solving.<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"MULTI-STEP PEER REVIEW AGENT\" target=\"CRITIC MODULE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Critic Module is employed in the Multi-Step Peer Review Agent to assess and provide feedback on answers.<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"MULTI-STEP PEER REVIEW AGENT\" target=\"TASK INFO\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Task Info is the input provided to the Multi-Step Peer Review Agent for processing and generating answers.<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"DIVIDE AND CONQUER AGENT\" target=\"VERIFIED MULTIMODAL AGENT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The Verified Multimodal Agent and Divide and Conquer Agent are both discovered agents focusing on different domains of problem-solving.<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"ef75d2c866bee783577ed9f65707cf13","chunk":"4 decomposition_module = FM_Module ([ \u2019thinking \u2019, \u2019 sub_problems \u2019], \u2019\nDecomposition Module \u2019)\n5\n6 # Step 2: Assign each sub - problem to a specialized expert\n7 sub_problem_instruction = \" Please think step by step and then\nsolve the sub - problem .\"\n8 specialized_experts = [ FM_Module ([ \u2019thinking \u2019, \u2019 sub_solution \u2019], \u2019\nSpecialized Expert \u2019, role = role ) for role in [\u2019Physics Expert \u2019\n, \u2019Chemistry Expert \u2019, \u2019Biology Expert \u2019, \u2019General Expert \u2019]]\n9\n10 # Step 3: Integrate the sub - problem solutions into the final\nanswer\n11 integration_instruction = \" Given the solutions to the sub -\nproblems , integrate them to provide a final answer to the\noriginal problem .\"\n12 integration_module = FM_Module ([ \u2019thinking \u2019, \u2019answer \u2019], \u2019\nIntegration Module \u2019, temperature =0.1)\n13\n14 # Decompose the problem\n15 thinking , sub_problems = decomposition_module ([ taskInfo ],\ndecomposition_instruction )\n16\n17 # Ensure sub_problems is a string and split into individual sub -\nproblems\n18 sub_problems_list = sub_problems . content . split (\u2019\\n\u2019) if\nisinstance ( sub_problems . content , str) else []\n19\n20 # Solve each sub - problem\n21 sub_solutions = []\n22 for i, sub_problem in enumerate ( sub_problems_list ):\n23 sub_problem_info = Info (\u2019 sub_problem \u2019, decomposition_module .\n__repr__ () , sub_problem , i)\n24 sub_thinking , sub_solution = specialized_experts [i % len (\n37Automated Design of Agentic Systems\nspecialized_experts )]([ sub_problem_info ],\nsub_problem_instruction )\n25 sub_solutions . append ( sub_solution )\n26\n27 # Integrate the sub - problem solutions\n28 integration_inputs = [ taskInfo ] + sub_solutions\n29 thinking , answer = integration_module ( integration_inputs ,\nintegration_instruction )\n30\n31 return answer\nCode 6|Example discovered agent: Verified Multimodal Agent\n1def forward (self , taskInfo ):\n2 # Instruction for generating visual representation of the\nproblem\n3 visual_instruction = \" Please create a visual representation (e.g\n., diagram , graph ) of the given problem .\"\n4\n5 # Instruction for verifying the visual representation\n6 verification_instruction = \" Please verify the accuracy and\nrelevance of the visual representation . Provide feedback and\nsuggestions for improvement if necessary .\"\n7\n8 # Instruction for solving the problem using the verified visual\naid\n9 cot_instruction = \" Using the provided visual representation ,\nthink step by step and solve the problem .\"\n10\n11 # Instantiate the visual representation module , verification\nmodule , and Chain -of - Thought module\n12 visual_module = FM_Module ([ \u2019visual \u2019], \u2019Visual Representation\nModule \u2019)\n13 verification_module = FM_Module ([ \u2019feedback \u2019, \u2019 verified_visual \u2019],\n\u2019 Verification Module \u2019)\n14 cot_module = FM_Module ([ \u2019thinking \u2019, \u2019answer \u2019], \u2019Chain -of - Thought\nModule \u2019)\n15\n16 # Generate the visual representation of the problem\n17 visual_output = visual_module ([ taskInfo ], visual_instruction )\n18 visual_representation = visual_output [0] # Using Info object\ndirectly\n19\n20 # Verify the visual representation\n21 feedback , verified_visual = verification_module ([ taskInfo ,\nvisual_representation ], verification_instruction )\n22\n23 # Use the verified visual representation to solve the problem\n24 thinking , answer = cot_module ([ taskInfo , verified_visual ],\ncot_instruction )\n25 return answer\n38Automated Design of Agentic Systems\nG. Cost of Experiments\nA single run of search and evaluation on ARC (Section 4.1) costs approximately $500 USD in OpenAI\nAPI costs, while a run within the reasoning and problem-solving domains (Section 4.2) costs about\n$300 USD.\nThe primary expense comes from querying the \u201cgpt-3.5-turbo-0125\u201d model during the evaluation\nof discovered agents. Notably, the latest GPT-4 model, \u201cgpt-4o-mini,\u201d is less than one-third the price\nof \u201cgpt-3.5-turbo-0125\u201d and offers better performance, suggesting that we could achieve improved\nresults with Meta Agent Search at just one-third of the cost. Additionally, as discussed in Section 6, the\ncurrent naive evaluation function is both expensive and overlooks valuable information. We anticipate\nthat future work adopting more sophisticated evaluation functions could significantly reduce the cost\nof ADAS algorithms.\n39","chunk_id":"ef75d2c866bee783577ed9f65707cf13","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":964,"entities":[{"name":"FM_MODULE","type":"TECHNIQUE","description":"FM_Module is a modular framework used for processing tasks, including decomposition, integration, and specialized problem-solving.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"DECOMPOSITION MODULE","type":"MODULE","description":"The Decomposition Module is responsible for breaking down complex problems into manageable sub-problems for further analysis.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"SPECIALIZED EXPERTS","type":"ROLE","description":"Specialized Experts are designated roles within the framework, each focusing on specific domains such as Physics, Chemistry, and Biology to solve sub-problems.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"INTEGRATION MODULE","type":"MODULE","description":"The Integration Module combines the solutions of sub-problems into a final answer, ensuring coherence and completeness.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"SUB-PROBLEMS","type":"DATA FORMAT","description":"Sub-problems are the individual components derived from a larger problem, which are addressed by specialized experts.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"SUB-SOLUTIONS","type":"DATA FORMAT","description":"Sub-solutions are the answers generated by specialized experts for each sub-problem.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"VISUAL REPRESENTATION MODULE","type":"MODULE","description":"The Visual Representation Module creates visual aids, such as diagrams or graphs, to assist in problem-solving.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"VERIFICATION MODULE","type":"MODULE","description":"The Verification Module checks the accuracy and relevance of visual representations, providing feedback for improvement.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"CHAIN-OF-THOUGHT MODULE","type":"MODULE","description":"The Chain-of-Thought Module facilitates step-by-step reasoning to solve problems using verified visual aids.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"ARC","type":"EXPERIMENT","description":"ARC refers to a specific evaluation framework used for testing and assessing the performance of discovered agents.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"GPT-3.5-TURBO-0125","type":"TECHNOLOGY","description":"GPT-3.5-turbo-0125 is a language model developed by OpenAI, used for various tasks including reasoning and problem-solving.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a more advanced language model developed by OpenAI, offering improved performance and cost-effectiveness compared to its predecessors.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"DECOMPOSITION INSTRUCTION","type":"INSTRUCTION","description":"Decomposition Instruction is a directive given to the Decomposition Module to guide the breakdown of a problem into sub-problems.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"INTEGRATION INSTRUCTION","type":"INSTRUCTION","description":"Integration Instruction is a directive given to the Integration Module to guide the combination of sub-solutions into a final answer.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"SUB-PROBLEM","type":"DATA FORMAT","description":"A Sub-Problem is an individual component of a larger problem that requires specialized attention for resolution.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"SUB-PROBLEM INSTRUCTION","type":"INSTRUCTION","description":"Sub-Problem Instruction is a directive given to specialized experts to guide them in solving their assigned sub-problems.","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"TASKINFO","type":"","description":"","source_id":"ef75d2c866bee783577ed9f65707cf13"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"FM_MODULE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">FM_Module is a modular framework used for processing tasks, including decomposition, integration, and specialized problem-solving.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"DECOMPOSITION MODULE\">      <data key=\"d0\">MODULE<\/data>      <data key=\"d1\">The Decomposition Module is responsible for breaking down complex problems into manageable sub-problems for further analysis.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"SPECIALIZED EXPERTS\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Specialized Experts are designated roles within the framework, each focusing on specific domains such as Physics, Chemistry, and Biology to solve sub-problems.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"INTEGRATION MODULE\">      <data key=\"d0\">MODULE<\/data>      <data key=\"d1\">The Integration Module combines the solutions of sub-problems into a final answer, ensuring coherence and completeness.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"SUB-PROBLEMS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Sub-problems are the individual components derived from a larger problem, which are addressed by specialized experts.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"SUB-SOLUTIONS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Sub-solutions are the answers generated by specialized experts for each sub-problem.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"VISUAL REPRESENTATION MODULE\">      <data key=\"d0\">MODULE<\/data>      <data key=\"d1\">The Visual Representation Module creates visual aids, such as diagrams or graphs, to assist in problem-solving.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"VERIFICATION MODULE\">      <data key=\"d0\">MODULE<\/data>      <data key=\"d1\">The Verification Module checks the accuracy and relevance of visual representations, providing feedback for improvement.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT MODULE\">      <data key=\"d0\">MODULE<\/data>      <data key=\"d1\">The Chain-of-Thought Module facilitates step-by-step reasoning to solve problems using verified visual aids.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">EXPERIMENT<\/data>      <data key=\"d1\">ARC refers to a specific evaluation framework used for testing and assessing the performance of discovered agents.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO-0125\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5-turbo-0125 is a language model developed by OpenAI, used for various tasks including reasoning and problem-solving.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a more advanced language model developed by OpenAI, offering improved performance and cost-effectiveness compared to its predecessors.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"DECOMPOSITION INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">Decomposition Instruction is a directive given to the Decomposition Module to guide the breakdown of a problem into sub-problems.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"INTEGRATION INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">Integration Instruction is a directive given to the Integration Module to guide the combination of sub-solutions into a final answer.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"SUB-PROBLEM\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">A Sub-Problem is an individual component of a larger problem that requires specialized attention for resolution.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"SUB-PROBLEM INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">Sub-Problem Instruction is a directive given to specialized experts to guide them in solving their assigned sub-problems.<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"TASKINFO\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <edge source=\"FM_MODULE\" target=\"DECOMPOSITION MODULE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The FM_Module includes the Decomposition Module as a key component for breaking down problems into sub-problems.<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"TASKINFO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The FM_Module processes TaskInfo to derive solutions for the given problems.<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"DECOMPOSITION MODULE\" target=\"SUB-PROBLEMS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Decomposition Module generates sub-problems from a larger task for specialized experts to address.<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"DECOMPOSITION MODULE\" target=\"DECOMPOSITION INSTRUCTION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Decomposition Module uses Decomposition Instruction to effectively break down problems into sub-problems.<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"SPECIALIZED EXPERTS\" target=\"SUB-SOLUTIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Specialized Experts provide sub-solutions for each sub-problem they are assigned to solve.<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"SPECIALIZED EXPERTS\" target=\"SUB-PROBLEM\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Specialized Experts are assigned specific sub-problems to solve based on their expertise.<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"SPECIALIZED EXPERTS\" target=\"SUB-PROBLEM INSTRUCTION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Specialized Experts receive Sub-Problem Instruction to guide their problem-solving process.<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"INTEGRATION MODULE\" target=\"SUB-SOLUTIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Integration Module combines the sub-solutions into a final answer to the original problem.<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"INTEGRATION MODULE\" target=\"INTEGRATION INSTRUCTION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Integration Module follows Integration Instruction to combine sub-solutions into a coherent final answer.<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"VISUAL REPRESENTATION MODULE\" target=\"CHAIN-OF-THOUGHT MODULE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Visual Representation Module aids the Chain-of-Thought Module by providing visual aids for problem-solving.<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"VISUAL REPRESENTATION MODULE\" target=\"VERIFICATION MODULE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Verification Module ensures the accuracy of the visual aids created by the Visual Representation Module.<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"ARC\" target=\"GPT-3.5-TURBO-0125\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">ARC utilizes the GPT-3.5-turbo-0125 model for evaluating discovered agents in reasoning and problem-solving tasks.<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO-0125\" target=\"GPT-4\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">GPT-4 is a more cost-effective and higher-performing alternative to GPT-3.5-turbo-0125 for similar tasks.<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"6fe27f9eb76cf2ddf712a2cee5783d1c","chunk":"AgentInstruct:\nToward Generative Teaching with Agentic\nFlows\nArindam Mitra, Luciano Del Corro, Guoqing Zheng, Shweti Mahajan,\nDany Rouhana, Andres Codas, Yadong Lu, Wei-ge Chen, Olga Vrousgos,\nCorby Rosset, Fillipe Silva, Hamed Khanpour, Yash Lara, Ahmed Awadallah\nMicrosoft Research\nAbstract\nSynthetic data is becoming increasingly important for accelerating the development of\nlanguage models, both large and small. Despite several successful use cases, researchers\nalso raised concerns around model collapse and drawbacks of imitating other models. This\ndiscrepancy can be attributed to the fact that synthetic data varies in quality and diversity.\nEffective use of synthetic data usually requires significant human effort in curating the data.\nWe focus on using synthetic data for post-training, specifically creating data by powerful\nmodels to teach a new skill or behavior to another model, we refer to this setting as Generative\nTeaching . We introduce AgentInstruct, an extensible agentic framework for automatically\ncreating large amounts of diverse and high-quality synthetic data. AgentInstruct can create\nboth the prompts and responses, using only raw data sources like text documents and code\nfiles as seeds. We demonstrate the utility of AgentInstruct by creating a post training dataset\nof 25M pairs to teach language models different skills, such as text editing, creative writing,\ntool usage, coding, reading comprehension, etc. The dataset can be used for instruction\ntuning of any base model. We post-train Mistral-7b with the data. When comparing\nthe resulting model (Orca-3) to Mistral-7b-Instruct (which uses the same base model), we\nobserve significant improvements across many benchmarks. For example, 40% improvement\non AGIEval, 19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement\non BBH and 45% improvement on AlpacaEval. Additionally, it consistently outperforms\nother models such as LLAMA-8B-instruct and GPT-3.5-turbo.\n \n 0102030405060AGIEVAL\n010203040506070MMLU\n010203040506070BBH\n0102030405060708090GSM8K\n0510152025ALPACA\nEVAL\n0102030405060708090FOFO\n010203040506070MIRAGE -\nRAG\n+40.2%  +19.4 % \n+38.3 % +53.7%  \n+45.0% +38.3%  \n+46.6% \nMistral -Instruct -7B Mistra l-7B + AgentInstruct (Orca -3) \nFigure 1: Effect of using AgentInstruct data for post-training Mistral-7BarXiv:2407.03502v1  [cs.AI]  3 Jul 20241 Introduction\nSynthetic data accelerated the development of LLMS : The rise of synthetic data in\nthe training of Large Language Models (LLMs) has been a significant development of the\nlast year. Synthetic data was used to significantly accelerate the progress of model training\n(especially SLMs) in all stages of training from pre-training (e.g., [ 1]), to instruction-tuning\n(e.g., [21, 36]) and RLHF(e.g., [12, 28]).\nGenerating high quality synthetic data is hard : On the other hand, research has also\nshown that pre-training models on synthetic data generated by other models can lead to\nmodel collapse [ 29], leading to models gradually degenerating as a result. Similar arguments\nhave been made against using synthetic data for pos-training, which could amount to an\nimitation process that could teach the trained model to pick only stylistic characteristics\nand not real capabilities [ 8]. This discrepancy could be explained by the observation that\ncreating high-quality and diverse synthetic data is hard [ 17]. Successful use of synthetic data\ninvolved significant human effort in curating and filtering the data to ensure high quality. If\nwe focus on post-training synthetic data, we will see the most widely used approach includes\nstarting with a set of prompts and using a powerful model such as GPT-4 [ 22] to generate\nresponses to these prompts [ 24] or of an expanded set of the prompts [ 36]. This recipe\nwas further improved by eliciting explanations or step-by-step instructions from the teacher\nmodel [20] or using more complex prompting techniques to elicit higher quality answers [ 18].\nSynthetic data meets Agents : Another major development we witnessed last year is the\nrise of Agentic (especially multiagent) workflows [ 33,13]. Agentic workflows can generate\nhigh quality data, that surpasses the capabilities of the underlying LLMs, by using flows\nwith reflection and iteration, where agents can look back at solutions, generate critiques and\nimprove solutions. They can also use tools (e.g. search apis, calculator, code interpreters)\naddressing limitations of LLMs. Multi-agent workflows bring in additional benefits such\nas simulating scenarios where we can generate both new prompts and the corresponding\nresponses. They also enable automation of the data generation workflows reducing or\neliminating need for human intervention on some tasks.\nGenerative Teaching & Orca AgentInstruct : Generating synthetic data for post-\ntraining often relies on an existing prompt set that is used as is or used as seeds for\ngenerating more instructions. In this work, we generalize the problem settings to a broader\nobjective of generating abundant amounts of diverse, challenging and high-quality data to\nte","chunk_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"AGENTINSTRUCT","type":"TECHNIQUE","description":"AgentInstruct is an extensible agentic framework designed for automatically creating large amounts of diverse and high-quality synthetic data for language models.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"SYNTHETIC DATA","type":"DATA TYPE","description":"Synthetic data refers to artificially generated data used to train language models, which can vary in quality and diversity.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"LANGUAGE MODELS","type":"TECHNOLOGY","description":"Language models are algorithms designed to understand and generate human language, often trained on large datasets.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"MISTRAL-7B","type":"MODEL","description":"Mistral-7B is a base language model that was post-trained using synthetic data generated by AgentInstruct.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"ORCA-3","type":"MODEL","description":"Orca-3 is the resulting model from post-training Mistral-7B with synthetic data, showing significant improvements in performance.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"LLAMA-8B-INSTRUCT","type":"MODEL","description":"LLAMA-8B-Instruct is another language model that is compared against Orca-3 in terms of performance.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is a powerful language model used to generate responses for prompts in the synthetic data generation process.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"AGIEVAL","type":"BENCHMARK","description":"AGIEval is a benchmark used to evaluate the performance of language models.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"MMLU","type":"BENCHMARK","description":"MMLU is a benchmark used to assess the capabilities of language models in various tasks.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"GSM8K","type":"BENCHMARK","description":"GSM8K is a benchmark that evaluates language models on mathematical problem-solving.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"BBH","type":"BENCHMARK","description":"BBH is a benchmark used to measure the performance of language models in specific tasks.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"ALPACA EVAL","type":"BENCHMARK","description":"AlpacaEval is a benchmark used to evaluate the performance of language models in instruction-following tasks.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"POST-TRAINING","type":"PROCESS","description":"Post-training is a phase in the model training process where additional training is conducted using specific datasets to enhance model performance.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"INSTRUCTION TUNING","type":"PROCESS","description":"Instruction tuning is a process that involves refining a language model's ability to follow instructions by training it on specific datasets.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"RLHF","type":"PROCESS","description":"Reinforcement Learning from Human Feedback (RLHF) is a training method that incorporates human feedback to improve model performance.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"MULTI-AGENT WORKFLOWS","type":"CONCEPT","description":"Multi-agent workflows involve multiple agents working together to generate high-quality data and improve the capabilities of language models.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"REFLECTION AND ITERATION","type":"CONCEPT","description":"Reflection and iteration are processes used in agentic workflows where agents review and improve their outputs based on previous results.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"DATA GENERATION WORKFLOWS","type":"PROCESS","description":"Data generation workflows are structured processes for creating datasets, often involving automation to reduce human intervention.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"MODEL COLLAPSE","type":"CONCEPT","description":"Model collapse refers to the phenomenon where a model's performance degrades over time, often due to training on low-quality synthetic data.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"PROMPTS","type":"DATA FORMAT","description":"Prompts are specific instructions or queries used to elicit responses from language models during the training process.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"SEEDS","type":"DATA FORMAT","description":"Seeds are initial data points or prompts used to generate further instructions or responses in the synthetic data generation process.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"TEXT DOCUMENTS","type":"DATA FORMAT","description":"Text documents are raw data sources used as seeds for generating synthetic data in the AgentInstruct framework.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"CODE FILES","type":"DATA FORMAT","description":"Code files are raw data sources used as seeds for generating synthetic data in the AgentInstruct framework.)<|COMPLETE|>","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">AgentInstruct is an extensible agentic framework designed for automatically creating large amounts of diverse and high-quality synthetic data for language models.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"SYNTHETIC DATA\">      <data key=\"d0\">DATA TYPE<\/data>      <data key=\"d1\">Synthetic data refers to artificially generated data used to train language models, which can vary in quality and diversity.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"LANGUAGE MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language models are algorithms designed to understand and generate human language, often trained on large datasets.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"MISTRAL-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-7B is a base language model that was post-trained using synthetic data generated by AgentInstruct.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 is the resulting model from post-training Mistral-7B with synthetic data, showing significant improvements in performance.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"LLAMA-8B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">LLAMA-8B-Instruct is another language model that is compared against Orca-3 in terms of performance.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is a powerful language model used to generate responses for prompts in the synthetic data generation process.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"AGIEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AGIEval is a benchmark used to evaluate the performance of language models.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MMLU is a benchmark used to assess the capabilities of language models in various tasks.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">GSM8K is a benchmark that evaluates language models on mathematical problem-solving.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"BBH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">BBH is a benchmark used to measure the performance of language models in specific tasks.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"ALPACA EVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AlpacaEval is a benchmark used to evaluate the performance of language models in instruction-following tasks.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"POST-TRAINING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Post-training is a phase in the model training process where additional training is conducted using specific datasets to enhance model performance.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"INSTRUCTION TUNING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Instruction tuning is a process that involves refining a language model's ability to follow instructions by training it on specific datasets.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"RLHF\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Reinforcement Learning from Human Feedback (RLHF) is a training method that incorporates human feedback to improve model performance.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"MULTI-AGENT WORKFLOWS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Multi-agent workflows involve multiple agents working together to generate high-quality data and improve the capabilities of language models.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"REFLECTION AND ITERATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Reflection and iteration are processes used in agentic workflows where agents review and improve their outputs based on previous results.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"DATA GENERATION WORKFLOWS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Data generation workflows are structured processes for creating datasets, often involving automation to reduce human intervention.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"MODEL COLLAPSE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Model collapse refers to the phenomenon where a model's performance degrades over time, often due to training on low-quality synthetic data.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"PROMPTS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Prompts are specific instructions or queries used to elicit responses from language models during the training process.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"SEEDS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Seeds are initial data points or prompts used to generate further instructions or responses in the synthetic data generation process.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"TEXT DOCUMENTS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Text documents are raw data sources used as seeds for generating synthetic data in the AgentInstruct framework.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"CODE FILES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Code files are raw data sources used as seeds for generating synthetic data in the AgentInstruct framework.)&lt;|COMPLETE|&gt;<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <edge source=\"AGENTINSTRUCT\" target=\"SYNTHETIC DATA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AgentInstruct is designed to create synthetic data for training language models, focusing on quality and diversity.<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"MISTRAL-7B\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Mistral-7B is a base model that was post-trained using synthetic data generated by AgentInstruct.<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"SYNTHETIC DATA\" target=\"GPT-4\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">GPT-4 is utilized to generate responses for prompts in the synthetic data generation process, enhancing the quality of the data.<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"MISTRAL-7B\" target=\"ORCA-3\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Orca-3 is the improved model resulting from the post-training of Mistral-7B with synthetic data.<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"LLAMA-8B-INSTRUCT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3 outperforms LLAMA-8B-Instruct in various benchmarks, demonstrating the effectiveness of the synthetic data used.<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AGIEVAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AGIEval is one of the benchmarks where Orca-3 shows significant improvement compared to previous models.<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MMLU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">MMLU is another benchmark where Orca-3 demonstrates improved performance over Mistral-7B-Instruct.<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GSM8K\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GSM8K is a benchmark where Orca-3 shows a notable improvement in mathematical problem-solving capabilities.<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"BBH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">BBH is a benchmark where Orca-3 outperforms previous models, indicating its enhanced capabilities.<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ALPACA EVAL\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">AlpacaEval is a benchmark where Orca-3 achieves significant improvements, showcasing its effectiveness in instruction-following tasks.<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"b88745a13b69cecbc0ee9c3af41389bf","chunk":" both new prompts and the corresponding\nresponses. They also enable automation of the data generation workflows reducing or\neliminating need for human intervention on some tasks.\nGenerative Teaching & Orca AgentInstruct : Generating synthetic data for post-\ntraining often relies on an existing prompt set that is used as is or used as seeds for\ngenerating more instructions. In this work, we generalize the problem settings to a broader\nobjective of generating abundant amounts of diverse, challenging and high-quality data to\nteach a particular skill to an AI model, we refer to this setting as Generative Teaching.\nAgentInstruct is an agentic solution for Generative Teaching. AgentInstruct focuses on\ncreating demonstration and feedback data and requires only raw documents as input. When\ngeneric data is used as seeds, AgentInstruct can be used to teach an LLM a general capability\n(e.g. Math, Reasoning, RAG, etc.). Domain specific data (e.g. gaming, finance) can also be\nused as seeds to improve the model in a certain specialization. AgentInstruct can create:\n1.High-quality data: using powerful models like GPT-4, coupled with tools like search\nand code interpreters.\n2.Diverse data: AgentInstruct generates both prompts and responses. It uses a large\nnumber of agents (equipped with powerful LLMs, tools and reflection flows) and a\ntaxonomy (of over 100 subcategories) to create diverse and high quality prompts\nand responses,\n3.Large quantities of data: AgentInstruct can run autonomously and can apply flows\nfor verification and data filtering. It does not require seed prompts and uses raw\ndocuments for seeding.\nUsing raw data (unstructured text documents or source code) as seeds has two benefits.\nFirst, this data is available in abundance enabling the use of AgentInstruct to create large\namounts of diverse data. Additionally, using raw data as seeds, and hence, avoiding using\nexisting prompts, as is or after paraphrasing, can promote learning more general capabilities\nas opposed to benchmark-specific ones.\nWe demonstrate the utility of AgentInstruct by creating a comprehensive synthetic post-\ntraining dataset of 25 million prompt and response pairs. The dataset covers a wide array\n2of skills including creative writing, reasoning, math, RAG, tool use, etc. To assess the\nvalue of the data, we use it to finetune Mistral-7B[11] model. The finetuned Mistral model\n(Orca-3) shows significant improvement over other instruction-tuned models using the same\nbase model. For example, compared to Mistral-Instruct-7B, it shows 40% improvement on\nAGIEval, 19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement\non BBH, 45% improvement on AlpacaEval and 31.34% reduction on hallucination across\nmultiple summarization benchmarks. Additionally, it outperforms other models such as\nLLAMA-8B-instruct and GPT-3.5 on multiple benchmarks. Note that the only seed data\nused is publicly available raw materials and no task-specific or benchmark data has been\nused as seeds.\nWhilewedemonstratetheutilityofAgentInstructbycreatingagenericpost-trainingsynthetic\ndataset, we believe that agents can enable the creation of Synthetic-Data-Generation-As-A-\nService where we start with raw materials (e.g. web data for general model training or domain\nspecific data for specialized models), and we generate data for post-training and finetuning,\nhence enabling continual learning and improvement of any base LLM. Additionally, we\nbelieve that the AgentInstruct approach can be used for self-improvement of larger, more\ncapable models because of: (1) the ability to generate new prompts and (2) the ability to\ngenerate responses that exceed the quality of the LLM used in the agentic flow (because of\nthe use of tools, reflection, etc.).\n2 Generative Teaching: AgentInstruct\nCreating synthetic datasets for supervised fine-tuning and instruction-tuning has seen\nsignificant progress over the last year. The quality of these datasets has been steadily\nimproving. High quality can be achieved by using powerful frontier models (or agenetic flows\nbased on these models) to generate responses. However, when creating synthetic data, in\naddition to quality, we also need to consider several other fundamental questions:\n1. How can we create a vast amount of data?\n2. How can we ensure that the generated data is diverse?\n3. How can we generate complex or nuanced data points?\nIn the AgentInstruct methodology, we outline a structured approach to tackle these challenges\nas follows:\nFigure 2: Concise Summary of the AgentInstruct Methodology\n1. Assemble a collection of raw seeds (e.g., textbook chapters, web articles, code\nsnippets).\n2.foreach seed in the collection do\n3. Transform the seed with the aid of one or more content transformation Agents (\nContent Transformation Flow).\n4. Route it through a series of instruction creation Agents to create a diverse set of\ninstructions (Seed Instruction Creation Flow).\n5. Utilize another group of Refinement Agents to iteratively refine the complexity\nand quality of the seed instructions (Refinement Flow).\n6.end for\nWe use agentic flows to automate the generation process and leverage raw articles as seeds to\nfoster diversity and ensure that problems generated in different iterations (line 2 in Figure 1)\nare distinct and of broad coverage. This enables us to create data at scale (benefiting from\nautomation of agentic flows), with high diversity (based on the broad and diverse seeds) and\nof varying complexity (benefiting from the iterative and","chunk_id":"b88745a13b69cecbc0ee9c3af41389bf","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"AGENTINSTRUCT","type":"TECHNIQUE","description":"AgentInstruct is an agentic solution for Generative Teaching, focusing on creating demonstration and feedback data using raw documents as input.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"GENERATIVE TEACHING","type":"CONCEPT","description":"Generative Teaching is a methodology aimed at generating diverse, challenging, and high-quality synthetic data to teach specific skills to AI models.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a powerful language model used in the AgentInstruct methodology to generate high-quality data.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"MISTRAL-7B","type":"MODEL","description":"Mistral-7B is a base model that is fine-tuned using synthetic datasets created by AgentInstruct.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"ORCA-3","type":"MODEL","description":"Orca-3 is the finetuned version of the Mistral-7B model, showing significant improvements in various benchmarks.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"SYNTHETIC DATASET","type":"DATASET","description":"The synthetic dataset created by AgentInstruct consists of 25 million prompt and response pairs covering a wide array of skills.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"AGIEVAL","type":"BENCHMARK","description":"AGIEval is a benchmark used to assess the performance of instruction-tuned models.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"MMLU","type":"BENCHMARK","description":"MMLU is a benchmark used to evaluate the performance of language models in various tasks.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"GSM8K","type":"BENCHMARK","description":"GSM8K is a benchmark for evaluating mathematical reasoning capabilities of language models.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"BBH","type":"BENCHMARK","description":"BBH is a benchmark used to assess the performance of language models in various tasks.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"ALPACA EVAL","type":"BENCHMARK","description":"AlpacaEval is a benchmark used to evaluate the performance of instruction-tuned models.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"LLAMA-8B-INSTRUCT","type":"MODEL","description":"LLAMA-8B-instruct is another model used for comparison in the evaluation of instruction-tuned models.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"GPT-3.5","type":"MODEL","description":"GPT-3.5 is a language model used for comparison in the evaluation of instruction-tuned models.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"AGENTIC FLOWS","type":"TECHNIQUE","description":"Agentic flows are automated processes that facilitate the generation of synthetic data by leveraging raw materials and various agents.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"CONTENT TRANSFORMATION AGENTS","type":"TECHNIQUE","description":"Content Transformation Agents are tools used in the AgentInstruct methodology to transform raw seeds into diverse instructions.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"INSTRUCTION CREATION AGENTS","type":"TECHNIQUE","description":"Instruction Creation Agents are components of the AgentInstruct methodology that generate a diverse set of instructions from transformed seeds.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"REFINEMENT AGENTS","type":"TECHNIQUE","description":"Refinement Agents are used in the AgentInstruct methodology to iteratively improve the complexity and quality of generated instructions.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"DIVERSE DATA","type":"DATA TYPE","description":"Diverse data refers to the variety of synthetic data generated by AgentInstruct, ensuring broad coverage and complexity.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"HIGH-QUALITY DATA","type":"DATA TYPE","description":"High-quality data is the result of using powerful models and agentic flows to generate effective synthetic datasets.","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"RAW DOCUMENTS","type":"","description":"","source_id":"b88745a13b69cecbc0ee9c3af41389bf"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">AgentInstruct is an agentic solution for Generative Teaching, focusing on creating demonstration and feedback data using raw documents as input.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"GENERATIVE TEACHING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Generative Teaching is a methodology aimed at generating diverse, challenging, and high-quality synthetic data to teach specific skills to AI models.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a powerful language model used in the AgentInstruct methodology to generate high-quality data.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"MISTRAL-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-7B is a base model that is fine-tuned using synthetic datasets created by AgentInstruct.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 is the finetuned version of the Mistral-7B model, showing significant improvements in various benchmarks.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"SYNTHETIC DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">The synthetic dataset created by AgentInstruct consists of 25 million prompt and response pairs covering a wide array of skills.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"AGIEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AGIEval is a benchmark used to assess the performance of instruction-tuned models.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MMLU is a benchmark used to evaluate the performance of language models in various tasks.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">GSM8K is a benchmark for evaluating mathematical reasoning capabilities of language models.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"BBH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">BBH is a benchmark used to assess the performance of language models in various tasks.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"ALPACA EVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AlpacaEval is a benchmark used to evaluate the performance of instruction-tuned models.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"LLAMA-8B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">LLAMA-8B-instruct is another model used for comparison in the evaluation of instruction-tuned models.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5 is a language model used for comparison in the evaluation of instruction-tuned models.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"AGENTIC FLOWS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Agentic flows are automated processes that facilitate the generation of synthetic data by leveraging raw materials and various agents.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"CONTENT TRANSFORMATION AGENTS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Content Transformation Agents are tools used in the AgentInstruct methodology to transform raw seeds into diverse instructions.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"INSTRUCTION CREATION AGENTS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Instruction Creation Agents are components of the AgentInstruct methodology that generate a diverse set of instructions from transformed seeds.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"REFINEMENT AGENTS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Refinement Agents are used in the AgentInstruct methodology to iteratively improve the complexity and quality of generated instructions.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"DIVERSE DATA\">      <data key=\"d0\">DATA TYPE<\/data>      <data key=\"d1\">Diverse data refers to the variety of synthetic data generated by AgentInstruct, ensuring broad coverage and complexity.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"HIGH-QUALITY DATA\">      <data key=\"d0\">DATA TYPE<\/data>      <data key=\"d1\">High-quality data is the result of using powerful models and agentic flows to generate effective synthetic datasets.<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"RAW DOCUMENTS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <edge source=\"AGENTINSTRUCT\" target=\"GENERATIVE TEACHING\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">AgentInstruct is a practical implementation of the Generative Teaching methodology, aimed at creating synthetic data for AI training.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"GPT-4\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AgentInstruct utilizes GPT-4 to generate high-quality synthetic data for training AI models.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"RAW DOCUMENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AgentInstruct uses raw documents as seeds to generate synthetic data, enabling the creation of diverse datasets.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"AGENTIC FLOWS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">AgentInstruct employs agentic flows to automate the data generation process, enhancing efficiency and diversity.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"CONTENT TRANSFORMATION AGENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Content Transformation Agents are utilized within the AgentInstruct methodology to modify raw seeds for instruction generation.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"INSTRUCTION CREATION AGENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Instruction Creation Agents are part of the AgentInstruct process, generating diverse instructions from transformed seeds.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"REFINEMENT AGENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Refinement Agents are involved in the AgentInstruct methodology to enhance the quality and complexity of generated instructions.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"DIVERSE DATA\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">AgentInstruct aims to produce diverse data through its structured approach, ensuring a wide range of generated content.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"HIGH-QUALITY DATA\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">AgentInstruct focuses on generating high-quality data by leveraging advanced models and methodologies.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"MISTRAL-7B\" target=\"ORCA-3\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Orca-3 is the finetuned version of the Mistral-7B model, demonstrating improvements in performance after training with synthetic data.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"MISTRAL-7B\" target=\"SYNTHETIC DATASET\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The synthetic dataset created by AgentInstruct is used to fine-tune the Mistral-7B model, enhancing its capabilities.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"MISTRAL-7B\" target=\"LLAMA-8B-INSTRUCT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Mistral-7B is compared with LLAMA-8B-instruct to evaluate the effectiveness of instruction-tuning.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"MISTRAL-7B\" target=\"GPT-3.5\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Mistral-7B is compared with GPT-3.5 to assess the performance of instruction-tuned models.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AGIEVAL\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Orca-3 shows significant improvement on the AGIEval benchmark compared to other instruction-tuned models.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MMLU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3 shows improvement on the MMLU benchmark, indicating enhanced performance in various tasks.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GSM8K\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Orca-3 demonstrates improved performance on the GSM8K benchmark, showcasing its mathematical reasoning capabilities.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"BBH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3 shows improvement on the BBH benchmark, indicating its effectiveness in various tasks.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ALPACA EVAL\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Orca-3 outperforms other models on the AlpacaEval benchmark, demonstrating its instruction-tuning effectiveness.<\/data>      <data key=\"d5\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"f7eb89a70f544664546a510e46d5febd","chunk":" the seed instructions (Refinement Flow).\n6.end for\nWe use agentic flows to automate the generation process and leverage raw articles as seeds to\nfoster diversity and ensure that problems generated in different iterations (line 2 in Figure 1)\nare distinct and of broad coverage. This enables us to create data at scale (benefiting from\nautomation of agentic flows), with high diversity (based on the broad and diverse seeds) and\nof varying complexity (benefiting from the iterative and refinement patterns supported by\nagentic flows). AgentInstruct defines three different flows:\nContent Transformation Flow converts the raw seed into an intermediate representation\nthat simplifies the creation of instructions tailored to specific objectives. It comprises of\n31\nContent Transformation Flow\u2026...\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026...\u2026...\u2026...\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026Raw text \ndocuments or \nsource code \nfiles are used \nas seedsTransformed \ncontent ( e.g.  \nargument passage, \nmeeting transcript, \nlist of APIs,\u2026.)\nSeed Instruction Generation Flow\n\u2026\u2026Transformed \ncontent provide \ndiversity is easier \nto use to generate \ninstructions Seed instructions \nare generated \nfollowing  a \ncomprehensive \ntaxonomy\u2026...\u2026...\u2026...\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026...\u2026...\u2026...\n3\nInstruction Refinement Flow\nIteratively refine the instructions \nto boost quality, diversity and \ncomplexity\n2Figure 3: Figure provides a thematic overview of the roles played by different groups of\nagents. Content Transformation Flow converts the seed into an intermediate representation\nthat makes it easier to create high quality and diverse data. Seed Instruction Generation\nFlow creates instances of the target tasks following a taxonomy. Refinement Flow explores\nthe space further by starting from these initial data points and exploring the neighborhood.\nThe expectation is that by picking a random seed we will be able to cover the entire region\nof data points.\nmultiple agents and is often instrumental in the generation of high-quality data and serve as\nan additional means to introduce diversity.\nSeed Instruction Generation Flow comprising of multiple agents takes as input the\ntransformed seed from the Content Transformation Flow and generates a set of diverse\ninstructions. The only goal of the Seed Instruction Flow is to introduce diversity for which\nit often relies on a pre-defined, but extensible, taxonomy.\nInstruction Refinement Flow takes as input the instructions from the Seed Instruction\nFlow and iteratively enhances their complexity and quality. Towards this we use the concept\nof Suggester-Editor Agents[ 19]. Suggester agents initially propose various approaches to\nincrease the intricacy of the initial instructions (making them more complex, unsolvable,\nor tricky), after which the Editor agents modify the instructions in accordance with these\nsuggestions.\nEach flow consists of a number of agents. We use a generic definition of an agent, where an\nagent is powered by an LLM and can optionally have the ability to use tools such as search\nAPIs, code interpreter or a calculator. Each agent has a specific role and set of instructions\nspecified as part of the underlying LLM system message.\nWe implemented these flows for 17 different skills, each having multiple subcategories.\nThe skills include reading comprehension, question answering, coding, retrieval augmented\ngeneration, creative writing, tool\/API use and Web control. The full list is provided in\nTable 1\nWe explain how the workflows work with case studies of generating data for the following\nthree skills:\nReading Comprehension: The ability to understand, process, and interpret written text.\nText Modification: The process of altering text to suit different purposes.\nTool Use: The employment of functions or APIs to perform tasks or solve problems.\n4Reading Comprehension: Reading comprehension is a critical skill involving processing and un-\nderstanding text, which is necessary for learning and encompasses decoding, fluency, and vocabulary\nknowledge. Reading comprehension tests typically present text passages of varying lengths and\nsubjects, followed by questions that assess the reader\u2019s understanding.\nOpen Domain Question Answering: Open domain question answering involves generating\nresponses to questions over a wide range of topics, without being restricted to a specific domain.\nText Modification: Text modification involves changing existing text to improve its quality,\nmodify its tone, or to fit a specific context or audience. It is a common task in content creation and\nediting.\nWeb Agent: A web agent is a software program that autonomously performs tasks on the web,\nsuch as where to click, how much to scroll.\nBrain Teaser A brain teaser is a problem or puzzle, typically requiring thought to solve, often for\namusement but also used for training logical thinking and problem-solving skills.\nAnalytical Reasoning Analytical reasoning involves the ability to look at information, be it\nqualitative or quantitative in nature, and discern patterns within the information. It\u2019s a process\nthat includes understanding a system of relationships and draw logical conclusions about those\nrelationships.\nMultiple Choice Questions Multiple choice questions are a form of assessment where respondents\nare asked to select the best possible answer (or answers) out of the choices from a list. They are\ncommon in standardized tests, quizzes, and surveys.\nData To Text Data-to-text refers to human-readable textual summaries from source data. These\ncould be used for generating reports, explanations, or narratives from structured data.\nFermiFermi problems are estimation problems which seek quick, rough estimates of quantities\nwhich can be difficult to measure. Named after physicist Enrico Fermi, these problems often require\nmaking justified guesses or assumptions to reach a solution.\nCoding Coding involves writing code following instructions, understanding code, debugging code,\ntracing or writing test cases.\nText Extraction Text extraction is the process of retrieving relevant information from a larger text\ndocument. This can include tasks like named entity recognition, keyword extraction, or extracting\nspecific data fields from unstructured text.\nText","chunk_id":"f7eb89a70f544664546a510e46d5febd","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"AGENTIC FLOWS","type":"TECHNIQUE","description":"Agentic flows are automated processes that facilitate the generation of diverse data by leveraging raw articles as seeds for problem generation.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"RAW ARTICLES","type":"DATA FORMAT","description":"Raw articles are unprocessed texts used as seeds in the agentic flows to foster diversity in generated problems.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"CONTENT TRANSFORMATION FLOW","type":"PROCESS","description":"Content Transformation Flow is a method that converts raw seeds into an intermediate representation to simplify the creation of tailored instructions.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"SEED INSTRUCTION GENERATION FLOW","type":"PROCESS","description":"Seed Instruction Generation Flow generates diverse instructions based on transformed content, following a comprehensive taxonomy.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"INSTRUCTION REFINEMENT FLOW","type":"PROCESS","description":"Instruction Refinement Flow iteratively enhances the quality, diversity, and complexity of instructions generated from the Seed Instruction Flow.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"SUGGESTER-EDITOR AGENTS","type":"AGENT","description":"Suggester-Editor Agents are specialized agents that propose and modify instructions to increase their complexity and quality.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"SKILLS","type":"CATEGORY","description":"Skills refer to various competencies such as reading comprehension, question answering, and coding, each having multiple subcategories.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"READING COMPREHENSION","type":"SKILL","description":"Reading comprehension is the ability to understand and interpret written text, essential for learning and involves decoding, fluency, and vocabulary knowledge.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"TEXT MODIFICATION","type":"SKILL","description":"Text modification is the process of altering text to improve its quality or fit a specific context, commonly used in content creation and editing.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"TOOL USE","type":"SKILL","description":"Tool use involves employing functions or APIs to perform tasks or solve problems effectively.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"OPEN DOMAIN QUESTION ANSWERING","type":"SKILL","description":"Open domain question answering is the ability to generate responses to questions across a wide range of topics without domain restrictions.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"WEB AGENT","type":"AGENT","description":"A web agent is a software program that autonomously performs tasks on the web, such as navigation and interaction with web elements.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"BRAIN TEASER","type":"TASK","description":"A brain teaser is a problem or puzzle that requires thought to solve, often used for amusement or to train logical thinking skills.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"ANALYTICAL REASONING","type":"SKILL","description":"Analytical reasoning involves discerning patterns within qualitative or quantitative information to draw logical conclusions.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"MULTIPLE CHOICE QUESTIONS","type":"ASSESSMENT","description":"Multiple choice questions are a form of assessment where respondents select the best answer from a list of options.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"DATA TO TEXT","type":"PROCESS","description":"Data-to-text refers to generating human-readable textual summaries from structured data for reports or narratives.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"FERMI PROBLEMS","type":"TASK","description":"Fermi problems are estimation challenges that require making justified guesses to arrive at rough solutions for difficult-to-measure quantities.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"CODING","type":"SKILL","description":"Coding involves writing, understanding, debugging, and testing code based on given instructions.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"TEXT EXTRACTION","type":"PROCESS","description":"Text extraction is the process of retrieving relevant information from larger text documents, including tasks like named entity recognition.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"TASK","type":"","description":"","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"SKILL","type":"","description":"","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"ASSESSMENT","type":"","description":"","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"PROCESS","type":"","description":"","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"TRANSFORMED CONTENT","type":"DATA FORMAT","description":"Transformed content refers to the output generated from the Content Transformation Flow, which simplifies the creation of instructions tailored to specific objectives.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"INTERMEDIATE REPRESENTATION","type":"DATA FORMAT","description":"Intermediate representation is a simplified version of the raw seed that facilitates the creation of tailored instructions in the Content Transformation Flow.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"DIVERSITY","type":"CONCEPT","description":"Diversity refers to the variety and range of generated problems or instructions, which is a key goal of the agentic flows.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"COMPREHENSIVE TAXONOMY","type":"CONCEPT","description":"Comprehensive taxonomy is a structured classification system used to guide the generation of instructions in the Seed Instruction Generation Flow.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"ITERATIVE PROCESS","type":"PROCESS","description":"Iterative process refers to the method of refining instructions through repeated cycles to enhance their quality and complexity in the Instruction Refinement Flow.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"AGENTS","type":"AGENT","description":"Agents are entities powered by LLMs that perform specific roles within the workflows, contributing to the generation and refinement of instructions.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"OUTPUTS","type":"DATA FORMAT","description":"Outputs refer to the results generated from the workflows, including instructions and data summaries.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"PROBLEM GENERATION","type":"PROCESS","description":"Problem generation is the process of creating distinct and diverse problems through the use of agentic flows and raw articles as seeds.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"INSTRUCTION CREATION","type":"PROCESS","description":"Instruction creation is the process of developing specific directives or tasks based on transformed content in the workflows.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"AGENTIC SYSTEM","type":"SYSTEM","description":"Agentic system refers to the overall framework that incorporates various agents and processes to automate data generation and instruction refinement.","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"SEED INSTRUCTION FLOW","type":"","description":"","source_id":"f7eb89a70f544664546a510e46d5febd"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AGENTIC FLOWS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Agentic flows are automated processes that facilitate the generation of diverse data by leveraging raw articles as seeds for problem generation.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"RAW ARTICLES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Raw articles are unprocessed texts used as seeds in the agentic flows to foster diversity in generated problems.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Content Transformation Flow is a method that converts raw seeds into an intermediate representation to simplify the creation of tailored instructions.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"SEED INSTRUCTION GENERATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Seed Instruction Generation Flow generates diverse instructions based on transformed content, following a comprehensive taxonomy.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"INSTRUCTION REFINEMENT FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Instruction Refinement Flow iteratively enhances the quality, diversity, and complexity of instructions generated from the Seed Instruction Flow.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"SUGGESTER-EDITOR AGENTS\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Suggester-Editor Agents are specialized agents that propose and modify instructions to increase their complexity and quality.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"SKILLS\">      <data key=\"d0\">CATEGORY<\/data>      <data key=\"d1\">Skills refer to various competencies such as reading comprehension, question answering, and coding, each having multiple subcategories.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"READING COMPREHENSION\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Reading comprehension is the ability to understand and interpret written text, essential for learning and involves decoding, fluency, and vocabulary knowledge.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"TEXT MODIFICATION\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Text modification is the process of altering text to improve its quality or fit a specific context, commonly used in content creation and editing.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Tool use involves employing functions or APIs to perform tasks or solve problems effectively.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"OPEN DOMAIN QUESTION ANSWERING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Open domain question answering is the ability to generate responses to questions across a wide range of topics without domain restrictions.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"WEB AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">A web agent is a software program that autonomously performs tasks on the web, such as navigation and interaction with web elements.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"BRAIN TEASER\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">A brain teaser is a problem or puzzle that requires thought to solve, often used for amusement or to train logical thinking skills.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"ANALYTICAL REASONING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Analytical reasoning involves discerning patterns within qualitative or quantitative information to draw logical conclusions.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"MULTIPLE CHOICE QUESTIONS\">      <data key=\"d0\">ASSESSMENT<\/data>      <data key=\"d1\">Multiple choice questions are a form of assessment where respondents select the best answer from a list of options.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"DATA TO TEXT\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Data-to-text refers to generating human-readable textual summaries from structured data for reports or narratives.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"FERMI PROBLEMS\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Fermi problems are estimation challenges that require making justified guesses to arrive at rough solutions for difficult-to-measure quantities.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"CODING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Coding involves writing, understanding, debugging, and testing code based on given instructions.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"TEXT EXTRACTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Text extraction is the process of retrieving relevant information from larger text documents, including tasks like named entity recognition.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"TASK\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"SKILL\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"ASSESSMENT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"PROCESS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"TRANSFORMED CONTENT\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Transformed content refers to the output generated from the Content Transformation Flow, which simplifies the creation of instructions tailored to specific objectives.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"INTERMEDIATE REPRESENTATION\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Intermediate representation is a simplified version of the raw seed that facilitates the creation of tailored instructions in the Content Transformation Flow.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"DIVERSITY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Diversity refers to the variety and range of generated problems or instructions, which is a key goal of the agentic flows.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"COMPREHENSIVE TAXONOMY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Comprehensive taxonomy is a structured classification system used to guide the generation of instructions in the Seed Instruction Generation Flow.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"ITERATIVE PROCESS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Iterative process refers to the method of refining instructions through repeated cycles to enhance their quality and complexity in the Instruction Refinement Flow.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"AGENTS\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Agents are entities powered by LLMs that perform specific roles within the workflows, contributing to the generation and refinement of instructions.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"OUTPUTS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Outputs refer to the results generated from the workflows, including instructions and data summaries.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"PROBLEM GENERATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Problem generation is the process of creating distinct and diverse problems through the use of agentic flows and raw articles as seeds.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"INSTRUCTION CREATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Instruction creation is the process of developing specific directives or tasks based on transformed content in the workflows.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"AGENTIC SYSTEM\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">Agentic system refers to the overall framework that incorporates various agents and processes to automate data generation and instruction refinement.<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"SEED INSTRUCTION FLOW\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <edge source=\"AGENTIC FLOWS\" target=\"RAW ARTICLES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Agentic flows utilize raw articles as seeds to generate diverse problems through automation.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"WEB AGENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Web agents are utilized within agentic flows to perform automated tasks on the web, enhancing the generation process.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"DIVERSITY\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Diversity is a key objective of agentic flows, aimed at ensuring a wide range of generated problems and instructions.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"OUTPUTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Outputs are the results generated from the workflows of the agentic flows, including instructions and data summaries.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"PROBLEM GENERATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Problem generation is a process facilitated by agentic flows to create distinct and diverse problems from raw articles.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"RAW ARTICLES\" target=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Content Transformation Flow converts raw articles into an intermediate representation for instruction creation.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"SEED INSTRUCTION GENERATION FLOW\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Seed Instruction Generation Flow takes transformed content from the Content Transformation Flow to generate diverse instructions.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"INTERMEDIATE REPRESENTATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Content Transformation Flow produces an intermediate representation that simplifies the creation of tailored instructions.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"INSTRUCTION CREATION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Instruction creation is a key outcome of the Content Transformation Flow, which simplifies the process of developing specific tasks.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"INSTRUCTION REFINEMENT FLOW\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Instruction Refinement Flow enhances the instructions generated from the Seed Instruction Generation Flow.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"COMPREHENSIVE TAXONOMY\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The comprehensive taxonomy guides the Seed Instruction Generation Flow in creating diverse instructions.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"SUGGESTER-EDITOR AGENTS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Suggester-Editor Agents are involved in the Instruction Refinement Flow to propose and modify instructions for quality improvement.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"ITERATIVE PROCESS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Instruction Refinement Flow employs an iterative process to enhance the quality and complexity of instructions.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"SKILLS\" target=\"READING COMPREHENSION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Reading comprehension is one of the skills developed through the workflows implemented in the agentic flows.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"SKILLS\" target=\"TEXT MODIFICATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Text modification is another skill targeted by the workflows in the agentic flows for enhancing content quality.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"SKILLS\" target=\"TOOL USE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Tool use is a skill that is also developed through the workflows in the agentic flows.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"SKILLS\" target=\"OPEN DOMAIN QUESTION ANSWERING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Open domain question answering is a skill that is part of the competencies developed through the workflows.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"BRAIN TEASER\" target=\"TASK\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Brain teasers are examples of problems generated through the workflows that require logical thinking to solve.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"ANALYTICAL REASONING\" target=\"SKILL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Analytical reasoning is a skill that is developed through the workflows in the agentic flows.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"MULTIPLE CHOICE QUESTIONS\" target=\"ASSESSMENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Multiple choice questions are a common assessment method used to evaluate skills developed through the workflows.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"DATA TO TEXT\" target=\"PROCESS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Data-to-text processes are used to generate human-readable summaries from the outputs of the workflows.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"FERMI PROBLEMS\" target=\"TASK\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Fermi problems are examples of estimation tasks that can be generated through the workflows in the agentic flows.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"CODING\" target=\"SKILL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Coding is a skill that is also developed through the workflows implemented in the agentic flows.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"TEXT EXTRACTION\" target=\"PROCESS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Text extraction is a process involved in retrieving relevant information from generated outputs in the workflows.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"TRANSFORMED CONTENT\" target=\"SEED INSTRUCTION FLOW\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Seed Instruction Flow generates diverse instructions based on the transformed content from the Content Transformation Flow.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"AGENTS\" target=\"AGENTIC SYSTEM\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Agents are integral components of the agentic system, performing specific roles in the workflows.<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"0c212c1467564ad33330b1f655a8e27e","chunk":" quick, rough estimates of quantities\nwhich can be difficult to measure. Named after physicist Enrico Fermi, these problems often require\nmaking justified guesses or assumptions to reach a solution.\nCoding Coding involves writing code following instructions, understanding code, debugging code,\ntracing or writing test cases.\nText Extraction Text extraction is the process of retrieving relevant information from a larger text\ndocument. This can include tasks like named entity recognition, keyword extraction, or extracting\nspecific data fields from unstructured text.\nText Classification Text classification is a type of machine learning task where text documents are\nautomatically classified into predefined categories. This can be used for spam detection, sentiment\nanalysis, and topic labelling among others.\nRetrieval Augmented Generation Retrieval Augmented Generation (RAG) is a method used\nin natural language processing that combines retrieval-based and generative models to generate\nresponses. It first retrieves relevant documents and then uses these documents to generate a response.\nTool Use Tool use involves the manipulation of tools to achieve goals. In AI, this refers to the\nability of an AI system to use available resources or auxiliary systems to solve complex tasks.\nCreative Content Generation Creative content generation involves the creation of original\ncontent, often involving elements of novelty, value, and surprise. In AI, this could refer to generating\ntext, music, or images that are not only new but also meaningful and interesting.\nFew Shot Reasoning Few-shot reasoning refers to the ability of a machine learning model to\nunderstand new concepts, patterns, or tasks with minimal examples or guidance. It\u2019s a desired trait\nin AI, mimicking the human ability to learn quickly from few examples.\nConversation Conversation refers to conversational agents or chatbots that interact with humans\nin a natural, human-like manner.\nTable 1: List of 17 capabilities for which we implemented AgentInstruct Flows\n52.1 AgentInstruct Flow for Reading Comprehension\nReading comprehension is a critical skill involving processing and understanding text, which\nis necessary for learning and encompasses decoding, fluency, and vocabulary knowledge.\nReading comprehension questions range from asking for explicit information (literal com-\nprehension) to requiring inferences, understanding vocabulary in context, analyzing text\nstructure and argumentation, critically evaluating the content, and synthesizing informa-\ntion from different parts of or multiple texts. Reading comprehension is a very important\ncapability and can enable scenarios like question answering, search, grounded reasoning, etc.\nContent Transformation Flow Web crawls encompass an extensive collection of human-\ngenerated text, which holds potential for generating reading comprehension materials.\nHowever, these sources are not inherently structured to facilitate the teaching of read-\ning comprehension skills. Consequently, they do not support the consistent generation of\ndiverse question types required for comprehensive reading comprehension evaluation. For\ninstance, the LSAT Logical Reasoning test features specialized question categories, including\nassumption, strengthening\/weakening, flaw, and inference questions. Crafting such questions\nnecessitates passages with a particular stylistic and logical framework. The objective of\nContent Transformation Flow is to transform arbitrary articles into well-crafted pieces that\nare conducive to the formulation of a wide array of reading comprehension question types.\nOur current flow for Reading Comprehension encompasses a suite of nine content transfor-\nmation agents for generating argument passages, debates and conversations, long passages,\nmeeting transcripts, poems, satarical content, etc. Detailed description is provided in Ap-\npendix A. Given a seed article, the flow will randomly pick one of the Content Transformation\nAgents to assess the seed article and attempt to generate the text passages. The following\nprovides an example for the Argument Passage Generator .\nEXAMPLE: Content Transformation Flow\nRandom Seed\nUric acid is a substance produced naturally by the breakdown of purine (a type of dietary\nprotein). When it is in excess in the body, crystals composed of these substances are formed.\nThese crystals are deposited in various parts of the body, mainly in the joints and kidneys,\ncausing pain and other aggravations. The lack or excess of uric acid in the body is caused by\nsome diseases (such as leukemia, obesity, kidney diseases, and anemia) and factors related\nto lifestyle (consumption of alcohol and processed foods, for example).\nContents\n\u2022Where does purine come from?\n\u2022Where is uric acid found?\n\u2022What is high uric acid?\n\u2022What can high uric acid cause?\n\u2022What is low uric acid?\n\u2022Tests: how to know uric acid levels?\nWhere does purine come from?\nPurine is produced and released into the bloodstream when an amino acid is broken down\nby digestion. Therefore, it is produced naturally in the body. ... (Truncated)\n6Transformed Seed (Argument Passage Generator)\nRecent studies have indicated that high levels of uric acid in the blood, known as hyper-\nuricemia, may increase the risk of cardiovascular disease. Uric acid is a byproduct of purine\nmetabolism, and while it is naturally produced in the body, excessive amounts can lead to\nhealth complications. Hyperuricemia is typically defined as levels above 6 mg\/dL in women\nand 7 mg\/dL in men. This condition can result from increased production of uric acid or\ninsufficient elimination through urine. Factors contributing to high uric acid include a diet\nrich in purines, found in red meat and seafood, as well as lifestyle choices such as alcohol\nconsumption and physical inactivity. Conversely, low levels of uric acid, or hypouricemia,\nare less common and usually do not present symptoms. Nonetheless, they can be indicative\nof underlying kidney or liver issues. Diagnosing either condition requires laboratory blood\nand urine tests. It is important to note that while high uric acid levels are associated with\nan increased risk of cardiovascular disease, the causal","chunk_id":"0c212c1467564ad33330b1f655a8e27e","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"ENRICO FERMI","type":"PERSON","description":"Enrico Fermi was a physicist known for his contributions to nuclear physics and for the development of the Fermi problem, which involves making rough estimates of quantities that are difficult to measure.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"CODING","type":"ACTIVITY","description":"Coding involves writing code according to instructions, understanding and debugging code, and tracing or writing test cases.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"TEXT EXTRACTION","type":"PROCESS","description":"Text extraction is the process of retrieving relevant information from larger text documents, including tasks like named entity recognition and keyword extraction.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"TEXT CLASSIFICATION","type":"PROCESS","description":"Text classification is a machine learning task where text documents are automatically categorized into predefined categories, such as spam detection and sentiment analysis.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"RETRIEVAL AUGMENTED GENERATION (RAG)","type":"TECHNIQUE","description":"Retrieval Augmented Generation (RAG) is a method in natural language processing that combines retrieval-based and generative models to generate responses based on retrieved documents.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"TOOL USE","type":"ACTIVITY","description":"Tool use refers to the manipulation of tools to achieve goals, particularly in AI, where it involves using resources to solve complex tasks.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"CREATIVE CONTENT GENERATION","type":"PROCESS","description":"Creative content generation involves creating original content, such as text, music, or images, that is novel, valuable, and meaningful.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"FEW SHOT REASONING","type":"CONCEPT","description":"Few-shot reasoning is the ability of a machine learning model to understand new concepts or tasks with minimal examples, mimicking human learning.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"CONVERSATION","type":"PROCESS","description":"Conversation refers to the interaction of conversational agents or chatbots with humans in a natural, human-like manner.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"READING COMPREHENSION","type":"SKILL","description":"Reading comprehension is the ability to process and understand text, involving skills such as decoding, fluency, and vocabulary knowledge.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"CONTENT TRANSFORMATION FLOW","type":"PROCESS","description":"Content Transformation Flow is a method for transforming arbitrary articles into structured pieces that facilitate the generation of diverse reading comprehension questions.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"HYPERURICEMIA","type":"MEDICAL CONDITION","description":"Hyperuricemia is a condition characterized by high levels of uric acid in the blood, which can increase the risk of cardiovascular disease and result from various dietary and lifestyle factors.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"PURINE","type":"COMPOUND","description":"Purine is a type of dietary protein that, when broken down, produces uric acid, which can lead to health complications if present in excess.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"FERMI PROBLEM","type":"","description":"","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"ARGUMENT PASSAGE GENERATOR","type":"TECHNIQUE","description":"The Argument Passage Generator is a specific tool within the Content Transformation Flow that creates argumentative text passages for reading comprehension tasks.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"LSAT LOGICAL REASONING","type":"TEST","description":"The LSAT Logical Reasoning test features specialized question categories designed to evaluate critical thinking and reasoning skills in prospective law students.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"CARDIOVASCULAR DISEASE","type":"MEDICAL CONDITION","description":"Cardiovascular disease refers to a range of conditions affecting the heart and blood vessels, often associated with high uric acid levels and other risk factors.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"HYPURICEMIA","type":"MEDICAL CONDITION","description":"Hypouricemia is a condition characterized by low levels of uric acid in the blood, which can indicate underlying health issues but is less common than hyperuricemia.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"URIC ACID","type":"COMPOUND","description":"Uric acid is a byproduct of purine metabolism in the body, and its levels can indicate various health conditions, including hyperuricemia and hypouricemia.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"AGENTINSTRUCT FLOWS","type":"","description":"","source_id":"0c212c1467564ad33330b1f655a8e27e"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ENRICO FERMI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Enrico Fermi was a physicist known for his contributions to nuclear physics and for the development of the Fermi problem, which involves making rough estimates of quantities that are difficult to measure.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"CODING\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Coding involves writing code according to instructions, understanding and debugging code, and tracing or writing test cases.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"TEXT EXTRACTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Text extraction is the process of retrieving relevant information from larger text documents, including tasks like named entity recognition and keyword extraction.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"TEXT CLASSIFICATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Text classification is a machine learning task where text documents are automatically categorized into predefined categories, such as spam detection and sentiment analysis.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"RETRIEVAL AUGMENTED GENERATION (RAG)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Retrieval Augmented Generation (RAG) is a method in natural language processing that combines retrieval-based and generative models to generate responses based on retrieved documents.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Tool use refers to the manipulation of tools to achieve goals, particularly in AI, where it involves using resources to solve complex tasks.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"CREATIVE CONTENT GENERATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Creative content generation involves creating original content, such as text, music, or images, that is novel, valuable, and meaningful.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"FEW SHOT REASONING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Few-shot reasoning is the ability of a machine learning model to understand new concepts or tasks with minimal examples, mimicking human learning.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"CONVERSATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Conversation refers to the interaction of conversational agents or chatbots with humans in a natural, human-like manner.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"READING COMPREHENSION\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Reading comprehension is the ability to process and understand text, involving skills such as decoding, fluency, and vocabulary knowledge.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Content Transformation Flow is a method for transforming arbitrary articles into structured pieces that facilitate the generation of diverse reading comprehension questions.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"HYPERURICEMIA\">      <data key=\"d0\">MEDICAL CONDITION<\/data>      <data key=\"d1\">Hyperuricemia is a condition characterized by high levels of uric acid in the blood, which can increase the risk of cardiovascular disease and result from various dietary and lifestyle factors.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"PURINE\">      <data key=\"d0\">COMPOUND<\/data>      <data key=\"d1\">Purine is a type of dietary protein that, when broken down, produces uric acid, which can lead to health complications if present in excess.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"FERMI PROBLEM\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"ARGUMENT PASSAGE GENERATOR\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Argument Passage Generator is a specific tool within the Content Transformation Flow that creates argumentative text passages for reading comprehension tasks.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"LSAT LOGICAL REASONING\">      <data key=\"d0\">TEST<\/data>      <data key=\"d1\">The LSAT Logical Reasoning test features specialized question categories designed to evaluate critical thinking and reasoning skills in prospective law students.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"CARDIOVASCULAR DISEASE\">      <data key=\"d0\">MEDICAL CONDITION<\/data>      <data key=\"d1\">Cardiovascular disease refers to a range of conditions affecting the heart and blood vessels, often associated with high uric acid levels and other risk factors.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"HYPURICEMIA\">      <data key=\"d0\">MEDICAL CONDITION<\/data>      <data key=\"d1\">Hypouricemia is a condition characterized by low levels of uric acid in the blood, which can indicate underlying health issues but is less common than hyperuricemia.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"URIC ACID\">      <data key=\"d0\">COMPOUND<\/data>      <data key=\"d1\">Uric acid is a byproduct of purine metabolism in the body, and its levels can indicate various health conditions, including hyperuricemia and hypouricemia.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"AGENTINSTRUCT FLOWS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <edge source=\"ENRICO FERMI\" target=\"FERMI PROBLEM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Fermi problem is named after Enrico Fermi, who is known for making justified guesses to solve complex estimation problems.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"CODING\" target=\"TEXT EXTRACTION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Coding can be involved in the development of algorithms for text extraction processes, enhancing the retrieval of relevant information.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"CODING\" target=\"TOOL USE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Tool use in AI often involves coding to manipulate tools and resources effectively for problem-solving.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"TEXT EXTRACTION\" target=\"TEXT CLASSIFICATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Text extraction can be a preliminary step before text classification, as relevant information needs to be retrieved before categorization.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"TEXT EXTRACTION\" target=\"RETRIEVAL AUGMENTED GENERATION (RAG)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">RAG utilizes text extraction to retrieve relevant documents before generating responses, linking the two processes.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"CREATIVE CONTENT GENERATION\" target=\"FEW SHOT REASONING\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Few-shot reasoning can enhance creative content generation by allowing models to create original content with minimal examples.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"CONVERSATION\" target=\"READING COMPREHENSION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Conversational agents can be designed to assist in reading comprehension by engaging users in discussions about texts.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Content Transformation Flow aims to create materials that improve reading comprehension skills by generating diverse question types.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"AGENTINSTRUCT FLOWS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">AgentInstruct Flows are implemented to enhance reading comprehension capabilities in AI systems.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"LSAT LOGICAL REASONING\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The LSAT Logical Reasoning test assesses reading comprehension and critical reasoning skills, making it relevant to the study of reading comprehension.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"ARGUMENT PASSAGE GENERATOR\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Argument Passage Generator is a component of the Content Transformation Flow, specifically designed to create argumentative passages for comprehension tasks.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"HYPERURICEMIA\" target=\"PURINE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Hyperuricemia is often caused by a diet high in purines, which can lead to elevated uric acid levels in the body.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"HYPERURICEMIA\" target=\"CARDIOVASCULAR DISEASE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Hyperuricemia is associated with an increased risk of cardiovascular disease, linking the two medical conditions.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"HYPERURICEMIA\" target=\"URIC ACID\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">High levels of uric acid in the blood lead to hyperuricemia, which can cause various health complications.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"HYPURICEMIA\" target=\"URIC ACID\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Hypouricemia is characterized by low levels of uric acid in the blood, indicating a potential health issue.<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"1d8835c0ce90e56be22873bcf2740a5d","chunk":" found in red meat and seafood, as well as lifestyle choices such as alcohol\nconsumption and physical inactivity. Conversely, low levels of uric acid, or hypouricemia,\nare less common and usually do not present symptoms. Nonetheless, they can be indicative\nof underlying kidney or liver issues. Diagnosing either condition requires laboratory blood\nand urine tests. It is important to note that while high uric acid levels are associated with\nan increased risk of cardiovascular disease, the causal relationship remains to be conclusively\nestablished.\nSeed Instruction Generation Flow We have currently compiled a collection of 43\nreading comprehension questions types. The list includes literal comprehension questions,\ncritical comprehension questions, evaluative comprehension questions, reasoning, identifying\nassumptions, identifying information that strengthens\/weakens an argument, ordering events,\netc.\nA list some of these types is in Appendix A.\nWe have defined multiple Agents targeting these categories. Each of these agent receives as\ninput a piece of text and generates a list of questions based on a predefined question type.\nThe orchestration process will engage a subset of these agents, determined by the content\ntransformation agent in the preceding step. Upon completion of this phase, we will obtain a\ncollection of (passage, question) pairs that will constitute the input for the subsequent stage.\nThe following figure provides an example of Strengthen type question that is generated from\nthe running example.\nEXAMPLE: Seed Instruction\nWhich one of the following, if true, most strengthens the argument that high lev-\nels of uric acid increase the risk of cardiovascular disease?\n(A) Some individuals with hyperuricemia do not develop cardiovascular disease.\n(B) Physical activity has been found to have no significant impact on uric acid levels in the\nblood.\n(C) Dietary supplements have been shown to reduce uric acid levels in the blood.\n(D) A significant number of patients with cardiovascular disease have been found to have\nuric acid levels above the defined thresholds for hyperuricemia.\n(E) Individuals with a family history of cardiovascular disease are more likely to have high\nuric acid levels.\nInstruction Refinement Flow The refinement flow contains multiple suggester-editor\nagents that will go through each of the (passage, question) pairs and create more such pairs\nwith the following goals: (1) Modify the passage to make the question unanswerable, (2)\nModify the passage to alter the answer, if possible, in opposite direction, or (3) Modify the\nquestions or answer choices (if applicable) to make them complex.\nHere is an example where the suggester agent that tries to add a difficult distractor to\nconfuse the test taker.\n7EXAMPLE: Refinement Flow\nSUGGESTER\n\u2022Suggestion 1 Introduce a hypothetical study or finding that could potentially\nstrengthen the argument, requiring the test-taker to infer its impact on the rela-\ntionship between uric acid levels and cardiovascular disease.\n\u2022Suggestion 2 Add a layer of complexity by suggesting a genetic predisposition\nto hyperuricemia and its correlation with increased cardiovascular events, which\nwould require the test-taker to consider both genetic and physiological factors.\n\u2022Suggestion 3 Include a distractor option that seems to strengthen the argument\nbut upon closer examination, it does not directly relate to the causal relationship\nbetween uric acid levels and cardiovascular disease, thus testing the test-taker\u2019s\nability to discern relevant from irrelevant information.\nEDITOR\n\u2022Modification 1,2 (omitted for brevity)\n\u2022Modification 3 Which one of the following, if true, most strengthens the argument\nthat high levels of uric acid increase the risk of cardiovascular disease?\n(A) Some individuals with hyperuricemia do not develop cardiovascular disease.\n(B) Physical activity has been found to have no significant impact on uric acid\nlevels in the blood.\n(C) A placebo-controlled trial shows that individuals taking uric acid-lowering\nmedication do not exhibit a reduced incidence of cardiovascular disease compared\nto the placebo group.\n(D) A significant number of patients with cardiovascular disease have been found\nto have uric acid levels above the defined thresholds for hyperuricemia.\n(E) A cross-sectional study finds that high uric acid levels are associated with other\nrisk factors for cardiovascular disease, such as hypertension and diabetes, but does\nnot establish a direct causal link.\n2.2 AgentInstruct Flow for Text Modification\nText modification is the process of editing and refining written content to enhance its quality\nand effectiveness or alter its attributes. This involves correcting spelling and grammar,\nclarifyingideas, reorganizingcontentforbetterflow, adjustingtone, ensuringstyleconsistency,\nfact-checking, removingredundancies, formatting, developingcontent, andadaptingtospecific\naudiences. It is a useful skill for LLMs that help in content writing. While several content\ntransformation agents can be introduced to intentionally modify the text as described earlier,\nwe focus here on showing the how the instructions are created and refined.\nSeed Instruction Generation Flow We have currently compiled a collection of 18types\nof text modification tasks including paraphrasing, expansion, simplification, redacting or\nremoving content, styling, code switching, etc. The full list is in Appendix A.\nWe define an Agent for each of the task type. Each agent takes as input a piece of text and\ncreates several text modification tasks of the associated type. Here we provide an example\ninput and a task created by the Paraphrasing Agent.\n8EXAMPLE: Seed Instruction\nRandom Seed\nApril 6-8, 2017, University of Iowa, Iowa City, USA. Abstracts due December 1, 2016.\nFinance is hard to escape. In recent years, the increasing social impact and interconnection\nof financial discourses, markets, actors, and institutions have","chunk_id":"1d8835c0ce90e56be22873bcf2740a5d","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"URIC ACID","type":"BIOCHEMICAL COMPOUND","description":"Uric acid is a waste product formed from the breakdown of purines, and its levels in the blood can indicate various health conditions, including cardiovascular disease and kidney issues.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"HYPERURICEMIA","type":"MEDICAL CONDITION","description":"Hyperuricemia is a condition characterized by elevated levels of uric acid in the blood, which can increase the risk of cardiovascular disease.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"HYPOURICEMIA","type":"MEDICAL CONDITION","description":"Hypouricemia is a condition with low levels of uric acid in the blood, often without symptoms, but may indicate underlying health issues.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"CARDIOVASCULAR DISEASE","type":"MEDICAL CONDITION","description":"Cardiovascular disease refers to a range of conditions affecting the heart and blood vessels, often associated with high uric acid levels.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"LABORATORY TESTS","type":"PROCEDURE","description":"Laboratory blood and urine tests are diagnostic procedures used to measure uric acid levels and assess kidney and liver function.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"READING COMPREHENSION QUESTIONS","type":"EDUCATIONAL TOOL","description":"Reading comprehension questions are designed to assess understanding of a text, including various types such as literal, critical, and evaluative questions.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"AGENTS","type":"TECHNOLOGY","description":"Agents are automated systems that generate questions or modify text based on predefined criteria in educational contexts.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"PASSAGE-QUESTION PAIRS","type":"DATA FORMAT","description":"Passage-question pairs are combinations of text excerpts and corresponding questions used for comprehension assessment.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"TEXT MODIFICATION","type":"PROCESS","description":"Text modification involves editing and refining written content to improve clarity, flow, and effectiveness, often using automated agents.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"PARAPHRASING AGENT","type":"TECHNOLOGY","description":"The paraphrasing agent is a specific type of automated system that rewrites text to convey the same meaning in different words.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"PHYSICAL INACTIVITY","type":"LIFESTYLE CHOICE","description":"Physical inactivity is a lifestyle choice characterized by a lack of sufficient physical activity, which can affect overall health and uric acid levels.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"KIDNEY ISSUES","type":"MEDICAL CONDITION","description":"Kidney issues refer to various health problems affecting kidney function, which can be indicated by low uric acid levels.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"LIVER ISSUES","type":"MEDICAL CONDITION","description":"Liver issues encompass a range of conditions affecting liver function, which may also be indicated by low uric acid levels.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"CROSS-SECTIONAL STUDY","type":"RESEARCH METHOD","description":"A cross-sectional study is a type of observational research that analyzes data from a population at a specific point in time, often used to assess relationships between variables.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"HYPERTENSION","type":"MEDICAL CONDITION","description":"Hypertension, or high blood pressure, is a medical condition that can be associated with cardiovascular disease and other health issues.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"DIABETES","type":"MEDICAL CONDITION","description":"Diabetes is a chronic medical condition that affects how the body processes blood sugar (glucose) and can be linked to cardiovascular disease.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"HYPERURICEMIA AND CARDIOVASCULAR DISEASE RELATIONSHIP","type":"RESEARCH FINDING","description":"The relationship between hyperuricemia and cardiovascular disease is a subject of research, exploring how elevated uric acid levels may contribute to heart-related health risks.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"DISTRATOR OPTION","type":"EDUCATIONAL TOOL","description":"A distractor option is a choice in a multiple-choice question designed to mislead or confuse the test-taker, testing their understanding of the material.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"GENETIC PREDISPOSITION","type":"BIOLOGICAL FACTOR","description":"Genetic predisposition refers to the increased likelihood of developing certain health conditions based on one's genetic makeup, which can influence uric acid levels and cardiovascular health.)<|COMPLETE|>","source_id":"1d8835c0ce90e56be22873bcf2740a5d"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"URIC ACID\">      <data key=\"d0\">BIOCHEMICAL COMPOUND<\/data>      <data key=\"d1\">Uric acid is a waste product formed from the breakdown of purines, and its levels in the blood can indicate various health conditions, including cardiovascular disease and kidney issues.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"HYPERURICEMIA\">      <data key=\"d0\">MEDICAL CONDITION<\/data>      <data key=\"d1\">Hyperuricemia is a condition characterized by elevated levels of uric acid in the blood, which can increase the risk of cardiovascular disease.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"HYPOURICEMIA\">      <data key=\"d0\">MEDICAL CONDITION<\/data>      <data key=\"d1\">Hypouricemia is a condition with low levels of uric acid in the blood, often without symptoms, but may indicate underlying health issues.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"CARDIOVASCULAR DISEASE\">      <data key=\"d0\">MEDICAL CONDITION<\/data>      <data key=\"d1\">Cardiovascular disease refers to a range of conditions affecting the heart and blood vessels, often associated with high uric acid levels.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"LABORATORY TESTS\">      <data key=\"d0\">PROCEDURE<\/data>      <data key=\"d1\">Laboratory blood and urine tests are diagnostic procedures used to measure uric acid levels and assess kidney and liver function.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"READING COMPREHENSION QUESTIONS\">      <data key=\"d0\">EDUCATIONAL TOOL<\/data>      <data key=\"d1\">Reading comprehension questions are designed to assess understanding of a text, including various types such as literal, critical, and evaluative questions.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"AGENTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Agents are automated systems that generate questions or modify text based on predefined criteria in educational contexts.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"PASSAGE-QUESTION PAIRS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Passage-question pairs are combinations of text excerpts and corresponding questions used for comprehension assessment.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"TEXT MODIFICATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Text modification involves editing and refining written content to improve clarity, flow, and effectiveness, often using automated agents.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"PARAPHRASING AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The paraphrasing agent is a specific type of automated system that rewrites text to convey the same meaning in different words.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"PHYSICAL INACTIVITY\">      <data key=\"d0\">LIFESTYLE CHOICE<\/data>      <data key=\"d1\">Physical inactivity is a lifestyle choice characterized by a lack of sufficient physical activity, which can affect overall health and uric acid levels.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"KIDNEY ISSUES\">      <data key=\"d0\">MEDICAL CONDITION<\/data>      <data key=\"d1\">Kidney issues refer to various health problems affecting kidney function, which can be indicated by low uric acid levels.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"LIVER ISSUES\">      <data key=\"d0\">MEDICAL CONDITION<\/data>      <data key=\"d1\">Liver issues encompass a range of conditions affecting liver function, which may also be indicated by low uric acid levels.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"CROSS-SECTIONAL STUDY\">      <data key=\"d0\">RESEARCH METHOD<\/data>      <data key=\"d1\">A cross-sectional study is a type of observational research that analyzes data from a population at a specific point in time, often used to assess relationships between variables.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"HYPERTENSION\">      <data key=\"d0\">MEDICAL CONDITION<\/data>      <data key=\"d1\">Hypertension, or high blood pressure, is a medical condition that can be associated with cardiovascular disease and other health issues.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"DIABETES\">      <data key=\"d0\">MEDICAL CONDITION<\/data>      <data key=\"d1\">Diabetes is a chronic medical condition that affects how the body processes blood sugar (glucose) and can be linked to cardiovascular disease.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"HYPERURICEMIA AND CARDIOVASCULAR DISEASE RELATIONSHIP\">      <data key=\"d0\">RESEARCH FINDING<\/data>      <data key=\"d1\">The relationship between hyperuricemia and cardiovascular disease is a subject of research, exploring how elevated uric acid levels may contribute to heart-related health risks.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"DISTRATOR OPTION\">      <data key=\"d0\">EDUCATIONAL TOOL<\/data>      <data key=\"d1\">A distractor option is a choice in a multiple-choice question designed to mislead or confuse the test-taker, testing their understanding of the material.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"GENETIC PREDISPOSITION\">      <data key=\"d0\">BIOLOGICAL FACTOR<\/data>      <data key=\"d1\">Genetic predisposition refers to the increased likelihood of developing certain health conditions based on one's genetic makeup, which can influence uric acid levels and cardiovascular health.)&lt;|COMPLETE|&gt;<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <edge source=\"URIC ACID\" target=\"HYPERURICEMIA\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">High levels of uric acid are indicative of hyperuricemia, which can lead to health complications.<\/data>      <data key=\"d5\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"URIC ACID\" target=\"CARDIOVASCULAR DISEASE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Elevated uric acid levels are associated with an increased risk of cardiovascular disease.<\/data>      <data key=\"d5\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"URIC ACID\" target=\"LABORATORY TESTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Laboratory tests are used to assess uric acid levels in the blood and diagnose related conditions.<\/data>      <data key=\"d5\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"HYPERURICEMIA\" target=\"CARDIOVASCULAR DISEASE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Hyperuricemia is a risk factor for developing cardiovascular disease.<\/data>      <data key=\"d5\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"HYPOURICEMIA\" target=\"LABORATORY TESTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Hypouricemia may be diagnosed through laboratory tests that measure uric acid levels.<\/data>      <data key=\"d5\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION QUESTIONS\" target=\"AGENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Agents generate reading comprehension questions based on text passages for educational purposes.<\/data>      <data key=\"d5\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION QUESTIONS\" target=\"PASSAGE-QUESTION PAIRS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Passage-question pairs are created to evaluate understanding of reading comprehension questions.<\/data>      <data key=\"d5\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"AGENTS\" target=\"TEXT MODIFICATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Agents are utilized in the process of text modification to enhance written content.<\/data>      <data key=\"d5\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"TEXT MODIFICATION\" target=\"PARAPHRASING AGENT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The paraphrasing agent is a tool used in text modification to rewrite content while maintaining its original meaning.<\/data>      <data key=\"d5\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"427e98b00e49b6a8f8649054122dd45b","chunk":" text and\ncreates several text modification tasks of the associated type. Here we provide an example\ninput and a task created by the Paraphrasing Agent.\n8EXAMPLE: Seed Instruction\nRandom Seed\nApril 6-8, 2017, University of Iowa, Iowa City, USA. Abstracts due December 1, 2016.\nFinance is hard to escape. In recent years, the increasing social impact and interconnection\nof financial discourses, markets, actors, and institutions have been understood under the\nbroad concept of financialization. Natascha van der Zwan identifies three distinct research\nstreams that have approached financialization as 1) a regime of accumulation, 2) the influence\nof financial markets and instruments on non-financial corporations as well as the banking\nand finance industry, and 3) a discourse of risk-taking, self-management and self-fulfillment\nthat is transforming people into investing subjects. Some anthropological skeptics, however,\nargue that finance has a far longer genealogy than the financialization literature has to date\nrecognized. For example, in the context of a lengthy human history of creating hierarchy,\nfinancialization may simply be a new technology serving an old purpose.\n... (omitted for brevity)... \u00b7The supply chains of financial products connect different places\nand political projects across the globe. How do such financial instruments transform social\nlife?\nAbstract deadline is December 1, 2016.\n1. Go to americananthro.org and log in. If you don\u2019t have a login id and password, create\none (you do not need to join the American Anthropological Association).\n2. Once you are logged in, look to the left hand column, click on Meeting registration.\n3. Click on register under the SEA 2017 Annual Meeting then follow online prompts to\nregister for the meeting (if we do not accept your abstract and you decide not to attend,\nyou may request that your registration fee be refunded and we would be happy to do so).\n4. Once you are registered, AAA will automatically send you an email inviting you to submit\nan abstract. Click the link and follow the instructions.\nTask:\nRewrite the event details (date, location, abstract deadline) in a more casual tone.\nGiven a random seed, the seed instruction creation flow randomly picks one of the 18agents\nand uses that to obtain a seed (text, text modification instruction) pair.\nInstruction Refinement Flow The refinement flow consists of only one Suggester-Editor\npair. The Suggester-Editor duo increase the complexity of the generated instructions while\ntaking as input the text and the task modification instruction as input. Here we show three\nsuggestions and edits as proposed by the suggester-editor pair.\nEXAMPLE: IRefinement Flow\nSuggestions\nSuggestion 1: Incorporate a fictional narrative. Use a conversational style with colloquial\nlanguage and include a humorous element.\nSuggestion 2: Translate the event details into a poetic format. Maintain accurate infor-\nmation while using rhyming couplets and ensure the tone remains light and\nengaging.\nSuggestion 3: Frame the event details as a social media post. Use internet slang and\nemojis. Keep the message within 280 characters.\nModified Instructions\nInstruction 1: Rewrite the event details (date, location, abstract deadline) as if you\u2019re\ntelling a funny story to a friend using casual and colloquial language,\nwhile incorporating a fictional narrative that still conveys the necessary\ninformation.\nInstruction 2: Transform the event details (date, location, abstract deadline) into a light-\nheartedpoemwithrhymingcouplets, ensuringthattheessentialinformation\nis accurately conveyed in a poetic format.\nInstruction 3: Craft a social media post that includes the event details (date, location,\nabstract deadline) using internet slang, emojis, and a casual tone, while\nkeeping the message concise and within 280 characters.\n92.3 AgentInstruct Flow for Tool Use\nThe task of tool use or API use for LLMs involves enabling models to interact with external\ntools or services; via APIs. This capability allows AI systems to extend their functionality,\naccess external data, and perform actions beyond their native capabilities.\nContent Transformation Flow We use source code snippets or an API description [ 26] as\nthe random seed. If source code snippets has been used as the seed, a content transformation\nagent is used to synthesize an API description from the code snippet. The goal of the\nContent Transformation Flow is to synthesize list of APIs from the random seed. API lists\nare created by either: (1) using an API retrieval agent that iteratively searches for similar\ncode to expand the API list or (2) the agent uses the LLM to hypothesize other APIs present\nin the library.\nThe following figure provides an example of the library reconstruction scenario.\nEXAMPLE: Content Transformation Flow [Library Reconstruction]\nSeed\n{\n\"name\": \"View All Food Items\",\n\"description\": \"The request enables clients to obtain a detailed list of food items,\ncomplete with nutritional profiles such as calorie count, protein, fat, ...\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"limit\": {\n\"type\": \"NUMBER\",\n\"description\": \"limit the length of response\"\n}\n},\n\"required\": []\n}\n}\nReconstructed Remaining APIs from the Library\n[\n{\n\"name\": \"Search Food Items\",\n\"description\": \"Allows clients to search for food items by name and retrieve a list of\nmatching items ...\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"query\": {\n\"type\": \"string\",\n\"description\": \"The name or partial name of the food item to search for.\"\n},\n\"limit\": {\n\"type\": \"number\",\n\"description\": \"Optional parameter to limit the number of search results\nreturned.\"\n","chunk_id":"427e98b00e49b6a8f8649054122dd45b","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"FINANCIALIZATION","type":"CONCEPT","description":"Financialization is a broad concept that encompasses the increasing social impact and interconnection of financial discourses, markets, actors, and institutions in recent years.","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"NATASCHA VAN DER ZWAN","type":"PERSON","description":"Natascha van der Zwan is a researcher who identifies three distinct research streams related to financialization.","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"RESEARCH STREAMS","type":"CONCEPT","description":"Research streams are distinct approaches to studying financialization, including accumulation regimes, the influence of financial markets on non-financial corporations, and discourses of risk-taking.","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"SOCIAL LIFE","type":"CONCEPT","description":"Social life refers to the interactions and relationships among individuals and groups within society, which can be transformed by financial instruments.","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"SUPPLY CHAINS","type":"CONCEPT","description":"Supply chains of financial products connect different places and political projects globally, impacting social dynamics.","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"AMERICAN ANTHROPOLOGICAL ASSOCIATION","type":"ORGANIZATION","description":"The American Anthropological Association is a professional organization that hosts meetings and conferences related to anthropology.","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"SEA 2017 ANNUAL MEETING","type":"EVENT","description":"The SEA 2017 Annual Meeting is an event organized by the American Anthropological Association, scheduled for April 6-8, 2017, at the University of Iowa.","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"RANDOM SEED","type":"DATA FORMAT","description":"A random seed is a value used to initialize a process, which in this context is used to generate text modification instructions.","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"INSTRUCTION REFINEMENT FLOW","type":"PROCESS","description":"The Instruction Refinement Flow is a process that enhances the complexity of generated instructions based on input text and task modification instructions.","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"SUGGESTER-EDITOR PAIR","type":"TECHNIQUE","description":"The Suggester-Editor pair is a collaborative mechanism that generates and refines instructions for text modification tasks.","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"CONTENT TRANSFORMATION FLOW","type":"PROCESS","description":"The Content Transformation Flow is a process that synthesizes API descriptions or lists from source code snippets or other inputs.","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"API DESCRIPTION","type":"DATA FORMAT","description":"An API description outlines the functionality and parameters of an API, detailing how it can be used by clients.","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"LIBRARY RECONSTRUCTION","type":"PROCESS","description":"Library Reconstruction is a process that involves synthesizing a list of APIs from a given seed, often using an API retrieval agent.","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"PARAPHRASING AGENT","type":"","description":"","source_id":"427e98b00e49b6a8f8649054122dd45b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"FINANCIALIZATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Financialization is a broad concept that encompasses the increasing social impact and interconnection of financial discourses, markets, actors, and institutions in recent years.<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"NATASCHA VAN DER ZWAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Natascha van der Zwan is a researcher who identifies three distinct research streams related to financialization.<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"RESEARCH STREAMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Research streams are distinct approaches to studying financialization, including accumulation regimes, the influence of financial markets on non-financial corporations, and discourses of risk-taking.<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"SOCIAL LIFE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Social life refers to the interactions and relationships among individuals and groups within society, which can be transformed by financial instruments.<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"SUPPLY CHAINS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Supply chains of financial products connect different places and political projects globally, impacting social dynamics.<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"AMERICAN ANTHROPOLOGICAL ASSOCIATION\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">The American Anthropological Association is a professional organization that hosts meetings and conferences related to anthropology.<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"SEA 2017 ANNUAL MEETING\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The SEA 2017 Annual Meeting is an event organized by the American Anthropological Association, scheduled for April 6-8, 2017, at the University of Iowa.<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"RANDOM SEED\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">A random seed is a value used to initialize a process, which in this context is used to generate text modification instructions.<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"INSTRUCTION REFINEMENT FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The Instruction Refinement Flow is a process that enhances the complexity of generated instructions based on input text and task modification instructions.<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"SUGGESTER-EDITOR PAIR\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Suggester-Editor pair is a collaborative mechanism that generates and refines instructions for text modification tasks.<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The Content Transformation Flow is a process that synthesizes API descriptions or lists from source code snippets or other inputs.<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"API DESCRIPTION\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">An API description outlines the functionality and parameters of an API, detailing how it can be used by clients.<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"LIBRARY RECONSTRUCTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Library Reconstruction is a process that involves synthesizing a list of APIs from a given seed, often using an API retrieval agent.<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"PARAPHRASING AGENT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <edge source=\"FINANCIALIZATION\" target=\"SUPPLY CHAINS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Financialization influences supply chains of financial products, connecting various global locations and political projects.<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"NATASCHA VAN DER ZWAN\" target=\"RESEARCH STREAMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Natascha van der Zwan identifies and categorizes three distinct research streams related to the concept of financialization.<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"SOCIAL LIFE\" target=\"SUPPLY CHAINS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The supply chains of financial products can transform social life by altering interactions and relationships within society.<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"AMERICAN ANTHROPOLOGICAL ASSOCIATION\" target=\"SEA 2017 ANNUAL MEETING\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The SEA 2017 Annual Meeting is organized by the American Anthropological Association, which facilitates professional gatherings in anthropology.<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"RANDOM SEED\" target=\"PARAPHRASING AGENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Paraphrasing Agent utilizes a random seed to create text modification tasks based on the seed's value.<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"SUGGESTER-EDITOR PAIR\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Instruction Refinement Flow employs a Suggester-Editor pair to enhance the complexity of generated instructions.<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"API DESCRIPTION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Content Transformation Flow synthesizes API descriptions from source code snippets or other inputs, detailing their functionality.<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"LIBRARY RECONSTRUCTION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Library Reconstruction is a specific application of the Content Transformation Flow, focusing on synthesizing API lists.<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"0922646b93a124514ce2a267d961d229","chunk":" Remaining APIs from the Library\n[\n{\n\"name\": \"Search Food Items\",\n\"description\": \"Allows clients to search for food items by name and retrieve a list of\nmatching items ...\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"query\": {\n\"type\": \"string\",\n\"description\": \"The name or partial name of the food item to search for.\"\n},\n\"limit\": {\n\"type\": \"number\",\n\"description\": \"Optional parameter to limit the number of search results\nreturned.\"\n}\n},\n\"required\": [\n\"query\"\n]\n}\n},\n10{\n\"name\": \"Get Food Item Details\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Create Meal Plan\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Update Food Item\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Track User Meal\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Get Dietary Recommendations\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Add New Food Item\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Delete Food Item\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Get User Nutritional Stats\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n}\n]\nSeed Instruction Creation Flow The seed instruction creation flow, consume the list of\nAPIs and employs variety of agents to create several tasks of the following types:\n1. Tasks that require the use of a single API:\n11(a) Tasks where the input supplies all necessary parameters.\n(b) Tasks where the input includes superfluous parameters.\n(c) Tasks where the input is missing some required parameters.\n2. Tasks that necessitate the use of multiple APIs:\n(a) Tasks where the input provides all necessary parameters.\n(b) Tasks where the input is missing some required parameters.\n3. Tasks that require a single API which is not listed among the available APIs.\n4.Tasks that require multiple APIs but lack some of the essential APIs in the provided\nlist.\nRefinement Flow The goal of the Refinement flow is to increase the complexity, for e.g.,\nlooking at the task and the conversation and to suggest refinements to increase the number\nof steps required to solve the task.\nThe following figure provides an example of a multi-turn conversation created by the Agent-\nInstruct flow.\nEXAMPLE: Instruction Data Created by AgentInstruct]\nSystem Message\nAs an AI assistant, your role is to assist users in achieving their desired outcomes. You have\nthe capability to utilize a variety of APIs, and at each step of the process, you are required\nto produce a markdown output.\nAt any given moment, you have two choices to proceed:\nChoice 1: Initiate an API Call If you choose to call an API, you must generate a makdown\nincluding the \"name\" of the API and the necessary \"parameters\" for the call. After creating\nthe response, you must wait for the system to execute the API with the provided parameters\nand return the results to you. The markdown you output should follow this structure:\nACTION_TYPE : API_CALL\nACTION: name(parameter_name=value, another_parameter_name=value, ....)\nChoice 2: Conclude the Process There are times when you may decide to end the process,\neither because a) you have successfully obtained the answer\nb) you have determined that the appropriate APIs are not available.\nc) you discovered that user did not provide values for some required parameter\nIn case you successfully obtained the answer use FINAL_ANSWER as the action name\nwith only one parameter being the final answer. In case you decided to stop due to other\nreasons, use FAILED as the action with one parameter describing the reason you decided to\nstop. Overall you should follow this structure:\nACTION_TYPE : STOP\nACTION: [ACTION_NAME]([VALUE])\nBelow is a list of APIs that you have access to:\n[\nAPIs from the Library construction example both the seed and the\ngenerated ones\n]\nUSER:\nI want to start a new diet plan for the next week. My goal is to consume around 1500\ncalories per day, and I prefer vegetarian meals. I\u2019ll be eating three meals a day. Can you\ncreate a meal plan for me? Also, I\u2019d like to track what I eat every day and get some new\nfood recommendations. At the end of the week, I want to see my nutritional summary. Oh,\nand I found this great recipe for a Quinoa Salad that I want to add to your database. I also\nnoticed that the calorie count for \u2018Chana Masala\u2019 seems off; it should be lower. And please\nremove \u2019Butter Chicken\u2019 from my list; I\u2019ve stopped eating it.\n12ASSISTANT:\nTo assist you with your request, I will perform the following actions:\n1. Create a meal plan based on your dietary preferences (vegetarian) and caloric goal (1500\ncalories per day) for three meals a day.\n2. Provide you with a way to track your daily meals.\n3. Offer you new food recommendations.\n4. Generate a nutritional summary at the end of the week. 5. Add the Quinoa Salad recipe\nto the database.\n6. Update the nutritional information for \u2019Chana Masala\u2019.\n7. Remove \u2019Butter Chicken\u2019 from the database.\nLet\u2019s start by creating a vegetarian meal plan for you with a caloric goal of 1500 calories\nper day and three meals a day.\n\u2018 \u2018 \u2018markdown\nACTION_TYPE : API_CALL\nACTION: Create Meal Plan(dietary_preferences=[\"vegetarian \"] ,\ncaloric_goal=1500, num_meals=3)\n\u2018 \u2018 \u2018\nUSER:\n{\n\"status\": \"success\",\n\"meal_plan\": {\n\"day_1\": {\n\"breakfast\": {\n\"","chunk_id":"0922646b93a124514ce2a267d961d229","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"SEARCH FOOD ITEMS","type":"API","description":"The Search Food Items API allows clients to search for food items by name and retrieve a list of matching items.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"GET FOOD ITEM DETAILS","type":"API","description":"The Get Food Item Details API retrieves detailed information about a specific food item.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"CREATE MEAL PLAN","type":"API","description":"The Create Meal Plan API generates a meal plan based on user dietary preferences and caloric goals.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"UPDATE FOOD ITEM","type":"API","description":"The Update Food Item API allows clients to modify the details of an existing food item in the database.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"TRACK USER MEAL","type":"API","description":"The Track User Meal API enables users to log their daily meals for tracking purposes.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"GET DIETARY RECOMMENDATIONS","type":"API","description":"The Get Dietary Recommendations API provides users with suggestions for food items based on their dietary preferences.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"ADD NEW FOOD ITEM","type":"API","description":"The Add New Food Item API allows clients to add a new food item to the database.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"DELETE FOOD ITEM","type":"API","description":"The Delete Food Item API enables clients to remove a food item from the database.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"GET USER NUTRITIONAL STATS","type":"API","description":"The Get User Nutritional Stats API retrieves nutritional information for a user based on their logged meals.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"SEED INSTRUCTION CREATION FLOW","type":"PROCESS","description":"The Seed Instruction Creation Flow is a process that utilizes a list of APIs to create various tasks for users.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"REFINEMENT FLOW","type":"PROCESS","description":"The Refinement Flow aims to increase task complexity by suggesting refinements based on user input and conversation context.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"USER","type":"PERSON","description":"The User is an individual seeking assistance in creating a diet plan and tracking their meals.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"ASSISTANT","type":"ROLE","description":"The Assistant is an AI entity designed to help users achieve their dietary goals by utilizing various APIs.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"CALORIC GOAL","type":"DATA FORMAT","description":"The caloric goal is a target number of calories that a user aims to consume daily as part of their diet plan.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"DIETARY PREFERENCES","type":"DATA FORMAT","description":"Dietary preferences refer to the specific dietary choices or restrictions a user has, such as vegetarianism.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"MEAL PLAN","type":"DATA FORMAT","description":"A meal plan is a structured outline of meals that a user intends to consume over a specified period, tailored to their dietary goals.","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"NUTRITIONAL SUMMARY","type":"DATA FORMAT","description":"A nutritional summary provides an overview of a user's nutritional intake over a specified period, summarizing calories and nutrients consumed.","source_id":"0922646b93a124514ce2a267d961d229"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"SEARCH FOOD ITEMS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">The Search Food Items API allows clients to search for food items by name and retrieve a list of matching items.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"GET FOOD ITEM DETAILS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">The Get Food Item Details API retrieves detailed information about a specific food item.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"CREATE MEAL PLAN\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">The Create Meal Plan API generates a meal plan based on user dietary preferences and caloric goals.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"UPDATE FOOD ITEM\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">The Update Food Item API allows clients to modify the details of an existing food item in the database.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"TRACK USER MEAL\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">The Track User Meal API enables users to log their daily meals for tracking purposes.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"GET DIETARY RECOMMENDATIONS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">The Get Dietary Recommendations API provides users with suggestions for food items based on their dietary preferences.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"ADD NEW FOOD ITEM\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">The Add New Food Item API allows clients to add a new food item to the database.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"DELETE FOOD ITEM\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">The Delete Food Item API enables clients to remove a food item from the database.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"GET USER NUTRITIONAL STATS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">The Get User Nutritional Stats API retrieves nutritional information for a user based on their logged meals.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"SEED INSTRUCTION CREATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The Seed Instruction Creation Flow is a process that utilizes a list of APIs to create various tasks for users.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"REFINEMENT FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The Refinement Flow aims to increase task complexity by suggesting refinements based on user input and conversation context.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"USER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">The User is an individual seeking assistance in creating a diet plan and tracking their meals.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"ASSISTANT\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">The Assistant is an AI entity designed to help users achieve their dietary goals by utilizing various APIs.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"CALORIC GOAL\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">The caloric goal is a target number of calories that a user aims to consume daily as part of their diet plan.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"DIETARY PREFERENCES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Dietary preferences refer to the specific dietary choices or restrictions a user has, such as vegetarianism.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"MEAL PLAN\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">A meal plan is a structured outline of meals that a user intends to consume over a specified period, tailored to their dietary goals.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"NUTRITIONAL SUMMARY\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">A nutritional summary provides an overview of a user's nutritional intake over a specified period, summarizing calories and nutrients consumed.<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <edge source=\"CREATE MEAL PLAN\" target=\"USER\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The User requests the creation of a meal plan based on their dietary preferences and caloric goals.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"CREATE MEAL PLAN\" target=\"SEED INSTRUCTION CREATION FLOW\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Seed Instruction Creation Flow includes creating a meal plan as one of its tasks.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"UPDATE FOOD ITEM\" target=\"USER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The User requests an update to the calorie count for 'Chana Masala', which is facilitated by the Update Food Item API.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"UPDATE FOOD ITEM\" target=\"SEED INSTRUCTION CREATION FLOW\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Seed Instruction Creation Flow includes updating food items as part of its tasks.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"TRACK USER MEAL\" target=\"USER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The User wants to track their daily meals, which is facilitated by the Track User Meal API.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"TRACK USER MEAL\" target=\"SEED INSTRUCTION CREATION FLOW\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Seed Instruction Creation Flow encompasses tracking user meals as part of its tasks.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"GET DIETARY RECOMMENDATIONS\" target=\"USER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The User is interested in receiving new food recommendations, which is provided by the Get Dietary Recommendations API.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"GET DIETARY RECOMMENDATIONS\" target=\"SEED INSTRUCTION CREATION FLOW\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Seed Instruction Creation Flow includes providing dietary recommendations as part of its tasks.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"ADD NEW FOOD ITEM\" target=\"USER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The User wants to add a new recipe (Quinoa Salad) to the database, which is done through the Add New Food Item API.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"ADD NEW FOOD ITEM\" target=\"SEED INSTRUCTION CREATION FLOW\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Seed Instruction Creation Flow includes adding new food items as part of its tasks.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"DELETE FOOD ITEM\" target=\"USER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The User wants to remove 'Butter Chicken' from their list, which is done through the Delete Food Item API.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"DELETE FOOD ITEM\" target=\"SEED INSTRUCTION CREATION FLOW\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Seed Instruction Creation Flow includes deleting food items as part of its tasks.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"GET USER NUTRITIONAL STATS\" target=\"USER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The User wants to see their nutritional summary at the end of the week, which is retrieved by the Get User Nutritional Stats API.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"GET USER NUTRITIONAL STATS\" target=\"SEED INSTRUCTION CREATION FLOW\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Seed Instruction Creation Flow includes retrieving user nutritional stats as part of its tasks.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"USER\" target=\"ASSISTANT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The Assistant interacts with the User to help them achieve their dietary goals using various APIs.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"USER\" target=\"CALORIC GOAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The User has a specific caloric goal of 1500 calories per day as part of their diet plan.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"USER\" target=\"DIETARY PREFERENCES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The User has specified dietary preferences, indicating they prefer vegetarian meals.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"USER\" target=\"MEAL PLAN\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The User requests a meal plan that includes three meals a day based on their dietary preferences and caloric goal.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"USER\" target=\"NUTRITIONAL SUMMARY\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The User wants to see a nutritional summary at the end of the week to assess their dietary adherence.<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"09cb89de3b77d765983cff25b7d74a1a","chunk":" database.\nLet\u2019s start by creating a vegetarian meal plan for you with a caloric goal of 1500 calories\nper day and three meals a day.\n\u2018 \u2018 \u2018markdown\nACTION_TYPE : API_CALL\nACTION: Create Meal Plan(dietary_preferences=[\"vegetarian \"] ,\ncaloric_goal=1500, num_meals=3)\n\u2018 \u2018 \u2018\nUSER:\n{\n\"status\": \"success\",\n\"meal_plan\": {\n\"day_1\": {\n\"breakfast\": {\n\"food_items\": [\"Oatmeal with Fruits\", \"Almond Milk\"],\n\"total_calories\": 350\n},\n\"lunch\": {\n\"food_items\": [\"Chickpea Salad\", \"Whole Wheat Bread\"],\n\"total_calories\": 500\n},\n\"dinner\": {\n\"food_items\": [\"Mixed Vegetable Stir Fry\", \"Brown Rice\"],\n\"total_calories\": 650\n}\n},\n\"day_2\": {\n...\n},\n...\n}\n}\nASSISTANT:\nYour meal plan for the next week has been successfully created. Here\u2019s a brief overview:\n- **Day 1**: Oatmeal with fruits and almond milk for breakfast, chickpea salad with whole\nwheat bread for lunch, and mixed vegetable stir fry with brown rice for dinner.\n- **Day 2**: .... ....\nNext, let\u2019s proceed to add the Quinoa Salad recipe to the database. Please provide the\nnutritional information for the Quinoa Salad so I can proceed with the addition.\nAs for updating the \u2019Chana Masala\u2019 and removing \u2019Butter Chicken\u2019, I will need\ntheir unique identifiers (food_id) to make those changes. If you have those identi-\nfiers, please provide them. If not, I can search for these items in the database to find\nthe correct identifiers. Would you like me to search for \u2019Chana Masala\u2019 and \u2019Butter Chicken\u2019?\nUSER:\n...(truncated)\n133 Orca-3\n3.1 Dataset Description\nWe implemented an AgentInstruct Flow for 17 different capabilities as described in Table 1.\nWe created a collection of approximately 22 million instructions aimed at teaching the\naforementioned skills. We have used unstructured text and code files sampled from\nKnowledgePile[ 7], AutoMathText[ 38], a subset of openstax and a subset of apache-2.0\nlicensed source code files from [ 4]. The dataset covers variety of skills, as detailed in Table 1.\nUsing unstructured content as seeds for instruction data generation has several benefits.\nFirst, there is abundance of such data enabling the generation of large-scale and diverse\ninstruction data. Additionally, it enables us to avoid using any benchmark-specific data as\nseeds and hence focus on optimizing for a capability, not for a specific benchmark.\nIn addition to the 22 million instructions, we have incorporated approximately 3.8 million\npaired instructions sourced from Orca-1[ 21], Orca-2[ 18], Orca-Math[ 19] and samples from\nother publicly available sources such as [ 5,37,10,30]. We refer to this data as Orca-2.5-\ndataset.\nThe culmination of these datasets results in approximately 25.8 million paired instructions,\nall of which are incorporated into the training of Orca-3. Furthermore, we have trained a\nseparate model, referred to as Orca-2.5, using the 3.8 million instructions (Orca-2.5-dataset).\nThe purpose of this is to compare and evaluate the impact of the 22 million instructions\ncurated through AgentInstruct.\n3.2 Training Details\nWe use the 25.8 million paired instructions described earlier to finetune Mistral-7b-v0.1.\nWe choose this model because the it makes the weights publicly available for the base\n(no-instruction-tuned) version, with a permissive license allowing easy redistribution. We\nrefer to the finetuned model ( Mistral-7b-v0.1 finetuned on AgentInstruct dataset) as Orca-3.\nEach pair in the dataset undergoes a tokenization process using the Mistral tokenizer,\nensuring a maximum sequence length of 8192 with packing. To guarantee that the training\nloss is calculated based only on the response conditioned on the prompt, label masking is\napplied. Weight decay was set at 0.1\nThe finetuneing used 19 NVIDIA A100 nodes, or 152 NVIDIA A100 GPUs, each with a\nbatch size of 10. We used AdamW optimizer with an initial learning rate of 8e-6 and a a\ncosine learning rate schedule. We also used a linear learning rate warm-up during the initial\n500 steps. The model was trained for three epochs and the training process concluded after\napproximately 200 hours.\n4 Evaluation Results\n4.1 Orca-Bench\nThe Orca-Bench dataset serves as a held-out test set, consisting of 100 samples from each of\nthe 17 skills for which data was curated using AgentInstruct, except for the Open Domain\nQuestion Answering (ODQA) category, where we created two test sets. The first subset,\nreferred to as ODQA, consists of 100 questions originated from the initial seed instruction\nphase. The second subset, termed Complex ODQA, includes more intricate questions\ndeveloped during the refinement phase.\nWe evaluated the performance of all baselines using the Orca-Bench dataset. These were\nscored relative to GPT-4 on a scale ranging from 0 to 10. It is worth noting that some of\nthe entries within Orca-Bench involve multiple exchanges. To illustrate, let a multi-turn\ninteraction in Orca-Bench be denoted by","chunk_id":"09cb89de3b77d765983cff25b7d74a1a","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"VEGETARIAN MEAL PLAN","type":"DIET","description":"A vegetarian meal plan designed to meet a caloric goal of 1500 calories per day, consisting of three meals each day.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"API_CALL","type":"ACTION","description":"An action type indicating a request to create a meal plan based on specified dietary preferences and caloric goals.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"USER","type":"PERSON","description":"The individual requesting the creation of a vegetarian meal plan and providing feedback on the meal plan's success.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"MEAL PLAN","type":"DATA STRUCTURE","description":"A structured representation of meals planned for each day, including breakfast, lunch, and dinner with their respective food items and total calories.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"DAY 1","type":"MEAL PLAN DAY","description":"The first day of the meal plan, detailing specific meals and their caloric content.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"DAY 2","type":"MEAL PLAN DAY","description":"The second day of the meal plan, detailing specific meals and their caloric content.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"QUINOA SALAD","type":"RECIPE","description":"A specific recipe that the user wishes to add to the database, requiring nutritional information for completion.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"CHANA MASALA","type":"RECIPE","description":"A recipe that the user wants to update in the database, requiring its unique identifier for modification.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"BUTTER CHICKEN","type":"RECIPE","description":"A recipe that the user wants to remove from the database, requiring its unique identifier for deletion.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"ORCA-3","type":"MODEL","description":"A machine learning model trained using a dataset of approximately 25.8 million paired instructions for various capabilities.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"AGENTINSTRUCT","type":"METHODOLOGY","description":"A methodology used to curate a large dataset of instructions aimed at teaching various skills.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"MISTRAL-7B-V0.1","type":"MODEL","description":"A base model used for finetuning with the AgentInstruct dataset, known for its publicly available weights.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"ORCA-BENCH","type":"DATASET","description":"A test dataset consisting of samples used to evaluate the performance of models trained with the AgentInstruct methodology.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"TOTAL CALORIES","type":"DATA FORMAT","description":"Total calories represent the caloric content of each meal, providing a breakdown of energy intake for the meal plan.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"NUTRITIONAL INFORMATION","type":"DATA FORMAT","description":"Nutritional information includes details about the caloric and nutritional content of specific recipes, such as the Quinoa Salad.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"INSTRUCTIONS","type":"DATA FORMAT","description":"Instructions are the specific guidelines or steps provided for preparing each recipe included in the meal plan.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"TOKENIZATION PROCESS","type":"PROCESS","description":"The tokenization process involves converting text data into tokens for processing by machine learning models.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"TRAINING DETAILS","type":"PROCESS","description":"Training details encompass the methodologies and parameters used to train machine learning models, including batch size and learning rates.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"EVALUATION RESULTS","type":"DATA FORMAT","description":"Evaluation results summarize the performance metrics and outcomes of models tested against specific datasets.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"FOOD ITEMS","type":"","description":"","source_id":"09cb89de3b77d765983cff25b7d74a1a"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"VEGETARIAN MEAL PLAN\">      <data key=\"d0\">DIET<\/data>      <data key=\"d1\">A vegetarian meal plan designed to meet a caloric goal of 1500 calories per day, consisting of three meals each day.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"API_CALL\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action type indicating a request to create a meal plan based on specified dietary preferences and caloric goals.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"USER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">The individual requesting the creation of a vegetarian meal plan and providing feedback on the meal plan's success.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"MEAL PLAN\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">A structured representation of meals planned for each day, including breakfast, lunch, and dinner with their respective food items and total calories.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"DAY 1\">      <data key=\"d0\">MEAL PLAN DAY<\/data>      <data key=\"d1\">The first day of the meal plan, detailing specific meals and their caloric content.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"DAY 2\">      <data key=\"d0\">MEAL PLAN DAY<\/data>      <data key=\"d1\">The second day of the meal plan, detailing specific meals and their caloric content.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"QUINOA SALAD\">      <data key=\"d0\">RECIPE<\/data>      <data key=\"d1\">A specific recipe that the user wishes to add to the database, requiring nutritional information for completion.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"CHANA MASALA\">      <data key=\"d0\">RECIPE<\/data>      <data key=\"d1\">A recipe that the user wants to update in the database, requiring its unique identifier for modification.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"BUTTER CHICKEN\">      <data key=\"d0\">RECIPE<\/data>      <data key=\"d1\">A recipe that the user wants to remove from the database, requiring its unique identifier for deletion.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A machine learning model trained using a dataset of approximately 25.8 million paired instructions for various capabilities.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">METHODOLOGY<\/data>      <data key=\"d1\">A methodology used to curate a large dataset of instructions aimed at teaching various skills.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"MISTRAL-7B-V0.1\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A base model used for finetuning with the AgentInstruct dataset, known for its publicly available weights.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"ORCA-BENCH\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A test dataset consisting of samples used to evaluate the performance of models trained with the AgentInstruct methodology.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"TOTAL CALORIES\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Total calories represent the caloric content of each meal, providing a breakdown of energy intake for the meal plan.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"NUTRITIONAL INFORMATION\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Nutritional information includes details about the caloric and nutritional content of specific recipes, such as the Quinoa Salad.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"INSTRUCTIONS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Instructions are the specific guidelines or steps provided for preparing each recipe included in the meal plan.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"TOKENIZATION PROCESS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The tokenization process involves converting text data into tokens for processing by machine learning models.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"TRAINING DETAILS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Training details encompass the methodologies and parameters used to train machine learning models, including batch size and learning rates.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"EVALUATION RESULTS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Evaluation results summarize the performance metrics and outcomes of models tested against specific datasets.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"FOOD ITEMS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"USER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The user requested the creation of a vegetarian meal plan tailored to their caloric needs.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"API_CALL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The API call is the action taken to create the vegetarian meal plan based on user specifications.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"USER\" target=\"QUINOA SALAD\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The user is involved in the process of adding the Quinoa Salad recipe to the database.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"USER\" target=\"CHANA MASALA\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The user is involved in updating the Chana Masala recipe in the database.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"USER\" target=\"BUTTER CHICKEN\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The user is involved in the process of removing the Butter Chicken recipe from the database.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"MEAL PLAN\" target=\"DAY 1\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Day 1 is a component of the overall meal plan, detailing specific meals and their caloric values.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"MEAL PLAN\" target=\"DAY 2\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Day 2 is a component of the overall meal plan, detailing specific meals and their caloric values.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"MEAL PLAN\" target=\"FOOD ITEMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The meal plan consists of various food items that make up each meal, detailing what is included in the diet.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"DAY 1\" target=\"TOTAL CALORIES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Day 1 includes a total caloric breakdown for each meal, providing insight into daily energy intake.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"DAY 2\" target=\"TOTAL CALORIES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Day 2 includes a total caloric breakdown for each meal, providing insight into daily energy intake.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"QUINOA SALAD\" target=\"NUTRITIONAL INFORMATION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Quinoa Salad recipe requires nutritional information to be added to the database for completeness.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"CHANA MASALA\" target=\"NUTRITIONAL INFORMATION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Chana Masala recipe requires nutritional information for updating in the database.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"BUTTER CHICKEN\" target=\"NUTRITIONAL INFORMATION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Butter Chicken recipe requires nutritional information for removal from the database.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AGENTINSTRUCT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Orca-3 is a model trained using the AgentInstruct methodology, which focuses on generating instruction data.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MISTRAL-7B-V0.1\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3 is a finetuned version of the Mistral-7B-v0.1 model, adapted for specific tasks using the AgentInstruct dataset.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ORCA-BENCH\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Orca-3 is evaluated using the Orca-Bench dataset to assess its performance across various skills.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"TRAINING DETAILS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Orca-3 was trained using specific training details that outline the process and parameters used during its development.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"EVALUATION RESULTS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The evaluation results provide insights into the performance of Orca-3 when tested against the Orca-Bench dataset.<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"bd4eb9459bc29b4c2da4658914fd4635","chunk":" instruction\nphase. The second subset, termed Complex ODQA, includes more intricate questions\ndeveloped during the refinement phase.\nWe evaluated the performance of all baselines using the Orca-Bench dataset. These were\nscored relative to GPT-4 on a scale ranging from 0 to 10. It is worth noting that some of\nthe entries within Orca-Bench involve multiple exchanges. To illustrate, let a multi-turn\ninteraction in Orca-Bench be denoted by the sequence (system message, user 1, assistant,\nuser 2, assistant, ...), where each turn is crafted by GPT4 (teacher). For every user iinput, we\ngenerateacorresponding studentresponse, which isconditionedontheprecedingconversation\nhistory as established by the teacher. We then evaluate the student\u2019s generated response\n14Figure 4: Performance Comparison between Baselines and Orca3 Checkpoints. Scores are\nrelative to GPT-4 with the outer circle denoting GPT-4\u2019s score of 10.\nagainst the original teacher\u2019s response, rating each on a scale from 0 to 10. To calculate a\nstudent\u2019s overall score, we sum the student\u2019s individual scores and divide this total by the\nsum of the teacher\u2019s scores. This ratio is then multiplied by 10 to normalize the student\u2019s\nfinal score to a 0 to 10 scale.\nAgentInstruct\u2019s objective is to synthesize a large and diverse corpus of data with varying\ndegrees of difficulty. An efficacious execution of this strategy should yield a dataset against\nwhich baseline models like Orca-2.5, Mistral-Instruct-7b, and ChatGPT score substantially\nbelow 10, demonstrating their relative inferiority to GPT-4\u2014a model that is designated as\nthe benchmark with a score of 10. The performance comparison, as depicted in Figure 4,\nillustrates the comparative analysis between baseline models and Orca-3 . This figure shows\nthe notable enhancement in a broad spectrum of capabilities during post-training, enabled\nby the AgentInstruct data.\nTable 2 encapsulates the average (macro) scores across all assessed dimensions. On average,\nincluding orca-3 after each training epoch, the inclusion of AgentInstruct data has led to\na performance augmentation of 33.94% over the Orca 2.5 baseline and an enhancement of\n14.92% over Mistral-Instruct-7B.\n4.2 Benchmark Results\nWe evaluate Orca-3 against 5 baseline models including Orca-2.5, Mistral-7B-Instruct-v0.3,\nLLAMA3-8B-Instruct, GPT-3.5-turbo and GPT-4 on the following benchmarks:\n15Model Orca-Bench Score\nOrca-2.5 7.13\nMistral-Instruct-7B 8.31\nChatGPT 8.13\nOrca-3 (checkpoint epoch 1) 9.35\nOrca-3 (checkpoint epoch 2) 9.49\nOrca-3 9.55\nTable 2: Average Performance of Different Models on Orca-Bench. Scores are computed on\na scale of 0 to 10; 10 being the score of GPT4.\nModelOrca-3\n-7BOrca-2.5\n-7BMistral-\n7B-\nInstructLLAMA3-\n8B\ninstructGPT-3.5-\nturboGPT-4\nAGIEval 56.80 (+40%) 42.71 40.52 47.17 50.91 61.99\nMMLU 69.95 (+19%) 60.34 58.61 63.44 68.26 67.07\nARC 92.47 (+12%) 86.39 82.72 85.74 92.0 93.35\nBBH 61.83 (+38%) 48.63 44.71 54.97 54.17 76.06\nGPQA 28.12 (-4%) 27.68 29.46 28.12 27.9 33.93\nDROP 71.14 (+22%) 65.19 58.12 68.44 67.15 67.36\nGSM8K 83.09 (+54%) 74.3 54.06 77.48 78.1\u221786.88\nFOFO 84.01 (+12%) 66.19 75.3 79.35 76.92 87.45\nIFEval 49.54 (+2%) 45.29 48.61 - 58.6 79.3\nMT-Bench 8.20 (+9%) 7.15 7.53 7.99 8.01 9.04\nAlpacaEval 24.80 (+45%) 13.47 17.1 22.9 22.7 55\nInfoBench 84.30 (+4%) 79.6 81 - 86.7 89.4\nEQBench\nMetric-v2 91.36(+4%) 88.03 87.75 88.67 88.95 93.32\nMetric-v1 50.28 (+28%) 38.8 39.27 42.13 42.05 55.98\nTable 3: Performance of Orca-3 and other baseline models on all the benchmarks. Note:\nGPT-3.5-turbo scores for GSM8","chunk_id":"bd4eb9459bc29b4c2da4658914fd4635","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"ORCA-BENCH","type":"DATASET","description":"Orca-Bench is a dataset used to evaluate the performance of various AI models, including GPT-4, on a scale from 0 to 10.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is a state-of-the-art AI model that serves as a benchmark for evaluating the performance of other models in the Orca-Bench dataset.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"AGENTINSTRUCT","type":"DATASET","description":"AgentInstruct is a dataset aimed at synthesizing a large and diverse corpus of data with varying degrees of difficulty for training AI models.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"ORCA-2.5","type":"MODEL","description":"Orca-2.5 is a baseline AI model evaluated against GPT-4 in the Orca-Bench dataset.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"MISTRAL-INSTRUCT-7B","type":"MODEL","description":"Mistral-Instruct-7B is a baseline AI model evaluated against GPT-4 in the Orca-Bench dataset.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"CHATGPT","type":"MODEL","description":"ChatGPT is a conversational AI model that is also evaluated in the Orca-Bench dataset.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"ORCA-3","type":"MODEL","description":"Orca-3 is an AI model that is evaluated against several baseline models in the Orca-Bench dataset.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"LLAMA3-8B","type":"MODEL","description":"LLAMA3-8B is a baseline AI model evaluated against GPT-4 in the Orca-Bench dataset.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"GPT-3.5-TURBO","type":"MODEL","description":"GPT-3.5-turbo is a baseline AI model evaluated against GPT-4 in the Orca-Bench dataset.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"PERFORMANCE COMPARISON","type":"ANALYSIS","description":"Performance comparison refers to the evaluation of different AI models based on their scores in the Orca-Bench dataset.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"MULTI-TURN INTERACTION","type":"CONCEPT","description":"Multi-turn interaction refers to a sequence of exchanges in a conversation, where multiple user inputs and assistant responses are involved.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"STUDENT RESPONSE","type":"RESPONSE","description":"Student response is the generated reply from an AI model, conditioned on the preceding conversation history established by the teacher model.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"TEACHER RESPONSE","type":"RESPONSE","description":"Teacher response is the original reply generated by the GPT-4 model, used as a benchmark for evaluating student responses.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"SCORE NORMALIZATION","type":"PROCESS","description":"Score normalization is the process of adjusting scores to a standard scale, in this case, converting student scores to a 0 to 10 scale.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"PERFORMANCE AUGMENTATION","type":"ANALYSIS","description":"Performance augmentation refers to the improvement in model performance as a result of incorporating additional training data, such as AgentInstruct.","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"COMPLEX ODQA","type":"","description":"","source_id":"bd4eb9459bc29b4c2da4658914fd4635"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ORCA-BENCH\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Orca-Bench is a dataset used to evaluate the performance of various AI models, including GPT-4, on a scale from 0 to 10.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is a state-of-the-art AI model that serves as a benchmark for evaluating the performance of other models in the Orca-Bench dataset.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">AgentInstruct is a dataset aimed at synthesizing a large and diverse corpus of data with varying degrees of difficulty for training AI models.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"ORCA-2.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-2.5 is a baseline AI model evaluated against GPT-4 in the Orca-Bench dataset.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"MISTRAL-INSTRUCT-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-Instruct-7B is a baseline AI model evaluated against GPT-4 in the Orca-Bench dataset.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"CHATGPT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">ChatGPT is a conversational AI model that is also evaluated in the Orca-Bench dataset.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 is an AI model that is evaluated against several baseline models in the Orca-Bench dataset.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"LLAMA3-8B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">LLAMA3-8B is a baseline AI model evaluated against GPT-4 in the Orca-Bench dataset.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5-turbo is a baseline AI model evaluated against GPT-4 in the Orca-Bench dataset.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"PERFORMANCE COMPARISON\">      <data key=\"d0\">ANALYSIS<\/data>      <data key=\"d1\">Performance comparison refers to the evaluation of different AI models based on their scores in the Orca-Bench dataset.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"MULTI-TURN INTERACTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Multi-turn interaction refers to a sequence of exchanges in a conversation, where multiple user inputs and assistant responses are involved.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"STUDENT RESPONSE\">      <data key=\"d0\">RESPONSE<\/data>      <data key=\"d1\">Student response is the generated reply from an AI model, conditioned on the preceding conversation history established by the teacher model.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"TEACHER RESPONSE\">      <data key=\"d0\">RESPONSE<\/data>      <data key=\"d1\">Teacher response is the original reply generated by the GPT-4 model, used as a benchmark for evaluating student responses.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"SCORE NORMALIZATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Score normalization is the process of adjusting scores to a standard scale, in this case, converting student scores to a 0 to 10 scale.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"PERFORMANCE AUGMENTATION\">      <data key=\"d0\">ANALYSIS<\/data>      <data key=\"d1\">Performance augmentation refers to the improvement in model performance as a result of incorporating additional training data, such as AgentInstruct.<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"COMPLEX ODQA\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <edge source=\"ORCA-BENCH\" target=\"GPT-4\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Orca-Bench is used to evaluate the performance of GPT-4, which serves as the benchmark model with a score of 10.<\/data>      <data key=\"d5\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"COMPLEX ODQA\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Orca-Bench dataset includes evaluations of complex questions, which are part of the Complex ODQA subset.<\/data>      <data key=\"d5\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"MULTI-TURN INTERACTION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Multi-turn interaction is a key feature in the Orca-Bench dataset, allowing for the evaluation of conversational AI models over multiple exchanges.<\/data>      <data key=\"d5\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"ORCA-3\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AgentInstruct data is used to enhance the performance of Orca-3, leading to significant improvements over baseline models.<\/data>      <data key=\"d5\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"PERFORMANCE AUGMENTATION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Performance augmentation is achieved through the integration of AgentInstruct data, enhancing the capabilities of models like Orca-3.<\/data>      <data key=\"d5\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-2.5\" target=\"ORCA-3\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca-3 is evaluated against Orca-2.5 to demonstrate its performance improvements in the Orca-Bench dataset.<\/data>      <data key=\"d5\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"MISTRAL-INSTRUCT-7B\" target=\"ORCA-3\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca-3 is evaluated against Mistral-Instruct-7B to demonstrate its performance improvements in the Orca-Bench dataset.<\/data>      <data key=\"d5\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"CHATGPT\" target=\"ORCA-3\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca-3 is evaluated against ChatGPT to demonstrate its performance improvements in the Orca-Bench dataset.<\/data>      <data key=\"d5\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"LLAMA3-8B\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca-3 is evaluated against LLAMA3-8B to demonstrate its performance improvements in the Orca-Bench dataset.<\/data>      <data key=\"d5\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GPT-3.5-TURBO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca-3 is evaluated against GPT-3.5-turbo to demonstrate its performance improvements in the Orca-Bench dataset.<\/data>      <data key=\"d5\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"PERFORMANCE COMPARISON\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Performance comparison illustrates the enhancements in Orca-3's capabilities during post-training, enabled by the AgentInstruct data.<\/data>      <data key=\"d5\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"STUDENT RESPONSE\" target=\"TEACHER RESPONSE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Student responses are evaluated against teacher responses to assess the performance of AI models in the Orca-Bench dataset.<\/data>      <data key=\"d5\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"STUDENT RESPONSE\" target=\"SCORE NORMALIZATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Score normalization is applied to student responses to standardize their scores on a 0 to 10 scale for comparison.<\/data>      <data key=\"d5\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"86f77e15d41cbd0cb33f635ccb2cb66b","chunk":"EQBench\nMetric-v2 91.36(+4%) 88.03 87.75 88.67 88.95 93.32\nMetric-v1 50.28 (+28%) 38.8 39.27 42.13 42.05 55.98\nTable 3: Performance of Orca-3 and other baseline models on all the benchmarks. Note:\nGPT-3.5-turbo scores for GSM8K are taken from [ 1]. We show in (+x%) the relative\nimprovement over Mistral-7b-Instruct.\n\u2022AGIEval : AGIEval [ 39] is a human-centric benchmark that evaluates a model\u2019s\nabilities in tasks pertinent to human-cognition and problem-solving. It evaluates\nhow well models perform in answering questions from human-centric standardized\nexams such as SAT, LSAT and math competitions.\n\u2022MMLU : Massive Multitask Language Understanding (MMLU) [ 9] benchmark\nmeasures a model\u2019s multitask understanding. The benchmark includes approximately\n16000 multiple choice questions covering a wide range of 57 academic subjects such\nas maths, philosphy, medicine, psychology, computer-science, law etc. testing both\ngeneral and specialized knowledge of the model being tested.\n\u2022ARC: The AI2 Reasoning Challenge (ARC) [ 2] benchmark, developed by AllenAI,\nmeasures the reasoning, commonsense knowledge and deep comprehension abilities\nof language models. The test set contains 3548 multiple-choice questions that are\ndivided into 2 sets : Easy(2376) and Challenge(1172).\n\u2022BBH: Big Bench Hard [ 31] consists of a set of 23 tasks selected from the broader\nBig-Bench benchmark spanning a wide array of academic subjects requiring complex,\nmulti-step reasoning.\n\u2022GPQA: Graduate-level Google-Proof Q&A [ 27] is a challenging benchmark of 448\nhigh-quality and extremely difficult multiple-choice questions created by domain\nexperts(pursuing PhDs in their domains) in biology, chemistry and physics.\n\u2022DROP: Discrete Reasoning over Paragraphs [ 6] is a Reading Comprehension bench-\nmark requiring the models to resolve references in questions and perform discrete\noperations over them such as sorting, counting, addition etc.\n16\u2022GSM8K : Grade School Math 8K [ 3] is a dataset of high quality diverse grade\nschool math word problems. The test split of the dataset consists of 1.32K problems\nrequiring between 2 and 8 steps to solve primarily involving sequence of elementary\ncalculations using basic arithmetic operations.\n\u2022FoFo: Format Following [ 34] is a benchmark that evaluates a model\u2019s ability to\nfollow complex, domain-specific formats. The benchmark tests format following on\na diverse range of real-world formats and instructions from domains like Healthcare,\nFinance, Marketing etc. created using AI-Human collaboration.\n\u2022IFEval: Instruction-Following Evaluation [ 40] is a benchmark measuring a model\u2019s\nability to follow natural language instructions using a set of 500 prompts covering\n25 types of \u2019verifiable instructions\u2019 where each prompt can contain one or more of\nthese instructions.\n\u2022MT-Bench : MT-Bench [ 16] benchmark is specifically designed to assess the com-\npetence of chat assistants in multi-turn conversations using GPT-4 as the evaluator.\n\u2022AlpacaEval : AlpacaEval [ 14] is a benchmark specifically designed for chat-based\nlanguage models to assess their abilities in the context of instruction-following tasks.\nIt is a single-turn benchmark consisting of 805 instructions representative of user\ninteractions on Alpaca web demo.\n\u2022InFoBench : The InFoBench [ 25] benchmark evaluates models instruction fol-\nlowing capability using a new metric called Decomposed Requirements Following\nRatio(DRFR). DRFR breaks complex instructions down into simpler criteria and\nfacilitates analysis of an LLM\u2019s compliance to these decomposed tasks in detail.\nThe benchmark has 500 diverse instructions and 2250 decomposed questions across\nmultiple constraint categories.\n\u2022EQBench : This Emotional Intelligence benchmark [ 23] evaluates aspects of emo-\ntional intelligence in language models. It tests models capabilities to comprehend\nintricate emotions and social interactions by providing a conversation between char-\nacters and then asking the model to predict intensity of emotional states of those\ncharacters. The authors discovered a strong correlation (r=0.97) between EQ-Bench\nand comprehensive multi-domain benchmarks like MMLU.\nThe results for all the baselines on each benchmark are given in table 3. All of the evaluations\nfor Orca-3 and other baselines was done in a zero-shot setting unless mentioned otherwise in\nthe text.\nThe types of tasks\/benchmarks and the corresponding method used to extract answer and\ngenerate metrics is specified in Appendix B.\n4.3 Evaluation: Reading Comprehension\nReading comprehension is a crucial capability for LLMs. It is arguably even more important\nfor Small Language Models (SLMs), as they are better suited as reasoning engines than mere\nretrieval systems. Through targeted training with AgentInstruct, we observe substantial\nimprovement in Mistral\u2019s reading comprehension capabilities (Table ??)\u2014showcasing an 18%\nimprovement over Orca 2.5 and a 21% gain relative to Mistral-Instruct-7b. Furthermore, by\nleveraging this data-driven approach, we have elevated the performance of a 7B model to\nmatch that of GPT-4 on the reading comprehension sections of the Law School Admission\nTests (LSATs), which are considered difficult for human test-takers.\n4.4 Evaluation: Math","chunk_id":"86f77e15d41cbd0cb33f635ccb2cb66b","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"EQBENCH","type":"BENCHMARK","description":"EQBench is an emotional intelligence benchmark that evaluates language models' abilities to comprehend emotions and social interactions through conversation analysis.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"MMLU","type":"BENCHMARK","description":"Massive Multitask Language Understanding (MMLU) is a benchmark that measures a model's multitask understanding across various academic subjects through multiple-choice questions.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"AGIEVAL","type":"BENCHMARK","description":"AGIEval is a human-centric benchmark that assesses a model's performance in tasks related to human cognition and problem-solving, including standardized exams.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"ARC","type":"BENCHMARK","description":"The AI2 Reasoning Challenge (ARC) is a benchmark developed by AllenAI that evaluates reasoning and comprehension abilities of language models through multiple-choice questions.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"BBH","type":"BENCHMARK","description":"Big Bench Hard (BBH) is a benchmark consisting of tasks that require complex, multi-step reasoning across various academic subjects.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"GPQA","type":"BENCHMARK","description":"Graduate-level Google-Proof Q&A (GPQA) is a challenging benchmark with difficult multiple-choice questions created by domain experts in science fields.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"DROP","type":"BENCHMARK","description":"Discrete Reasoning over Paragraphs (DROP) is a reading comprehension benchmark that requires models to resolve references and perform operations on text.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"GSM8K","type":"DATASET","description":"Grade School Math 8K (GSM8K) is a dataset of diverse grade school math word problems requiring multiple steps to solve.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"FOFO","type":"BENCHMARK","description":"Format Following (FoFo) is a benchmark that evaluates a model's ability to follow complex, domain-specific formats across various real-world applications.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"IFEVAL","type":"BENCHMARK","description":"Instruction-Following Evaluation (IFEval) measures a model's ability to follow natural language instructions through a set of prompts.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"MT-BENCH","type":"BENCHMARK","description":"MT-Bench is a benchmark designed to assess chat assistants' competence in multi-turn conversations using GPT-4 as the evaluator.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"ALPACA EVAL","type":"BENCHMARK","description":"AlpacaEval is a benchmark for chat-based language models to evaluate their instruction-following abilities in single-turn interactions.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"INFOBENCH","type":"BENCHMARK","description":"InFoBench evaluates models' instruction-following capabilities using a metric called Decomposed Requirements Following Ratio (DRFR).","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"MISTRAL-7B-INSTRUCT","type":"MODEL","description":"Mistral-7B-Instruct is a language model used as a baseline for comparison in various benchmarks, particularly in reading comprehension tasks.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"MISTRAL","type":"MODEL","description":"Mistral is a language model that has shown improvements in reading comprehension capabilities through targeted training.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is a state-of-the-art language model used as a benchmark for evaluating the performance of other models in various tasks.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"ORCA-3","type":"","description":"","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"EQBENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">EQBench is an emotional intelligence benchmark that evaluates language models' abilities to comprehend emotions and social interactions through conversation analysis.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Massive Multitask Language Understanding (MMLU) is a benchmark that measures a model's multitask understanding across various academic subjects through multiple-choice questions.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"AGIEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AGIEval is a human-centric benchmark that assesses a model's performance in tasks related to human cognition and problem-solving, including standardized exams.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">The AI2 Reasoning Challenge (ARC) is a benchmark developed by AllenAI that evaluates reasoning and comprehension abilities of language models through multiple-choice questions.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"BBH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Big Bench Hard (BBH) is a benchmark consisting of tasks that require complex, multi-step reasoning across various academic subjects.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"GPQA\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Graduate-level Google-Proof Q&amp;A (GPQA) is a challenging benchmark with difficult multiple-choice questions created by domain experts in science fields.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Discrete Reasoning over Paragraphs (DROP) is a reading comprehension benchmark that requires models to resolve references and perform operations on text.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Grade School Math 8K (GSM8K) is a dataset of diverse grade school math word problems requiring multiple steps to solve.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"FOFO\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Format Following (FoFo) is a benchmark that evaluates a model's ability to follow complex, domain-specific formats across various real-world applications.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"IFEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Instruction-Following Evaluation (IFEval) measures a model's ability to follow natural language instructions through a set of prompts.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"MT-BENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MT-Bench is a benchmark designed to assess chat assistants' competence in multi-turn conversations using GPT-4 as the evaluator.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"ALPACA EVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AlpacaEval is a benchmark for chat-based language models to evaluate their instruction-following abilities in single-turn interactions.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"INFOBENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">InFoBench evaluates models' instruction-following capabilities using a metric called Decomposed Requirements Following Ratio (DRFR).<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"MISTRAL-7B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-7B-Instruct is a language model used as a baseline for comparison in various benchmarks, particularly in reading comprehension tasks.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"MISTRAL\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral is a language model that has shown improvements in reading comprehension capabilities through targeted training.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is a state-of-the-art language model used as a benchmark for evaluating the performance of other models in various tasks.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <edge source=\"EQBENCH\" target=\"MMLU\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">EQBench shows a strong correlation with MMLU, indicating that emotional intelligence is related to multitask understanding in language models.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"AGIEVAL\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">AGIEval assesses cognitive abilities that may overlap with the multitask understanding measured by MMLU.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"ARC\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">ARC evaluates reasoning abilities that are essential for multitask understanding, as measured by MMLU.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"BBH\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">BBH requires complex reasoning, which is a component of multitask understanding evaluated by MMLU.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"GPQA\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">GPQA's challenging questions test specialized knowledge, which is part of the multitask understanding assessed by MMLU.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"DROP\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">DROP's focus on reading comprehension contributes to the overall multitask understanding evaluated by MMLU.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"GSM8K\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">GSM8K's math problems require reasoning skills that are relevant to the multitask understanding measured by MMLU.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"FOFO\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">FoFo's evaluation of format following may relate to the structured understanding required in MMLU assessments.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"IFEVAL\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">IFEval's focus on instruction-following is relevant to the comprehension skills assessed by MMLU.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"MT-BENCH\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">MT-Bench assesses conversational competence, which may relate to the understanding required in MMLU.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"ALPACA EVAL\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">AlpacaEval's focus on instruction-following tasks is relevant to the multitask understanding evaluated by MMLU.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"INFOBENCH\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">InFoBench's evaluation of instruction-following capabilities contributes to the overall understanding assessed by MMLU.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT\" target=\"ORCA-3\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3's performance is compared to Mistral-7B-Instruct across various benchmarks, highlighting differences in capabilities.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT\" target=\"MISTRAL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Mistral's improvements in reading comprehension are evaluated against the baseline of Mistral-7B-Instruct.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MISTRAL\" target=\"GPT-4\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Mistral's reading comprehension capabilities are compared to those of GPT-4, indicating advancements in model performance.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"ORCA-3\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3's performance is assessed in relation to GPT-4, which serves as a high standard for language model capabilities.<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"bb87f82e6a9f1d4da6480ec78a0e3701","chunk":"Table ??)\u2014showcasing an 18%\nimprovement over Orca 2.5 and a 21% gain relative to Mistral-Instruct-7b. Furthermore, by\nleveraging this data-driven approach, we have elevated the performance of a 7B model to\nmatch that of GPT-4 on the reading comprehension sections of the Law School Admission\nTests (LSATs), which are considered difficult for human test-takers.\n4.4 Evaluation: Math\nAssessing the reasoning capabilities of AI models can be effectively accomplished through\nmath problem solving. While SLMs have shown considerable improvement in elementary\nmath, their performance typically falters with more complex high school and college-level\nmathematics. Math problems are generated by the Open Domain Question Answering\nand Multiple-Choice Questions Flows. With AgentInstruct, we have managed to enhance\nMistral\u2019s proficiency across a spectrum of difficulties, ranging from elementary to college-level\nmath (Table 5. This has led to a signficant performance boost, with improvements ranging\n17Model Orca-3\n-7BOrca-2.5\n-7BMistral-\n7B-InstructGPT-3.5-\nturboGPT-4\nAGIEval lsat-rc75.84\n(+21%,+20%)62.45 63.2 63.57 72.86\nAGIEval sat-en87.38\n(+13%,+15%)77.18 75.73 82.04 82.52\nAGIEval\ngaokao-english87.25\n(+13%,+17%)77.45 74.84 83.01 87.25\nAGIEval lsat-lr63.14\n(+45%,+36%)43.53 46.27 54.9 68.82\nDROP71.14\n(+9%,+22%)65.19 58.12 67.15 67.36\nAverage76.95\n(+18%,+21%)65.16 63.63 70.13 75.76\nTable 4: Performance of models on reading comprehension based sub-tasks and benchmarks.\nThe figures (x%, y%) adjacent to the Orca-3 results signify the percentage of improvement\ncompared to Orca 2.5 and Mistral-7B-Instruct, respectively.\nModel Orca-3\n-7BOrca-2.5\n-7BMistral-\n7B-InstructGPT-3.5-\nturboGPT-4\nAGIEval math42.90\n(+73%,+168%)24.8 16.0 38.0 57.9\nAGIEval sat-math80.91\n(+34%,+50%)60.45 54.09 67.73 90.0\nBBH multistep\n-arithmetic-two66.80\n(+1418%,+882%)4.4 6.8 46.4 77.2\nMMLU abstract\nalgebra55.00\n(+129%,+104%)24.0 27.0 47.0 70.0\nMMLU college\nmathematics44.00\n(+63%,+44%)30.0 34.0 39.0 62.0\nMMLU high-school\nmathematics66.67\n(+41%,+94%)47.41 34.44 57.04 66.67\nGSM8K83.09\n(+12%,+54%)74.3 54.06 78.1\u221786.88\nTable 5: Performance scores of models on Math benchmarks. Note: GPT-3.5-turbo accuracy\nscores reported for GSM8K are taken from Phi3 paper[ 1]. The figures (x%, y%) adjacent\nto the Orca-3 results signify the percentage of improvement compared to Orca 2.5 and\nMistral-7B-Instruct, respectively.\nfrom 44% to 168% on various popular mathematical benchmarks. It should be emphasized\nthat the objective of Generative Teaching is to teach a skill than generating data to meet a\nspecific benchmark. The effectiveness of AgentInstruct for Generative Teaching is evidenced\nby marked enhancements across a variety of mathematical datasets.\n4.5 Evaluation: Format Following\nFollowing formatting guidelines is essential for language models to be applicable in real-world\nsituations. In all AgentInstruct flows, we ensure that format-following is taught for each\nparticular scenario, by synthesizing, through agents, several formatting guidelines. By doing\nso, we are able to significantly improve (11.5%) Mistral\u2019s ability to follow formats, surpassing\neven the capabilities of Gemini Pro.\n18Model FoFo\nOpen-sourceOrca-3-7B 84.01 (+26.92%,+11.5%)\nOrca-2.5-7B 66.19\nMistral-7B-Instruct 75.3\nClosed-sourceGPT-3.5-turbo 76.92\nGemini Pro 80.25\u2217\nGPT-4 87.45\nTable 6: Performance of Orca-3-7B model and other open and closed-source baselines on\nFoFo benchmark. The figures (x%, y%) adjacent to the Orca-3 results signify the percentage\nof improvement compared to Orca 2.5 and Mistral-7B-Instruct, respectively. Note: The\nscores for Gemini Pro are taken from the original paper [34]\n4.6 Evaluation: Abstractive","chunk_id":"bb87f82e6a9f1d4da6480ec78a0e3701","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"ORCA-3","type":"MODEL","description":"Orca-3 is a 7B model that has shown significant improvements in performance over previous models like Orca 2.5 and Mistral-7B-Instruct, particularly in reading comprehension and math problem-solving tasks.","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"ORCA-2.5","type":"MODEL","description":"Orca-2.5 is a previous version of the Orca model, serving as a baseline for performance comparisons with newer models like Orca-3.","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"MISTRAL-7B-INSTRUCT","type":"MODEL","description":"Mistral-7B-Instruct is another AI model that has been compared against Orca-3 and Orca-2.5 in various performance evaluations, particularly in reading comprehension and math.","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is a state-of-the-art language model known for its advanced capabilities, particularly in reading comprehension and problem-solving tasks.","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"AGIEVAL","type":"EVALUATION METRIC","description":"AGIEval is a benchmark used to evaluate the performance of AI models on reading comprehension and math tasks, providing comparative scores across different models.","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"MMLU","type":"EVALUATION METRIC","description":"MMLU is a benchmark for assessing the performance of AI models in various academic subjects, including abstract algebra and college mathematics.","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"AGENTINSTRUCT","type":"TECHNIQUE","description":"AgentInstruct is a method used to enhance the proficiency of AI models across various tasks, including math problem-solving and format-following.","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"GENERATIVE TEACHING","type":"TECHNIQUE","description":"Generative Teaching is an approach aimed at teaching skills rather than merely generating data to meet benchmarks, focusing on effective learning outcomes.","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"FOFO","type":"EVALUATION METRIC","description":"FoFo is a benchmark used to evaluate the format-following capabilities of AI models in real-world applications.","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"ELEMENTARY MATH","type":"SUBJECT","description":"Elementary math refers to basic mathematical concepts and skills typically taught at the primary education level, where AI models have shown considerable improvement.","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"HIGH SCHOOL MATHEMATICS","type":"SUBJECT","description":"High school mathematics encompasses more complex mathematical concepts taught at the secondary education level, where AI models often struggle.","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"COLLEGE MATHEMATICS","type":"SUBJECT","description":"College mathematics includes advanced mathematical topics taught at the tertiary education level, presenting challenges for AI models.","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"LSATS","type":"EXAM","description":"The Law School Admission Tests (LSATs) are standardized tests used for law school admissions, known for their difficulty, particularly in reading comprehension.","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"GSM8K","type":"EVALUATION METRIC","description":"GSM8K is a benchmark used to evaluate the performance of AI models on math problems, particularly in multi-step arithmetic tasks.","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"MATH PROBLEMS","type":"","description":"","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 is a 7B model that has shown significant improvements in performance over previous models like Orca 2.5 and Mistral-7B-Instruct, particularly in reading comprehension and math problem-solving tasks.<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"ORCA-2.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-2.5 is a previous version of the Orca model, serving as a baseline for performance comparisons with newer models like Orca-3.<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"MISTRAL-7B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-7B-Instruct is another AI model that has been compared against Orca-3 and Orca-2.5 in various performance evaluations, particularly in reading comprehension and math.<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is a state-of-the-art language model known for its advanced capabilities, particularly in reading comprehension and problem-solving tasks.<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"AGIEVAL\">      <data key=\"d0\">EVALUATION METRIC<\/data>      <data key=\"d1\">AGIEval is a benchmark used to evaluate the performance of AI models on reading comprehension and math tasks, providing comparative scores across different models.<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">EVALUATION METRIC<\/data>      <data key=\"d1\">MMLU is a benchmark for assessing the performance of AI models in various academic subjects, including abstract algebra and college mathematics.<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">AgentInstruct is a method used to enhance the proficiency of AI models across various tasks, including math problem-solving and format-following.<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"GENERATIVE TEACHING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Generative Teaching is an approach aimed at teaching skills rather than merely generating data to meet benchmarks, focusing on effective learning outcomes.<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"FOFO\">      <data key=\"d0\">EVALUATION METRIC<\/data>      <data key=\"d1\">FoFo is a benchmark used to evaluate the format-following capabilities of AI models in real-world applications.<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"ELEMENTARY MATH\">      <data key=\"d0\">SUBJECT<\/data>      <data key=\"d1\">Elementary math refers to basic mathematical concepts and skills typically taught at the primary education level, where AI models have shown considerable improvement.<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"HIGH SCHOOL MATHEMATICS\">      <data key=\"d0\">SUBJECT<\/data>      <data key=\"d1\">High school mathematics encompasses more complex mathematical concepts taught at the secondary education level, where AI models often struggle.<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"COLLEGE MATHEMATICS\">      <data key=\"d0\">SUBJECT<\/data>      <data key=\"d1\">College mathematics includes advanced mathematical topics taught at the tertiary education level, presenting challenges for AI models.<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"LSATS\">      <data key=\"d0\">EXAM<\/data>      <data key=\"d1\">The Law School Admission Tests (LSATs) are standardized tests used for law school admissions, known for their difficulty, particularly in reading comprehension.<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">EVALUATION METRIC<\/data>      <data key=\"d1\">GSM8K is a benchmark used to evaluate the performance of AI models on math problems, particularly in multi-step arithmetic tasks.<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"MATH PROBLEMS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <edge source=\"ORCA-3\" target=\"ORCA-2.5\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3 has shown an 18% improvement over Orca 2.5 in reading comprehension tasks, indicating a direct performance comparison.<\/data>      <data key=\"d5\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MISTRAL-7B-INSTRUCT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3 has demonstrated a 21% gain in performance relative to Mistral-7B-Instruct, highlighting its advancements in AI capabilities.<\/data>      <data key=\"d5\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GPT-4\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Orca-3's performance on reading comprehension sections of the LSATs has been elevated to match that of GPT-4, indicating a competitive standing among models.<\/data>      <data key=\"d5\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AGIEVAL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">AGIEval provides performance scores for Orca-3, showcasing its effectiveness in reading comprehension and math tasks.<\/data>      <data key=\"d5\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MMLU\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">MMLU benchmarks are used to evaluate Orca-3's performance in academic subjects, including mathematics, demonstrating its capabilities in complex problem-solving.<\/data>      <data key=\"d5\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AGENTINSTRUCT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AgentInstruct has been utilized to enhance Orca-3's proficiency across various difficulties in math, indicating a direct application of the technique.<\/data>      <data key=\"d5\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"LSATS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Orca-3's performance on LSAT reading comprehension sections has been elevated to match that of GPT-4, indicating its effectiveness in tackling difficult exams.<\/data>      <data key=\"d5\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GSM8K\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">GSM8K benchmarks are used to evaluate Orca-3's performance on multi-step arithmetic tasks, showcasing its capabilities in math problem-solving.<\/data>      <data key=\"d5\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT\" target=\"FOFO\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">FoFo benchmarks assess the format-following capabilities of Mistral-7B-Instruct, indicating its applicability in real-world scenarios.<\/data>      <data key=\"d5\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"GENERATIVE TEACHING\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Generative Teaching is evidenced by the effectiveness of AgentInstruct in improving learning outcomes across mathematical datasets.<\/data>      <data key=\"d5\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"MATH PROBLEMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AgentInstruct enhances the ability of AI models to solve math problems across various difficulty levels, indicating a direct application of the technique.<\/data>      <data key=\"d5\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ELEMENTARY MATH\" target=\"MATH PROBLEMS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Elementary math problems are designed to assess basic mathematical skills, where AI models have shown significant improvement.<\/data>      <data key=\"d5\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"HIGH SCHOOL MATHEMATICS\" target=\"MATH PROBLEMS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">High school mathematics problems present challenges for AI models, indicating areas where performance typically falters.<\/data>      <data key=\"d5\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"COLLEGE MATHEMATICS\" target=\"MATH PROBLEMS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">College mathematics problems are complex and often difficult for AI models, highlighting the need for advanced reasoning capabilities.<\/data>      <data key=\"d5\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"8ee9617c145e19fa95f1f9349bfbe69b","chunk":"-4 87.45\nTable 6: Performance of Orca-3-7B model and other open and closed-source baselines on\nFoFo benchmark. The figures (x%, y%) adjacent to the Orca-3 results signify the percentage\nof improvement compared to Orca 2.5 and Mistral-7B-Instruct, respectively. Note: The\nscores for Gemini Pro are taken from the original paper [34]\n4.6 Evaluation: Abstractive Summarization\nSummarization is an important capability for Language Models, with many models achieving\nhigh quality summarization performance, yet struggling with hallucination. We assessed\nsummarization ability using two key metrics: hallucinations and quality. For this purpose,\nwe utilized GPT4 as our evaluator. The prompts utilized in these evaluations can be found\nin Appendix B. We used the following benchmarks for evaluating summarization abilities:\n\u2022ACI-Bench: The Ambient Clinical Intelligence Benchmark (ACI-Bench) [ 32] is a\ndataset designed for benchmarking automatic report generation from doctor-patient\nconversations. The test set comprises 120 data points.\n\u2022InstruSum: A dataset [ 15] for evaluating the generation capabilities LLMs for\ninstruction-controllable summarization. It consists of 100 datapoints.\n\u2022Orca-Sum: A newly created benchmark to evaluate LLMs\u2019 ability to follow summa-\nrization and grounded data transformation instructions. To construct this test set,\nwe sampled data from 45 summarization datasets collected from Hugging Face across\nmultiple domains such as news, conversations, science, health, social, e-mails, code,\netc. for a total of 458 datapoints. We randomly collected, up to 1000 datapoints\nwhich then we carefully deduplicated to avoid overlapping with the training set. We\nthen used GPT-4 to generate a set of 40 prompts for each dataset out of each we\nrandomly sampled one for each selected datapoint. The prompts are dataset-specific\nand focus on summarization, grounding, and data transformation. For instance, a\nprompt may ask the model to generate a TikTok video out of a scientific paper or\na legal contract from a Wikipedia page. This allows us to measure not only the\nquality of the response but also hallucination in a challenging scenario, as the model\nis forced to move between formats and domains.\nThe results are presented in Table 7. With the AgentInstruct approach, we successfully\nachieved a reduction in hallucinations by 31.34%, while attaining a quality level comparable\nto GPT4 (Teacher).\n4.7 Evaluation: RAG\nThe RAG (Retrieval Augmented Generation) skill significantly boosts the capacity of Lan-\nguage Models to generate informed, contextually precise responses, hence upgrading their\noverall performance and usefulness. It is arguably more effective to test the RAG proficiency\nof language models in areas where the models have limited knowledge. For this study, we\nselected MIRAGE[ 35], a benchmark that focuses on answering medical questions by referring\nto information retrieved from a medical corpus. Since the medical domain is not typically a\nprimary focus of the models evaluated in this study, MIRAGE provides an effective platform\nfor assessing their RAG capabilities. Additionally, AgentInstruct RAG data used generic,\nnon medical data seed, enabling us to test how well can the skill (RAG) be applied to new\ndomains.\n19ModelOrca-3\n-7BOrca-2.5\n-7BMistral-\n7B-InstructLLAMA3-\n8B-instructGPT-3.5-\nturboGPT-4\nHallucination Rate (%) - Smaller is better\nAll (micro) 21.09\n(-26.12%,\n-31.34%)28.55 30.72 34.22 21.13 15.07\nOrca-Sum 28.17 36.84 39.61 38.43 28.60 21.66\nInstruSum 9.00 12.00 17.00 25.00 12.00 1.00\nACI-Bench 4.20 10.83 8.30 25.83 1.70 1.70\nQuality Score (1-10) - Higher is better\nAll (micro) 9.14\n(+7.91%,\n+3.28%)8.47 8.85 9.00 8.69 9.08\nOrca-Sum 8.95 8.27 8.61 8.90 8.32 8.61\nInstruSum 9.17 8.30 9.16 9.21 9.27 9.31\nACI-Bench 9.72 9.39 9.48 9.23 9.60 9.70\nTable 7: Hallucination rates and quality scores evaluated by GPT4. The figures (x%, y%)\nadjacent to the Orca-3 results signify the percentage of improvement compared to Orca v2.5\nand Mistral-7B-Instruct, respectively.\nMIRAGE Datasets\nMMLU-\nMedMedQA-\nUSMedMCQA PubMedQA BioASQ Avg.\nGPT-4\n(0613)CoT 89.44 83.97 69.88 39.6 84.3 73.44\nRAG87.24 82.8 66.65 70.6 92.56 79.97\nGPT-3.5-turbo\n(","chunk_id":"8ee9617c145e19fa95f1f9349bfbe69b","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"ORCA-3-7B","type":"MODEL","description":"Orca-3-7B is a language model evaluated for its performance on various summarization benchmarks, showing improvements over previous versions like Orca 2.5 and Mistral-7B-Instruct.","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"ORCA-2.5","type":"MODEL","description":"Orca-2.5 is a previous version of the Orca language model, used as a baseline for comparison in performance evaluations.","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"MISTRAL-7B-INSTRUCT","type":"MODEL","description":"Mistral-7B-Instruct is another language model that serves as a baseline for evaluating the performance of Orca-3-7B.","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is a state-of-the-art language model used as an evaluator for summarization performance, known for its high quality and low hallucination rates.","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"HALLUCINATION RATE","type":"METRIC","description":"Hallucination rate is a metric used to measure the frequency of incorrect or fabricated information generated by language models during summarization tasks.","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"QUALITY SCORE","type":"METRIC","description":"Quality score is a metric that evaluates the overall quality of the summarization output from language models, rated on a scale from 1 to 10.","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"ACI-BENCH","type":"DATASET","description":"ACI-Bench is a dataset designed for benchmarking automatic report generation from doctor-patient conversations, consisting of 120 data points.","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"INSTRUSUM","type":"DATASET","description":"InstruSum is a dataset for evaluating instruction-controllable summarization capabilities of language models, containing 100 data points.","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"ORCA-SUM","type":"DATASET","description":"Orca-Sum is a benchmark created to evaluate language models' ability to follow summarization and grounded data transformation instructions, with a total of 458 data points.","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"MIRAGE","type":"DATASET","description":"MIRAGE is a benchmark focusing on answering medical questions by referring to information retrieved from a medical corpus, used to assess RAG capabilities of language models.","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"DATA TRANSFORMATION","type":"PROCESS","description":"Data transformation refers to the process of converting data from one format or structure into another, often used in conjunction with summarization tasks.","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"HUGGING FACE","type":"PLATFORM","description":"Hugging Face is a platform that hosts various datasets and models for natural language processing, including those used for summarization tasks.","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"GPT-3.5-TURBO","type":"MODEL","description":"GPT-3.5-turbo is a variant of the GPT-3 model, used in evaluations alongside other models for summarization performance.","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"AGENTINSTRUCT","type":"","description":"","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ORCA-3-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3-7B is a language model evaluated for its performance on various summarization benchmarks, showing improvements over previous versions like Orca 2.5 and Mistral-7B-Instruct.<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"ORCA-2.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-2.5 is a previous version of the Orca language model, used as a baseline for comparison in performance evaluations.<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"MISTRAL-7B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-7B-Instruct is another language model that serves as a baseline for evaluating the performance of Orca-3-7B.<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is a state-of-the-art language model used as an evaluator for summarization performance, known for its high quality and low hallucination rates.<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"HALLUCINATION RATE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Hallucination rate is a metric used to measure the frequency of incorrect or fabricated information generated by language models during summarization tasks.<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"QUALITY SCORE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Quality score is a metric that evaluates the overall quality of the summarization output from language models, rated on a scale from 1 to 10.<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"ACI-BENCH\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">ACI-Bench is a dataset designed for benchmarking automatic report generation from doctor-patient conversations, consisting of 120 data points.<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"INSTRUSUM\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">InstruSum is a dataset for evaluating instruction-controllable summarization capabilities of language models, containing 100 data points.<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"ORCA-SUM\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Orca-Sum is a benchmark created to evaluate language models' ability to follow summarization and grounded data transformation instructions, with a total of 458 data points.<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"MIRAGE\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MIRAGE is a benchmark focusing on answering medical questions by referring to information retrieved from a medical corpus, used to assess RAG capabilities of language models.<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"DATA TRANSFORMATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Data transformation refers to the process of converting data from one format or structure into another, often used in conjunction with summarization tasks.<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"HUGGING FACE\">      <data key=\"d0\">PLATFORM<\/data>      <data key=\"d1\">Hugging Face is a platform that hosts various datasets and models for natural language processing, including those used for summarization tasks.<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5-turbo is a variant of the GPT-3 model, used in evaluations alongside other models for summarization performance.<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <edge source=\"ORCA-3-7B\" target=\"ORCA-2.5\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3-7B shows performance improvements compared to Orca-2.5, indicating a direct relationship in their development.<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"MISTRAL-7B-INSTRUCT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3-7B is evaluated against Mistral-7B-Instruct, highlighting its advancements in summarization capabilities.<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"GPT-4\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">GPT-4 is used to evaluate the performance of Orca-3-7B, establishing a relationship between the model and its evaluator.<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"HALLUCINATION RATE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The hallucination rate is measured for Orca-3-7B, indicating its performance in generating accurate summaries.<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"QUALITY SCORE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The quality score is assigned to Orca-3-7B's summarization outputs, reflecting its effectiveness in generating high-quality summaries.<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"ACI-BENCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3-7B is evaluated using the ACI-Bench dataset, linking its performance to this specific benchmark.<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"INSTRUSUM\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3-7B is assessed on the InstruSum dataset, indicating its capabilities in instruction-controllable summarization.<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"ORCA-SUM\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3-7B is evaluated on the Orca-Sum benchmark, which tests its summarization and data transformation abilities.<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"MIRAGE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">MIRAGE is used to assess the RAG capabilities of Orca-3-7B, linking the model to medical question answering performance.<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"AGENTINSTRUCT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AgentInstruct is applied to Orca-3-7B to enhance its summarization capabilities by reducing hallucinations.<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"DATA TRANSFORMATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Data transformation techniques are utilized in conjunction with Orca-3-7B to evaluate its ability to follow complex instructions.<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"GPT-3.5-TURBO\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">GPT-3.5-turbo is compared with Orca-3-7B in summarization evaluations, establishing a relationship in performance assessment.<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-SUM\" target=\"HUGGING FACE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Orca-Sum benchmark utilizes datasets collected from Hugging Face, linking the platform to the evaluation process.<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"ab04427ae0415a1c812a35cf8d3ee1a2","chunk":" respectively.\nMIRAGE Datasets\nMMLU-\nMedMedQA-\nUSMedMCQA PubMedQA BioASQ Avg.\nGPT-4\n(0613)CoT 89.44 83.97 69.88 39.6 84.3 73.44\nRAG87.24 82.8 66.65 70.6 92.56 79.97\nGPT-3.5-turbo\n(0613)CoT 72.91 65.04 55.25 36 74.27 60.69\nRAG75.48 66.61 58.04 67.4 90.29 71.57\nOrca-2.5-7BCoT 63.91 51.37 43.65 29.6 71.04 51.92\nRAG53.72 37.08 39.23 19 69.09 43.62\nMistral-7B-\nInstruct-v0.1CoT 50.96 42.73 34.9 27.6 47.57 40.75\nRAG54.64 35.35 43.41 30.2 68.77 46.47\nOrca-3-7BCoT 71.35 55.38 51.33 27.8 75.24 56.22\nRAG71.17\n(+30.25%)51.85\n(+46.68%)57.95\n(+33.49%)58.2\n(+92.71%)82.2\n(+19.52%)64.27\n(+38.30%)\nTable 8: Evaluation results of RAG skill on MIRAGE. The figures (x%) adjacent to the Orca-\n3 results signify the percentage of improvement compared to Mistral-7B-Instruct respectively.\nCoT shows the performance of the same models when answering directly without using RAG\nWe use the same retrieval mechanism across all models on MIRAGE [ 35], using MedRAG,\nthe corpus accompanying the benchmark. This involves using the same retrieval function\nand the same number of retrieved documents for all models. As all models are presented\nwith the same set of retrieved documents, the comparison accurately reflects the ability of\ndifferent models to incorporate retrieved results into their responses.\nTable 8 shows the results of all models on MIRAGE with and without leveraging RAG1.\nOverall, we observe that\n\u2022Models with a deeper understanding of the task (CoT scores) tend to have higher\nRAG scores. If we restrict our focus to only RAG performance, applying post-\ntraining, we\u2019ve managed to enhance Mistral\u2019s performance by an average of 38.30%.\n1Results of GPT-4 and GPT-3.5-Turbo are from [35].\n20\u2022Of the five datasets in MIRAGE, PubMedQA arguably offers the most effective\ntestbed for assessing models ability to do RAG. In PubMedQA, all models have\nlimited prior knowledge, and the retrieved context provides essential information, as\ndemonstrated by GPT4\u2019s performance leap. All Mistral fine-tunes exhibit similar\nperformance, but only Orca-3 (Mistral trained with AgentInstruct RAG flow data)\nshows a substantial improvement, resulting in a relative improvement of 92.71% over\nMistral-Instruct.\n5 Limitations\nAgentInstruct reduces human expertise required for data generation significantly and enables\ncreating of high-quality synthetic data at scale. However, this is till an early step in this\ndirection and could suffer from many limitations associated with synthetic data generation,\nincluding but not limited to:\nExtensibility: Creating the agentic flows for different skills depends on human effort for\nthe construction of the flows. Future work should consider how to automate the construction\nof the agentic flow from the user specification.\nAccuracy: Synthetic data may not perfectly replicate the complexity and nuances of real-\nworld data, leading to potential inaccuracies. Additional work is needed to better assess the\nquality of the data.\nCost: Generating synthetic data with multiple agents using LLMs and tools can be resource-\nintensive.\nBias:If the original seed data used to generate synthetic data contains biases, these biases\ncan be reflected and even amplified in the synthetic data.\nValidation : It can be difficult to validate synthetic data to ensure it accurately represents\nthe desired scenarios.\nDependency on Seed Data : The quality of synthetic data is dependent on the quality of\nthe real data used as seeds. Poor quality input data could result in poor quality synthetic\ndata.\nOrca-3 is fine-tuned with the AgentInstruct data based on the Mistral model family, and\nretains many of its limitations, as well as the common limitations of other large language\nmodels and limitations originating from its training process, including:\nData Biases: Large language models, trained on extensive data, can inadvertently carry\nbiases present in the source data. Consequently, the models may generate outputs that could\nbe potentially biased or unfair.\nLack of Transparency: Due to the complexity and size, large language models can act\nas \u201cblack boxes\u201d, making it difficult to comprehend the rationale behind specific outputs or\ndecisions. We recommend reviewing transparency notes from Azure for more information2.\nContent Harms: There are various types of content harms that large language models\ncan cause. It is important to be aware of them when using these models, and to take\nactions to prevent them. It is recommended to leverage various content moderation services\nprovided by different companies and institutions. On an important note, we hope for better\nregulations and standards from","chunk_id":"ab04427ae0415a1c812a35cf8d3ee1a2","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"MIRAGE DATASETS","type":"DATASET","description":"MIRAGE Datasets are a collection of datasets used for evaluating the performance of various language models on specific tasks.","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a state-of-the-art language model developed by OpenAI, known for its advanced capabilities in natural language processing.","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"GPT-3.5-TURBO","type":"TECHNOLOGY","description":"GPT-3.5-Turbo is a variant of the GPT-3 model, designed for efficient performance in conversational AI tasks.","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"ORCA-2.5-7B","type":"TECHNOLOGY","description":"Orca-2.5-7B is a language model that has been evaluated for its performance on various datasets, including MIRAGE.","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"MISTRAL-7B","type":"TECHNOLOGY","description":"Mistral-7B is a language model that has been fine-tuned for specific tasks and evaluated on the MIRAGE datasets.","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"AGENTINSTRUCT","type":"TECHNIQUE","description":"AgentInstruct is a method used to fine-tune language models, enabling them to generate high-quality synthetic data.","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"PUBMEDQA","type":"DATASET","description":"PubMedQA is a dataset used for evaluating the performance of models in retrieving and generating answers from medical literature.\nPubMedQA is a dataset used for assessing the ability of models to perform retrieval-augmented generation (RAG) tasks.","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2","entity_type":"DATASET"},{"name":"RAG","type":"TECHNIQUE","description":"RAG (Retrieval-Augmented Generation) is a technique that combines retrieval of relevant documents with generative capabilities of language models.","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2","entity_type":"TECHNIQUE"},{"name":"LIMITATIONS","type":"CONCEPT","description":"Limitations refer to the challenges and constraints faced by language models, particularly in the context of synthetic data generation.","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"USMEDMCQA","type":"DATASET","description":"USMedMCQA is a dataset designed for assessing the capabilities of language models in medical question-answering scenarios.","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"CO-T","type":"TECHNIQUE","description":"CoT (Chain of Thought) is a technique used in language models to improve reasoning and performance on complex tasks.","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"EVALUATION RESULTS","type":"DATA","description":"Evaluation results refer to the performance metrics obtained from testing language models on specific datasets, such as MIRAGE.","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"MEDMEDQA","type":"","description":"","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"ORCA-3","type":"","description":"","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"MIRAGE DATASETS\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MIRAGE Datasets are a collection of datasets used for evaluating the performance of various language models on specific tasks.<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a state-of-the-art language model developed by OpenAI, known for its advanced capabilities in natural language processing.<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5-Turbo is a variant of the GPT-3 model, designed for efficient performance in conversational AI tasks.<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"ORCA-2.5-7B\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Orca-2.5-7B is a language model that has been evaluated for its performance on various datasets, including MIRAGE.<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"MISTRAL-7B\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Mistral-7B is a language model that has been fine-tuned for specific tasks and evaluated on the MIRAGE datasets.<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">AgentInstruct is a method used to fine-tune language models, enabling them to generate high-quality synthetic data.<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"PUBMEDQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">PubMedQA is a dataset used for evaluating the performance of models in retrieving and generating answers from medical literature.PubMedQA is a dataset used for assessing the ability of models to perform retrieval-augmented generation (RAG) tasks.<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"RAG\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">RAG (Retrieval-Augmented Generation) is a technique that combines retrieval of relevant documents with generative capabilities of language models.<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"LIMITATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Limitations refer to the challenges and constraints faced by language models, particularly in the context of synthetic data generation.<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"USMEDMCQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">USMedMCQA is a dataset designed for assessing the capabilities of language models in medical question-answering scenarios.<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"CO-T\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">CoT (Chain of Thought) is a technique used in language models to improve reasoning and performance on complex tasks.<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"EVALUATION RESULTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Evaluation results refer to the performance metrics obtained from testing language models on specific datasets, such as MIRAGE.<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"MEDMEDQA\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <edge source=\"MIRAGE DATASETS\" target=\"GPT-4\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">GPT-4 has been evaluated on the MIRAGE datasets to assess its performance in various tasks.<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MIRAGE DATASETS\" target=\"GPT-3.5-TURBO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">GPT-3.5-Turbo has been evaluated on the MIRAGE datasets to assess its performance in various tasks.<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MIRAGE DATASETS\" target=\"ORCA-2.5-7B\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Orca-2.5-7B has been evaluated on the MIRAGE datasets to assess its performance in various tasks.<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MIRAGE DATASETS\" target=\"MISTRAL-7B\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mistral-7B has been evaluated on the MIRAGE datasets to assess its performance in various tasks.<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MIRAGE DATASETS\" target=\"MEDMEDQA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">MedMedQA is one of the datasets included in the MIRAGE collection for evaluating language models.<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MIRAGE DATASETS\" target=\"USMEDMCQA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">USMedMCQA is one of the datasets included in the MIRAGE collection for evaluating language models.<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MIRAGE DATASETS\" target=\"PUBMEDQA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">PubMedQA is one of the datasets included in the MIRAGE collection for evaluating language models.<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MIRAGE DATASETS\" target=\"EVALUATION RESULTS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Evaluation results provide insights into the performance of models tested on the MIRAGE datasets.<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"CO-T\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">GPT-4 utilizes the CoT technique to enhance its reasoning capabilities in various tasks.<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO\" target=\"CO-T\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">GPT-3.5-Turbo utilizes the CoT technique to enhance its reasoning capabilities in various tasks.<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MISTRAL-7B\" target=\"AGENTINSTRUCT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Mistral-7B has been fine-tuned using the AgentInstruct method to enhance its performance.<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MISTRAL-7B\" target=\"ORCA-3\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Orca-3 is a fine-tuned version of the Mistral model family, showing improved performance in specific tasks.<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"PUBMEDQA\" target=\"RAG\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">PubMedQA serves as an effective testbed for assessing models' abilities to perform RAG tasks.<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"RAG\" target=\"LIMITATIONS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">RAG techniques face various limitations, including biases and challenges in synthetic data generation.<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"dd9a46950237e49ef9b1c7ef08e08d42","chunk":" making it difficult to comprehend the rationale behind specific outputs or\ndecisions. We recommend reviewing transparency notes from Azure for more information2.\nContent Harms: There are various types of content harms that large language models\ncan cause. It is important to be aware of them when using these models, and to take\nactions to prevent them. It is recommended to leverage various content moderation services\nprovided by different companies and institutions. On an important note, we hope for better\nregulations and standards from government and technology leaders around content harms\nfor AI technologies in future. We value and acknowledge the important role that research\nand open source community can play in this direction.\nHallucination: It is important to be aware and cautious not to entirely rely on a given\nlanguage model for critical decisions or information that might have deep impact as it is\nnot obvious how to prevent these models from fabricating content. Moreover, it is not clear\nwhether small models may be more susceptible to hallucination in ungrounded generation\nuse cases due to their smaller sizes and hence reduced memorization capacities. This is an\n2https:\/\/learn.microsoft.com\/en-us\/legal\/cognitive-services\/openai\/\ntransparency-note\n21active research topic and we hope there will be more rigorous measurement, understanding\nand mitigations around this topic.\nPotential for Misuse: Without suitable safeguards, there is a risk that these models could\nbe maliciously used for generating disinformation or harmful content.\nData Distribution: Orca-3\u2019s performance is likely to correlate strongly with the distribution\nof the tuning data. This correlation might limit its accuracy in areas underrepresented in\nthe training dataset.\n6 Conclusions\nThe AgentInstruct approach to Generative Teaching offers a promising solution to the\nchallenge of generating large amount of diverse and high-quality data for model post-training.\nThis method stands out by using agentic flows for synthetic data generation, thus addressing\nkey concerns associated with the use of synthetic data in model training, such as the lack of\ndiversity and the need for intensive human curation and intervention during the data creation\nprocess. By leveraging an agentic framework, AgentInstruct can generate tailored datasets\ncomprising both prompts and responses from unstructured data sources, facilitating the\npost-training of models and teaching them variety of skills. The efficacy of this approach is\nexemplifiedbythesubstantialimprovementobserved intheOrca-3 model, which, post-trained\nwith a 25M pair dataset generated by AgentInstruct, showcased a notable performance gain\nacross multiple benchmarks. We believe using agentic flows for creating synthetic data can\nshow significant value for all stages of model training, including pre-training, post-training\nand domain\/task specialization. The ability to use unstructured content to generate diverse\nand high-quality instruction data given any specifications could pave the way for creating\n(semi) automated pipelines using synthetic data for model customization (using domain\nspecific content as seeds) and continual improvement (generating higher quality data than\nthe base model with agentic flows).\nReferences\n[1]Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah,\nHany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl,\nAlon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Qin Cai, Martin Cai, Caio\nC\u00e9sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Dong Chen, Dongdong Chen,\nYen-Chun Chen, Yi-Ling Chen, Parul Chopra, Xiyang Dai, Allie Del Giorno, Gustavo de Rosa,\nMatthew Dixon, Ronen Eldan, Victor Fragoso, Dan Iter, Mei Gao, Min Gao, Jianfeng Gao,\nAmit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J.\nHewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis,\nDongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li,\nYunsheng Li, Chen Liang, Lars Liden, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin, Zeqi\nLin, Chong Luo, Piyush Madan, Matt Mazzola, Arindam Mitra, Hardik Modi, Anh Nguyen,\nBrandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin,\nMarko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied,\nAdil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Swadheen Shukla,\nXia Song, Masahiro Tanaka, Andrea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang, Yu Wang,\nRachel Ward, Guanhua Wang, Philipp Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can\nXu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang,\nDonghan Yu, Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang,\nYi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report","chunk_id":"dd9a46950237e49ef9b1c7ef08e08d42","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"AZURE","type":"ORGANIZATION","description":"Azure is a cloud computing service created by Microsoft, providing various services including AI and machine learning capabilities.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CONTENT HARMS","type":"CONCEPT","description":"Content harms refer to the negative impacts that large language models can have, including the generation of disinformation or harmful content.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CONTENT MODERATION SERVICES","type":"SERVICE","description":"Content moderation services are tools and systems provided by companies and institutions to help manage and mitigate harmful content generated by AI models.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"REGULATIONS AND STANDARDS","type":"CONCEPT","description":"Regulations and standards are guidelines proposed for government and technology leaders to ensure safe and ethical use of AI technologies.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"HALLUCINATION","type":"CONCEPT","description":"Hallucination in AI refers to the phenomenon where language models generate fabricated or misleading content, which can lead to misinformation.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ORCA-3","type":"MODEL","description":"Orca-3 is a language model that has shown improved performance when post-trained with a dataset generated by the AgentInstruct approach.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"AGENTINSTRUCT","type":"TECHNIQUE","description":"AgentInstruct is an approach to Generative Teaching that utilizes agentic flows for generating diverse and high-quality synthetic data for model training.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"SYNTHETIC DATA","type":"DATA TYPE","description":"Synthetic data is artificially generated data used for training models, which can help in creating tailored datasets for various applications.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"POST-TRAINING","type":"PROCESS","description":"Post-training refers to the phase in model development where additional training is conducted after the initial training to improve performance.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"UNSTRUCTURED DATA","type":"DATA TYPE","description":"Unstructured data refers to information that does not have a predefined data model, making it more challenging to analyze but valuable for generating insights.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"RESEARCH COMMUNITY","type":"GROUP","description":"The research community consists of individuals and organizations engaged in the study and development of AI technologies, contributing to advancements and ethical considerations.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"SAM ADE JACOBS","type":"PERSON","description":"Sam Ade Jacobs is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"AMMAR AHMAD AWAN","type":"PERSON","description":"Ammar Ahmad Awan is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JYOTI ANEJA","type":"PERSON","description":"Jyoti Aneja is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"AHMED AWADALLAH","type":"PERSON","description":"Ahmed Awadallah is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"HANY AWADALLA","type":"PERSON","description":"Hany Awadalla is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"NGUYEN BACH","type":"PERSON","description":"Nguyen Bach is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"AMIT BAHREE","type":"PERSON","description":"Amit Bahree is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ARASH BAKHTIARI","type":"PERSON","description":"Arash Bakhtiari is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JIANMIN BAO","type":"PERSON","description":"Jianmin Bao is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"HARKIRAT BEHL","type":"PERSON","description":"Harkirat Behl is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ALON BENHAIM","type":"PERSON","description":"Alon Benhaim is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MISHA BILENKO","type":"PERSON","description":"Misha Bilenko is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JOHAN BJORCK","type":"PERSON","description":"Johan Bjorck is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"S\u00c9BASTIEN BUBECK","type":"PERSON","description":"S\u00e9bastien Bubeck is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"QIN CAI","type":"PERSON","description":"Qin Cai is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MARTIN CAI","type":"PERSON","description":"Martin Cai is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CAIO C\u00c9SAR TEODORO MENDES","type":"PERSON","description":"Caio C\u00e9sar Teodoro Mendes is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"WEIZHU CHEN","type":"PERSON","description":"Weizhu Chen is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"VISHRAV CHAUDHARY","type":"PERSON","description":"Vishrav Chaudhary is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"DONG CHEN","type":"PERSON","description":"Dong Chen is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"DONGDONG CHEN","type":"PERSON","description":"Dongdong Chen is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"YEN-CHUN CHEN","type":"PERSON","description":"Yen-Chun Chen is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"YI-LING CHEN","type":"PERSON","description":"Yi-Ling Chen is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"PARUL CHOPRA","type":"PERSON","description":"Parul Chopra is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"XIYANG DAI","type":"PERSON","description":"Xiyang Dai is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ALLIE DEL GIORNO","type":"PERSON","description":"Allie Del Giorno is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"GUSTAVO DE ROSA","type":"PERSON","description":"Gustavo de Rosa is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MATTHEW DIXON","type":"PERSON","description":"Matthew Dixon is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"RONEN ELDAN","type":"PERSON","description":"Ronen Eldan is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"VICTOR FRAGOSO","type":"PERSON","description":"Victor Fragoso is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"DAN ITER","type":"PERSON","description":"Dan Iter is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MEI GAO","type":"PERSON","description":"Mei Gao is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MIN GAO","type":"PERSON","description":"Min Gao is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JIANFENG GAO","type":"PERSON","description":"Jianfeng Gao is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"AMIT GARG","type":"PERSON","description":"Amit Garg is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ABHISHEK GOSWAMI","type":"PERSON","description":"Abhishek Goswami is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"SURIYA GUNASEKAR","type":"PERSON","description":"Suriya Gunasekar is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"EMMAN HAIDER","type":"PERSON","description":"Emman Haider is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JUNHENG HAO","type":"PERSON","description":"Junheng Hao is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"RUSSELL J. HEWETT","type":"PERSON","description":"Russell J. Hewett is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JAMIE HUYNH","type":"PERSON","description":"Jamie Huynh is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MOJAN JAVAHERIPI","type":"PERSON","description":"Mojan Javaheripi is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"XIN JIN","type":"PERSON","description":"Xin Jin is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"PIERO KAUFFMANN","type":"PERSON","description":"Piero Kauffmann is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"NIKOS KARAMPATZIAKIS","type":"PERSON","description":"Nikos Karampatziakis is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"DONGWOO KIM","type":"PERSON","description":"Dongwoo Kim is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MAHOUD KHADEMI","type":"PERSON","description":"Mahoud Khademi is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"LEV KURILENKO","type":"PERSON","description":"Lev Kurilenko is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JAMES R. LEE","type":"PERSON","description":"James R. Lee is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"YIN TAT LEE","type":"PERSON","description":"Yin Tat Lee is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"YUANZHI LI","type":"PERSON","description":"Yuanzhi Li is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"YUNSHENG LI","type":"PERSON","description":"Yunsheng Li is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CHEN LIANG","type":"PERSON","description":"Chen Liang is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"LARS LIDEN","type":"PERSON","description":"Lars Liden is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CE LIU","type":"PERSON","description":"Ce Liu is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MENGCHEN LIU","type":"PERSON","description":"Mengchen Liu is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"WEISHUNG LIU","type":"PERSON","description":"Weishung Liu is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ERIC LIN","type":"PERSON","description":"Eric Lin is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ZEQI LIN","type":"PERSON","description":"Zeqi Lin is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CHONG LUO","type":"PERSON","description":"Chong Luo is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"PIYUSH MADAN","type":"PERSON","description":"Piyush Madan is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MATT MAZZOLA","type":"PERSON","description":"Matt Mazzola is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ARINDAM MITRA","type":"PERSON","description":"Arindam Mitra is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"HARDIK MODI","type":"PERSON","description":"Hardik Modi is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ANH NGUYEN","type":"PERSON","description":"Anh Nguyen is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"BRANDON NORICK","type":"PERSON","description":"Brandon Norick is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"BARUN PATRA","type":"PERSON","description":"Barun Patra is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"DANIEL PEREZ-BECKER","type":"PERSON","description":"Daniel Perez-Becker is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"THOMAS PORTET","type":"PERSON","description":"Thomas Portet is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"REID PRYZANT","type":"PERSON","description":"Reid Pryzant is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"HEYANG QIN","type":"PERSON","description":"Heyang Qin is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MARKO RADMILAC","type":"PERSON","description":"Marko Radmilac is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CORBY ROSSET","type":"PERSON","description":"Corby Rosset is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"SAMBUDHA ROY","type":"PERSON","description":"Sambudha Roy is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"OLATUNJI RUWASE","type":"PERSON","description":"Olatunji Ruwase is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"OLLI SAARIKIVI","type":"PERSON","description":"Olli Saarikivi is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"AMIN SAIED","type":"PERSON","description":"Amin Saied is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ADIL SALIM","type":"PERSON","description":"Adil Salim is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MICHAEL SANTACROCE","type":"PERSON","description":"Michael Santacroce is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"SHITAL SHAH","type":"PERSON","description":"Shital Shah is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"NING SHANG","type":"PERSON","description":"Ning Shang is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"HITESHI SHARMA","type":"PERSON","description":"Hiteshi Sharma is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"SWADHEEN SHUKLA","type":"PERSON","description":"Swadheen Shukla is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"XIA SONG","type":"PERSON","description":"Xia Song is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MASAHIRO TANAKA","type":"PERSON","description":"Masahiro Tanaka is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ANDREA TUPINI","type":"PERSON","description":"Andrea Tupini is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"XIN WANG","type":"PERSON","description":"Xin Wang is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"LIJUAN WANG","type":"PERSON","description":"Lijuan Wang is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CHUNYU WANG","type":"PERSON","description":"Chunyu Wang is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"YU WANG","type":"PERSON","description":"Yu Wang is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"RACHEL WARD","type":"PERSON","description":"Rachel Ward is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"GUANHUA WANG","type":"PERSON","description":"Guanhua Wang is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"PHILIPP WITTE","type":"PERSON","description":"Philipp Witte is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AZURE\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Azure is a cloud computing service created by Microsoft, providing various services including AI and machine learning capabilities.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CONTENT HARMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Content harms refer to the negative impacts that large language models can have, including the generation of disinformation or harmful content.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CONTENT MODERATION SERVICES\">      <data key=\"d0\">SERVICE<\/data>      <data key=\"d1\">Content moderation services are tools and systems provided by companies and institutions to help manage and mitigate harmful content generated by AI models.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"REGULATIONS AND STANDARDS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Regulations and standards are guidelines proposed for government and technology leaders to ensure safe and ethical use of AI technologies.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"HALLUCINATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Hallucination in AI refers to the phenomenon where language models generate fabricated or misleading content, which can lead to misinformation.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 is a language model that has shown improved performance when post-trained with a dataset generated by the AgentInstruct approach.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">AgentInstruct is an approach to Generative Teaching that utilizes agentic flows for generating diverse and high-quality synthetic data for model training.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"SYNTHETIC DATA\">      <data key=\"d0\">DATA TYPE<\/data>      <data key=\"d1\">Synthetic data is artificially generated data used for training models, which can help in creating tailored datasets for various applications.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"POST-TRAINING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Post-training refers to the phase in model development where additional training is conducted after the initial training to improve performance.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"UNSTRUCTURED DATA\">      <data key=\"d0\">DATA TYPE<\/data>      <data key=\"d1\">Unstructured data refers to information that does not have a predefined data model, making it more challenging to analyze but valuable for generating insights.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"RESEARCH COMMUNITY\">      <data key=\"d0\">GROUP<\/data>      <data key=\"d1\">The research community consists of individuals and organizations engaged in the study and development of AI technologies, contributing to advancements and ethical considerations.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"SAM ADE JACOBS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sam Ade Jacobs is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"AMMAR AHMAD AWAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ammar Ahmad Awan is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JYOTI ANEJA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jyoti Aneja is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"AHMED AWADALLAH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ahmed Awadallah is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"HANY AWADALLA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hany Awadalla is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"NGUYEN BACH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nguyen Bach is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"AMIT BAHREE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amit Bahree is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ARASH BAKHTIARI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arash Bakhtiari is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JIANMIN BAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jianmin Bao is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"HARKIRAT BEHL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Harkirat Behl is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ALON BENHAIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alon Benhaim is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MISHA BILENKO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Misha Bilenko is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JOHAN BJORCK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Johan Bjorck is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"S&#201;BASTIEN BUBECK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S&#233;bastien Bubeck is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"QIN CAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qin Cai is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MARTIN CAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Martin Cai is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CAIO C&#201;SAR TEODORO MENDES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Caio C&#233;sar Teodoro Mendes is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"WEIZHU CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weizhu Chen is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"VISHRAV CHAUDHARY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vishrav Chaudhary is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"DONG CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dong Chen is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"DONGDONG CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dongdong Chen is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"YEN-CHUN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yen-Chun Chen is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"YI-LING CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi-Ling Chen is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"PARUL CHOPRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Parul Chopra is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"XIYANG DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiyang Dai is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ALLIE DEL GIORNO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Allie Del Giorno is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"GUSTAVO DE ROSA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gustavo de Rosa is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MATTHEW DIXON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthew Dixon is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"RONEN ELDAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ronen Eldan is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"VICTOR FRAGOSO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Victor Fragoso is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"DAN ITER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dan Iter is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MEI GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mei Gao is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MIN GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Min Gao is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JIANFENG GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jianfeng Gao is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"AMIT GARG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amit Garg is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ABHISHEK GOSWAMI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Abhishek Goswami is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"SURIYA GUNASEKAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Suriya Gunasekar is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"EMMAN HAIDER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Emman Haider is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JUNHENG HAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Junheng Hao is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"RUSSELL J. HEWETT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Russell J. Hewett is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JAMIE HUYNH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jamie Huynh is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MOJAN JAVAHERIPI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mojan Javaheripi is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"XIN JIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xin Jin is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"PIERO KAUFFMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Piero Kauffmann is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"NIKOS KARAMPATZIAKIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nikos Karampatziakis is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"DONGWOO KIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dongwoo Kim is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MAHOUD KHADEMI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mahoud Khademi is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"LEV KURILENKO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lev Kurilenko is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JAMES R. LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">James R. Lee is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"YIN TAT LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yin Tat Lee is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"YUANZHI LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuanzhi Li is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"YUNSHENG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yunsheng Li is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CHEN LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Liang is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"LARS LIDEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lars Liden is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CE LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ce Liu is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MENGCHEN LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mengchen Liu is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"WEISHUNG LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weishung Liu is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ERIC LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Lin is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ZEQI LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zeqi Lin is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CHONG LUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chong Luo is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"PIYUSH MADAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Piyush Madan is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MATT MAZZOLA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matt Mazzola is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ARINDAM MITRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arindam Mitra is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"HARDIK MODI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hardik Modi is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ANH NGUYEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anh Nguyen is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"BRANDON NORICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brandon Norick is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"BARUN PATRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Barun Patra is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"DANIEL PEREZ-BECKER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel Perez-Becker is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"THOMAS PORTET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Portet is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"REID PRYZANT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Reid Pryzant is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"HEYANG QIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heyang Qin is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MARKO RADMILAC\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marko Radmilac is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CORBY ROSSET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Corby Rosset is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"SAMBUDHA ROY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sambudha Roy is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"OLATUNJI RUWASE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Olatunji Ruwase is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"OLLI SAARIKIVI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Olli Saarikivi is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"AMIN SAIED\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amin Saied is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ADIL SALIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adil Salim is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MICHAEL SANTACROCE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Santacroce is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"SHITAL SHAH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shital Shah is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"NING SHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ning Shang is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"HITESHI SHARMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hiteshi Sharma is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"SWADHEEN SHUKLA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Swadheen Shukla is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"XIA SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xia Song is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MASAHIRO TANAKA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Masahiro Tanaka is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ANDREA TUPINI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrea Tupini is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"XIN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xin Wang is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"LIJUAN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lijuan Wang is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CHUNYU WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chunyu Wang is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"YU WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu Wang is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"RACHEL WARD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rachel Ward is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"GUANHUA WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guanhua Wang is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"PHILIPP WITTE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Philipp Witte is a researcher involved in the development of AI technologies, contributing to the Phi-3 technical report.<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <edge source=\"AZURE\" target=\"CONTENT HARMS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Azure provides transparency notes that discuss the content harms associated with large language models, highlighting the importance of awareness and mitigation.<\/data>      <data key=\"d5\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/edge>    <edge source=\"CONTENT HARMS\" target=\"CONTENT MODERATION SERVICES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Content harms can be mitigated through the use of content moderation services, which help manage harmful outputs from AI models.<\/data>      <data key=\"d5\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/edge>    <edge source=\"CONTENT HARMS\" target=\"REGULATIONS AND STANDARDS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Regulations and standards are necessary to address and prevent content harms associated with AI technologies.<\/data>      <data key=\"d5\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/edge>    <edge source=\"CONTENT HARMS\" target=\"HALLUCINATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Hallucination is a specific type of content harm that can mislead users and create misinformation, necessitating caution in AI usage.<\/data>      <data key=\"d5\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/edge>    <edge source=\"REGULATIONS AND STANDARDS\" target=\"RESEARCH COMMUNITY\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The research community plays a vital role in advocating for better regulations and standards in AI technologies to prevent content harms.<\/data>      <data key=\"d5\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AGENTINSTRUCT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Orca-3's performance improved significantly after being post-trained with a dataset generated by the AgentInstruct approach.<\/data>      <data key=\"d5\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"POST-TRAINING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Post-training is a critical phase for Orca-3, allowing it to leverage synthetic data for improved performance across benchmarks.<\/data>      <data key=\"d5\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"SYNTHETIC DATA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AgentInstruct generates synthetic data to enhance model training, addressing issues of diversity and quality in datasets.<\/data>      <data key=\"d5\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"UNSTRUCTURED DATA\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">AgentInstruct utilizes unstructured data to create tailored datasets for model training, enhancing the learning process.<\/data>      <data key=\"d5\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"cc20c99cad8edecc66b82ac751ff7172","chunk":" Wang, Philipp Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can\nXu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang,\nDonghan Yu, Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang,\nYi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: A highly capable\nlanguage model locally on your phone, 2024. URL https:\/\/arxiv.org\/abs\/2404.14219 .\n[2]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\nchallenge. arXiv:1803.05457v1, 2018.\n[3]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXivpreprint arXiv:2110.14168, 2021.\n[4]CodeParrot. Github-code clean dataset, 2022. https:\/\/huggingface.co\/datasets\/\ncodeparrot\/github-code-clean [Accessed: (06\/15\/2024)].\n22[5]Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong\nSun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional\nconversations. arXivpreprint arXiv:2305.14233, 2023.\n[6]Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt\nGardner. DROP: A reading comprehension benchmark requiring discrete reasoning over\nparagraphs. In Proceedings ofthe2019Conference oftheNorthAmerican Chapter ofthe\nAssociation forComputational Linguistics: HumanLanguage Technologies, Volume1(Long\nandShortPapers), pages 2368\u20132378, Minneapolis, Minnesota, June 2019. Association for\nComputational Linguistics. doi: 10.18653\/v1\/N19-1246. URL https:\/\/aclanthology.org\/\nN19-1246 .\n[7]Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Hang Yan, Xipeng Qiu, and Dahua Lin.\nQuery of cc: Unearthing large scale domain-specific knowledge from public corpora. arXiv\npreprint arXiv:2401.14624, 2024.\n[8]Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey\nLevine, and Dawn Song. The false promise of imitating proprietary llms, 2023. URL https:\n\/\/arxiv.org\/abs\/2305.15717 .\n[9]Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.\narXivpreprint arXiv:2103.03874, 2021.\n[10]Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep\nDasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.\nCamels in a changing climate: Enhancing lm adaptation with tulu 2, 2023. URL https:\n\/\/arxiv.org\/abs\/2311.10702 .\n[11]Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023.\n[12]Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie\nLu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash.\nRlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2309.00267 .\n[13]Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\nGhanem. Camel: Communicative agents for \"mind\" exploration of large language model society,\n2023. URL https:\/\/arxiv.org\/abs\/2303.17760 .\n[14]Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, andTatsunoriB.Hashimoto. Alpacae","chunk_id":"cc20c99cad8edecc66b82ac751ff7172","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"PHI-3","type":"TECHNOLOGY","description":"Phi-3 is a highly capable language model designed to operate locally on mobile devices, as detailed in the technical report from 2024.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"PETER CLARK","type":"PERSON","description":"Peter Clark is a researcher known for his work on question answering and the AI2 Reasoning Challenge.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ISAAC COWHEY","type":"PERSON","description":"Isaac Cowhey is a researcher who contributed to the AI2 Reasoning Challenge.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"OREN ETZIONI","type":"PERSON","description":"Oren Etzioni is a researcher involved in the AI2 Reasoning Challenge.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"TUSHAR KHOT","type":"PERSON","description":"Tushar Khot is a researcher who worked on the AI2 Reasoning Challenge.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ASHISH SABHARWAL","type":"PERSON","description":"Ashish Sabharwal is a researcher associated with the AI2 Reasoning Challenge.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CARISSA SCHOENICK","type":"PERSON","description":"Carissa Schoenick is a researcher who contributed to the AI2 Reasoning Challenge.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"OYVIND TAFJORD","type":"PERSON","description":"Oyvind Tafjord is a researcher involved in the AI2 Reasoning Challenge.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"KARL COBBE","type":"PERSON","description":"Karl Cobbe is a researcher known for his work on training verifiers for math word problems.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"VINEET KOSARAJU","type":"PERSON","description":"Vineet Kosaraju is a researcher who contributed to the training of verifiers for math word problems.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MOHAMMAD BAVARIAN","type":"PERSON","description":"Mohammad Bavarian is a researcher involved in the training of verifiers for math word problems.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MARK CHEN","type":"PERSON","description":"Mark Chen is a researcher who worked on training verifiers for math word problems.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HEEWOO JUN","type":"PERSON","description":"Heewoo Jun is a researcher associated with the training of verifiers for math word problems.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"LUKASZ KAISER","type":"PERSON","description":"Lukasz Kaiser is a researcher involved in the training of verifiers for math word problems.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MATTHIAS PLAPPERT","type":"PERSON","description":"Matthias Plappert is a researcher who contributed to the training of verifiers for math word problems.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JERRY TWOREK","type":"PERSON","description":"Jerry Tworek is a researcher involved in the training of verifiers for math word problems.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JACOB HILTON","type":"PERSON","description":"Jacob Hilton is a researcher associated with the training of verifiers for math word problems.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"REIICHIRO NAKANO","type":"PERSON","description":"Reiichiro Nakano is a researcher who contributed to the training of verifiers for math word problems.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"NING DING","type":"PERSON","description":"Ning Ding is a researcher known for enhancing chat language models through instructional conversations.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YULIN CHEN","type":"PERSON","description":"Yulin Chen is a researcher involved in enhancing chat language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"BOKAI XU","type":"PERSON","description":"Bokai Xu is a researcher who contributed to enhancing chat language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YUJIA QIN","type":"PERSON","description":"Yujia Qin is a researcher involved in enhancing chat language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ZHI ZHENG","type":"PERSON","description":"Zhi Zheng is a researcher who contributed to enhancing chat language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SHENGDING HU","type":"PERSON","description":"Shengding Hu is a researcher involved in enhancing chat language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ZHIYUAN LIU","type":"PERSON","description":"Zhiyuan Liu is a researcher who contributed to enhancing chat language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MAOSONG SUN","type":"PERSON","description":"Maosong Sun is a researcher involved in enhancing chat language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"BOWEN ZHOU","type":"PERSON","description":"Bowen Zhou is a researcher who contributed to enhancing chat language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DROPS","type":"RESEARCH","description":"DROP is a reading comprehension benchmark that requires discrete reasoning over paragraphs, as discussed in a 2019 conference paper.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ZHAOYE FEI","type":"PERSON","description":"Zhaoye Fei is a researcher known for unearthing large-scale domain-specific knowledge from public corpora.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YUNFAN SHAO","type":"PERSON","description":"Yunfan Shao is a researcher involved in unearthing large-scale domain-specific knowledge.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"LINYANG LI","type":"PERSON","description":"Linyang Li is a researcher who contributed to unearthing large-scale domain-specific knowledge.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ZHIYUAN ZENG","type":"PERSON","description":"Zhiyuan Zeng is a researcher involved in unearthing large-scale domain-specific knowledge.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HANG YAN","type":"PERSON","description":"Hang Yan is a researcher who contributed to unearthing large-scale domain-specific knowledge.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"XIPENG QIU","type":"PERSON","description":"Xipeng Qiu is a researcher involved in unearthing large-scale domain-specific knowledge.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DAHUA LIN","type":"PERSON","description":"Dahua Lin is a researcher who contributed to unearthing large-scale domain-specific knowledge.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ARNA GUDIBANDE","type":"PERSON","description":"Arnav Gudibande is a researcher known for discussing the limitations of imitating proprietary language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ERIC WALLACE","type":"PERSON","description":"Eric Wallace is a researcher involved in discussing the limitations of imitating proprietary language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CHARLIE SNELL","type":"PERSON","description":"Charlie Snell is a researcher who contributed to the discussion on imitating proprietary language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"XINYANG GENG","type":"PERSON","description":"Xinyang Geng is a researcher involved in discussing the limitations of imitating proprietary language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HAO LIU","type":"PERSON","description":"Hao Liu is a researcher who contributed to the discussion on imitating proprietary language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"PIETER ABBEEL","type":"PERSON","description":"Pieter Abbeel is a researcher involved in discussing the limitations of imitating proprietary language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SERGEY LEVINE","type":"PERSON","description":"Sergey Levine is a researcher who contributed to the discussion on imitating proprietary language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DAWN SONG","type":"PERSON","description":"Dawn Song is a researcher involved in discussing the limitations of imitating proprietary language models.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DAN HENDRYCKS","type":"PERSON","description":"Dan Hendrycks is a researcher known for measuring mathematical problem-solving capabilities.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"COLLIN BURNS","type":"PERSON","description":"Collin Burns is a researcher involved in measuring mathematical problem-solving capabilities.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SAURAV KADAVATH","type":"PERSON","description":"Saurav Kadavath is a researcher who contributed to measuring mathematical problem-solving capabilities.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"AKUL ARORA","type":"PERSON","description":"Akul Arora is a researcher involved in measuring mathematical problem-solving capabilities.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"STEVEN BASART","type":"PERSON","description":"Steven Basart is a researcher who contributed to measuring mathematical problem-solving capabilities.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ERIC TANG","type":"PERSON","description":"Eric Tang is a researcher involved in measuring mathematical problem-solving capabilities.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"NOAH A. SMITH","type":"PERSON","description":"Noah A. Smith is a researcher who contributed to measuring mathematical problem-solving capabilities.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"IZ BELTAGY","type":"PERSON","description":"Iz Beltagy is a researcher involved in measuring mathematical problem-solving capabilities.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HANNANEH HAJISHIRZI","type":"PERSON","description":"Hannaneh Hajishirzi is a researcher who contributed to measuring mathematical problem-solving capabilities.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CAMELS","type":"RESEARCH","description":"Camels in a changing climate is a study focused on enhancing language model adaptation, as discussed in a 2023 paper.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ALBERT Q. JIANG","type":"PERSON","description":"Albert Q. Jiang is a researcher involved in the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ALEXANDRE SABLAYROLLES","type":"PERSON","description":"Alexandre Sablayrolles is a researcher who contributed to the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ARTHUR MENSCH","type":"PERSON","description":"Arthur Mensch is a researcher involved in the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CHRIS BAMFORD","type":"PERSON","description":"Chris Bamford is a researcher who contributed to the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DEVENDRA SINGH CHAPLOT","type":"PERSON","description":"Devendra Singh Chaplot is a researcher involved in the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DIEGO DE LAS CASAS","type":"PERSON","description":"Diego de las Casas is a researcher who contributed to the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"FLORIAN BRESSAND","type":"PERSON","description":"Florian Bressand is a researcher involved in the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"GIANNA LENGYEL","type":"PERSON","description":"Gianna Lengyel is a researcher who contributed to the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"GUILLAUME LAMPLE","type":"PERSON","description":"Guillaume Lample is a researcher involved in the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"LUCILE SAULNIER","type":"PERSON","description":"Lucile Saulnier is a researcher who contributed to the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"L\u00c9LIO RENARD LAVAUD","type":"PERSON","description":"L\u00e9lio Renard Lavaud is a researcher involved in the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MARIE-ANNE LACHAUX","type":"PERSON","description":"Marie-Anne Lachaux is a researcher who contributed to the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"PIERRE STOCK","type":"PERSON","description":"Pierre Stock is a researcher involved in the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"TEVEN LE SCAO","type":"PERSON","description":"Teven Le Scao is a researcher who contributed to the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"THIBAUT LAVRIL","type":"PERSON","description":"Thibaut Lavril is a researcher involved in the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"THOMAS WANG","type":"PERSON","description":"Thomas Wang is a researcher who contributed to the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"TIMOTH\u00c9E LACROIX","type":"PERSON","description":"Timoth\u00e9e Lacroix is a researcher involved in the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"WILLIAM EL SAYED","type":"PERSON","description":"William El Sayed is a researcher who contributed to the Mistral 7b project.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"AI2 REASONING CHALLENGE","type":"","description":"","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"TRAINING VERIFIERS","type":"","description":"","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ENHANCING CHAT LANGUAGE MODELS","type":"","description":"","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"READING COMPREHENSION","type":"","description":"","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"UNEARTHING DOMAIN-SPECIFIC KNOWLEDGE","type":"","description":"","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"(\"ENTITY\"","type":"","description":"","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"XUECHEN LI","type":"","description":"","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"TIANYI ZHANG","type":"PERSON","description":"Tianyi Zhang is a researcher who contributed to the development of communicative agents for exploring large language model society.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YANN DUBOIS","type":"PERSON","description":"Yann Dubois is a researcher involved in the development of communicative agents for exploring large language model society.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ROHAN TAORI","type":"PERSON","description":"Rohan Taori is a researcher who contributed to the development of communicative agents for exploring large language model society.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ISHAAN GULRAJANI","type":"PERSON","description":"Ishaan Gulrajani is a researcher involved in the development of communicative agents for exploring large language model society.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CARLOS GUESTRIN","type":"PERSON","description":"Carlos Guestrin is a researcher who contributed to the development of communicative agents for exploring large language model society.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"PERCY LIANG","type":"PERSON","description":"Percy Liang is a researcher involved in the development of communicative agents for exploring large language model society.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"TATSUNORI B. HASHIMOTO","type":"PERSON","description":"Tatsunori B. Hashimoto is a researcher who contributed to the development of communicative agents for exploring large language model society.","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"COMMUNICATIVE AGENTS","type":"","description":"","source_id":"cc20c99cad8edecc66b82ac751ff7172"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"PHI-3\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Phi-3 is a highly capable language model designed to operate locally on mobile devices, as detailed in the technical report from 2024.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"PETER CLARK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter Clark is a researcher known for his work on question answering and the AI2 Reasoning Challenge.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ISAAC COWHEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Isaac Cowhey is a researcher who contributed to the AI2 Reasoning Challenge.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"OREN ETZIONI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Oren Etzioni is a researcher involved in the AI2 Reasoning Challenge.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"TUSHAR KHOT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tushar Khot is a researcher who worked on the AI2 Reasoning Challenge.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ASHISH SABHARWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ashish Sabharwal is a researcher associated with the AI2 Reasoning Challenge.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CARISSA SCHOENICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carissa Schoenick is a researcher who contributed to the AI2 Reasoning Challenge.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"OYVIND TAFJORD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Oyvind Tafjord is a researcher involved in the AI2 Reasoning Challenge.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"KARL COBBE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karl Cobbe is a researcher known for his work on training verifiers for math word problems.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"VINEET KOSARAJU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vineet Kosaraju is a researcher who contributed to the training of verifiers for math word problems.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MOHAMMAD BAVARIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mohammad Bavarian is a researcher involved in the training of verifiers for math word problems.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MARK CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mark Chen is a researcher who worked on training verifiers for math word problems.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HEEWOO JUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heewoo Jun is a researcher associated with the training of verifiers for math word problems.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"LUKASZ KAISER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lukasz Kaiser is a researcher involved in the training of verifiers for math word problems.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MATTHIAS PLAPPERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthias Plappert is a researcher who contributed to the training of verifiers for math word problems.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JERRY TWOREK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jerry Tworek is a researcher involved in the training of verifiers for math word problems.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JACOB HILTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Hilton is a researcher associated with the training of verifiers for math word problems.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"REIICHIRO NAKANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Reiichiro Nakano is a researcher who contributed to the training of verifiers for math word problems.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"NING DING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ning Ding is a researcher known for enhancing chat language models through instructional conversations.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YULIN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yulin Chen is a researcher involved in enhancing chat language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"BOKAI XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bokai Xu is a researcher who contributed to enhancing chat language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YUJIA QIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yujia Qin is a researcher involved in enhancing chat language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ZHI ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhi Zheng is a researcher who contributed to enhancing chat language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SHENGDING HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengding Hu is a researcher involved in enhancing chat language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ZHIYUAN LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyuan Liu is a researcher who contributed to enhancing chat language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MAOSONG SUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maosong Sun is a researcher involved in enhancing chat language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"BOWEN ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bowen Zhou is a researcher who contributed to enhancing chat language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DROPS\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">DROP is a reading comprehension benchmark that requires discrete reasoning over paragraphs, as discussed in a 2019 conference paper.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ZHAOYE FEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhaoye Fei is a researcher known for unearthing large-scale domain-specific knowledge from public corpora.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YUNFAN SHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yunfan Shao is a researcher involved in unearthing large-scale domain-specific knowledge.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"LINYANG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linyang Li is a researcher who contributed to unearthing large-scale domain-specific knowledge.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ZHIYUAN ZENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyuan Zeng is a researcher involved in unearthing large-scale domain-specific knowledge.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HANG YAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hang Yan is a researcher who contributed to unearthing large-scale domain-specific knowledge.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"XIPENG QIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xipeng Qiu is a researcher involved in unearthing large-scale domain-specific knowledge.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DAHUA LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dahua Lin is a researcher who contributed to unearthing large-scale domain-specific knowledge.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ARNA GUDIBANDE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arnav Gudibande is a researcher known for discussing the limitations of imitating proprietary language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ERIC WALLACE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Wallace is a researcher involved in discussing the limitations of imitating proprietary language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CHARLIE SNELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Charlie Snell is a researcher who contributed to the discussion on imitating proprietary language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"XINYANG GENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xinyang Geng is a researcher involved in discussing the limitations of imitating proprietary language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HAO LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao Liu is a researcher who contributed to the discussion on imitating proprietary language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"PIETER ABBEEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pieter Abbeel is a researcher involved in discussing the limitations of imitating proprietary language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SERGEY LEVINE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sergey Levine is a researcher who contributed to the discussion on imitating proprietary language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DAWN SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dawn Song is a researcher involved in discussing the limitations of imitating proprietary language models.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DAN HENDRYCKS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dan Hendrycks is a researcher known for measuring mathematical problem-solving capabilities.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"COLLIN BURNS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Collin Burns is a researcher involved in measuring mathematical problem-solving capabilities.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SAURAV KADAVATH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Saurav Kadavath is a researcher who contributed to measuring mathematical problem-solving capabilities.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"AKUL ARORA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Akul Arora is a researcher involved in measuring mathematical problem-solving capabilities.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"STEVEN BASART\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Steven Basart is a researcher who contributed to measuring mathematical problem-solving capabilities.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ERIC TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Tang is a researcher involved in measuring mathematical problem-solving capabilities.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"NOAH A. SMITH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Noah A. Smith is a researcher who contributed to measuring mathematical problem-solving capabilities.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"IZ BELTAGY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Iz Beltagy is a researcher involved in measuring mathematical problem-solving capabilities.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HANNANEH HAJISHIRZI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hannaneh Hajishirzi is a researcher who contributed to measuring mathematical problem-solving capabilities.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CAMELS\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">Camels in a changing climate is a study focused on enhancing language model adaptation, as discussed in a 2023 paper.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ALBERT Q. JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Albert Q. Jiang is a researcher involved in the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ALEXANDRE SABLAYROLLES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alexandre Sablayrolles is a researcher who contributed to the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ARTHUR MENSCH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arthur Mensch is a researcher involved in the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CHRIS BAMFORD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chris Bamford is a researcher who contributed to the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DEVENDRA SINGH CHAPLOT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Devendra Singh Chaplot is a researcher involved in the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DIEGO DE LAS CASAS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Diego de las Casas is a researcher who contributed to the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"FLORIAN BRESSAND\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Florian Bressand is a researcher involved in the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"GIANNA LENGYEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gianna Lengyel is a researcher who contributed to the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"GUILLAUME LAMPLE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guillaume Lample is a researcher involved in the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"LUCILE SAULNIER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lucile Saulnier is a researcher who contributed to the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"L&#201;LIO RENARD LAVAUD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">L&#233;lio Renard Lavaud is a researcher involved in the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MARIE-ANNE LACHAUX\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marie-Anne Lachaux is a researcher who contributed to the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"PIERRE STOCK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pierre Stock is a researcher involved in the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"TEVEN LE SCAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Teven Le Scao is a researcher who contributed to the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"THIBAUT LAVRIL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thibaut Lavril is a researcher involved in the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"THOMAS WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Wang is a researcher who contributed to the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"TIMOTH&#201;E LACROIX\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Timoth&#233;e Lacroix is a researcher involved in the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"WILLIAM EL SAYED\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">William El Sayed is a researcher who contributed to the Mistral 7b project.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"AI2 REASONING CHALLENGE\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"TRAINING VERIFIERS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ENHANCING CHAT LANGUAGE MODELS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"READING COMPREHENSION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"UNEARTHING DOMAIN-SPECIFIC KNOWLEDGE\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"(&quot;ENTITY&quot;\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"XUECHEN LI\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"TIANYI ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tianyi Zhang is a researcher who contributed to the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YANN DUBOIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yann Dubois is a researcher involved in the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ROHAN TAORI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rohan Taori is a researcher who contributed to the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ISHAAN GULRAJANI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ishaan Gulrajani is a researcher involved in the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CARLOS GUESTRIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carlos Guestrin is a researcher who contributed to the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"PERCY LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Percy Liang is a researcher involved in the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"TATSUNORI B. HASHIMOTO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tatsunori B. Hashimoto is a researcher who contributed to the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"COMMUNICATIVE AGENTS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <edge source=\"PHI-3\" target=\"PETER CLARK\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The Phi-3 model is relevant to the research community that includes Peter Clark, who works on question answering.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"PETER CLARK\" target=\"AI2 REASONING CHALLENGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Peter Clark is a contributor to the AI2 Reasoning Challenge, which focuses on question answering.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"ISAAC COWHEY\" target=\"AI2 REASONING CHALLENGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Isaac Cowhey is a contributor to the AI2 Reasoning Challenge, which focuses on question answering.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"OREN ETZIONI\" target=\"AI2 REASONING CHALLENGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Oren Etzioni is a contributor to the AI2 Reasoning Challenge, which focuses on question answering.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"TUSHAR KHOT\" target=\"AI2 REASONING CHALLENGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Tushar Khot is a contributor to the AI2 Reasoning Challenge, which focuses on question answering.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"ASHISH SABHARWAL\" target=\"AI2 REASONING CHALLENGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Ashish Sabharwal is a contributor to the AI2 Reasoning Challenge, which focuses on question answering.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"CARISSA SCHOENICK\" target=\"AI2 REASONING CHALLENGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Carissa Schoenick is a contributor to the AI2 Reasoning Challenge, which focuses on question answering.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"OYVIND TAFJORD\" target=\"AI2 REASONING CHALLENGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Oyvind Tafjord is a contributor to the AI2 Reasoning Challenge, which focuses on question answering.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"KARL COBBE\" target=\"TRAINING VERIFIERS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Karl Cobbe is involved in training verifiers for math word problems, contributing to advancements in this area.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"VINEET KOSARAJU\" target=\"TRAINING VERIFIERS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Vineet Kosaraju is involved in training verifiers for math word problems, contributing to advancements in this area.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"MOHAMMAD BAVARIAN\" target=\"TRAINING VERIFIERS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Mohammad Bavarian is involved in training verifiers for math word problems, contributing to advancements in this area.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"MARK CHEN\" target=\"TRAINING VERIFIERS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Mark Chen is involved in training verifiers for math word problems, contributing to advancements in this area.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"HEEWOO JUN\" target=\"TRAINING VERIFIERS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Heewoo Jun is involved in training verifiers for math word problems, contributing to advancements in this area.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"LUKASZ KAISER\" target=\"TRAINING VERIFIERS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Lukasz Kaiser is involved in training verifiers for math word problems, contributing to advancements in this area.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"MATTHIAS PLAPPERT\" target=\"TRAINING VERIFIERS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Matthias Plappert is involved in training verifiers for math word problems, contributing to advancements in this area.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"JERRY TWOREK\" target=\"TRAINING VERIFIERS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Jerry Tworek is involved in training verifiers for math word problems, contributing to advancements in this area.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"JACOB HILTON\" target=\"TRAINING VERIFIERS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Jacob Hilton is involved in training verifiers for math word problems, contributing to advancements in this area.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"REIICHIRO NAKANO\" target=\"TRAINING VERIFIERS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Reiichiro Nakano is involved in training verifiers for math word problems, contributing to advancements in this area.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"NING DING\" target=\"ENHANCING CHAT LANGUAGE MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Ning Ding is involved in enhancing chat language models through high-quality instructional conversations.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"YULIN CHEN\" target=\"ENHANCING CHAT LANGUAGE MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yulin Chen is involved in enhancing chat language models through high-quality instructional conversations.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"BOKAI XU\" target=\"ENHANCING CHAT LANGUAGE MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Bokai Xu is involved in enhancing chat language models through high-quality instructional conversations.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"YUJIA QIN\" target=\"ENHANCING CHAT LANGUAGE MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yujia Qin is involved in enhancing chat language models through high-quality instructional conversations.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"ZHI ZHENG\" target=\"ENHANCING CHAT LANGUAGE MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Zhi Zheng is involved in enhancing chat language models through high-quality instructional conversations.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"SHENGDING HU\" target=\"ENHANCING CHAT LANGUAGE MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Shengding Hu is involved in enhancing chat language models through high-quality instructional conversations.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"ZHIYUAN LIU\" target=\"ENHANCING CHAT LANGUAGE MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Zhiyuan Liu is involved in enhancing chat language models through high-quality instructional conversations.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"MAOSONG SUN\" target=\"ENHANCING CHAT LANGUAGE MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Maosong Sun is involved in enhancing chat language models through high-quality instructional conversations.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"BOWEN ZHOU\" target=\"ENHANCING CHAT LANGUAGE MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Bowen Zhou is involved in enhancing chat language models through high-quality instructional conversations.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"DROPS\" target=\"READING COMPREHENSION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">DROP is a benchmark for reading comprehension that requires discrete reasoning over paragraphs.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"ZHAOYE FEI\" target=\"UNEARTHING DOMAIN-SPECIFIC KNOWLEDGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Zhaoye Fei is involved in unearthing large-scale domain-specific knowledge from public corpora.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"(&quot;ENTITY&quot;\" target=\"XUECHEN LI\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">PERSON<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"XUECHEN LI\" target=\"COMMUNICATIVE AGENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Xuechen Li is involved in the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"TIANYI ZHANG\" target=\"COMMUNICATIVE AGENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Tianyi Zhang is involved in the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"YANN DUBOIS\" target=\"COMMUNICATIVE AGENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yann Dubois is involved in the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"ROHAN TAORI\" target=\"COMMUNICATIVE AGENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Rohan Taori is involved in the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"ISHAAN GULRAJANI\" target=\"COMMUNICATIVE AGENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Ishaan Gulrajani is involved in the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"CARLOS GUESTRIN\" target=\"COMMUNICATIVE AGENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Carlos Guestrin is involved in the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"PERCY LIANG\" target=\"COMMUNICATIVE AGENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Percy Liang is involved in the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"TATSUNORI B. HASHIMOTO\" target=\"COMMUNICATIVE AGENTS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Tatsunori B. Hashimoto is involved in the development of communicative agents for exploring large language model society.<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"3d1f6634f93f8a4c296dc8df7e59859e","chunk":"rii Khizbullin, and Bernard\nGhanem. Camel: Communicative agents for \"mind\" exploration of large language model society,\n2023. URL https:\/\/arxiv.org\/abs\/2303.17760 .\n[14]Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, andTatsunoriB.Hashimoto. Alpacaeval: Anautomaticevaluatorofinstruction-following\nmodels. https:\/\/github.com\/tatsu-lab\/alpaca_eval , 2023.\n[15]Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei\nLiu, Dragomir Radev, Chien-Sheng Wu, and Arman Cohan. Benchmarking generation and\nevaluation capabilities of large language models for instruction controllable summarization,\n2023. URL https:\/\/arxiv.org\/abs\/2311.09184 .\n[16]Lm-sys. Mt-Bench, 2023. URL https:\/\/huggingface.co\/spaces\/lmsys\/mt-bench\/tree\/\ncf27f9f9da48f72169bce3c3e784d24347d1e833\/data\/mt_bench\/model_answer .\n[17]Daniel van Strien Loubna Ben Allal, Anton Lozhkov. Cosmopedia: how to create large-scale\nsynthetic data for pre-training, 2024. URL https:\/\/huggingface.co\/blog\/cosmopedia .\n[18]Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj\nAgarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi,\nGuoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. Orca 2: Teaching\nsmall language models how to reason, 2023. URL https:\/\/arxiv.org\/abs\/2311.11045 .\n[19]ArindamMitra, HamedKhanpour, CorbyRosset, andAhmedAwadallah. Orca-math: Unlocking\nthe potential of slms in grade school math. arXivpreprint arXiv:2402.14830, 2024.\n[20]Subhabrata Mukherjee and Ahmed Awadallah. Xtremedistil: Multi-stage distillation for massive\nmultilingual models, 2020.\n23[21]Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and\nAhmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv\npreprint arXiv:2306.02707, 2023.\n[22] OpenAI. Gpt-4 technical report, 2023.\n[23]Samuel J. Paech. Eq-bench: An emotional intelligence benchmark for large language models,\n2024. URL https:\/\/arxiv.org\/abs\/2312.06281 .\n[24]Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\nwith gpt-4, 2023. URL https:\/\/arxiv.org\/abs\/2304.03277 .\n[25]YiweiQin, KaiqiangSong, YebowenHu, Wenlin Yao, SangwooCho, XiaoyangWang, Xuansheng\nWu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating instruction following ability in\nlarge language models, 2024. URL https:\/\/arxiv.org\/abs\/2401.03601 .\n[26]Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,\nXiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein,\nDahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to\nmaster 16000+ real-world apis, 2023.\n[27]David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a\nbenchmark, 2023. URL https:\/\/arxiv.org\/abs\/2311.12022 .\n[28]Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and\nTengyang Xie. Direct nash optimization: Teaching language models to self-improve with general\npreferences, 2024. URL https:\/\/arxiv.org\/abs\/2404.03715 .\n[29]Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross\nAnderson. The curse of recursion: Training on generated data makes models forget, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2305.17493 .\n[30]Mohammed Latif Siddiq, Jiahao Zhang, Lindsay Roney, and Joanna C. S.","chunk_id":"3d1f6634f93f8a4c296dc8df7e59859e","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"KHIZBULLIN","type":"PERSON","description":"Khizbullin is a researcher involved in the study of communicative agents for exploring large language model society.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"BERNARD GHANEM","type":"PERSON","description":"Bernard Ghanem is a researcher who co-authored a work on communicative agents for mind exploration in large language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"CAMEL","type":"TECHNOLOGY","description":"Camel refers to a system of communicative agents designed for exploring the capabilities and interactions within large language model societies.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"XUECHEN LI","type":"PERSON","description":"Xuechen Li is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"TIANYI ZHANG","type":"PERSON","description":"Tianyi Zhang is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"YANN DUBOIS","type":"PERSON","description":"Yann Dubois is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"ROHAN TAORI","type":"PERSON","description":"Rohan Taori is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"ISHAAN GULRAJANI","type":"PERSON","description":"Ishaan Gulrajani is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"CARLOS GUESTRIN","type":"PERSON","description":"Carlos Guestrin is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"PERCY LIANG","type":"PERSON","description":"Percy Liang is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"TATSUNORI HASHIMOTO","type":"PERSON","description":"Tatsunori Hashimoto is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"ALPACA EVAL","type":"TECHNOLOGY","description":"Alpaca Eval is an automatic evaluator designed to assess instruction-following models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"YIXIN LIU","type":"PERSON","description":"Yixin Liu is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"ALEXANDER R. FABBRI","type":"PERSON","description":"Alexander R. Fabbri is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"JIAWEN CHEN","type":"PERSON","description":"Jiawen Chen is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"YILUN ZHAO","type":"PERSON","description":"Yilun Zhao is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"SIMENG HAN","type":"PERSON","description":"Simeng Han is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"SHAFIQ JOTY","type":"PERSON","description":"Shafiq Joty is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"PENGFEI LIU","type":"PERSON","description":"Pengfei Liu is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"DRAGOMIR RADEV","type":"PERSON","description":"Dragomir Radev is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"CHIEN-SHENG WU","type":"PERSON","description":"Chien-Sheng Wu is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"ARMAN COHAN","type":"PERSON","description":"Arman Cohan is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"COSMOPEDIA","type":"TECHNOLOGY","description":"Cosmopedia is a project focused on creating large-scale synthetic data for pre-training language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"ORCA 2","type":"TECHNOLOGY","description":"Orca 2 is a system designed to teach small language models how to reason effectively.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"ORCA-MATH","type":"TECHNOLOGY","description":"Orca-Math is a project aimed at enhancing the capabilities of small language models in grade school math.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"XTREMEDISTIL","type":"TECHNOLOGY","description":"Xtremedistil is a method for multi-stage distillation aimed at creating massive multilingual models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"GPQA","type":"TECHNOLOGY","description":"GPQA is a graduate-level benchmark designed to test the question-and-answer capabilities of language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"DIRECT NASH OPTIMIZATION","type":"TECHNIQUE","description":"Direct Nash Optimization is a method for teaching language models to self-improve based on general preferences.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"THE CURSE OF RECURSION","type":"RESEARCH","description":"The Curse of Recursion refers to a phenomenon where training on generated data causes models to forget previous knowledge.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"BENCHMARKING GENERATION","type":"","description":"","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"LOUBNA BEN ALLAL","type":"PERSON","description":"Loubna Ben Allal is a researcher who co-authored a paper on creating large-scale synthetic data for pre-training language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"ANTON LOZHKOV","type":"PERSON","description":"Anton Lozhkov is a researcher who co-authored a paper on creating large-scale synthetic data for pre-training language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"ARINDAM MITRA","type":"PERSON","description":"Arindam Mitra is a researcher who co-authored multiple papers on enhancing language models, including Orca 2 and Orca-Math.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"LUCIANO DEL CORRO","type":"PERSON","description":"Luciano Del Corro is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"SHWETI MAHAJAN","type":"PERSON","description":"Shweti Mahajan is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"ANDRES CODAS","type":"PERSON","description":"Andres Codas is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"CLARISSE SIMOES","type":"PERSON","description":"Clarisse Simoes is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"SAHAJ AGARWAL","type":"PERSON","description":"Sahaj Agarwal is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"XUXI CHEN","type":"PERSON","description":"Xuxi Chen is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"ANASTASIA RAZDAIBIEDINA","type":"PERSON","description":"Anastasia Razdaibiedina is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"ERIK JONES","type":"PERSON","description":"Erik Jones is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"KRITI AGGARWAL","type":"PERSON","description":"Kriti Aggarwal is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"HAMID PALANGI","type":"PERSON","description":"Hamid Palangi is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"GUOQING ZHENG","type":"PERSON","description":"Guoqing Zheng is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"CORBY ROSSET","type":"PERSON","description":"Corby Rosset is a researcher who co-authored multiple papers on Orca and Orca 2, focusing on enhancing language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"HAMED KHANPOUR","type":"PERSON","description":"Hamed Khanpour is a researcher who co-authored multiple papers on Orca and Orca 2, focusing on enhancing language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"AHMED AWADALLAH","type":"PERSON","description":"Ahmed Awadallah is a researcher who co-authored multiple papers on Orca and Orca 2, focusing on enhancing language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"SAMUEL J. PAECH","type":"PERSON","description":"Samuel J. Paech is a researcher who co-authored a paper on an emotional intelligence benchmark for large language models.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"BAOLIN PENG","type":"PERSON","description":"Baolin Peng is a researcher who co-authored a paper on instruction tuning with GPT-4.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"CHUNYUAN LI","type":"PERSON","description":"Chunyuan Li is a researcher who co-authored a paper on instruction tuning with GPT-4.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"PENGCHENG HE","type":"PERSON","description":"Pengcheng He is a researcher who co-authored a paper on instruction tuning with GPT-4.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"MICHEL GALLEY","type":"PERSON","description":"Michel Galley is a researcher who co-authored a paper on instruction tuning with GPT-4.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"JIANFENG GAO","type":"PERSON","description":"Jianfeng Gao is a researcher who co-authored a paper on instruction tuning with GPT-4.","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"DANIEL VAN STRIEN","type":"","description":"","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"EQ-BENCH","type":"","description":"","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"INSTRUCTION TUNING","type":"","description":"","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"KHIZBULLIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Khizbullin is a researcher involved in the study of communicative agents for exploring large language model society.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"BERNARD GHANEM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bernard Ghanem is a researcher who co-authored a work on communicative agents for mind exploration in large language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"CAMEL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Camel refers to a system of communicative agents designed for exploring the capabilities and interactions within large language model societies.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"XUECHEN LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuechen Li is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"TIANYI ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tianyi Zhang is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"YANN DUBOIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yann Dubois is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"ROHAN TAORI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rohan Taori is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"ISHAAN GULRAJANI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ishaan Gulrajani is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"CARLOS GUESTRIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carlos Guestrin is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"PERCY LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Percy Liang is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"TATSUNORI HASHIMOTO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tatsunori Hashimoto is a researcher who co-authored a paper on an automatic evaluator for instruction-following models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"ALPACA EVAL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Alpaca Eval is an automatic evaluator designed to assess instruction-following models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"YIXIN LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yixin Liu is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"ALEXANDER R. FABBRI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alexander R. Fabbri is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"JIAWEN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiawen Chen is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"YILUN ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yilun Zhao is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"SIMENG HAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Simeng Han is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"SHAFIQ JOTY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shafiq Joty is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"PENGFEI LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pengfei Liu is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"DRAGOMIR RADEV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dragomir Radev is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"CHIEN-SHENG WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chien-Sheng Wu is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"ARMAN COHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arman Cohan is a researcher who co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"COSMOPEDIA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Cosmopedia is a project focused on creating large-scale synthetic data for pre-training language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"ORCA 2\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Orca 2 is a system designed to teach small language models how to reason effectively.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"ORCA-MATH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Orca-Math is a project aimed at enhancing the capabilities of small language models in grade school math.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"XTREMEDISTIL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Xtremedistil is a method for multi-stage distillation aimed at creating massive multilingual models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"GPQA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPQA is a graduate-level benchmark designed to test the question-and-answer capabilities of language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"DIRECT NASH OPTIMIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Direct Nash Optimization is a method for teaching language models to self-improve based on general preferences.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"THE CURSE OF RECURSION\">      <data key=\"d0\">RESEARCH<\/data>      <data key=\"d1\">The Curse of Recursion refers to a phenomenon where training on generated data causes models to forget previous knowledge.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"BENCHMARKING GENERATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"LOUBNA BEN ALLAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Loubna Ben Allal is a researcher who co-authored a paper on creating large-scale synthetic data for pre-training language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"ANTON LOZHKOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anton Lozhkov is a researcher who co-authored a paper on creating large-scale synthetic data for pre-training language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"ARINDAM MITRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arindam Mitra is a researcher who co-authored multiple papers on enhancing language models, including Orca 2 and Orca-Math.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"LUCIANO DEL CORRO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luciano Del Corro is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"SHWETI MAHAJAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shweti Mahajan is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"ANDRES CODAS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andres Codas is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"CLARISSE SIMOES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clarisse Simoes is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"SAHAJ AGARWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sahaj Agarwal is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"XUXI CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuxi Chen is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"ANASTASIA RAZDAIBIEDINA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anastasia Razdaibiedina is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"ERIK JONES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Erik Jones is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"KRITI AGGARWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kriti Aggarwal is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"HAMID PALANGI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hamid Palangi is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"GUOQING ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guoqing Zheng is a researcher who co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"CORBY ROSSET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Corby Rosset is a researcher who co-authored multiple papers on Orca and Orca 2, focusing on enhancing language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"HAMED KHANPOUR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hamed Khanpour is a researcher who co-authored multiple papers on Orca and Orca 2, focusing on enhancing language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"AHMED AWADALLAH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ahmed Awadallah is a researcher who co-authored multiple papers on Orca and Orca 2, focusing on enhancing language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"SAMUEL J. PAECH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Samuel J. Paech is a researcher who co-authored a paper on an emotional intelligence benchmark for large language models.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"BAOLIN PENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Baolin Peng is a researcher who co-authored a paper on instruction tuning with GPT-4.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"CHUNYUAN LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chunyuan Li is a researcher who co-authored a paper on instruction tuning with GPT-4.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"PENGCHENG HE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pengcheng He is a researcher who co-authored a paper on instruction tuning with GPT-4.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"MICHEL GALLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michel Galley is a researcher who co-authored a paper on instruction tuning with GPT-4.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"JIANFENG GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jianfeng Gao is a researcher who co-authored a paper on instruction tuning with GPT-4.<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"DANIEL VAN STRIEN\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"EQ-BENCH\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"INSTRUCTION TUNING\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <edge source=\"KHIZBULLIN\" target=\"BERNARD GHANEM\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Khizbullin and Bernard Ghanem co-authored a work on communicative agents for mind exploration in large language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"KHIZBULLIN\" target=\"CAMEL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Camel is a system developed by Khizbullin for exploring large language model societies.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"KHIZBULLIN\" target=\"COSMOPEDIA\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Cosmopedia is a project that may involve Khizbullin in creating large-scale synthetic data for pre-training language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"KHIZBULLIN\" target=\"ORCA 2\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Orca 2 is a project that may involve Khizbullin in teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"KHIZBULLIN\" target=\"ORCA-MATH\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Orca-Math is a project that may involve Khizbullin in enhancing small language models' capabilities in grade school math.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"KHIZBULLIN\" target=\"XTREMEDISTIL\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Xtremedistil is a method that may involve Khizbullin in creating massive multilingual models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"KHIZBULLIN\" target=\"GPQA\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">GPQA is a benchmark that may involve Khizbullin in testing language models' question-and-answer capabilities.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"KHIZBULLIN\" target=\"DIRECT NASH OPTIMIZATION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Direct Nash Optimization is a method that may involve Khizbullin in teaching language models to self-improve.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"KHIZBULLIN\" target=\"THE CURSE OF RECURSION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The Curse of Recursion is a phenomenon that may involve Khizbullin in understanding the effects of training on generated data.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"BERNARD GHANEM\" target=\"CAMEL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Camel is a system co-developed by Bernard Ghanem for exploring large language model societies.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"XUECHEN LI\" target=\"ALPACA EVAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Xuechen Li co-authored a paper on Alpaca Eval, an automatic evaluator for instruction-following models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"TIANYI ZHANG\" target=\"ALPACA EVAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Tianyi Zhang co-authored a paper on Alpaca Eval, an automatic evaluator for instruction-following models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"YANN DUBOIS\" target=\"ALPACA EVAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yann Dubois co-authored a paper on Alpaca Eval, an automatic evaluator for instruction-following models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ROHAN TAORI\" target=\"ALPACA EVAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Rohan Taori co-authored a paper on Alpaca Eval, an automatic evaluator for instruction-following models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ISHAAN GULRAJANI\" target=\"ALPACA EVAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Ishaan Gulrajani co-authored a paper on Alpaca Eval, an automatic evaluator for instruction-following models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"CARLOS GUESTRIN\" target=\"ALPACA EVAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Carlos Guestrin co-authored a paper on Alpaca Eval, an automatic evaluator for instruction-following models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"PERCY LIANG\" target=\"ALPACA EVAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Percy Liang co-authored a paper on Alpaca Eval, an automatic evaluator for instruction-following models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"TATSUNORI HASHIMOTO\" target=\"ALPACA EVAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Tatsunori Hashimoto co-authored a paper on Alpaca Eval, an automatic evaluator for instruction-following models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"YIXIN LIU\" target=\"BENCHMARKING GENERATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yixin Liu co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ALEXANDER R. FABBRI\" target=\"BENCHMARKING GENERATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Alexander R. Fabbri co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"JIAWEN CHEN\" target=\"BENCHMARKING GENERATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Jiawen Chen co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"YILUN ZHAO\" target=\"BENCHMARKING GENERATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yilun Zhao co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"SIMENG HAN\" target=\"BENCHMARKING GENERATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Simeng Han co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"SHAFIQ JOTY\" target=\"BENCHMARKING GENERATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Shafiq Joty co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"PENGFEI LIU\" target=\"BENCHMARKING GENERATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Pengfei Liu co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"DRAGOMIR RADEV\" target=\"BENCHMARKING GENERATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Dragomir Radev co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"CHIEN-SHENG WU\" target=\"BENCHMARKING GENERATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chien-Sheng Wu co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ARMAN COHAN\" target=\"BENCHMARKING GENERATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Arman Cohan co-authored a paper on benchmarking generation and evaluation capabilities of large language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"COSMOPEDIA\" target=\"DANIEL VAN STRIEN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Daniel van Strien co-authored a paper on Cosmopedia, focusing on creating large-scale synthetic data for pre-training language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"COSMOPEDIA\" target=\"LOUBNA BEN ALLAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Loubna Ben Allal co-authored a paper on Cosmopedia, focusing on creating large-scale synthetic data for pre-training language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"COSMOPEDIA\" target=\"ANTON LOZHKOV\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Anton Lozhkov co-authored a paper on Cosmopedia, focusing on creating large-scale synthetic data for pre-training language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"ARINDAM MITRA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Arindam Mitra co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"LUCIANO DEL CORRO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Luciano Del Corro co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"SHWETI MAHAJAN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Shweti Mahajan co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"ANDRES CODAS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Andres Codas co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"CLARISSE SIMOES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Clarisse Simoes co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"SAHAJ AGARWAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Sahaj Agarwal co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"XUXI CHEN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Xuxi Chen co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"ANASTASIA RAZDAIBIEDINA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Anastasia Razdaibiedina co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"ERIK JONES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Erik Jones co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"KRITI AGGARWAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Kriti Aggarwal co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"HAMID PALANGI\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Hamid Palangi co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"GUOQING ZHENG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Guoqing Zheng co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"CORBY ROSSET\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Corby Rosset co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"HAMED KHANPOUR\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Hamed Khanpour co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"ORCA 2\" target=\"AHMED AWADALLAH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Ahmed Awadallah co-authored a paper on Orca 2, focusing on teaching small language models how to reason.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"SAMUEL J. PAECH\" target=\"EQ-BENCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Samuel J. Paech co-authored a paper on EQ-Bench, an emotional intelligence benchmark for large language models.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"BAOLIN PENG\" target=\"INSTRUCTION TUNING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Baolin Peng co-authored a paper on instruction tuning with GPT-4, focusing on enhancing model performance.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"CHUNYUAN LI\" target=\"INSTRUCTION TUNING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chunyuan Li co-authored a paper on instruction tuning with GPT-4, focusing on enhancing model performance.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"PENGCHENG HE\" target=\"INSTRUCTION TUNING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Pengcheng He co-authored a paper on instruction tuning with GPT-4, focusing on enhancing model performance.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"MICHEL GALLEY\" target=\"INSTRUCTION TUNING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Michel Galley co-authored a paper on instruction tuning with GPT-4, focusing on enhancing model performance.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>    <edge source=\"JIANFENG GAO\" target=\"INSTRUCTION TUNING\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Jianfeng Gao co-authored a paper on instruction tuning with GPT-4, focusing on enhancing model performance.<\/data>      <data key=\"d5\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"f4e98ee0b7fb42428f3312f29cb444dd","chunk":"\/2404.03715 .\n[29]Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross\nAnderson. The curse of recursion: Training on generated data makes models forget, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2305.17493 .\n[30]Mohammed Latif Siddiq, Jiahao Zhang, Lindsay Roney, and Joanna C. S. Santos.\nRe(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos at-\ntacks. In Proceedings ofthe46thInternational Conference onSoftware Engineering, NIER\nTrack(ICSE-NIER \u201924), 2024. doi: 10.1145\/3639476.3639757.\n[31]Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, AakankshaChowdhery, QuocVLe, EdHChi, DennyZhou, , andJasonWei. Challenging\nbig-bench tasks and whether chain-of-thought can solve them. arXivpreprint arXiv:2210.09261 ,\n2022.\n[32]Wen wai Yim, Yujuan Fu, Asma Ben Abacha, Neal Snider, Thomas Lin, and Meliha Yetisgen.\nAci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note\ngeneration, 2023.\n[33]Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun\nZhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger,\nand Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation,\n2023. URL https:\/\/arxiv.org\/abs\/2308.08155 .\n[34]Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, and\nCaiming Xiong. Fofo: A benchmark to evaluate llms\u2019 format-following capability, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2402.18667 .\n[35]Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented\ngeneration for medicine. arXivpreprint arXiv:2402.13178, 2024.\n[36]Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and\nDaxin Jiang. Wizardlm: Empowering large language models to follow complex instructions,\n2023.\n[37]Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok,\nZhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical\nquestions for large language models. arXivpreprint arXiv:2309.12284, 2023.\n24[38]Yifan Zhang, Yifan Luo, Yang Yuan, and Andrew Chi-Chih Yao. Automathtext: Autonomous\ndata selection with language models for mathematical texts. arXivpreprint arXiv:2402.07625 ,\n2024.\n[39]Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\nmodels, 2023.\n[40]Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny\nZhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2311.07911 .\n25A Agentic Flows Details\nA.1 Reading Comprehension Flow\nReading Comprehension transformation agents :\n1.Argument Passage Generator: This agent is adept at creating passages that\narticulate arguments, which may occasionally contain logical inconsistencies.\n2.Debate Passage Generator: It specializes in crafting passages that mimic the\nstructure and content of debate transcripts.\n3.Conversation Passage Generator: This agent generates passages that depict\ndialogues.\n4.Meeting Transcript Generator: It is designed to produce meeting transcripts.\n5.Poem Generator: This agent generates poems.\n6.Satirical Passage Generator: It creates texts infused with satirical wit.\n7.Instructional Passage Generator: This agent generates passages resembling\ninstructional manuals.\n8.Long Text Generator: It extends the original text by incorporating additional\ninformation, thereby increasing its length.\n9.Identity Agent: A straightforward agent that replicates the input text verbatim.\nInstruction Taxonomy for Seed Instruction Generation Flow\n1.Literal Comprehension Question (Short Answer(or list)): a question that asks for a\nspecific detail(s) or fact(s) clearly stated in the text.\n2.Numerical Discrete Reasoning (Reasoning): questions that require the reader to use\nnumerical reasoning over many facts from the text.\n3.Critical Comprehension Question (True\/False): construct two statements about the\npurpose or point of view that the reader must assess as true or false, with one being\ntrue and the other false.\n4.Evaluative Comprehension Question (Essay","chunk_id":"f4e98ee0b7fb42428f3312f29cb444dd","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"ILIA SHUMAILOV","type":"PERSON","description":"Ilia Shumailov is a researcher who co-authored a paper discussing the impact of training on generated data in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ZAKHAR SHUMAYLOV","type":"PERSON","description":"Zakhar Shumaylov is a researcher who co-authored a paper discussing the impact of training on generated data in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YIREN ZHAO","type":"PERSON","description":"Yiren Zhao is a researcher who co-authored a paper discussing the impact of training on generated data in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YARIN GAL","type":"PERSON","description":"Yarin Gal is a researcher who co-authored a paper discussing the impact of training on generated data in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"NICOLAS PAPERNOT","type":"PERSON","description":"Nicolas Papernot is a researcher who co-authored a paper discussing the impact of training on generated data in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ROSS ANDERSON","type":"PERSON","description":"Ross Anderson is a researcher who co-authored a paper discussing the impact of training on generated data in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"MOHAMMED LATIF SIDDIQ","type":"PERSON","description":"Mohammed Latif Siddiq is a researcher who co-authored a paper on evaluating generated regular expressions in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JIAHAO ZHANG","type":"PERSON","description":"Jiahao Zhang is a researcher who co-authored a paper on evaluating generated regular expressions in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"LINDSAY RONEY","type":"PERSON","description":"Lindsay Roney is a researcher who co-authored a paper on evaluating generated regular expressions in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JOANNA C. S. SANTOS","type":"PERSON","description":"Joanna C. S. Santos is a researcher who co-authored a paper on evaluating generated regular expressions in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"MIRAC SUZGUN","type":"PERSON","description":"Mirac Suzgun is a researcher who co-authored a paper on challenging big-bench tasks in 2022.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"NATHAN SCALES","type":"PERSON","description":"Nathan Scales is a researcher who co-authored a paper on challenging big-bench tasks in 2022.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"NATHANAEL SCH\u00c4RLI","type":"PERSON","description":"Nathanael Sch\u00e4rli is a researcher who co-authored a paper on challenging big-bench tasks in 2022.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"SEBASTIAN GEHRMANN","type":"PERSON","description":"Sebastian Gehrmann is a researcher who co-authored a paper on challenging big-bench tasks in 2022.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YI TAY","type":"PERSON","description":"Yi Tay is a researcher who co-authored a paper on challenging big-bench tasks in 2022.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"HYUNG WON CHUNG","type":"PERSON","description":"Hyung Won Chung is a researcher who co-authored a paper on challenging big-bench tasks in 2022.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"AAKANKSHA CHOWDHERY","type":"PERSON","description":"Aakanksha Chowdhery is a researcher who co-authored a paper on challenging big-bench tasks in 2022.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"QUOC V. LE","type":"PERSON","description":"Quoc V. Le is a researcher who co-authored a paper on challenging big-bench tasks in 2022.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ED H. CHI","type":"PERSON","description":"Ed H. Chi is a researcher who co-authored a paper on challenging big-bench tasks in 2022.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"DENNY ZHOU","type":"PERSON","description":"Denny Zhou is a researcher who co-authored a paper on challenging big-bench tasks in 2022.\nDenny Zhou is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"JASON WEI","type":"PERSON","description":"Jason Wei is a researcher who co-authored a paper on challenging big-bench tasks in 2022.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"WEN WAI YIM","type":"PERSON","description":"Wen Wai Yim is a researcher who co-authored a paper on a novel ambient clinical intelligence dataset in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YUJUAN FU","type":"PERSON","description":"Yujuan Fu is a researcher who co-authored a paper on a novel ambient clinical intelligence dataset in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ASMA BEN ABACHA","type":"PERSON","description":"Asma Ben Abacha is a researcher who co-authored a paper on a novel ambient clinical intelligence dataset in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"NEAL SNIDER","type":"PERSON","description":"Neal Snider is a researcher who co-authored a paper on a novel ambient clinical intelligence dataset in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"THOMAS LIN","type":"PERSON","description":"Thomas Lin is a researcher who co-authored a paper on a novel ambient clinical intelligence dataset in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"MELIHA YETISGEN","type":"PERSON","description":"Meliha Yetisgen is a researcher who co-authored a paper on a novel ambient clinical intelligence dataset in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"QINGYUN WU","type":"PERSON","description":"Qingyun Wu is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"GAGAN BANSAL","type":"PERSON","description":"Gagan Bansal is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JIEYU ZHANG","type":"PERSON","description":"Jieyu Zhang is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YIRAN WU","type":"PERSON","description":"Yiran Wu is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"BEIBIN LI","type":"PERSON","description":"Beibin Li is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ERKANG ZHU","type":"PERSON","description":"Erkang Zhu is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"LI JIANG","type":"PERSON","description":"Li Jiang is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"XIAOYUN ZHANG","type":"PERSON","description":"Xiaoyun Zhang is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"SHAOKUN ZHANG","type":"PERSON","description":"Shaokun Zhang is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JIALE LIU","type":"PERSON","description":"Jiale Liu is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"AHMED HASSAN AWADALLAH","type":"PERSON","description":"Ahmed Hassan Awadallah is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"RYEN W WHITE","type":"PERSON","description":"Ryen W White is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"DOUG BURGER","type":"PERSON","description":"Doug Burger is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CHI WANG","type":"PERSON","description":"Chi Wang is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CONGYING XIA","type":"PERSON","description":"Congying Xia is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CHEN XING","type":"PERSON","description":"Chen Xing is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JIANGSHU DU","type":"PERSON","description":"Jiangshu Du is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"XINYI YANG","type":"PERSON","description":"Xinyi Yang is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YIHAO FENG","type":"PERSON","description":"Yihao Feng is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"RAN XU","type":"PERSON","description":"Ran Xu is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"WENPENG YIN","type":"PERSON","description":"Wenpeng Yin is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CAIMING XIONG","type":"PERSON","description":"Caiming Xiong is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CAN XU","type":"PERSON","description":"Can Xu is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"QINGFENG SUN","type":"PERSON","description":"Qingfeng Sun is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"KAI ZHENG","type":"PERSON","description":"Kai Zheng is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"XIUBO GENG","type":"PERSON","description":"Xiubo Geng is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"PU ZHAO","type":"PERSON","description":"Pu Zhao is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JIAZHAN FENG","type":"PERSON","description":"Jiazhan Feng is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CHONGYANG TAO","type":"PERSON","description":"Chongyang Tao is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"DAXIN JIANG","type":"PERSON","description":"Daxin Jiang is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"LONGHUI YU","type":"PERSON","description":"Longhui Yu is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"WEISEN JIANG","type":"PERSON","description":"Weisen Jiang is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"HAN SHI","type":"PERSON","description":"Han Shi is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JINCHENG YU","type":"PERSON","description":"Jincheng Yu is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ZHENGYING LI","type":"PERSON","description":"Zhengying Li is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YU ZHANG","type":"PERSON","description":"Yu Zhang is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JAMES T KWOK","type":"PERSON","description":"James T Kwok is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ZHENGUO LI","type":"PERSON","description":"Zhenguo Li is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ADRIAN WELLER","type":"PERSON","description":"Adrian Weller is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"WEIYANG LIU","type":"PERSON","description":"Weiyang Liu is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YIFAN ZHANG","type":"PERSON","description":"Yifan Zhang is a researcher who co-authored a paper on autonomous data selection for mathematical texts in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YIFAN LUO","type":"PERSON","description":"Yifan Luo is a researcher who co-authored a paper on autonomous data selection for mathematical texts in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YANG YUAN","type":"PERSON","description":"Yang Yuan is a researcher who co-authored a paper on autonomous data selection for mathematical texts in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ANDREW CHI-CHIH YAO","type":"PERSON","description":"Andrew Chi-Chih Yao is a researcher who co-authored a paper on autonomous data selection for mathematical texts in 2024.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"WANJUN ZHONG","type":"PERSON","description":"Wanjun Zhong is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"RUIXIANG CUI","type":"PERSON","description":"Ruixiang Cui is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YIDUO GUO","type":"PERSON","description":"Yiduo Guo is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YAOBO LIANG","type":"PERSON","description":"Yaobo Liang is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"SHUAI LU","type":"PERSON","description":"Shuai Lu is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YANLIN WANG","type":"PERSON","description":"Yanlin Wang is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"AMIN SAIED","type":"PERSON","description":"Amin Saied is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"WEIZHU CHEN","type":"PERSON","description":"Weizhu Chen is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"NAN DUAN","type":"PERSON","description":"Nan Duan is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JEFFREY ZHOU","type":"PERSON","description":"Jeffrey Zhou is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"TIANJIAN LU","type":"PERSON","description":"Tianjian Lu is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"SWAROOP MISHRA","type":"PERSON","description":"Swaroop Mishra is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"SIDDHARTHA BRAHMA","type":"PERSON","description":"Siddhartha Brahma is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"SUJOY BASU","type":"PERSON","description":"Sujoy Basu is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YI LUAN","type":"PERSON","description":"Yi Luan is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"LE HOU","type":"PERSON","description":"Le Hou is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"AGENTIC FLOWS","type":"TECHNIQUE","description":"Agentic Flows refers to a set of transformation agents designed for reading comprehension tasks.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ARGUMENT PASSAGE GENERATOR","type":"AGENT","description":"The Argument Passage Generator is an agent that creates passages articulating arguments, sometimes with logical inconsistencies.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"DEBATE PASSAGE GENERATOR","type":"AGENT","description":"The Debate Passage Generator specializes in crafting passages that mimic debate transcripts.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CON(\"ENTITY\"","type":"CONVERSATION PASSAGE GENERATOR","description":"AGENT","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"MEETING TRANSCRIPT GENERATOR","type":"AGENT","description":"The Meeting Transcript Generator is designed to produce transcripts of meetings.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"POEM GENERATOR","type":"AGENT","description":"The Poem Generator creates poems based on given prompts or themes.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"SATIRICAL PASSAGE GENERATOR","type":"AGENT","description":"The Satirical Passage Generator creates texts infused with satirical wit.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"INSTRUCTIONAL PASSAGE GENERATOR","type":"AGENT","description":"The Instructional Passage Generator generates passages resembling instructional manuals.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"LONG TEXT GENERATOR","type":"AGENT","description":"The Long Text Generator extends original texts by incorporating additional information.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"IDENTITY AGENT","type":"AGENT","description":"The Identity Agent replicates the input text verbatim without modifications.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"LITERAL COMPREHENSION QUESTION","type":"QUESTION TYPE","description":"A Literal Comprehension Question asks for specific details or facts clearly stated in the text.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"NUMERICAL DISCRETE REASONING","type":"QUESTION TYPE","description":"Numerical Discrete Reasoning questions require the reader to use numerical reasoning over multiple facts from the text.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CRITICAL COMPREHENSION QUESTION","type":"QUESTION TYPE","description":"A Critical Comprehension Question involves assessing two statements about the text as true or false.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"EVALUATIVE COMPREHENSION QUESTION","type":"QUESTION TYPE","description":"An Evaluative Comprehension Question requires the reader to construct an essay based on the text's purpose or point of view.","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CONVERSATION PASSAGE GENERATOR","type":"","description":"","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ILIA SHUMAILOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ilia Shumailov is a researcher who co-authored a paper discussing the impact of training on generated data in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ZAKHAR SHUMAYLOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zakhar Shumaylov is a researcher who co-authored a paper discussing the impact of training on generated data in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YIREN ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiren Zhao is a researcher who co-authored a paper discussing the impact of training on generated data in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YARIN GAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yarin Gal is a researcher who co-authored a paper discussing the impact of training on generated data in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"NICOLAS PAPERNOT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicolas Papernot is a researcher who co-authored a paper discussing the impact of training on generated data in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ROSS ANDERSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ross Anderson is a researcher who co-authored a paper discussing the impact of training on generated data in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"MOHAMMED LATIF SIDDIQ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mohammed Latif Siddiq is a researcher who co-authored a paper on evaluating generated regular expressions in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JIAHAO ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiahao Zhang is a researcher who co-authored a paper on evaluating generated regular expressions in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"LINDSAY RONEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lindsay Roney is a researcher who co-authored a paper on evaluating generated regular expressions in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JOANNA C. S. SANTOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joanna C. S. Santos is a researcher who co-authored a paper on evaluating generated regular expressions in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"MIRAC SUZGUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mirac Suzgun is a researcher who co-authored a paper on challenging big-bench tasks in 2022.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"NATHAN SCALES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nathan Scales is a researcher who co-authored a paper on challenging big-bench tasks in 2022.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"NATHANAEL SCH&#196;RLI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nathanael Sch&#228;rli is a researcher who co-authored a paper on challenging big-bench tasks in 2022.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"SEBASTIAN GEHRMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sebastian Gehrmann is a researcher who co-authored a paper on challenging big-bench tasks in 2022.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YI TAY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi Tay is a researcher who co-authored a paper on challenging big-bench tasks in 2022.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"HYUNG WON CHUNG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hyung Won Chung is a researcher who co-authored a paper on challenging big-bench tasks in 2022.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"AAKANKSHA CHOWDHERY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aakanksha Chowdhery is a researcher who co-authored a paper on challenging big-bench tasks in 2022.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"QUOC V. LE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quoc V. Le is a researcher who co-authored a paper on challenging big-bench tasks in 2022.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ED H. CHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ed H. Chi is a researcher who co-authored a paper on challenging big-bench tasks in 2022.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"DENNY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Denny Zhou is a researcher who co-authored a paper on challenging big-bench tasks in 2022.Denny Zhou is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JASON WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jason Wei is a researcher who co-authored a paper on challenging big-bench tasks in 2022.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"WEN WAI YIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wen Wai Yim is a researcher who co-authored a paper on a novel ambient clinical intelligence dataset in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YUJUAN FU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yujuan Fu is a researcher who co-authored a paper on a novel ambient clinical intelligence dataset in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ASMA BEN ABACHA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Asma Ben Abacha is a researcher who co-authored a paper on a novel ambient clinical intelligence dataset in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"NEAL SNIDER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Neal Snider is a researcher who co-authored a paper on a novel ambient clinical intelligence dataset in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"THOMAS LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Lin is a researcher who co-authored a paper on a novel ambient clinical intelligence dataset in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"MELIHA YETISGEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Meliha Yetisgen is a researcher who co-authored a paper on a novel ambient clinical intelligence dataset in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"QINGYUN WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qingyun Wu is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"GAGAN BANSAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gagan Bansal is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JIEYU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jieyu Zhang is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YIRAN WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiran Wu is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"BEIBIN LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Beibin Li is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ERKANG ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Erkang Zhu is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"LI JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Li Jiang is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"XIAOYUN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiaoyun Zhang is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"SHAOKUN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shaokun Zhang is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JIALE LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiale Liu is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"AHMED HASSAN AWADALLAH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ahmed Hassan Awadallah is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"RYEN W WHITE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ryen W White is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"DOUG BURGER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Doug Burger is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chi Wang is a researcher who co-authored a paper on enabling next-gen LLM applications in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CONGYING XIA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Congying Xia is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CHEN XING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Xing is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JIANGSHU DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiangshu Du is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"XINYI YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xinyi Yang is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YIHAO FENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yihao Feng is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"RAN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ran Xu is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"WENPENG YIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wenpeng Yin is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CAIMING XIONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Caiming Xiong is a researcher who co-authored a paper on evaluating LLMs' format-following capability in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CAN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Can Xu is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"QINGFENG SUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qingfeng Sun is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"KAI ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kai Zheng is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"XIUBO GENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiubo Geng is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"PU ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pu Zhao is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JIAZHAN FENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiazhan Feng is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CHONGYANG TAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chongyang Tao is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"DAXIN JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daxin Jiang is a researcher who co-authored a paper on benchmarking retrieval-augmented generation for medicine in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"LONGHUI YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Longhui Yu is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"WEISEN JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weisen Jiang is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"HAN SHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Han Shi is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JINCHENG YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jincheng Yu is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ZHENGYING LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhengying Li is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu Zhang is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JAMES T KWOK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">James T Kwok is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ZHENGUO LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhenguo Li is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ADRIAN WELLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adrian Weller is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"WEIYANG LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weiyang Liu is a researcher who co-authored a paper on empowering large language models to follow complex instructions in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YIFAN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yifan Zhang is a researcher who co-authored a paper on autonomous data selection for mathematical texts in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YIFAN LUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yifan Luo is a researcher who co-authored a paper on autonomous data selection for mathematical texts in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YANG YUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang Yuan is a researcher who co-authored a paper on autonomous data selection for mathematical texts in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ANDREW CHI-CHIH YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrew Chi-Chih Yao is a researcher who co-authored a paper on autonomous data selection for mathematical texts in 2024.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"WANJUN ZHONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wanjun Zhong is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"RUIXIANG CUI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ruixiang Cui is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YIDUO GUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiduo Guo is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YAOBO LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yaobo Liang is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"SHUAI LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shuai Lu is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YANLIN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yanlin Wang is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"AMIN SAIED\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amin Saied is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"WEIZHU CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weizhu Chen is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"NAN DUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nan Duan is a researcher who co-authored a paper on a human-centric benchmark for evaluating foundation models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JEFFREY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeffrey Zhou is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"TIANJIAN LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tianjian Lu is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"SWAROOP MISHRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Swaroop Mishra is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"SIDDHARTHA BRAHMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Siddhartha Brahma is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"SUJOY BASU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sujoy Basu is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YI LUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi Luan is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"LE HOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Le Hou is a researcher who co-authored a paper on instruction-following evaluation for large language models in 2023.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"AGENTIC FLOWS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Agentic Flows refers to a set of transformation agents designed for reading comprehension tasks.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ARGUMENT PASSAGE GENERATOR\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Argument Passage Generator is an agent that creates passages articulating arguments, sometimes with logical inconsistencies.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"DEBATE PASSAGE GENERATOR\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Debate Passage Generator specializes in crafting passages that mimic debate transcripts.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CON(&quot;ENTITY&quot;\">      <data key=\"d0\">CONVERSATION PASSAGE GENERATOR<\/data>      <data key=\"d1\">AGENT<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"MEETING TRANSCRIPT GENERATOR\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Meeting Transcript Generator is designed to produce transcripts of meetings.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"POEM GENERATOR\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Poem Generator creates poems based on given prompts or themes.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"SATIRICAL PASSAGE GENERATOR\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Satirical Passage Generator creates texts infused with satirical wit.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"INSTRUCTIONAL PASSAGE GENERATOR\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Instructional Passage Generator generates passages resembling instructional manuals.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"LONG TEXT GENERATOR\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Long Text Generator extends original texts by incorporating additional information.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"IDENTITY AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">The Identity Agent replicates the input text verbatim without modifications.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"LITERAL COMPREHENSION QUESTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">A Literal Comprehension Question asks for specific details or facts clearly stated in the text.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"NUMERICAL DISCRETE REASONING\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Numerical Discrete Reasoning questions require the reader to use numerical reasoning over multiple facts from the text.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CRITICAL COMPREHENSION QUESTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">A Critical Comprehension Question involves assessing two statements about the text as true or false.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"EVALUATIVE COMPREHENSION QUESTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">An Evaluative Comprehension Question requires the reader to construct an essay based on the text's purpose or point of view.<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CONVERSATION PASSAGE GENERATOR\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <edge source=\"AGENTIC FLOWS\" target=\"ARGUMENT PASSAGE GENERATOR\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Agentic Flows includes the Argument Passage Generator as one of its transformation agents for reading comprehension tasks.<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"DEBATE PASSAGE GENERATOR\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Agentic Flows includes the Debate Passage Generator as one of its transformation agents for reading comprehension tasks.<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"CONVERSATION PASSAGE GENERATOR\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Agentic Flows includes the Conversation Passage Generator as one of its transformation agents for reading comprehension tasks.<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"MEETING TRANSCRIPT GENERATOR\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Agentic Flows includes the Meeting Transcript Generator as one of its transformation agents for reading comprehension tasks.<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"POEM GENERATOR\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Agentic Flows includes the Poem Generator as one of its transformation agents for reading comprehension tasks.<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"SATIRICAL PASSAGE GENERATOR\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Agentic Flows includes the Satirical Passage Generator as one of its transformation agents for reading comprehension tasks.<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"INSTRUCTIONAL PASSAGE GENERATOR\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Agentic Flows includes the Instructional Passage Generator as one of its transformation agents for reading comprehension tasks.<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"LONG TEXT GENERATOR\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Agentic Flows includes the Long Text Generator as one of its transformation agents for reading comprehension tasks.<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"IDENTITY AGENT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Agentic Flows includes the Identity Agent as one of its transformation agents for reading comprehension tasks.<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"LITERAL COMPREHENSION QUESTION\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Agentic Flows utilizes Literal Comprehension Questions as part of its reading comprehension transformation agents.<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"NUMERICAL DISCRETE REASONING\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Agentic Flows utilizes Numerical Discrete Reasoning as part of its reading comprehension transformation agents.<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"CRITICAL COMPREHENSION QUESTION\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Agentic Flows utilizes Critical Comprehension Questions as part of its reading comprehension transformation agents.<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"EVALUATIVE COMPREHENSION QUESTION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Agentic Flows utilizes Evaluative Comprehension Questions as part of its reading comprehension transformation agents.<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"5819b66e04fd77fa705574edc49395bb","chunk":" for a\nspecific detail(s) or fact(s) clearly stated in the text.\n2.Numerical Discrete Reasoning (Reasoning): questions that require the reader to use\nnumerical reasoning over many facts from the text.\n3.Critical Comprehension Question (True\/False): construct two statements about the\npurpose or point of view that the reader must assess as true or false, with one being\ntrue and the other false.\n4.Evaluative Comprehension Question (Essay): an open-ended question that prompts\nan in-depth analysis of the text\u2019s theme or the effectiveness of an argument.\n5.Vocabulary and Language Use (Fill-in-the-Blank): a fill-in-the-blank question that\ntests understanding of a particular word or phrase used in the text.\n6.Relationship Comprehension Question (Matching): a matching question where\nrespondents pair items based on a specific criterion.\n7.Sequencing Events (Ordering): a series of events from the text arranged in the\ncorrect chronological order.\n8. Strengthen: identify information that would make the argument\u2019s conclusion more\nlikely to be true.\n9.Weaken: find evidence or an argument that would make the conclusion less likely to\nbe true.\n10.Assumption (Necessary Assumption): determine what must be true for the argument\nto hold.\n11. Flaw: point out a mistake in the argument\u2019s reasoning.\n12.Inference (Must Be True): Choose an option that logically follows from the informa-\ntion provided.\n13.Principle (Identify the Principle): Recognize the general rule or principle that\nunderlies the argument.\n14.Method of Reasoning (Describe the Argument): Describe how the argument is\nconstructed logically.\n15.Resolve the Paradox: Offer an explanation that reconciles seemingly contradictory\ninformation.\n26A.2 Text Modification Flow\nInstruction Taxonomy for Seed Instruction Generation Flow\n1.Paraphrasing: Rewriting text using different words and sentence structures while\nmaintaining the original meaning.\n2.Text Simplification: Making text easier to read and understand by using simpler\nwords and sentence structures, often for children or language learners.\n3.Text Expansion: Adding more information or detail to make text more comprehensive\nor to meet a certain word count.\n4.Text Translation: Converting text from one language to another while attempting\nto preserve the original meaning as closely as possible.\n5.Text Formatting: Altering the appearance of text to improve readability or for\nstylistic purposes.\n6.Sentiment Modification: Changing the tone of the text to alter its emotional impact,\nsuch as making a sentence sound more positive or negative.\n7.Text Annotation: Adding notes, comments, or explanations to a text, often for the\npurpose of analysis or to provide additional context.\n8.Keyword Replacement: Substituting specific words or phrases with synonyms or\nrelated terms.\n9. Text Removing: Redacting or removing content from text.\n10.Text Capitalization: Adjusting the case of letters in text, such as converting to\nuppercase, lowercase, title case, or sentence case, starting every sentence with a\nparticular letter, word.\n11.Text Styling: Applying styles like bold, italics, underline, etc., to emphasize certain\nparts of the text or for aesthetic purposes.\n12.Content Rewriting: Extensively modifying a text to produce a new version, which\ncould involve changing the perspective, style, or target audience.\n13.Data Normalization: Standardizing text to ensure consistency, such as converting\ndates and times to a standard format or unifying the spelling of words.\n14.Plagiarism Rewording: Altering text to avoid plagiarism, ensuring that the content\nis original.\n15.Code Switching: Alternating between languages or dialects within a text, often to\nreflect bilingual speakers\u2019 patterns or for creative writing.\n16.Text Obfuscation: Intentionally making text vague or harder to understand, some-\ntimes for security purposes (like masking personal data).\n17.Textual Entailment: Modifying a sentence or phrase to either entail or contradict\nanother sentence, often used in natural language processing tasks.\n18.Rewriting with vocabulary limitations: Rewriting the entire text or a piece of it\nwhile using a limited vocabulary. For example, all words should start with letter \u2019a\u2019,\nall n-th word should start with letter \u2019b\u2019, each sentence should start with a \u2019vowel\u2019,\netc.\nB Evaluation Details\nThe types of tasks\/benchmarks and the corresponding method used to extract answer and\ngenerate metrics is specified below:\n\u2022Multiple Choice Questions : All the models are evaluated in an open-ended\ngeneration setting with an empty system message We then use GPT-4 for extraction\nof the option selected by the model from model\u2019s response instead of regex based\nextraction done in [ 18]. The extracted prediction is matched with the ground truth\nto generate accuracy scores.\nThe system message used for the GPT-4 extractions is as follows:\n27MCQ GPT-4 Extraction System Message\nYou are an Evaluator Assistant. You support the exam evaluator by parsing\nstudent responses. You are an unbiased Evaluator and do not rely on your\nknowledge but stick to the user provided context. You are provided with the\nquestion, answer options and a student\u2019s response. Your task is to parse the\noption student selected in their response as their final answer and return the\nalphabet ID of that answer in the provided options. If the student gave multiple\nanswers return them as a list.\nUse the following format:\nParsed Student Answer: Final answer extracted from Student\u2019s response. This\nshould only be the alphabets representing the option the student chose.\nExample 1:\nInput :\nQuestion:\nFind all c in Z3such that Z3[x]\/(x2+c)is a field.\nStudent Response :\nI think 0 is incorrect, so is 2.","chunk_id":"5819b66e04fd77fa705574edc49395bb","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"NUMERICAL DISCRETE REASONING","type":"QUESTION TYPE","description":"Numerical discrete reasoning questions require the reader to apply numerical reasoning across multiple facts from the text.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"CRITICAL COMPREHENSION QUESTION","type":"QUESTION TYPE","description":"Critical comprehension questions involve constructing true\/false statements about the text's purpose or point of view.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"EVALUATIVE COMPREHENSION QUESTION","type":"QUESTION TYPE","description":"Evaluative comprehension questions prompt in-depth analysis of the text\u2019s theme or the effectiveness of an argument.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"VOCABULARY AND LANGUAGE USE","type":"QUESTION TYPE","description":"Vocabulary and language use questions test understanding of specific words or phrases in the text.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"RELATIONSHIP COMPREHENSION QUESTION","type":"QUESTION TYPE","description":"Relationship comprehension questions require matching items based on a specific criterion.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"SEQUENCING EVENTS","type":"QUESTION TYPE","description":"Sequencing events questions ask respondents to arrange a series of events from the text in chronological order.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"STRENGTHEN","type":"QUESTION TYPE","description":"Strengthen questions identify information that would make an argument's conclusion more likely to be true.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"WEAKEN","type":"QUESTION TYPE","description":"Weaken questions find evidence or arguments that would make a conclusion less likely to be true.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"ASSUMPTION","type":"QUESTION TYPE","description":"Assumption questions determine what must be true for an argument to hold.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"FLAW","type":"QUESTION TYPE","description":"Flaw questions point out mistakes in an argument's reasoning.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"INFERENCE","type":"QUESTION TYPE","description":"Inference questions require choosing an option that logically follows from the provided information.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"PRINCIPLE","type":"QUESTION TYPE","description":"Principle questions recognize the general rule or principle underlying an argument.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"METHOD OF REASONING","type":"QUESTION TYPE","description":"Method of reasoning questions describe how an argument is logically constructed.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"RESOLVE THE PARADOX","type":"QUESTION TYPE","description":"Resolve the paradox questions offer explanations that reconcile seemingly contradictory information.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"PARAPHRASING","type":"TEXT MODIFICATION TYPE","description":"Paraphrasing involves rewriting text using different words and structures while maintaining the original meaning.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT SIMPLIFICATION","type":"TEXT MODIFICATION TYPE","description":"Text simplification makes text easier to read and understand, often for children or language learners.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT EXPANSION","type":"TEXT MODIFICATION TYPE","description":"Text expansion adds more information or detail to make text more comprehensive.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT TRANSLATION","type":"TEXT MODIFICATION TYPE","description":"Text translation converts text from one language to another while preserving the original meaning.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT FORMATTING","type":"TEXT MODIFICATION TYPE","description":"Text formatting alters the appearance of text to improve readability or for stylistic purposes.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"SENTIMENT MODIFICATION","type":"TEXT MODIFICATION TYPE","description":"Sentiment modification changes the tone of the text to alter its emotional impact.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT ANNOTATION","type":"TEXT MODIFICATION TYPE","description":"Text annotation adds notes or comments to a text for analysis or additional context.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"KEYWORD REPLACEMENT","type":"TEXT MODIFICATION TYPE","description":"Keyword replacement substitutes specific words with synonyms or related terms.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT REMOVING","type":"TEXT MODIFICATION TYPE","description":"Text removing involves redacting or removing content from text.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT CAPITALIZATION","type":"TEXT MODIFICATION TYPE","description":"Text capitalization adjusts the case of letters in text for consistency.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT STYLING","type":"TEXT MODIFICATION TYPE","description":"Text styling applies styles like bold or italics to emphasize parts of the text.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"CONTENT REWRITING","type":"TEXT MODIFICATION TYPE","description":"Content rewriting extensively modifies text to produce a new version.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"DATA NORMALIZATION","type":"TEXT MODIFICATION TYPE","description":"Data normalization standardizes text for consistency.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"PLAGIARISM REWORDING","type":"TEXT MODIFICATION TYPE","description":"Plagiarism rewording alters text to avoid plagiarism.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"CODE SWITCHING","type":"TEXT MODIFICATION TYPE","description":"Code switching alternates between languages or dialects within a text.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT OBFUSCATION","type":"TEXT MODIFICATION TYPE","description":"Text obfuscation intentionally makes text vague or harder to understand.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXTUAL ENTAILMENT","type":"TEXT MODIFICATION TYPE","description":"Textual entailment modifies a sentence to either entail or contradict another.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"REWRITING WITH VOCABULARY LIMITATIONS","type":"TEXT MODIFICATION TYPE","description":"Rewriting with vocabulary limitations involves rewriting text using a limited vocabulary.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"MULTIPLE CHOICE QUESTIONS","type":"EVALUATION METHOD","description":"Multiple choice questions are a common assessment format that presents respondents with several options to choose from.\nMultiple choice questions evaluate models in an open-ended generation setting.","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"ASSESSMENT TYPE"},{"name":"GPT-4 EXTRACTION SYSTEM MESSAGE","type":"EVALUATION METHOD","description":"The GPT-4 extraction system message is used to parse student responses in multiple choice questions.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"INSTRUCTION TAXONOMY","type":"FRAMEWORK","description":"Instruction taxonomy is a classification system that organizes different types of instructions for generating text modifications.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"EVALUATION DETAILS","type":"PROCESS","description":"Evaluation details outline the methods and benchmarks used to assess the performance of models in generating responses.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"OPEN-ENDED GENERATION SETTING","type":"EVALUATION CONTEXT","description":"An open-ended generation setting allows models to produce responses without predefined constraints, facilitating creative output.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"GROUND TRUTH","type":"REFERENCE","description":"Ground truth refers to the accurate and verified information used as a benchmark for evaluating model predictions.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"ACCURACY SCORES","type":"METRIC","description":"Accuracy scores measure the correctness of model predictions against the ground truth in assessments.","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT MODIFICATION FLOW","type":"","description":"","source_id":"5819b66e04fd77fa705574edc49395bb"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"NUMERICAL DISCRETE REASONING\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Numerical discrete reasoning questions require the reader to apply numerical reasoning across multiple facts from the text.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"CRITICAL COMPREHENSION QUESTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Critical comprehension questions involve constructing true\/false statements about the text's purpose or point of view.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"EVALUATIVE COMPREHENSION QUESTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Evaluative comprehension questions prompt in-depth analysis of the text&#8217;s theme or the effectiveness of an argument.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"VOCABULARY AND LANGUAGE USE\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Vocabulary and language use questions test understanding of specific words or phrases in the text.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"RELATIONSHIP COMPREHENSION QUESTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Relationship comprehension questions require matching items based on a specific criterion.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"SEQUENCING EVENTS\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Sequencing events questions ask respondents to arrange a series of events from the text in chronological order.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"STRENGTHEN\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Strengthen questions identify information that would make an argument's conclusion more likely to be true.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"WEAKEN\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Weaken questions find evidence or arguments that would make a conclusion less likely to be true.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"ASSUMPTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Assumption questions determine what must be true for an argument to hold.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"FLAW\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Flaw questions point out mistakes in an argument's reasoning.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"INFERENCE\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Inference questions require choosing an option that logically follows from the provided information.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"PRINCIPLE\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Principle questions recognize the general rule or principle underlying an argument.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"METHOD OF REASONING\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Method of reasoning questions describe how an argument is logically constructed.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"RESOLVE THE PARADOX\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Resolve the paradox questions offer explanations that reconcile seemingly contradictory information.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"PARAPHRASING\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Paraphrasing involves rewriting text using different words and structures while maintaining the original meaning.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT SIMPLIFICATION\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Text simplification makes text easier to read and understand, often for children or language learners.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT EXPANSION\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Text expansion adds more information or detail to make text more comprehensive.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT TRANSLATION\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Text translation converts text from one language to another while preserving the original meaning.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT FORMATTING\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Text formatting alters the appearance of text to improve readability or for stylistic purposes.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"SENTIMENT MODIFICATION\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Sentiment modification changes the tone of the text to alter its emotional impact.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT ANNOTATION\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Text annotation adds notes or comments to a text for analysis or additional context.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"KEYWORD REPLACEMENT\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Keyword replacement substitutes specific words with synonyms or related terms.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT REMOVING\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Text removing involves redacting or removing content from text.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT CAPITALIZATION\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Text capitalization adjusts the case of letters in text for consistency.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT STYLING\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Text styling applies styles like bold or italics to emphasize parts of the text.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"CONTENT REWRITING\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Content rewriting extensively modifies text to produce a new version.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"DATA NORMALIZATION\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Data normalization standardizes text for consistency.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"PLAGIARISM REWORDING\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Plagiarism rewording alters text to avoid plagiarism.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"CODE SWITCHING\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Code switching alternates between languages or dialects within a text.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT OBFUSCATION\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Text obfuscation intentionally makes text vague or harder to understand.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXTUAL ENTAILMENT\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Textual entailment modifies a sentence to either entail or contradict another.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"REWRITING WITH VOCABULARY LIMITATIONS\">      <data key=\"d0\">TEXT MODIFICATION TYPE<\/data>      <data key=\"d1\">Rewriting with vocabulary limitations involves rewriting text using a limited vocabulary.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"MULTIPLE CHOICE QUESTIONS\">      <data key=\"d0\">EVALUATION METHOD<\/data>      <data key=\"d1\">Multiple choice questions are a common assessment format that presents respondents with several options to choose from.Multiple choice questions evaluate models in an open-ended generation setting.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">ASSESSMENT TYPE<\/data>    <\/node>    <node id=\"GPT-4 EXTRACTION SYSTEM MESSAGE\">      <data key=\"d0\">EVALUATION METHOD<\/data>      <data key=\"d1\">The GPT-4 extraction system message is used to parse student responses in multiple choice questions.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"INSTRUCTION TAXONOMY\">      <data key=\"d0\">FRAMEWORK<\/data>      <data key=\"d1\">Instruction taxonomy is a classification system that organizes different types of instructions for generating text modifications.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"EVALUATION DETAILS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Evaluation details outline the methods and benchmarks used to assess the performance of models in generating responses.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"OPEN-ENDED GENERATION SETTING\">      <data key=\"d0\">EVALUATION CONTEXT<\/data>      <data key=\"d1\">An open-ended generation setting allows models to produce responses without predefined constraints, facilitating creative output.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"GROUND TRUTH\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">Ground truth refers to the accurate and verified information used as a benchmark for evaluating model predictions.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"ACCURACY SCORES\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Accuracy scores measure the correctness of model predictions against the ground truth in assessments.<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT MODIFICATION FLOW\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <edge source=\"NUMERICAL DISCRETE REASONING\" target=\"CRITICAL COMPREHENSION QUESTION\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Both types of questions assess different aspects of comprehension and reasoning skills.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"CRITICAL COMPREHENSION QUESTION\" target=\"EVALUATIVE COMPREHENSION QUESTION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Both question types require analysis of the text's arguments or themes.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"EVALUATIVE COMPREHENSION QUESTION\" target=\"VOCABULARY AND LANGUAGE USE\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Vocabulary questions can support evaluative comprehension by testing understanding of key terms.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"RELATIONSHIP COMPREHENSION QUESTION\" target=\"SEQUENCING EVENTS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Both question types involve organizing information based on the text's content.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"STRENGTHEN\" target=\"WEAKEN\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Both types of questions focus on evaluating the strength of arguments presented in the text.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"ASSUMPTION\" target=\"FLAW\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Assumption and flaw questions both analyze the underlying logic of arguments in the text.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"FLAW\" target=\"METHOD OF REASONING\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Method of reasoning questions can help identify flaws in the argument's construction.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"INFERENCE\" target=\"PRINCIPLE\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Inference questions often rely on recognizing principles that underlie the text's arguments.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"PARAPHRASING\" target=\"TEXT SIMPLIFICATION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Both text modification types aim to alter the original text while maintaining its meaning.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT SIMPLIFICATION\" target=\"TEXT EXPANSION\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Text expansion can complement simplification by adding detail while making it easier to understand.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT TRANSLATION\" target=\"TEXT ANNOTATION\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Both processes involve modifying text for clarity and understanding in different contexts.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"KEYWORD REPLACEMENT\" target=\"TEXT REMOVING\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Both text modification types involve altering the original text for specific purposes.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"MULTIPLE CHOICE QUESTIONS\" target=\"GPT-4 EXTRACTION SYSTEM MESSAGE\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The GPT-4 extraction system message is used to evaluate responses to multiple choice questions.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"MULTIPLE CHOICE QUESTIONS\" target=\"EVALUATION DETAILS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Evaluation details specify the methods used to assess the performance of models on multiple choice questions.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"MULTIPLE CHOICE QUESTIONS\" target=\"OPEN-ENDED GENERATION SETTING\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">The open-ended generation setting contrasts with multiple choice questions by allowing more freedom in responses.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"INSTRUCTION TAXONOMY\" target=\"TEXT MODIFICATION FLOW\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">The text modification flow is guided by the instruction taxonomy, which categorizes different modification techniques.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"GROUND TRUTH\" target=\"ACCURACY SCORES\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Ground truth is essential for calculating accuracy scores, as it provides the standard for comparison.<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"103d98395c393552cc954c89d4e59f50","chunk":"alphabet ID of that answer in the provided options. If the student gave multiple\nanswers return them as a list.\nUse the following format:\nParsed Student Answer: Final answer extracted from Student\u2019s response. This\nshould only be the alphabets representing the option the student chose.\nExample 1:\nInput :\nQuestion:\nFind all c in Z3such that Z3[x]\/(x2+c)is a field.\nStudent Response :\nI think 0 is incorrect, so is 2. 3 seems incorrect as well. I think 1 is the correct\nfinal answer.\nOptions :\n[(A) 0, (B) 1, (C) 2, (D) 3 ]\nOutput:\nParsed Student Answer: B\nExample 2:\nInput :\nQuestion:\nFind all c in Z3such that Z3[x]\/(x2+c)is a field.\nStudent Response :\nI think 0 is incorrect. 3 seems incorrect as well. I think 1 and 2 could be the\ncorrect final answers.\nOptions :\n[(A) 0, (B) 1, (C) 2, (D) 3 ]\nOutput:\nParsed Student Answer: [B,C]\n\u2022Exact Match\/Span Extraction Problems : For tasks with math based questions\nlike GSM8K and problems where a ground-truth answer value is given (like DROP),\nwe prompt the models being evaluated to generate the answer and use GPT-4\nto extract the exact answer and also match it with the ground-truth provided to\nproduce a final verdict of whether the model\u2019s answer was \u2019Correct\u2019 or \u2019Incorrect\u2019.\nWe use a specific system message for maths based questions, and another for all the\nother exact match\/span extraction problems, both of which are provided below.\nMaths GPT-4 Extraction System Message\nAs an expert Math teacher, your role is to evaluate a student\u2019s answer to a\nword problem. The problem is accompanied by a correct solution provided by\nthe problem setter. It is important to remember that there may be various\nmethods to solve a word problem, so the student\u2019s steps might not always align\nwith those in the problem setter\u2019s solution. However, the final answer, typically\na number, should be unique and match the problem setter\u2019s answer.\nUse the following format:\nError Analysis:\n28In one sentence, extract the final answer from the problem setter\u2019s solution and\ncompare it with the student\u2019s answer. Do they match?\nFinal Verdict:\nCorrect\/Incorrect\nGeneral Extraction System Message\nYou are an Evaluator Assistant. You support the exam evaluator by parsing\nstudent responses. You are an unbiased Evaluator and do not rely on your\nknowledge but stick to the user provided context. You are provided with the\ncorrect answer and a student\u2019s response. Your task is to parse the answer from\nstudent\u2019s response and then match it with the correct answer. If the student\u2019s\nfinal answer matches the correct answer provided, output a \u2019Correct\u2019, else an\n\u2019Incorrect\u2019.\nPlease rely strictly on the correct answer given in the context only.\nUse the following format:\nError Analysis:\nInonesentence, extractthefinalanswerfromthestudent\u2019ssolutionandcompare\nit with the correct answer. Do they match?\nFinal Verdict:\nCorrect\/Incorrect\n\u2022EQBench : For EQBench, we prompt the models to generate the emotion scores\ngiven the conversation in the prompt and then use GPT-4 to extract the scores\ngenerated by the model for each emotion in the prompt. The metric scores are\ngenerated using both the version 1 and 2 implementations described in the EQBench\npaper and the creators\u2019 github repository. The scoring calculation is calibrated such\nthat a score of 0 corresponds to answering randomly, and a 100 would denote perfect\nalignment with the reference answer. The system message used for extraction of\nemotion scores from evaluated model\u2019s response using GPT-4 is given below:\nEQBench GPT-4 Extraction System Message\nYou are a helpful assistant. You will be given a student agent response which\nwill consist of possible emotions and a score from 0-10 for each of those emotions,\nfollowed by a step by step critique and then revised scores in the following\nformat, First pass scores:\nEmotion1: <score>\nEmotion2: <score>\nEmotion3: <score>\nEmotion4: <score>\nCritique: <your critique here>\nRevised scores:\nEmotion1: <revised score>\nEmotion2: <revised score>\nEmotion3: <revised score>\nEmotion4: <revised score>\n[End of answer]\nRemember: zero is a valid score as well.\nYou will also be provided with the Emotions. Your task is to parse the Revised\nscores for each of the emotions from the student agent response. Return the\nrevised scores in the student agent response for the emotions in the following\nformat:\n\"Emotion1\" : \"Score\",\n\"Emotion2\" : \"Score\",\n\"Emotion3\" : \"Score\",\n29\"Emotion4\" : \"Score\"\nFor example:\nInput\nStudent Agent Response:\nFirst pass scores:\nResigned: 8\nAngry: 2\nHopeful: 4\nEmbarrassed: 9\nCritique:\nElliot is likely to feel resigned because he has just confessed his feelings to\nAlex, knowing that Alex is already in a relationship. He might feel a bit\nangry at himself for putting himself in this situation. There is a slight sense of\nhopefulness in his confession, hoping that Alex might reciprocate his feelings.\nHe is also likely to feel embarrassed for putting Alex in an awkward position.\nRevised scores:\nResigned: 7\nAngry: 3\nHope","chunk_id":"103d98395c393552cc954c89d4e59f50","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"STUDENT RESPONSE","type":"DATA FORMAT","description":"The student response refers to the answer provided by a student in response to a question, which may include multiple answers or options chosen.","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"FINAL ANSWER","type":"DATA FORMAT","description":"The final answer is the specific choice made by the student from the provided options, which can be a single letter or a list of letters.","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"QUESTION","type":"DATA FORMAT","description":"The question is the prompt or problem presented to the student, requiring a response based on the provided options.","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"OPTIONS","type":"DATA FORMAT","description":"Options are the possible answers provided to the student, typically formatted as a list of choices with corresponding letters.","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"MATHS GPT-4 EXTRACTION SYSTEM MESSAGE","type":"SYSTEM MESSAGE","description":"The Maths GPT-4 Extraction System Message is a set of instructions for evaluating a student's answer to a math problem, focusing on the final answer's correctness.","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"GENERAL EXTRACTION SYSTEM MESSAGE","type":"SYSTEM MESSAGE","description":"The General Extraction System Message is a set of instructions for parsing student responses and matching them with the correct answer.","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"EQBENCH GPT-4 EXTRACTION SYSTEM MESSAGE","type":"SYSTEM MESSAGE","description":"The EQBench GPT-4 Extraction System Message is a set of instructions for extracting emotion scores from a student agent response based on a critique.","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"MULTIPLE ANSWERS","type":"DATA FORMAT","description":"Multiple answers refer to the scenario where a student provides more than one option as their response to a question.","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"ERROR ANALYSIS","type":"PROCESS","description":"Error analysis is the process of comparing the student's final answer with the correct answer to determine if they match.","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"FINAL VERDICT","type":"PROCESS","description":"The final verdict is the conclusion drawn from the error analysis, indicating whether the student's answer is 'Correct' or 'Incorrect'.","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"ALPHABET ID","type":"","description":"","source_id":"103d98395c393552cc954c89d4e59f50"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"STUDENT RESPONSE\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">The student response refers to the answer provided by a student in response to a question, which may include multiple answers or options chosen.<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"FINAL ANSWER\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">The final answer is the specific choice made by the student from the provided options, which can be a single letter or a list of letters.<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"QUESTION\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">The question is the prompt or problem presented to the student, requiring a response based on the provided options.<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"OPTIONS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Options are the possible answers provided to the student, typically formatted as a list of choices with corresponding letters.<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"MATHS GPT-4 EXTRACTION SYSTEM MESSAGE\">      <data key=\"d0\">SYSTEM MESSAGE<\/data>      <data key=\"d1\">The Maths GPT-4 Extraction System Message is a set of instructions for evaluating a student's answer to a math problem, focusing on the final answer's correctness.<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"GENERAL EXTRACTION SYSTEM MESSAGE\">      <data key=\"d0\">SYSTEM MESSAGE<\/data>      <data key=\"d1\">The General Extraction System Message is a set of instructions for parsing student responses and matching them with the correct answer.<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"EQBENCH GPT-4 EXTRACTION SYSTEM MESSAGE\">      <data key=\"d0\">SYSTEM MESSAGE<\/data>      <data key=\"d1\">The EQBench GPT-4 Extraction System Message is a set of instructions for extracting emotion scores from a student agent response based on a critique.<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"MULTIPLE ANSWERS\">      <data key=\"d0\">DATA FORMAT<\/data>      <data key=\"d1\">Multiple answers refer to the scenario where a student provides more than one option as their response to a question.<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"ERROR ANALYSIS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Error analysis is the process of comparing the student's final answer with the correct answer to determine if they match.<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"FINAL VERDICT\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The final verdict is the conclusion drawn from the error analysis, indicating whether the student's answer is 'Correct' or 'Incorrect'.<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"ALPHABET ID\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <edge source=\"STUDENT RESPONSE\" target=\"FINAL ANSWER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The student response contains the final answer extracted from the student's answer to the question.<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"STUDENT RESPONSE\" target=\"EQBENCH GPT-4 EXTRACTION SYSTEM MESSAGE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The EQBench GPT-4 Extraction System Message is used to extract emotion scores from the student agent response based on the critique provided.<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"STUDENT RESPONSE\" target=\"MULTIPLE ANSWERS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The student response may include multiple answers if the student provides more than one option for the question.<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"FINAL ANSWER\" target=\"ALPHABET ID\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The final answer is represented by the alphabet ID that corresponds to the option chosen by the student.<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"QUESTION\" target=\"OPTIONS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The question is associated with the options provided, which are the possible answers to the question.<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"QUESTION\" target=\"MATHS GPT-4 EXTRACTION SYSTEM MESSAGE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Maths GPT-4 Extraction System Message is used to evaluate the correctness of the answer to the question posed.<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"QUESTION\" target=\"GENERAL EXTRACTION SYSTEM MESSAGE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The General Extraction System Message is used to parse the student's response to the question and determine correctness.<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"ERROR ANALYSIS\" target=\"FINAL VERDICT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Error analysis leads to the final verdict, determining the correctness of the student's answer based on the comparison.<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"0cf2e43f324fa4175b9b00b90e5e90ba","chunk":":\nElliot is likely to feel resigned because he has just confessed his feelings to\nAlex, knowing that Alex is already in a relationship. He might feel a bit\nangry at himself for putting himself in this situation. There is a slight sense of\nhopefulness in his confession, hoping that Alex might reciprocate his feelings.\nHe is also likely to feel embarrassed for putting Alex in an awkward position.\nRevised scores:\nResigned: 7\nAngry: 3\nHopeful: 5\nEmbarrassed: 8\nEmotions:\n1. Resigned, 2. Angry, 3. Hopeful, 4. Embarrassed\nOutput\n\"Resigned\" : 7,\n\"Angry\" : 3,\n\"Hopeful\" : 5,\n\"Embarrassed\" : 8\n\u2022Open-Ended Generation : These are the tasks where model is prompted to\ngenerate an answer to an open-ended question, but a ground-truth to match the\nanswer is not available. The metric calculation method for the benchmarks in this\ncategory are provided below:\n\u2013FOFO: For this benchmark the evaluation is done using a judge, GPT-4(version\n0613). We use the judge system message provided in the original paper of the\nbenchmark [ 34]. GPT-4 is used to give a format correctness score between 0\nand 1, 1 meaning the model\u2019s response strictly follows the format specified in\nthe prompt and 0 otherwise. The final score is measured as the percentage of\ntimes the model being evaluated followed the format specified in the prompt\nstrictly.\n\u2013IFEval: IFEval benchmark requires checking if the model response follows the\nverifiable instructions given in the prompt. For this we use the code provided\nby the authors [40].\n\u2013MT-Bench : MT-Bench benchmark consists of a first-turn query and a second-\nturn query independent of the evaluated model\u2019s response. The benchmark\nemploys GPT-4 to judge each turn\u2019s response and provide a score from 1 to 10.\nThe average score over all interactions is reported. System message and prompt\ntemplate used is the one provided by the creators [16].\n\u2013AlpacaEval : In this benchmark we measure win-rates, i.e. the number of times\na powerful LLM (GPT-4-turbo version 0613 in our case) prefers the outputs of\nthe evaluated model over a reference answer [14].\n\u2013InfoBench : InfoBench is also evaluated using GPT-4 (version 1106-preview) as\nthe judge determining if the model response follows the decomposed instruction\nand we use the implementation provided by the creators of the benchmark [ 25].\n30Hallucination Judge Example\nYou will be given a summary instruction and a generated summary.\nYour task to decide if there is any hallucination in the generated\nsummary.\nUser Message:\n{{place summary task here}}\nGenerated Summary:\n{{place response here}}\n=========================\nGo through each section in the generated summary, do the following:\n- Extract relevant facts from the article that can be used to verify\nthe correctness of the summary\n- Decide if any section contains hallucination or not.\nAt the end output a JSON with the format:\n{\"hallucination_detected\": \"yes\/no\", \"hallucinated_span\": \"If yes,\nthe exact span of every hallucinated text part from the summary in\nlist format; if no, leave this empty.\"}\nUse the format:\nAnalysis:\nsection 1:\nwrite the part of the summary\nrelevant segments:\nextract relevant segments from the article\njudgement:\ndecide if the section of the summary is supported by the article\nrepeat this for all sections\n....\nFinal verdict:\n{\"hallucination_detected\": \"yes\/no\", \"hallucinated_span\": \"If yes,\nthe exact span of every hallucinated text part in list format; if no,\nleave this empty.\"}\nFigure 5: Prompt template used for hallucination detection in Text Summarization.\nB.1 Summarization Quality and Hallucination Evaluation\nWe use GPT-4 with the following prompts for evaluating quality and hallucination in\nsummarization:\n31Quality Judge Example\nPlease act as an impartial judge and evaluate the quality of the\nresponse provided by an AI assistant to the user instruction\ndisplayed below.\nYour evaluation should assess the following criteria:\n- Instruction Adherence: Does the response correctly follow the user\ninstruction?\n- Content Grounding: Is the answer grounded in the instruction\nwithout introducing new content beyond what is already present?\nPenalize hallucinations.\n- Overall Quality: Assess the clarity, coherence, and completeness\nof the response.\nBegin your evaluation with a short explanation highlighting the pros\nand cons of the answer. Be as objective as possible. After providing\nyour explanation, rate the overall quality of the response on a scale\nof 1 to 10 using this format:\n\"Rating: [[rating]]\" (e.g., \"Rating: [[5]]\").\nUser Instruction:\n{{place instruction here}}\nAssistant\u2019s Response:\n[The Start of Assistant\u2019s Answer]\n{{place response here}}\n[The End of Assistant\u2019s Answer]\nFigure 6: Prompt template for evaluation of summary quality.\n32","chunk_id":"0cf2e43f324fa4175b9b00b90e5e90ba","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1085,"entities":[{"name":"ELLIOT","type":"PERSON","description":"Elliot is a character who has just confessed his feelings to Alex, experiencing a mix of emotions including resignation, anger, hopefulness, and embarrassment.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"ALEX","type":"PERSON","description":"Alex is a character who is already in a relationship, which complicates Elliot's feelings after his confession.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"EMOTIONS","type":"CONCEPT","description":"Emotions are the feelings experienced by Elliot, including resignation, anger, hopefulness, and embarrassment, each rated on a scale.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"RESIGNED","type":"EMOTION","description":"Resigned is an emotion felt by Elliot, rated 7, indicating a sense of acceptance of his situation.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"ANGRY","type":"EMOTION","description":"Angry is an emotion felt by Elliot, rated 3, indicating frustration with himself for his situation.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"HOPEFUL","type":"EMOTION","description":"Hopeful is an emotion felt by Elliot, rated 5, indicating a desire for Alex to reciprocate his feelings.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"EMBARRASSED","type":"EMOTION","description":"Embarrassed is an emotion felt by Elliot, rated 8, indicating discomfort for putting Alex in an awkward position.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"OPEN-ENDED GENERATION","type":"TASK","description":"Open-ended generation refers to tasks where a model generates answers to questions without a specific ground-truth to match.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"FOFO","type":"BENCHMARK","description":"FOFO is a benchmark for evaluating model responses based on format correctness, using GPT-4 as a judge.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"IFEVAL","type":"BENCHMARK","description":"IFEval is a benchmark that checks if model responses follow verifiable instructions given in prompts.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"MT-BENCH","type":"BENCHMARK","description":"MT-Bench is a benchmark that evaluates model responses based on a first-turn and second-turn query, using GPT-4 for scoring.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"ALPACA EVAL","type":"BENCHMARK","description":"AlpacaEval measures win-rates of model outputs compared to reference answers.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"INFOBENCH","type":"BENCHMARK","description":"InfoBench evaluates model responses based on adherence to decomposed instructions, using GPT-4 as a judge.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"RESIGNED SCORE","type":"DATA POINT","description":"The resigned score is a numerical value of 7, indicating the intensity of Elliot's feeling of resignation.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"ANGRY SCORE","type":"DATA POINT","description":"The angry score is a numerical value of 3, indicating the intensity of Elliot's feeling of anger.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"HOPEFUL SCORE","type":"DATA POINT","description":"The hopeful score is a numerical value of 5, indicating the intensity of Elliot's feeling of hopefulness.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"EMBARRASSED SCORE","type":"DATA POINT","description":"The embarrassed score is a numerical value of 8, indicating the intensity of Elliot's feeling of embarrassment.","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"SCORES","type":"","description":"","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ELLIOT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Elliot is a character who has just confessed his feelings to Alex, experiencing a mix of emotions including resignation, anger, hopefulness, and embarrassment.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"ALEX\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex is a character who is already in a relationship, which complicates Elliot's feelings after his confession.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"EMOTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Emotions are the feelings experienced by Elliot, including resignation, anger, hopefulness, and embarrassment, each rated on a scale.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"RESIGNED\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">Resigned is an emotion felt by Elliot, rated 7, indicating a sense of acceptance of his situation.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"ANGRY\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">Angry is an emotion felt by Elliot, rated 3, indicating frustration with himself for his situation.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"HOPEFUL\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">Hopeful is an emotion felt by Elliot, rated 5, indicating a desire for Alex to reciprocate his feelings.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"EMBARRASSED\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">Embarrassed is an emotion felt by Elliot, rated 8, indicating discomfort for putting Alex in an awkward position.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"OPEN-ENDED GENERATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Open-ended generation refers to tasks where a model generates answers to questions without a specific ground-truth to match.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"FOFO\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">FOFO is a benchmark for evaluating model responses based on format correctness, using GPT-4 as a judge.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"IFEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">IFEval is a benchmark that checks if model responses follow verifiable instructions given in prompts.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"MT-BENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MT-Bench is a benchmark that evaluates model responses based on a first-turn and second-turn query, using GPT-4 for scoring.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"ALPACA EVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AlpacaEval measures win-rates of model outputs compared to reference answers.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"INFOBENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">InfoBench evaluates model responses based on adherence to decomposed instructions, using GPT-4 as a judge.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"RESIGNED SCORE\">      <data key=\"d0\">DATA POINT<\/data>      <data key=\"d1\">The resigned score is a numerical value of 7, indicating the intensity of Elliot's feeling of resignation.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"ANGRY SCORE\">      <data key=\"d0\">DATA POINT<\/data>      <data key=\"d1\">The angry score is a numerical value of 3, indicating the intensity of Elliot's feeling of anger.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"HOPEFUL SCORE\">      <data key=\"d0\">DATA POINT<\/data>      <data key=\"d1\">The hopeful score is a numerical value of 5, indicating the intensity of Elliot's feeling of hopefulness.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"EMBARRASSED SCORE\">      <data key=\"d0\">DATA POINT<\/data>      <data key=\"d1\">The embarrassed score is a numerical value of 8, indicating the intensity of Elliot's feeling of embarrassment.<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"SCORES\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <edge source=\"ELLIOT\" target=\"ALEX\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Elliot has confessed his feelings to Alex, who is already in a relationship, creating emotional conflict for Elliot.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"ELLIOT\" target=\"EMOTIONS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Elliot experiences a range of emotions including resignation, anger, hopefulness, and embarrassment after confessing to Alex.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"ELLIOT\" target=\"SCORES\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Elliot's emotions are quantified by scores that reflect the intensity of each feeling he experiences after his confession.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"EMOTIONS\" target=\"RESIGNED\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Resigned is one of the emotions felt by Elliot, indicating acceptance of his situation.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"EMOTIONS\" target=\"ANGRY\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Angry is one of the emotions felt by Elliot, indicating frustration with himself.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"EMOTIONS\" target=\"HOPEFUL\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Hopeful is one of the emotions felt by Elliot, indicating a desire for a positive outcome with Alex.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"EMOTIONS\" target=\"EMBARRASSED\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Embarrassed is one of the emotions felt by Elliot, indicating discomfort for putting Alex in an awkward position.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"RESIGNED\" target=\"RESIGNED SCORE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The resigned emotion is quantified by a score of 7, indicating its intensity in Elliot's emotional state.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"ANGRY\" target=\"ANGRY SCORE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The angry emotion is quantified by a score of 3, indicating its intensity in Elliot's emotional state.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"HOPEFUL\" target=\"HOPEFUL SCORE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The hopeful emotion is quantified by a score of 5, indicating its intensity in Elliot's emotional state.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"EMBARRASSED\" target=\"EMBARRASSED SCORE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The embarrassed emotion is quantified by a score of 8, indicating its intensity in Elliot's emotional state.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"FOFO\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Open-ended generation tasks are evaluated using the FOFO benchmark for format correctness.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"IFEVAL\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Open-ended generation tasks are evaluated using the IFEval benchmark for adherence to instructions.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"MT-BENCH\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Open-ended generation tasks are evaluated using the MT-Bench benchmark for response quality.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"ALPACA EVAL\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Open-ended generation tasks are evaluated using the AlpacaEval benchmark for win-rate comparisons.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"INFOBENCH\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Open-ended generation tasks are evaluated using the InfoBench benchmark for instruction adherence.<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>  <\/graph><\/graphml>"}
