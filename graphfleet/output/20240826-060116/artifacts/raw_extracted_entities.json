{"id":"0c932f7def033fa2b1bf210fbb771e7d","chunk":"From Local to Global: A Graph RAG Approach to\nQuery-Focused Summarization\nDarren Edge1\u2020Ha Trinh1\u2020Newman Cheng2Joshua Bradley2Alex Chao3\nApurva Mody3Steven Truitt2\nJonathan Larson1\n1Microsoft Research\n2Microsoft Strategic Missions and Technologies\n3Microsoft Office of the CTO\n{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso }\n@microsoft.com\n\u2020These authors contributed equally to this work\nAbstract\nThe use of retrieval-augmented generation (RAG) to retrieve relevant informa-\ntion from an external knowledge source enables large language models (LLMs)\nto answer questions over private and\/or previously unseen document collections.\nHowever, RAG fails on global questions directed at an entire text corpus, such\nas \u201cWhat are the main themes in the dataset?\u201d, since this is inherently a query-\nfocused summarization (QFS) task, rather than an explicit retrieval task. Prior\nQFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\nRAG systems. To combine the strengths of these contrasting methods, we propose\na Graph RAG approach to question answering over private text corpora that scales\nwith both the generality of user questions and the quantity of source text to be in-\ndexed. Our approach uses an LLM to build a graph-based text index in two stages:\nfirst to derive an entity knowledge graph from the source documents, then to pre-\ngenerate community summaries for all groups of closely-related entities. Given a\nquestion, each community summary is used to generate a partial response, before\nall partial responses are again summarized in a final response to the user. For a\nclass of global sensemaking questions over datasets in the 1 million token range,\nwe show that Graph RAG leads to substantial improvements over a na \u00a8\u0131ve RAG\nbaseline for both the comprehensiveness and diversity of generated answers. An\nopen-source, Python-based implementation of both global and local Graph RAG\napproaches is forthcoming at https:\/\/aka .ms\/graphrag .\n1 Introduction\nHuman endeavors across a range of domains rely on our ability to read and reason about large\ncollections of documents, often reaching conclusions that go beyond anything stated in the source\ntexts themselves. With the emergence of large language models (LLMs), we are already witnessing\nattempts to automate human-like sensemaking in complex domains like scientific discovery (Mi-\ncrosoft, 2023) and intelligence analysis (Ranade and Joshi, 2023), where sensemaking is defined as\nPreprint. Under review.arXiv:2404.16130v1  [cs.CL]  24 Apr 2024Source Documents\nText Chunkstext extraction\nand chunking\nElement Instancesdomain-tailored\nsummarization\nElement Summariesdomain-tailored\nsummarization\nGraph Communitiescommunity\ndetectionCommunity Summaries\ndomain-tailored\nsummarizationCommunity Answers\nquery-focused\nsummarizationGlobal Answer\nquery-focused\nsummarization\nIndexing Time Query Time Pipeline Stage\nFigure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This\nindex spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that have\nbeen detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset.\nCommunity detection (e.g., Leiden, Traag et al., 2019) is used to partition the graph index into\ngroups of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both index-\ning time and query time. The \u201cglobal answer\u201d to a given query is produced using a final round of\nquery-focused summarization over all community summaries reporting relevance to that query.\n\u201ca motivated, continuous effort to understand connections (which can be among people, places, and\nevents) in order to anticipate their trajectories and act effectively \u201d (Klein et al., 2006a). Supporting\nhuman-led sensemaking over entire text corpora, however, needs a way for people to both apply and\nrefine their mental model of the data (Klein et al., 2006b) by asking questions of a global nature.\nRetrieval-augmented generation (RAG, Lewis et al., 2020) is an established approach to answering\nuser questions over entire datasets, but it is designed for situations where these answers are contained\nlocally within regions of text whose retrieval provides sufficient grounding for the generation task.\nInstead, a more appropriate task framing is query-focused summarization (QFS, Dang, 2006), and in\nparticular, query-focused abstractive summarization that generates natural language summaries and\nnot just concatenated excerpts (Baumel et al., 2018; Laskar et al., 2020; Yao et al., 2017) . In recent\nyears, however, such distinctions between summarization tasks that are abstractive versus extractive,\ngeneric versus query-focused, and single-document versus multi-document, have become less rele-\nvant. While early applications of the transformer architecture showed substantial improvements on\nthe state-of-the-art for all such summarization tasks (Goodwin et al., 2020; Laskar et al., 2022; Liu\nand Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam\net al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al.,","chunk_id":"0c932f7def033fa2b1bf210fbb771e7d","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"GRAPH RAG","type":"APPROACH, METHOD","description":"A method for question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. It uses an LLM to build a graph-based text index in two stages: deriving an entity knowledge graph from source documents and pre-generating community summaries for groups of closely-related entities.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"APPROACH, METHOD"},{"name":"QUERY-FOCUSED SUMMARIZATION (QFS)","type":"TASK, METHOD","description":"A task that involves generating summaries of text that are focused on answering specific user queries, rather than just retrieving relevant text excerpts.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"TASK, METHOD"},{"name":"RETRIEVAL-AUGMENTED GENERATION (RAG)","type":"TECHNIQUE, METHOD","description":"A technique that retrieves relevant information from an external knowledge source to enable large language models to answer questions over private and\/or previously unseen document collections.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"TECHNIQUE, METHOD"},{"name":"LARGE LANGUAGE MODELS (LLMS)","type":"TECHNOLOGY, MODEL","description":"Advanced machine learning models trained on vast amounts of text data to understand and generate human language, used in various applications including sensemaking and summarization.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"TECHNOLOGY, MODEL"},{"name":"COMMUNITY DETECTION","type":"TECHNIQUE, METHOD","description":"A process used to partition a graph into groups of elements (nodes, edges, covariates) that can be summarized in parallel at both indexing time and query time.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"TECHNIQUE, METHOD"},{"name":"SENSEMAKING","type":"PROCESS, ACTIVITY","description":"A motivated, continuous effort to understand connections among people, places, and events in order to anticipate their trajectories and act effectively.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"PROCESS, ACTIVITY"},{"name":"MICROSOFT RESEARCH","type":"ORGANIZATION","description":"A division of Microsoft responsible for conducting research in various domains, including the development of the Graph RAG approach.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"ORGANIZATION"},{"name":"MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES","type":"ORGANIZATION","description":"A division of Microsoft involved in strategic missions and technologies, contributing to the research on Graph RAG.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"ORGANIZATION"},{"name":"MICROSOFT OFFICE OF THE CTO","type":"ORGANIZATION","description":"A division of Microsoft led by the Chief Technology Officer, involved in the research on Graph RAG.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"ORGANIZATION"},{"name":"DAREN EDGE","type":"PERSON","description":"A researcher at Microsoft Research who contributed to the development of the Graph RAG approach.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"HA TRINH","type":"PERSON","description":"A researcher at Microsoft Research who contributed to the development of the Graph RAG approach.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"PERSON"},{"name":"NEWMAN CHENG","type":"PERSON","description":"A researcher at Microsoft Strategic Missions and Technologies who contributed to the development of the Graph RAG approach.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"PERSON"},{"name":"JOSHUA BRADLEY","type":"PERSON","description":"A researcher at Microsoft Strategic Missions and Technologies who contributed to the development of the Graph RAG approach.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"PERSON"},{"name":"ALEX CHAO","type":"PERSON","description":"A researcher at Microsoft Office of the CTO who contributed to the development of the Graph RAG approach.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"PERSON"},{"name":"APURVA MODY","type":"PERSON","description":"A researcher at Microsoft Office of the CTO who contributed to the development of the Graph RAG approach.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"PERSON"},{"name":"STEVEN TRUITT","type":"PERSON","description":"A researcher at Microsoft Strategic Missions and Technologies who contributed to the development of the Graph RAG approach.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"PERSON"},{"name":"JONATHAN LARSON","type":"PERSON","description":"A researcher at Microsoft Research who contributed to the development of the Graph RAG approach.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"PERSON"},{"name":"LEIDEN","type":"METHOD, TECHNIQUE","description":"A community detection method used to partition a graph index into groups of elements that can be summarized in parallel.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"METHOD, TECHNIQUE"},{"name":"SENSEMAKING IN COMPLEX DOMAINS","type":"ACTIVITY, PROCESS","description":"The process of automating human-like sensemaking in complex domains like scientific discovery and intelligence analysis using large language models.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"ACTIVITY, PROCESS"},{"name":"GLOBAL SENSEMAKING QUESTIONS","type":"QUESTION TYPE","description":"Questions that require understanding connections and drawing conclusions from large text corpora, often beyond what is explicitly stated in the source texts.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"QUESTION TYPE"},{"name":"INDEXING TIME","type":"TIME PERIOD, STAGE","description":"The stage in the Graph RAG pipeline where the graph index of source document text is created and partitioned into communities.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"TIME PERIOD, STAGE"},{"name":"QUERY TIME","type":"TIME PERIOD, STAGE","description":"The stage in the Graph RAG pipeline where community summaries are used to generate partial responses to a query, which are then summarized into a final response.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"TIME PERIOD, STAGE"},{"name":"GRAPH INDEX","type":"DATA STRUCTURE, INDEX","description":"An index created by an LLM that spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) detected, extracted, and summarized from source documents.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"DATA STRUCTURE, INDEX"},{"name":"COMMUNITY SUMMARIES","type":"DATA, OUTPUT","description":"Summaries generated for groups of closely-related entities in the graph index, used to generate partial responses to queries.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"DATA, OUTPUT"},{"name":"GLOBAL ANSWER","type":"OUTPUT, RESPONSE","description":"The final response to a query, produced by summarizing all community summaries that report relevance to the query.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"OUTPUT, RESPONSE"},{"name":"LOCAL GRAPH RAG","type":"APPROACH, METHOD","description":"A variant of the Graph RAG approach that focuses on local regions of text for question answering.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"APPROACH, METHOD"},{"name":"GLOBAL GRAPH RAG","type":"APPROACH, METHOD","description":"A variant of the Graph RAG approach that scales to global questions over large text corpora.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"APPROACH, METHOD"},{"name":"OPEN-SOURCE IMPLEMENTATION","type":"SOFTWARE, TOOL","description":"An open-source, Python-based implementation of both global and local Graph RAG approaches, available at https:\/\/aka.ms\/graphrag.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"SOFTWARE, TOOL"},{"name":"TRANSFORMER ARCHITECTURE","type":"TECHNOLOGY, MODEL","description":"A type of neural network architecture that has shown substantial improvements in various summarization tasks.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"TECHNOLOGY, MODEL"},{"name":"GPT","type":"MODEL, TECHNOLOGY","description":"A large language model developed by OpenAI, used for various natural language processing tasks including summarization.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"MODEL, TECHNOLOGY"},{"name":"LLAMA","type":"MODEL, TECHNOLOGY","description":"A large language model used for natural language processing tasks, including summarization.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"MODEL, TECHNOLOGY"},{"name":"GEMINI","type":"MODEL, TECHNOLOGY","description":"A large language model used for natural language processing tasks, including summarization.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"MODEL, TECHNOLOGY"},{"name":"MICROSOFT","type":"ORGANIZATION","description":"A technology company involved in the development of the Graph RAG approach and various other research initiatives.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"ORGANIZATION"},{"name":"RANADE AND JOSHI","type":"AUTHORS, RESEARCHERS","description":"Researchers who contributed to the study of sensemaking in intelligence analysis.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"AUTHORS, RESEARCHERS"},{"name":"KLEIN ET AL.","type":"AUTHORS, RESEARCHERS","description":"Researchers who defined sensemaking as a motivated, continuous effort to understand connections among people, places, and events.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"AUTHORS, RESEARCHERS"},{"name":"LEWIS ET AL.","type":"AUTHORS, RESEARCHERS","description":"Researchers who contributed to the development of the retrieval-augmented generation (RAG) technique.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"AUTHORS, RESEARCHERS"},{"name":"DANG","type":"AUTHOR, RESEARCHER","description":"A researcher who contributed to the study of query-focused summarization (QFS).","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"AUTHOR, RESEARCHER"},{"name":"BAUMEL ET AL.","type":"AUTHORS, RESEARCHERS","description":"Researchers who contributed to the study of query-focused abstractive summarization.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"AUTHORS, RESEARCHERS"},{"name":"LASKAR ET AL.","type":"AUTHORS, RESEARCHERS","description":"Researchers who contributed to the study of query-focused abstractive summarization.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"AUTHORS, RESEARCHERS"},{"name":"YAO ET AL.","type":"AUTHORS, RESEARCHERS","description":"Researchers who contributed to the study of query-focused abstractive summarization.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"AUTHORS, RESEARCHERS"},{"name":"GOODWIN ET AL.","type":"AUTHORS, RESEARCHERS","description":"Researchers who contributed to the study of summarization tasks using transformer architecture.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"AUTHORS, RESEARCHERS"},{"name":"LIU AND LAPATA","type":"AUTHORS, RESEARCHERS","description":"Researchers who contributed to the study of summarization tasks using transformer architecture.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"AUTHORS, RESEARCHERS"},{"name":"ACHIAM ET AL.","type":"AUTHORS, RESEARCHERS","description":"Researchers who contributed to the development of the GPT model.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"AUTHORS, RESEARCHERS"},{"name":"BROWN ET AL.","type":"AUTHORS, RESEARCHERS","description":"Researchers who contributed to the development of the GPT model.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"AUTHORS, RESEARCHERS"},{"name":"TOUVRON ET AL.","type":"AUTHORS, RESEARCHERS","description":"Researchers who contributed to the development of the Llama model.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"AUTHORS, RESEARCHERS"},{"name":"ANIL ET AL.","type":"AUTHORS, RESEARCHERS","description":"Researchers who contributed to the development of the Gemini model.","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"AUTHORS, RESEARCHERS"},{"name":"DARREN EDGE","type":"PERSON","description":"A researcher at Microsoft Research who contributed to the development of the Graph RAG approach.","source_id":"0c932f7def033fa2b1bf210fbb771e7d"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GRAPH RAG\">      <data key=\"d0\">APPROACH, METHOD<\/data>      <data key=\"d1\">A method for question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. It uses an LLM to build a graph-based text index in two stages: deriving an entity knowledge graph from source documents and pre-generating community summaries for groups of closely-related entities.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">APPROACH, METHOD<\/data>    <\/node>    <node id=\"QUERY-FOCUSED SUMMARIZATION (QFS)\">      <data key=\"d0\">TASK, METHOD<\/data>      <data key=\"d1\">A task that involves generating summaries of text that are focused on answering specific user queries, rather than just retrieving relevant text excerpts.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">TASK, METHOD<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED GENERATION (RAG)\">      <data key=\"d0\">TECHNIQUE, METHOD<\/data>      <data key=\"d1\">A technique that retrieves relevant information from an external knowledge source to enable large language models to answer questions over private and\/or previously unseen document collections.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">TECHNIQUE, METHOD<\/data>    <\/node>    <node id=\"LARGE LANGUAGE MODELS (LLMS)\">      <data key=\"d0\">TECHNOLOGY, MODEL<\/data>      <data key=\"d1\">Advanced machine learning models trained on vast amounts of text data to understand and generate human language, used in various applications including sensemaking and summarization.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">TECHNOLOGY, MODEL<\/data>    <\/node>    <node id=\"COMMUNITY DETECTION\">      <data key=\"d0\">TECHNIQUE, METHOD<\/data>      <data key=\"d1\">A process used to partition a graph into groups of elements (nodes, edges, covariates) that can be summarized in parallel at both indexing time and query time.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">TECHNIQUE, METHOD<\/data>    <\/node>    <node id=\"SENSEMAKING\">      <data key=\"d0\">PROCESS, ACTIVITY<\/data>      <data key=\"d1\">A motivated, continuous effort to understand connections among people, places, and events in order to anticipate their trajectories and act effectively.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">PROCESS, ACTIVITY<\/data>    <\/node>    <node id=\"MICROSOFT RESEARCH\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">A division of Microsoft responsible for conducting research in various domains, including the development of the Graph RAG approach.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">A division of Microsoft involved in strategic missions and technologies, contributing to the research on Graph RAG.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"MICROSOFT OFFICE OF THE CTO\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">A division of Microsoft led by the Chief Technology Officer, involved in the research on Graph RAG.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"DAREN EDGE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A researcher at Microsoft Research who contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"HA TRINH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A researcher at Microsoft Research who contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NEWMAN CHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A researcher at Microsoft Strategic Missions and Technologies who contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOSHUA BRADLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A researcher at Microsoft Strategic Missions and Technologies who contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALEX CHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A researcher at Microsoft Office of the CTO who contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"APURVA MODY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A researcher at Microsoft Office of the CTO who contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"STEVEN TRUITT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A researcher at Microsoft Strategic Missions and Technologies who contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JONATHAN LARSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A researcher at Microsoft Research who contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEIDEN\">      <data key=\"d0\">METHOD, TECHNIQUE<\/data>      <data key=\"d1\">A community detection method used to partition a graph index into groups of elements that can be summarized in parallel.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">METHOD, TECHNIQUE<\/data>    <\/node>    <node id=\"SENSEMAKING IN COMPLEX DOMAINS\">      <data key=\"d0\">ACTIVITY, PROCESS<\/data>      <data key=\"d1\">The process of automating human-like sensemaking in complex domains like scientific discovery and intelligence analysis using large language models.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">ACTIVITY, PROCESS<\/data>    <\/node>    <node id=\"GLOBAL SENSEMAKING QUESTIONS\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Questions that require understanding connections and drawing conclusions from large text corpora, often beyond what is explicitly stated in the source texts.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"INDEXING TIME\">      <data key=\"d0\">TIME PERIOD, STAGE<\/data>      <data key=\"d1\">The stage in the Graph RAG pipeline where the graph index of source document text is created and partitioned into communities.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">TIME PERIOD, STAGE<\/data>    <\/node>    <node id=\"QUERY TIME\">      <data key=\"d0\">TIME PERIOD, STAGE<\/data>      <data key=\"d1\">The stage in the Graph RAG pipeline where community summaries are used to generate partial responses to a query, which are then summarized into a final response.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">TIME PERIOD, STAGE<\/data>    <\/node>    <node id=\"GRAPH INDEX\">      <data key=\"d0\">DATA STRUCTURE, INDEX<\/data>      <data key=\"d1\">An index created by an LLM that spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) detected, extracted, and summarized from source documents.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">DATA STRUCTURE, INDEX<\/data>    <\/node>    <node id=\"COMMUNITY SUMMARIES\">      <data key=\"d0\">DATA, OUTPUT<\/data>      <data key=\"d1\">Summaries generated for groups of closely-related entities in the graph index, used to generate partial responses to queries.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">DATA, OUTPUT<\/data>    <\/node>    <node id=\"GLOBAL ANSWER\">      <data key=\"d0\">OUTPUT, RESPONSE<\/data>      <data key=\"d1\">The final response to a query, produced by summarizing all community summaries that report relevance to the query.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">OUTPUT, RESPONSE<\/data>    <\/node>    <node id=\"LOCAL GRAPH RAG\">      <data key=\"d0\">APPROACH, METHOD<\/data>      <data key=\"d1\">A variant of the Graph RAG approach that focuses on local regions of text for question answering.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">APPROACH, METHOD<\/data>    <\/node>    <node id=\"GLOBAL GRAPH RAG\">      <data key=\"d0\">APPROACH, METHOD<\/data>      <data key=\"d1\">A variant of the Graph RAG approach that scales to global questions over large text corpora.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">APPROACH, METHOD<\/data>    <\/node>    <node id=\"OPEN-SOURCE IMPLEMENTATION\">      <data key=\"d0\">SOFTWARE, TOOL<\/data>      <data key=\"d1\">An open-source, Python-based implementation of both global and local Graph RAG approaches, available at https:\/\/aka.ms\/graphrag.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">SOFTWARE, TOOL<\/data>    <\/node>    <node id=\"TRANSFORMER ARCHITECTURE\">      <data key=\"d0\">TECHNOLOGY, MODEL<\/data>      <data key=\"d1\">A type of neural network architecture that has shown substantial improvements in various summarization tasks.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">TECHNOLOGY, MODEL<\/data>    <\/node>    <node id=\"GPT\">      <data key=\"d0\">MODEL, TECHNOLOGY<\/data>      <data key=\"d1\">A large language model developed by OpenAI, used for various natural language processing tasks including summarization.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">MODEL, TECHNOLOGY<\/data>    <\/node>    <node id=\"LLAMA\">      <data key=\"d0\">MODEL, TECHNOLOGY<\/data>      <data key=\"d1\">A large language model used for natural language processing tasks, including summarization.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">MODEL, TECHNOLOGY<\/data>    <\/node>    <node id=\"GEMINI\">      <data key=\"d0\">MODEL, TECHNOLOGY<\/data>      <data key=\"d1\">A large language model used for natural language processing tasks, including summarization.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">MODEL, TECHNOLOGY<\/data>    <\/node>    <node id=\"MICROSOFT\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">A technology company involved in the development of the Graph RAG approach and various other research initiatives.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"RANADE AND JOSHI\">      <data key=\"d0\">AUTHORS, RESEARCHERS<\/data>      <data key=\"d1\">Researchers who contributed to the study of sensemaking in intelligence analysis.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">AUTHORS, RESEARCHERS<\/data>    <\/node>    <node id=\"KLEIN ET AL.\">      <data key=\"d0\">AUTHORS, RESEARCHERS<\/data>      <data key=\"d1\">Researchers who defined sensemaking as a motivated, continuous effort to understand connections among people, places, and events.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">AUTHORS, RESEARCHERS<\/data>    <\/node>    <node id=\"LEWIS ET AL.\">      <data key=\"d0\">AUTHORS, RESEARCHERS<\/data>      <data key=\"d1\">Researchers who contributed to the development of the retrieval-augmented generation (RAG) technique.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">AUTHORS, RESEARCHERS<\/data>    <\/node>    <node id=\"DANG\">      <data key=\"d0\">AUTHOR, RESEARCHER<\/data>      <data key=\"d1\">A researcher who contributed to the study of query-focused summarization (QFS).<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">AUTHOR, RESEARCHER<\/data>    <\/node>    <node id=\"BAUMEL ET AL.\">      <data key=\"d0\">AUTHORS, RESEARCHERS<\/data>      <data key=\"d1\">Researchers who contributed to the study of query-focused abstractive summarization.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">AUTHORS, RESEARCHERS<\/data>    <\/node>    <node id=\"LASKAR ET AL.\">      <data key=\"d0\">AUTHORS, RESEARCHERS<\/data>      <data key=\"d1\">Researchers who contributed to the study of query-focused abstractive summarization.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">AUTHORS, RESEARCHERS<\/data>    <\/node>    <node id=\"YAO ET AL.\">      <data key=\"d0\">AUTHORS, RESEARCHERS<\/data>      <data key=\"d1\">Researchers who contributed to the study of query-focused abstractive summarization.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">AUTHORS, RESEARCHERS<\/data>    <\/node>    <node id=\"GOODWIN ET AL.\">      <data key=\"d0\">AUTHORS, RESEARCHERS<\/data>      <data key=\"d1\">Researchers who contributed to the study of summarization tasks using transformer architecture.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">AUTHORS, RESEARCHERS<\/data>    <\/node>    <node id=\"LIU AND LAPATA\">      <data key=\"d0\">AUTHORS, RESEARCHERS<\/data>      <data key=\"d1\">Researchers who contributed to the study of summarization tasks using transformer architecture.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">AUTHORS, RESEARCHERS<\/data>    <\/node>    <node id=\"ACHIAM ET AL.\">      <data key=\"d0\">AUTHORS, RESEARCHERS<\/data>      <data key=\"d1\">Researchers who contributed to the development of the GPT model.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">AUTHORS, RESEARCHERS<\/data>    <\/node>    <node id=\"BROWN ET AL.\">      <data key=\"d0\">AUTHORS, RESEARCHERS<\/data>      <data key=\"d1\">Researchers who contributed to the development of the GPT model.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">AUTHORS, RESEARCHERS<\/data>    <\/node>    <node id=\"TOUVRON ET AL.\">      <data key=\"d0\">AUTHORS, RESEARCHERS<\/data>      <data key=\"d1\">Researchers who contributed to the development of the Llama model.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">AUTHORS, RESEARCHERS<\/data>    <\/node>    <node id=\"ANIL ET AL.\">      <data key=\"d0\">AUTHORS, RESEARCHERS<\/data>      <data key=\"d1\">Researchers who contributed to the development of the Gemini model.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">AUTHORS, RESEARCHERS<\/data>    <\/node>    <node id=\"DARREN EDGE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A researcher at Microsoft Research who contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <edge source=\"GRAPH RAG\" target=\"QUERY-FOCUSED SUMMARIZATION (QFS)\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG is proposed as a method to combine the strengths of retrieval-augmented generation and query-focused summarization.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"RETRIEVAL-AUGMENTED GENERATION (RAG)\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG aims to address the limitations of RAG by incorporating query-focused summarization.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"LARGE LANGUAGE MODELS (LLMS)\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG uses large language models to build a graph-based text index and generate community summaries.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"COMMUNITY DETECTION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG uses community detection to partition the graph index into groups of elements that can be summarized in parallel.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"SENSEMAKING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG supports human-led sensemaking over entire text corpora by enabling the generation of comprehensive and diverse answers to global questions.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"MICROSOFT RESEARCH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Microsoft Research is involved in the development of the Graph RAG approach.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Microsoft Strategic Missions and Technologies is involved in the development of the Graph RAG approach.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"MICROSOFT OFFICE OF THE CTO\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Microsoft Office of the CTO is involved in the development of the Graph RAG approach.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"DAREN EDGE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Daren Edge contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"HA TRINH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Ha Trinh contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"NEWMAN CHENG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Newman Cheng contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"JOSHUA BRADLEY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Joshua Bradley contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"ALEX CHAO\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Alex Chao contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"APURVA MODY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Apurva Mody contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"STEVEN TRUITT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Steven Truitt contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"JONATHAN LARSON\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Jonathan Larson contributed to the development of the Graph RAG approach.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"INDEXING TIME\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Indexing time is a stage in the Graph RAG pipeline where the graph index is created and partitioned.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"QUERY TIME\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Query time is a stage in the Graph RAG pipeline where community summaries are used to generate partial and final responses to queries.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"GRAPH INDEX\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG uses a graph index created by an LLM to organize and summarize source document text.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"COMMUNITY SUMMARIES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG generates community summaries for groups of closely-related entities in the graph index.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"GLOBAL ANSWER\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG produces a global answer by summarizing all community summaries relevant to a query.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"LOCAL GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Local Graph RAG is a variant of the Graph RAG approach that focuses on local regions of text.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"GLOBAL GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Global Graph RAG is a variant of the Graph RAG approach that scales to global questions over large text corpora.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"OPEN-SOURCE IMPLEMENTATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">An open-source implementation of the Graph RAG approach is available for public use.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"MICROSOFT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Microsoft is involved in the development of the Graph RAG approach.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"QUERY-FOCUSED SUMMARIZATION (QFS)\" target=\"DANG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Dang contributed to the study of query-focused summarization.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"QUERY-FOCUSED SUMMARIZATION (QFS)\" target=\"BAUMEL ET AL.\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Baumel et al. contributed to the study of query-focused abstractive summarization.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"QUERY-FOCUSED SUMMARIZATION (QFS)\" target=\"LASKAR ET AL.\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Laskar et al. contributed to the study of query-focused abstractive summarization.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"QUERY-FOCUSED SUMMARIZATION (QFS)\" target=\"YAO ET AL.\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Yao et al. contributed to the study of query-focused abstractive summarization.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"RETRIEVAL-AUGMENTED GENERATION (RAG)\" target=\"LEWIS ET AL.\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Lewis et al. contributed to the development of the retrieval-augmented generation technique.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"LARGE LANGUAGE MODELS (LLMS)\" target=\"SENSEMAKING IN COMPLEX DOMAINS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Large language models are used to automate human-like sensemaking in complex domains.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"LARGE LANGUAGE MODELS (LLMS)\" target=\"TRANSFORMER ARCHITECTURE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Transformer architecture has shown substantial improvements in various summarization tasks and is used in large language models.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"LARGE LANGUAGE MODELS (LLMS)\" target=\"GPT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">GPT is a type of large language model used for natural language processing tasks.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"LARGE LANGUAGE MODELS (LLMS)\" target=\"LLAMA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Llama is a type of large language model used for natural language processing tasks.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"LARGE LANGUAGE MODELS (LLMS)\" target=\"GEMINI\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Gemini is a type of large language model used for natural language processing tasks.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"COMMUNITY DETECTION\" target=\"LEIDEN\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Leiden is a method used for community detection in the Graph RAG approach.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"SENSEMAKING\" target=\"GLOBAL SENSEMAKING QUESTIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Global sensemaking questions require understanding connections and drawing conclusions from large text corpora.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"SENSEMAKING\" target=\"KLEIN ET AL.\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Klein et al. defined sensemaking as a motivated, continuous effort to understand connections among people, places, and events.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"SENSEMAKING IN COMPLEX DOMAINS\" target=\"RANADE AND JOSHI\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Ranade and Joshi contributed to the study of sensemaking in complex domains.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"TRANSFORMER ARCHITECTURE\" target=\"GOODWIN ET AL.\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Goodwin et al. contributed to the study of summarization tasks using transformer architecture.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"TRANSFORMER ARCHITECTURE\" target=\"LIU AND LAPATA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Liu and Lapata contributed to the study of summarization tasks using transformer architecture.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GPT\" target=\"ACHIAM ET AL.\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Achiam et al. contributed to the development of the GPT model.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GPT\" target=\"BROWN ET AL.\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Brown et al. contributed to the development of the GPT model.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"LLAMA\" target=\"TOUVRON ET AL.\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Touvron et al. contributed to the development of the Llama model.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GEMINI\" target=\"ANIL ET AL.\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Anil et al. contributed to the development of the Gemini model.<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"64476a39d7d8b87b399e3bd3cead79c7","chunk":" on\nthe state-of-the-art for all such summarization tasks (Goodwin et al., 2020; Laskar et al., 2022; Liu\nand Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam\net al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) series,\nall of which can use in-context learning to summarize any content provided in their context window.\nThe challenge remains, however, for query-focused abstractive summarization over an entire corpus.\nSuch volumes of text can greatly exceed the limits of LLM context windows, and the expansion of\nsuch windows may not be enough given that information can be \u201clost in the middle\u201d of longer\ncontexts (Kuratov et al., 2024; Liu et al., 2023). In addition, although the direct retrieval of text\nchunks in na \u00a8\u0131ve RAG is likely inadequate for QFS tasks, it is possible that an alternative form of\npre-indexing could support a new RAG approach specifically targeting global summarization.\nIn this paper, we present a Graph RAG approach based on global summarization of an LLM-derived\nknowledge graph (Figure 1). In contrast with related work that exploits the structured retrieval\nand traversal affordances of graph indexes (subsection 4.2), we focus on a previously unexplored\nquality of graphs in this context: their inherent modularity (Newman, 2006) and the ability of com-\nmunity detection algorithms to partition graphs into modular communities of closely-related nodes\n(e.g., Louvain, Blondel et al., 2008; Leiden, Traag et al., 2019). LLM-generated summaries of these\n20 1 2 30100002000030000\nNumber of gleanings performedEntity references detected600 chunk size\n1200 chunk size\n2400 chunk size\nFigure 2: How the entity references detected in the HotPotQA dataset (Yang et al., 2018)\nvaries with chunk size and gleanings for our generic entity extraction prompt with gpt-4-turbo .\ncommunity descriptions provide complete coverage of the underlying graph index and the input doc-\numents it represents. Query-focused summarization of an entire corpus is then made possible using\na map-reduce approach: first using each community summary to answer the query independently\nand in parallel, then summarizing all relevant partial answers into a final global answer.\nTo evaluate this approach, we used an LLM to generate a diverse set of activity-centered sense-\nmaking questions from short descriptions of two representative real-world datasets, containing pod-\ncast transcripts and news articles respectively. For the target qualities of comprehensiveness, diver-\nsity, and empowerment (defined in subsection 3.4) that develop understanding of broad issues and\nthemes, we both explore the impact of varying the the hierarchical level of community summaries\nused to answer queries, as well as compare to na \u00a8\u0131ve RAG and global map-reduce summarization\nof source texts. We show that all global approaches outperform na \u00a8\u0131ve RAG on comprehensiveness\nand diversity, and that Graph RAG with intermediate- and low-level community summaries shows\nfavorable performance over source text summarization on these same metrics, at lower token costs.\n2 Graph RAG Approach & Pipeline\nWe now unpack the high-level data flow of the Graph RAG approach (Figure 1) and pipeline, de-\nscribing key design parameters, techniques, and implementation details for each step.\n2.1 Source Documents \u2192Text Chunks\nA fundamental design decision is the granularity with which input texts extracted from source doc-\numents should be split into text chunks for processing. In the following step, each of these chunks\nwill be passed to a set of LLM prompts designed to extract the various elements of a graph index.\nLonger text chunks require fewer LLM calls for such extraction, but suffer from the recall degrada-\ntion of longer LLM context windows (Kuratov et al., 2024; Liu et al., 2023). This behavior can be\nobserved in Figure 2 in the case of a single extraction round (i.e., with zero gleanings): on a sample\ndataset (HotPotQA, Yang et al., 2018), using a chunk size of 600 token extracted almost twice as\nmany entity references as when using a chunk size of 2400. While more references are generally\nbetter, any extraction process needs to balance recall and precision for the target activity.\n2.2 Text Chunks \u2192Element Instances\nThe baseline requirement for this step is to identify and extract instances of graph nodes and edges\nfrom each chunk of source text. We do this using a multipart LLM prompt that first identifies all\nentities in the text, including their name, type, and description, before identifying all relationships\nbetween clearly-related entities, including the source and target entities and a description of their\nrelationship. Both kinds of element instance are output in a single list of delimited tuples.\nThe primary opportunity to tailor this prompt to the domain of the document corpus lies in the\nchoice of few-shot examples provided to the LLM for in-context learning (Brown et al., 2020).\n3For example, while our default prompt extracting the broad class of \u201cnamed entities\u201d like people,\nplaces, and organizations is generally applicable, domains with specialized knowledge (e.g., science,\nmedicine, law) will benefit from few-shot examples specialized to those domains. We also support\na secondary extraction prompt for any additional covariates we would like to associate with the\nextracted node instances. Our","chunk_id":"64476a39d7d8b87b399e3bd3cead79c7","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"LLMS","type":"TECHNOLOGY","description":"Large Language Models, advanced machine learning models capable of understanding and generating human language","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"TECHNOLOGY"},{"name":"GPT","type":"MODEL","description":"A series of large language models developed by OpenAI, known for their ability to generate human-like text","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"MODEL"},{"name":"LLAMA","type":"MODEL","description":"A series of large language models developed by Meta, known for their in-context learning capabilities","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"MODEL"},{"name":"GEMINI","type":"MODEL","description":"A series of large language models developed by Google, capable of in-context learning and summarization","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"MODEL"},{"name":"IN-CONTEXT LEARNING","type":"TECHNIQUE","description":"A method where models learn to perform tasks by being given examples within the context of the input","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"TECHNIQUE"},{"name":"QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION","type":"TASK","description":"A summarization task that focuses on generating summaries based on specific queries over an entire corpus","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"TASK"},{"name":"RAG","type":"TECHNIQUE","description":"Retrieval-Augmented Generation, a method that combines information retrieval with text generation","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"TECHNIQUE"},{"name":"GRAPH RAG","type":"TECHNIQUE","description":"A novel approach that uses a graph-based method for retrieval-augmented generation, focusing on global summarization of an LLM-derived knowledge graph","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"TECHNIQUE"},{"name":"KNOWLEDGE GRAPH","type":"STRUCTURE","description":"A structured representation of knowledge in the form of entities and their relationships, derived from LLMs","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"STRUCTURE"},{"name":"MODULARITY","type":"PROPERTY","description":"An inherent quality of graphs that allows them to be partitioned into modular communities of closely-related nodes","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"PROPERTY"},{"name":"COMMUNITY DETECTION ALGORITHMS","type":"TECHNIQUE","description":"Algorithms used to partition graphs into modular communities, such as Louvain and Leiden","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"TECHNIQUE"},{"name":"LOUVAIN","type":"ALGORITHM","description":"A community detection algorithm used to partition graphs into modular communities","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"ALGORITHM"},{"name":"LEIDEN","type":"ALGORITHM","description":"A community detection algorithm that improves upon Louvain, used to partition graphs into modular communities","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"ALGORITHM"},{"name":"HOTPOTQA","type":"DATASET","description":"A dataset used for evaluating question answering systems, containing diverse and complex questions","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"DATASET"},{"name":"GPT-4-TURBO","type":"MODEL","description":"An advanced version of the GPT-4 model, used for entity extraction in the described approach","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"MODEL"},{"name":"MAP-REDUCE","type":"TECHNIQUE","description":"A programming model used for processing large data sets with a distributed algorithm","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"TECHNIQUE"},{"name":"PODCAST TRANSCRIPTS","type":"DATASET","description":"A dataset containing transcripts of podcast episodes","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"DATASET"},{"name":"NEWS ARTICLES","type":"DATASET","description":"A dataset containing articles from news sources","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"DATASET"},{"name":"COMPREHENSIVENESS","type":"METRIC","description":"A quality metric that measures the extent to which a summary covers all relevant information","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"METRIC"},{"name":"DIVERSITY","type":"METRIC","description":"A quality metric that measures the variety of information included in a summary","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"METRIC"},{"name":"EMPOWERMENT","type":"METRIC","description":"A quality metric that measures the extent to which a summary enables understanding of broad issues and themes","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"METRIC"},{"name":"SOURCE TEXT SUMMARIZATION","type":"TASK","description":"The process of creating summaries directly from source texts","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"TASK"},{"name":"TOKEN COSTS","type":"METRIC","description":"A measure of the computational cost associated with processing text, often related to the number of tokens used","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"METRIC"},{"name":"TEXT CHUNKS","type":"DATA UNIT","description":"Segments of text extracted from source documents for processing","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"DATA UNIT"},{"name":"ENTITY EXTRACTION","type":"TASK","description":"The process of identifying and extracting entities from text","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"TASK"},{"name":"ELEMENT INSTANCES","type":"DATA UNIT","description":"Instances of graph nodes and edges identified and extracted from text chunks","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"DATA UNIT"},{"name":"FEW-SHOT EXAMPLES","type":"TECHNIQUE","description":"Examples provided to a model to help it learn a task with minimal training data","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"TECHNIQUE"},{"name":"NAMED ENTITIES","type":"CATEGORY","description":"Entities such as people, places, and organizations that are identified in text","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"CATEGORY"},{"name":"COVARIATES","type":"ATTRIBUTE","description":"Additional variables associated with extracted node instances","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"ATTRIBUTE"},{"name":"SUMMARIZATION TASKS","type":"","description":"","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"DATASET","type":"","description":"","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"MODEL","type":"","description":"","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"METRIC","type":"","description":"","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"DATA UNIT","type":"","description":"","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"TASK","type":"","description":"","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"TECHNIQUE","type":"","description":"","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"CATEGORY","type":"","description":"","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"ATTRIBUTE","type":"","description":"","source_id":"64476a39d7d8b87b399e3bd3cead79c7"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LLMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Large Language Models, advanced machine learning models capable of understanding and generating human language<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GPT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A series of large language models developed by OpenAI, known for their ability to generate human-like text<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"LLAMA\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A series of large language models developed by Meta, known for their in-context learning capabilities<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"GEMINI\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A series of large language models developed by Google, capable of in-context learning and summarization<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"IN-CONTEXT LEARNING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method where models learn to perform tasks by being given examples within the context of the input<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">A summarization task that focuses on generating summaries based on specific queries over an entire corpus<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"RAG\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Retrieval-Augmented Generation, a method that combines information retrieval with text generation<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"GRAPH RAG\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A novel approach that uses a graph-based method for retrieval-augmented generation, focusing on global summarization of an LLM-derived knowledge graph<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"KNOWLEDGE GRAPH\">      <data key=\"d0\">STRUCTURE<\/data>      <data key=\"d1\">A structured representation of knowledge in the form of entities and their relationships, derived from LLMs<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">STRUCTURE<\/data>    <\/node>    <node id=\"MODULARITY\">      <data key=\"d0\">PROPERTY<\/data>      <data key=\"d1\">An inherent quality of graphs that allows them to be partitioned into modular communities of closely-related nodes<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">PROPERTY<\/data>    <\/node>    <node id=\"COMMUNITY DETECTION ALGORITHMS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Algorithms used to partition graphs into modular communities, such as Louvain and Leiden<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"LOUVAIN\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">A community detection algorithm used to partition graphs into modular communities<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">ALGORITHM<\/data>    <\/node>    <node id=\"LEIDEN\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">A community detection algorithm that improves upon Louvain, used to partition graphs into modular communities<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">ALGORITHM<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset used for evaluating question answering systems, containing diverse and complex questions<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"GPT-4-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">An advanced version of the GPT-4 model, used for entity extraction in the described approach<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"MAP-REDUCE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A programming model used for processing large data sets with a distributed algorithm<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"PODCAST TRANSCRIPTS\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset containing transcripts of podcast episodes<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"NEWS ARTICLES\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset containing articles from news sources<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"COMPREHENSIVENESS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A quality metric that measures the extent to which a summary covers all relevant information<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"DIVERSITY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A quality metric that measures the variety of information included in a summary<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"EMPOWERMENT\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A quality metric that measures the extent to which a summary enables understanding of broad issues and themes<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"SOURCE TEXT SUMMARIZATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">The process of creating summaries directly from source texts<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"TOKEN COSTS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A measure of the computational cost associated with processing text, often related to the number of tokens used<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"TEXT CHUNKS\">      <data key=\"d0\">DATA UNIT<\/data>      <data key=\"d1\">Segments of text extracted from source documents for processing<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">DATA UNIT<\/data>    <\/node>    <node id=\"ENTITY EXTRACTION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">The process of identifying and extracting entities from text<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"ELEMENT INSTANCES\">      <data key=\"d0\">DATA UNIT<\/data>      <data key=\"d1\">Instances of graph nodes and edges identified and extracted from text chunks<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">DATA UNIT<\/data>    <\/node>    <node id=\"FEW-SHOT EXAMPLES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Examples provided to a model to help it learn a task with minimal training data<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"NAMED ENTITIES\">      <data key=\"d0\">CATEGORY<\/data>      <data key=\"d1\">Entities such as people, places, and organizations that are identified in text<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">CATEGORY<\/data>    <\/node>    <node id=\"COVARIATES\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Additional variables associated with extracted node instances<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">ATTRIBUTE<\/data>    <\/node>    <node id=\"SUMMARIZATION TASKS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"DATASET\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"MODEL\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"METRIC\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"DATA UNIT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"TASK\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"TECHNIQUE\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"CATEGORY\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"ATTRIBUTE\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <edge source=\"LLMS\" target=\"GPT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">GPT is a type of LLM<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"LLMS\" target=\"LLAMA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Llama is a type of LLM<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"LLMS\" target=\"GEMINI\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Gemini is a type of LLM<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"LLMS\" target=\"IN-CONTEXT LEARNING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">In-context learning is a technique used by LLMs<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION\" target=\"SUMMARIZATION TASKS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Query-focused abstractive summarization is a type of summarization task<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION\" target=\"RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">RAG is a technique used for query-focused abstractive summarization<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION\" target=\"MAP-REDUCE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Map-Reduce is a technique used for query-focused abstractive summarization<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"RAG\" target=\"GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG is a novel approach based on RAG<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"KNOWLEDGE GRAPH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG uses a knowledge graph for summarization<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"MODULARITY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Modularity is a property of knowledge graphs<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"COMMUNITY DETECTION ALGORITHMS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Community detection algorithms are used to partition knowledge graphs<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"COMMUNITY DETECTION ALGORITHMS\" target=\"LOUVAIN\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Louvain is a type of community detection algorithm<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"COMMUNITY DETECTION ALGORITHMS\" target=\"LEIDEN\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Leiden is a type of community detection algorithm<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"DATASET\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">HotPotQA is a dataset used for evaluating question answering systems<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GPT-4-TURBO\" target=\"MODEL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">GPT-4-Turbo is a model used for entity extraction<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"PODCAST TRANSCRIPTS\" target=\"DATASET\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Podcast transcripts are a type of dataset<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"NEWS ARTICLES\" target=\"DATASET\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">News articles are a type of dataset<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"COMPREHENSIVENESS\" target=\"METRIC\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Comprehensiveness is a quality metric for summarization<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"DIVERSITY\" target=\"METRIC\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Diversity is a quality metric for summarization<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"EMPOWERMENT\" target=\"METRIC\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Empowerment is a quality metric for summarization<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"SOURCE TEXT SUMMARIZATION\" target=\"SUMMARIZATION TASKS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Source text summarization is a type of summarization task<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"TOKEN COSTS\" target=\"METRIC\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Token costs are a metric related to the computational cost of processing text<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"TEXT CHUNKS\" target=\"DATA UNIT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Text chunks are segments of text extracted from source documents<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"ENTITY EXTRACTION\" target=\"TASK\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Entity extraction is a task involving the identification and extraction of entities from text<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"ELEMENT INSTANCES\" target=\"DATA UNIT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Element instances are instances of graph nodes and edges extracted from text chunks<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"FEW-SHOT EXAMPLES\" target=\"TECHNIQUE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Few-shot examples are a technique used to help models learn tasks with minimal training data<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"NAMED ENTITIES\" target=\"CATEGORY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Named entities are a category of entities identified in text<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"COVARIATES\" target=\"ATTRIBUTE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Covariates are additional variables associated with extracted node instances<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"e66ed885a08f92cc69f4895302c33047","chunk":" examples provided to the LLM for in-context learning (Brown et al., 2020).\n3For example, while our default prompt extracting the broad class of \u201cnamed entities\u201d like people,\nplaces, and organizations is generally applicable, domains with specialized knowledge (e.g., science,\nmedicine, law) will benefit from few-shot examples specialized to those domains. We also support\na secondary extraction prompt for any additional covariates we would like to associate with the\nextracted node instances. Our default covariate prompt aims to extract claims linked to detected\nentities, including the subject, object, type, description, source text span, and start and end dates.\nTo balance the needs of efficiency and quality, we use multiple rounds of \u201cgleanings\u201d, up to a\nspecified maximum, to encourage the LLM to detect any additional entities it may have missed\non prior extraction rounds. This is a multi-stage process in which we first ask the LLM to assess\nwhether all entities were extracted, using a logit bias of 100 to force a yes\/no decision. If the LLM\nresponds that entities were missed, then a continuation indicating that \u201cMANY entities were missed\nin the last extraction\u201d encourages the LLM to glean these missing entities. This approach allows us\nto use larger chunk sizes without a drop in quality (Figure 2) or the forced introduction of noise.\n2.3 Element Instances \u2192Element Summaries\nThe use of an LLM to \u201cextract\u201d descriptions of entities, relationships, and claims represented in\nsource texts is already a form of abstractive summarization, relying on the LLM to create inde-\npendently meaningful summaries of concepts that may be implied but not stated by the text itself\n(e.g., the presence of implied relationships). To convert all such instance-level summaries into sin-\ngle blocks of descriptive text for each graph element (i.e., entity node, relationship edge, and claim\ncovariate) requires a further round of LLM summarization over matching groups of instances.\nA potential concern at this stage is that the LLM may not consistently extract references to the\nsame entity in the same text format, resulting in duplicate entity elements and thus duplicate nodes\nin the entity graph. However, since all closely-related \u201ccommunities\u201d of entities will be detected\nand summarized in the following step, and given that LLMs can understand the common entity\nbehind multiple name variations, our overall approach is resilient to such variations provided there\nis sufficient connectivity from all variations to a shared set of closely-related entities.\nOverall, our use of rich descriptive text for homogeneous nodes in a potentially noisy graph structure\nis aligned with both the capabilities of LLMs and the needs of global, query-focused summarization.\nThese qualities also differentiate our graph index from typical knowledge graphs, which rely on\nconcise and consistent knowledge triples (subject, predicate, object) for downstream reasoning tasks.\n2.4 Element Summaries \u2192Graph Communities\nThe index created in the previous step can be modelled as an homogeneous undirected weighted\ngraph in which entity nodes are connected by relationship edges, with edge weights representing the\nnormalized counts of detected relationship instances. Given such a graph, a variety of community\ndetection algorithms may be used to partition the graph into communities of nodes with stronger\nconnections to one another than to the other nodes in the graph (e.g., see the surveys by Fortu-\nnato, 2010 and Jin et al., 2021). In our pipeline, we use Leiden (Traag et al., 2019) on account of\nits ability to recover hierarchical community structure of large-scale graphs efficiently (Figure 3).\nEach level of this hierarchy provides a community partition that covers the nodes of the graph in a\nmutually-exclusive, collective-exhaustive way, enabling divide-and-conquer global summarization.\n2.5 Graph Communities \u2192Community Summaries\nThe next step is to create report-like summaries of each community in the Leiden hierarchy, using\na method designed to scale to very large datasets. These summaries are independently useful in\ntheir own right as a way to understand the global structure and semantics of the dataset, and may\nthemselves be used to make sense of a corpus in the absence of a question. For example, a user\nmay scan through community summaries at one level looking for general themes of interest, then\nfollow links to the reports at the lower level that provide more details for each of the subtopics. Here,\nhowever, we focus on their utility as part of a graph-based index used for answering global queries.\nCommunity summaries are generated in the following way:\n4(a) Root communities at level 0 (b) Sub-communities at level 1\nFigure 3: Graph communities detected using the Leiden algorithm (Traag et al., 2019) over the\nMultiHop-RAG (Tang and Yang, 2024) dataset as indexed. Circles represent entity nodes with size\nproportional to their degree. Node layout was performed via OpenORD (Martin et al., 2011) and\nForce Atlas 2 (Jacomy et al., 2014). Node colors represent entity communities, shown at two levels\nof hierarchical clustering: (a) Level 0, corresponding to the hierarchical partition with maximum\nmodularity, and (b) Level 1, which reveals internal structure within these root-level communities.\n\u2022Leaf-level communities . The element summaries of a leaf-level community (nodes, edges,\ncovariates) are prioritized and then iteratively added to the LLM context window until\nthe token limit is reached. The prioritization is as follows: for each community edge in\ndecreasing order of combined source and target node degree (i.e., overall prominance), add\ndescriptions of the source node, target node, linked covari","chunk_id":"e66ed885a08f92cc69f4895302c33047","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"NAMED ENTITIES","type":"CONCEPT","description":"Named entities refer to specific categories of information such as people, places, and organizations that are extracted from text.","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"CONCEPT"},{"name":"COVARIATES","type":"CONCEPT","description":"Covariates are additional variables that are associated with the extracted node instances, such as claims linked to detected entities.","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"CONCEPT"},{"name":"LOGIT BIAS","type":"TECHNIQUE","description":"Logit bias is a method used to influence the output of a language model by adjusting the probabilities of certain tokens.","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"TECHNIQUE"},{"name":"ELEMENT INSTANCES","type":"CONCEPT","description":"Element instances refer to individual occurrences of entities, relationships, and claims represented in source texts.","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"CONCEPT"},{"name":"ELEMENT SUMMARIES","type":"CONCEPT","description":"Element summaries are descriptive texts created by summarizing multiple instances of entities, relationships, and claims.","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"CONCEPT"},{"name":"ENTITY GRAPH","type":"CONCEPT","description":"An entity graph is a structured representation of entities and their relationships, often used for knowledge representation and reasoning.","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"CONCEPT"},{"name":"COMMUNITY DETECTION","type":"TECHNIQUE","description":"Community detection is a method used to partition a graph into communities of nodes with stronger connections to one another than to other nodes in the graph.","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"TECHNIQUE"},{"name":"LEIDEN ALGORITHM","type":"TECHNIQUE","description":"The Leiden algorithm is a method for detecting hierarchical community structures in large-scale graphs efficiently.","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"TECHNIQUE"},{"name":"GRAPH COMMUNITIES","type":"CONCEPT","description":"Graph communities are groups of nodes in a graph that have stronger connections to each other than to nodes in other communities.","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"CONCEPT"},{"name":"COMMUNITY SUMMARIES","type":"CONCEPT","description":"Community summaries are report-like summaries of each community in a hierarchical graph, used to understand the global structure and semantics of the dataset.","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"CONCEPT"},{"name":"OPENORD","type":"TOOL","description":"OpenORD is a tool used for node layout in graph visualization, helping to arrange nodes in a visually meaningful way.","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"TOOL"},{"name":"FORCE ATLAS 2","type":"TOOL","description":"Force Atlas 2 is a tool used for node layout in graph visualization, helping to arrange nodes based on their connections and relationships.","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"TOOL"},{"name":"LLM","type":"","description":"","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"MULTIHOP-RAG","type":"DATASET","description":"MultiHop-RAG is a dataset used for benchmarking retrieval-augmented generation for multi-hop queries.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"CLAIMS","type":"CONCEPT","description":"Claims are statements or assertions linked to detected entities, often including details such as subject, object, type, description, source text span, and dates.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"HIERARCHICAL COMMUNITY STRUCTURE","type":"CONCEPT","description":"Hierarchical community structure refers to the organization of graph communities in a multi-level hierarchy, where each level represents a different granularity of community partitioning.","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"GLOBAL SUMMARIZATION","type":"TECHNIQUE","description":"Global summarization is the process of creating comprehensive summaries that capture the overall structure and key information of a dataset or graph.","source_id":"e66ed885a08f92cc69f4895302c33047"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"NAMED ENTITIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Named entities refer to specific categories of information such as people, places, and organizations that are extracted from text.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"COVARIATES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Covariates are additional variables that are associated with the extracted node instances, such as claims linked to detected entities.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"LOGIT BIAS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Logit bias is a method used to influence the output of a language model by adjusting the probabilities of certain tokens.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"ELEMENT INSTANCES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Element instances refer to individual occurrences of entities, relationships, and claims represented in source texts.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"ELEMENT SUMMARIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Element summaries are descriptive texts created by summarizing multiple instances of entities, relationships, and claims.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"ENTITY GRAPH\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">An entity graph is a structured representation of entities and their relationships, often used for knowledge representation and reasoning.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"COMMUNITY DETECTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Community detection is a method used to partition a graph into communities of nodes with stronger connections to one another than to other nodes in the graph.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"LEIDEN ALGORITHM\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The Leiden algorithm is a method for detecting hierarchical community structures in large-scale graphs efficiently.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"GRAPH COMMUNITIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Graph communities are groups of nodes in a graph that have stronger connections to each other than to nodes in other communities.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"COMMUNITY SUMMARIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Community summaries are report-like summaries of each community in a hierarchical graph, used to understand the global structure and semantics of the dataset.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"OPENORD\">      <data key=\"d0\">TOOL<\/data>      <data key=\"d1\">OpenORD is a tool used for node layout in graph visualization, helping to arrange nodes in a visually meaningful way.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">TOOL<\/data>    <\/node>    <node id=\"FORCE ATLAS 2\">      <data key=\"d0\">TOOL<\/data>      <data key=\"d1\">Force Atlas 2 is a tool used for node layout in graph visualization, helping to arrange nodes based on their connections and relationships.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">TOOL<\/data>    <\/node>    <node id=\"LLM\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"MULTIHOP-RAG\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MultiHop-RAG is a dataset used for benchmarking retrieval-augmented generation for multi-hop queries.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"CLAIMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Claims are statements or assertions linked to detected entities, often including details such as subject, object, type, description, source text span, and dates.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"HIERARCHICAL COMMUNITY STRUCTURE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Hierarchical community structure refers to the organization of graph communities in a multi-level hierarchy, where each level represents a different granularity of community partitioning.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"GLOBAL SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Global summarization is the process of creating comprehensive summaries that capture the overall structure and key information of a dataset or graph.<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <edge source=\"COVARIATES\" target=\"LLM\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">LLMs are used to extract covariates associated with detected entities.<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"COVARIATES\" target=\"CLAIMS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Claims are a type of covariate linked to detected entities.<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LOGIT BIAS\" target=\"LLM\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Logit bias is used to influence the output of LLMs.<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"ELEMENT INSTANCES\" target=\"ELEMENT SUMMARIES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Element instances are summarized into element summaries.<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"ELEMENT SUMMARIES\" target=\"ENTITY GRAPH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Element summaries are used to create an entity graph.<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"ENTITY GRAPH\" target=\"COMMUNITY DETECTION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Community detection is applied to the entity graph to partition it into communities.<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"COMMUNITY DETECTION\" target=\"LEIDEN ALGORITHM\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The Leiden algorithm is a method used for community detection.<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"COMMUNITY DETECTION\" target=\"GRAPH COMMUNITIES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph communities are the result of community detection.<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LEIDEN ALGORITHM\" target=\"MULTIHOP-RAG\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The Leiden algorithm is used to detect graph communities in the MultiHop-RAG dataset.<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LEIDEN ALGORITHM\" target=\"HIERARCHICAL COMMUNITY STRUCTURE\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The Leiden algorithm is used to recover hierarchical community structures in large-scale graphs.<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"GRAPH COMMUNITIES\" target=\"COMMUNITY SUMMARIES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Community summaries are created for each graph community.<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"GRAPH COMMUNITIES\" target=\"OPENORD\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">OpenORD is used for node layout in visualizing graph communities.<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"GRAPH COMMUNITIES\" target=\"FORCE ATLAS 2\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Force Atlas 2 is used for node layout in visualizing graph communities.<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"GLOBAL SUMMARIZATION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Community summaries are used for global summarization of the dataset.<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"4930fce6da868f894757a9da465807ba","chunk":" which reveals internal structure within these root-level communities.\n\u2022Leaf-level communities . The element summaries of a leaf-level community (nodes, edges,\ncovariates) are prioritized and then iteratively added to the LLM context window until\nthe token limit is reached. The prioritization is as follows: for each community edge in\ndecreasing order of combined source and target node degree (i.e., overall prominance), add\ndescriptions of the source node, target node, linked covariates, and the edge itself.\n\u2022Higher-level communities . If all element summaries fit within the token limit of the con-\ntext window, proceed as for leaf-level communities and summarize all element summaries\nwithin the community. Otherwise, rank sub-communities in decreasing order of element\nsummary tokens and iteratively substitute sub-community summaries (shorter) for their\nassociated element summaries (longer) until fit within the context window is achieved.\n2.6 Community Summaries \u2192Community Answers \u2192Global Answer\nGiven a user query, the community summaries generated in the previous step can be used to generate\na final answer in a multi-stage process. The hierarchical nature of the community structure also\nmeans that questions can be answered using the community summaries from different levels, raising\nthe question of whether a particular level in the hierarchical community structure offers the best\nbalance of summary detail and scope for general sensemaking questions (evaluated in section 3).\nFor a given community level, the global answer to any user query is generated as follows:\n\u2022Prepare community summaries . Community summaries are randomly shuffled and divided\ninto chunks of pre-specified token size. This ensures relevant information is distributed\nacross chunks, rather than concentrated (and potentially lost) in a single context window.\n\u2022Map community answers . Generate intermediate answers in parallel, one for each chunk.\nThe LLM is also asked to generate a score between 0-100 indicating how helpful the gen-\nerated answer is in answering the target question. Answers with score 0 are filtered out.\n\u2022Reduce to global answer . Intermediate community answers are sorted in descending order\nof helpfulness score and iteratively added into a new context window until the token limit\nis reached. This final context is used to generate the global answer returned to the user.\n5Dataset Example activity framing and generation of global sensemaking questions\nPodcast\ntranscriptsUser : A tech journalist looking for insights and trends in the tech industry\nTask: Understanding how tech leaders view the role of policy and regulation\nQuestions :\n1. Which episodes deal primarily with tech policy and government regulation?\n2. How do guests perceive the impact of privacy laws on technology development?\n3. Do any guests discuss the balance between innovation and ethical considerations?\n4. What are the suggested changes to current policies mentioned by the guests?\n5. Are collaborations between tech companies and governments discussed and how?\nNews\narticlesUser : Educator incorporating current affairs into curricula\nTask: Teaching about health and wellness\nQuestions :\n1. What current topics in health can be integrated into health education curricula?\n2. How do news articles address the concepts of preventive medicine and wellness?\n3. Are there examples of health articles that contradict each other, and if so, why?\n4. What insights can be gleaned about public health priorities based on news coverage?\n5. How can educators use the dataset to highlight the importance of health literacy?\nTable 1: Examples of potential users, tasks, and questions generated by the LLM based on short\ndescriptions of the target datasets. Questions target global understanding rather than specific details.\n3 Evaluation\n3.1 Datasets\nWe selected two datasets in the one million token range, each equivalent to about 10 novels of text\nand representative of the kind of corpora that users may encounter in their real world activities:\n\u2022Podcast transcripts . Compiled transcripts of podcast conversations between Kevin Scott,\nMicrosoft CTO, and other technology leaders (Behind the Tech, Scott, 2024). Size: 1669\n\u00d7600-token text chunks, with 100-token overlaps between chunks ( \u223c1 million tokens).\n\u2022News articles . Benchmark dataset comprising news articles published from September\n2013 to December 2023 in a range of categories, including entertainment, business, sports,\ntechnology, health, and science (MultiHop-RAG; Tang and Yang, 2024). Size: 3197 \u00d7\n600-token text chunks, with 100-token overlaps between chunks ( \u223c1.7 million tokens).\n3.2 Queries\nMany benchmark datasets for open-domain question answering exist, including HotPotQA (Yang\net al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However,\nthe associated question sets target explicit fact retrieval rather than summarization for the purpose\nof data sensemaking, i.e., the process though which people inspect, engage with, and contextualize\ndata within the broader scope of real-world activities (Koesten et al., 2021). Similarly, methods for\nextracting latent summarization queries from source texts also exist (Xu and Lapata, 2021), but such\nextracted questions can target details that betray prior knowledge of the texts.\nTo evaluate the effectiveness of RAG systems for more global sensemaking tasks, we need questions\nthat convey only a high-level understanding of dataset contents, and not the details of specific texts.\nWe used an activity-centered approach to automate the generation of such questions: given a short\ndescription of a dataset, we asked the LLM to identify Npotential users and Ntasks per user,\nthen for each (user, task) combination, we asked the LLM to generate Nquestions that require\nunderstanding of the entire corpus. For our evaluation, a value of N= 5 resulted in 125 test questions\nper","chunk_id":"4930fce6da868f894757a9da465807ba","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"LEAF-LEVEL COMMUNITIES","type":"CONCEPT","description":"Communities at the leaf level where element summaries (nodes, edges, covariates) are prioritized and added to the LLM context window until the token limit is reached","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"CONCEPT"},{"name":"HIGHER-LEVEL COMMUNITIES","type":"CONCEPT","description":"Communities at a higher level where element summaries are summarized within the community, and sub-communities are ranked and substituted to fit within the context window","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"CONCEPT"},{"name":"COMMUNITY SUMMARIES","type":"CONCEPT","description":"Summaries generated for communities, which are used to generate final answers in a multi-stage process","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"CONCEPT"},{"name":"GLOBAL ANSWER","type":"CONCEPT","description":"The final answer generated from community summaries in response to a user query","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"CONCEPT"},{"name":"USER QUERY","type":"CONCEPT","description":"A question posed by the user that the system aims to answer using community summaries","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"CONCEPT"},{"name":"COMMUNITY ANSWERS","type":"CONCEPT","description":"Intermediate answers generated from community summaries, which are then used to form the global answer","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"CONCEPT"},{"name":"TOKEN LIMIT","type":"CONSTRAINT","description":"The maximum number of tokens that can be included in the LLM context window","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"CONSTRAINT"},{"name":"PODCAST TRANSCRIPTS","type":"DATASET","description":"Compiled transcripts of podcast conversations between Kevin Scott, Microsoft CTO, and other technology leaders","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"DATASET"},{"name":"NEWS ARTICLES","type":"DATASET","description":"A benchmark dataset comprising news articles published from September 2013 to December 2023 in various categories","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"DATASET"},{"name":"TECH JOURNALIST","type":"USER","description":"A user looking for insights and trends in the tech industry","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"USER"},{"name":"EDUCATOR","type":"USER","description":"A user incorporating current affairs into curricula","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"USER"},{"name":"TECH POLICY","type":"TOPIC","description":"The role of policy and regulation in the tech industry","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"TOPIC"},{"name":"GOVERNMENT REGULATION","type":"TOPIC","description":"Government rules and laws affecting the tech industry","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"TOPIC"},{"name":"PRIVACY LAWS","type":"TOPIC","description":"Laws related to the protection of personal information and privacy","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"TOPIC"},{"name":"INNOVATION","type":"TOPIC","description":"The process of creating new technologies and ideas","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"TOPIC"},{"name":"ETHICAL CONSIDERATIONS","type":"TOPIC","description":"Moral principles that affect decision-making in technology development","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"TOPIC"},{"name":"COLLABORATIONS","type":"TOPIC","description":"Partnerships between tech companies and governments","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"TOPIC"},{"name":"HEALTH EDUCATION","type":"TOPIC","description":"Teaching about health and wellness","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"TOPIC"},{"name":"PREVENTIVE MEDICINE","type":"TOPIC","description":"Medical practices aimed at preventing diseases","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"TOPIC"},{"name":"WELLNESS","type":"TOPIC","description":"The state of being in good health","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"TOPIC"},{"name":"PUBLIC HEALTH","type":"TOPIC","description":"The health of the population as a whole","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"TOPIC"},{"name":"HEALTH LITERACY","type":"TOPIC","description":"The ability to understand and use health information","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"TOPIC"},{"name":"MULTIHOP-RAG","type":"SYSTEM","description":"A system for retrieval-augmented generation used in the context of news articles","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"SYSTEM"},{"name":"HOTPOTQA","type":"DATASET","description":"A benchmark dataset for open-domain question answering","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"DATASET"},{"name":"MT-BENCH","type":"DATASET","description":"A benchmark dataset for open-domain question answering","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"DATASET"},{"name":"RAG SYSTEMS","type":"TECHNOLOGY","description":"Retrieval-augmented generation systems used for answering questions and summarizing data","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"TECHNOLOGY"},{"name":"DATA SENSEMAKING","type":"PROCESS","description":"The process through which people inspect, engage with, and contextualize data within the broader scope of real-world activities","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"PROCESS"},{"name":"LATENT SUMMARIZATION QUERIES","type":"TECHNIQUE","description":"Methods for extracting summarization queries from source texts","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"TECHNIQUE"},{"name":"LLM","type":"TECHNOLOGY","description":"Large Language Model used for generating community summaries and answers","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"TECHNOLOGY"},{"name":"KEVIN SCOTT","type":"PERSON","description":"Microsoft CTO who participates in podcast conversations","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"PERSON"},{"name":"KOESTEN ET AL.","type":"AUTHORS","description":"Researchers who discussed the process of data sensemaking in 2021","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"AUTHORS"},{"name":"XU AND LAPATA","type":"AUTHORS","description":"Researchers who discussed methods for extracting latent summarization queries in 2021","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"AUTHORS"},{"name":"TANG AND YANG","type":"AUTHORS","description":"Researchers who discussed the MultiHop-RAG system in 2024","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"AUTHORS"},{"name":"ZHENG ET AL.","type":"AUTHORS","description":"Researchers who discussed the MT-Bench dataset in 2024","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"AUTHORS"},{"name":"BEHIND THE TECH","type":"PODCAST","description":"A podcast featuring conversations between Kevin Scott and other technology leaders","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"PODCAST"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LEAF-LEVEL COMMUNITIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Communities at the leaf level where element summaries (nodes, edges, covariates) are prioritized and added to the LLM context window until the token limit is reached<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"HIGHER-LEVEL COMMUNITIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Communities at a higher level where element summaries are summarized within the community, and sub-communities are ranked and substituted to fit within the context window<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"COMMUNITY SUMMARIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Summaries generated for communities, which are used to generate final answers in a multi-stage process<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"GLOBAL ANSWER\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The final answer generated from community summaries in response to a user query<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"USER QUERY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A question posed by the user that the system aims to answer using community summaries<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"COMMUNITY ANSWERS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Intermediate answers generated from community summaries, which are then used to form the global answer<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"TOKEN LIMIT\">      <data key=\"d0\">CONSTRAINT<\/data>      <data key=\"d1\">The maximum number of tokens that can be included in the LLM context window<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">CONSTRAINT<\/data>    <\/node>    <node id=\"PODCAST TRANSCRIPTS\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Compiled transcripts of podcast conversations between Kevin Scott, Microsoft CTO, and other technology leaders<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"NEWS ARTICLES\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A benchmark dataset comprising news articles published from September 2013 to December 2023 in various categories<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"TECH JOURNALIST\">      <data key=\"d0\">USER<\/data>      <data key=\"d1\">A user looking for insights and trends in the tech industry<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">USER<\/data>    <\/node>    <node id=\"EDUCATOR\">      <data key=\"d0\">USER<\/data>      <data key=\"d1\">A user incorporating current affairs into curricula<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">USER<\/data>    <\/node>    <node id=\"TECH POLICY\">      <data key=\"d0\">TOPIC<\/data>      <data key=\"d1\">The role of policy and regulation in the tech industry<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">TOPIC<\/data>    <\/node>    <node id=\"GOVERNMENT REGULATION\">      <data key=\"d0\">TOPIC<\/data>      <data key=\"d1\">Government rules and laws affecting the tech industry<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">TOPIC<\/data>    <\/node>    <node id=\"PRIVACY LAWS\">      <data key=\"d0\">TOPIC<\/data>      <data key=\"d1\">Laws related to the protection of personal information and privacy<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">TOPIC<\/data>    <\/node>    <node id=\"INNOVATION\">      <data key=\"d0\">TOPIC<\/data>      <data key=\"d1\">The process of creating new technologies and ideas<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">TOPIC<\/data>    <\/node>    <node id=\"ETHICAL CONSIDERATIONS\">      <data key=\"d0\">TOPIC<\/data>      <data key=\"d1\">Moral principles that affect decision-making in technology development<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">TOPIC<\/data>    <\/node>    <node id=\"COLLABORATIONS\">      <data key=\"d0\">TOPIC<\/data>      <data key=\"d1\">Partnerships between tech companies and governments<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">TOPIC<\/data>    <\/node>    <node id=\"HEALTH EDUCATION\">      <data key=\"d0\">TOPIC<\/data>      <data key=\"d1\">Teaching about health and wellness<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">TOPIC<\/data>    <\/node>    <node id=\"PREVENTIVE MEDICINE\">      <data key=\"d0\">TOPIC<\/data>      <data key=\"d1\">Medical practices aimed at preventing diseases<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">TOPIC<\/data>    <\/node>    <node id=\"WELLNESS\">      <data key=\"d0\">TOPIC<\/data>      <data key=\"d1\">The state of being in good health<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">TOPIC<\/data>    <\/node>    <node id=\"PUBLIC HEALTH\">      <data key=\"d0\">TOPIC<\/data>      <data key=\"d1\">The health of the population as a whole<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">TOPIC<\/data>    <\/node>    <node id=\"HEALTH LITERACY\">      <data key=\"d0\">TOPIC<\/data>      <data key=\"d1\">The ability to understand and use health information<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">TOPIC<\/data>    <\/node>    <node id=\"MULTIHOP-RAG\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system for retrieval-augmented generation used in the context of news articles<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A benchmark dataset for open-domain question answering<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"MT-BENCH\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A benchmark dataset for open-domain question answering<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"RAG SYSTEMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Retrieval-augmented generation systems used for answering questions and summarizing data<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"DATA SENSEMAKING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The process through which people inspect, engage with, and contextualize data within the broader scope of real-world activities<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"LATENT SUMMARIZATION QUERIES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Methods for extracting summarization queries from source texts<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Large Language Model used for generating community summaries and answers<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"KEVIN SCOTT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Microsoft CTO who participates in podcast conversations<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KOESTEN ET AL.\">      <data key=\"d0\">AUTHORS<\/data>      <data key=\"d1\">Researchers who discussed the process of data sensemaking in 2021<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">AUTHORS<\/data>    <\/node>    <node id=\"XU AND LAPATA\">      <data key=\"d0\">AUTHORS<\/data>      <data key=\"d1\">Researchers who discussed methods for extracting latent summarization queries in 2021<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">AUTHORS<\/data>    <\/node>    <node id=\"TANG AND YANG\">      <data key=\"d0\">AUTHORS<\/data>      <data key=\"d1\">Researchers who discussed the MultiHop-RAG system in 2024<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">AUTHORS<\/data>    <\/node>    <node id=\"ZHENG ET AL.\">      <data key=\"d0\">AUTHORS<\/data>      <data key=\"d1\">Researchers who discussed the MT-Bench dataset in 2024<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">AUTHORS<\/data>    <\/node>    <node id=\"BEHIND THE TECH\">      <data key=\"d0\">PODCAST<\/data>      <data key=\"d1\">A podcast featuring conversations between Kevin Scott and other technology leaders<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">PODCAST<\/data>    <\/node>    <edge source=\"LEAF-LEVEL COMMUNITIES\" target=\"LLM\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Leaf-level communities are summarized and added to the LLM context window<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"HIGHER-LEVEL COMMUNITIES\" target=\"LLM\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Higher-level communities are summarized and added to the LLM context window<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"GLOBAL ANSWER\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Community summaries are used to generate the global answer<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"LLM\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">LLM generates community summaries<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"GLOBAL ANSWER\" target=\"USER QUERY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The global answer is generated in response to a user query<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"GLOBAL ANSWER\" target=\"COMMUNITY ANSWERS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Community answers are used to form the global answer<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"PODCAST TRANSCRIPTS\" target=\"TECH JOURNALIST\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Tech journalists use podcast transcripts to gain insights and trends in the tech industry<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"NEWS ARTICLES\" target=\"EDUCATOR\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Educators use news articles to incorporate current affairs into curricula<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"NEWS ARTICLES\" target=\"MULTIHOP-RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">MultiHop-RAG is used in the context of news articles<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"TECH JOURNALIST\" target=\"TECH POLICY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Tech journalists are interested in episodes dealing with tech policy<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"TECH JOURNALIST\" target=\"GOVERNMENT REGULATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Tech journalists are interested in episodes dealing with government regulation<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"TECH JOURNALIST\" target=\"PRIVACY LAWS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Tech journalists are interested in the impact of privacy laws on technology development<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"TECH JOURNALIST\" target=\"INNOVATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Tech journalists are interested in the balance between innovation and ethical considerations<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"TECH JOURNALIST\" target=\"ETHICAL CONSIDERATIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Tech journalists are interested in the balance between innovation and ethical considerations<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"TECH JOURNALIST\" target=\"COLLABORATIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Tech journalists are interested in collaborations between tech companies and governments<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"EDUCATOR\" target=\"HEALTH EDUCATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Educators are interested in integrating current topics in health into health education curricula<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"EDUCATOR\" target=\"PREVENTIVE MEDICINE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Educators are interested in how news articles address preventive medicine<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"EDUCATOR\" target=\"WELLNESS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Educators are interested in how news articles address wellness<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"EDUCATOR\" target=\"PUBLIC HEALTH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Educators are interested in public health priorities based on news coverage<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"EDUCATOR\" target=\"HEALTH LITERACY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Educators are interested in highlighting the importance of health literacy<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"MULTIHOP-RAG\" target=\"TANG AND YANG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Tang and Yang discussed the MultiHop-RAG system<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"RAG SYSTEMS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">HotPotQA is a benchmark dataset used to evaluate RAG systems<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"MT-BENCH\" target=\"RAG SYSTEMS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">MT-Bench is a benchmark dataset used to evaluate RAG systems<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"MT-BENCH\" target=\"ZHENG ET AL.\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Zheng et al. discussed the MT-Bench dataset<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"RAG SYSTEMS\" target=\"DATA SENSEMAKING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">RAG systems are used for data sensemaking tasks<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"DATA SENSEMAKING\" target=\"LATENT SUMMARIZATION QUERIES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Latent summarization queries are used for data sensemaking<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"DATA SENSEMAKING\" target=\"KOESTEN ET AL.\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Koesten et al. discussed data sensemaking<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"LATENT SUMMARIZATION QUERIES\" target=\"XU AND LAPATA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Xu and Lapata discussed latent summarization queries<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"KEVIN SCOTT\" target=\"BEHIND THE TECH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Kevin Scott is a participant in the Behind the Tech podcast<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"26b2dad01a219bc034ac7d6a32d07582","chunk":" understanding of dataset contents, and not the details of specific texts.\nWe used an activity-centered approach to automate the generation of such questions: given a short\ndescription of a dataset, we asked the LLM to identify Npotential users and Ntasks per user,\nthen for each (user, task) combination, we asked the LLM to generate Nquestions that require\nunderstanding of the entire corpus. For our evaluation, a value of N= 5 resulted in 125 test questions\nper dataset. Table 1 shows example questions for each of the two evaluation datasets.\n63.3 Conditions\nWe compare six different conditions in our analysis, including Graph RAG using four levels of graph\ncommunities ( C0,C1,C2,C3), a text summarization method applying our map-reduce approach\ndirectly to source texts ( TS), and a na \u00a8\u0131ve \u201csemantic search\u201d RAG approach ( SS):\n\u2022CO. Uses root-level community summaries (fewest in number) to answer user queries.\n\u2022C1. Uses high-level community summaries to answer queries. These are sub-communities\nof C0, if present, otherwise C0 communities projected down.\n\u2022C2. Uses intermediate-level community summaries to answer queries. These are sub-\ncommunities of C1, if present, otherwise C1 communities projected down.\n\u2022C3. Uses low-level community summaries (greatest in number) to answer queries. These\nare sub-communities of C2, if present, otherwise C2 communities projected down.\n\u2022TS. The same method as in subsection 2.6, except source texts (rather than community\nsummaries) are shuffled and chunked for the map-reduce summarization stages.\n\u2022SS. An implementation of na \u00a8\u0131ve RAG in which text chunks are retrieved and added to the\navailable context window until the specified token limit is reached.\nThe size of the context window and the prompts used for answer generation are the same across\nall six conditions (except for minor modifications to reference styles to match the types of context\ninformation used). Conditions only differ in how the contents of the context window are created.\nThe graph index supporting conditions C0-C3was created using our generic prompts for entity and\nrelationship extraction only, with entity types and few-shot examples tailored to the domain of the\ndata. The graph indexing process used a context window size of 600 tokens with 1 gleaning for the\nPodcast dataset and 0 gleanings for the News dataset.\n3.4 Metrics\nLLMs have been shown to be good evaluators of natural language generation, achieving state-of-\nthe-art or competitive results compared against human judgements (Wang et al., 2023a; Zheng et al.,\n2024). While this approach can generate reference-based metrics when gold standard answers are\nknown, it is also capable of measuring the qualities of generated texts (e.g., fluency) in a reference-\nfree style (Wang et al., 2023a) as well as in head-to-head comparison of competing outputs (LLM-\nas-a-judge, Zheng et al., 2024). LLMs have also shown promise at evaluating the performance of\nconventional RAG systems, automatically evaluating qualities like context relevance, faithfulness,\nand answer relevance (RAGAS, Es et al., 2023).\nGiven the multi-stage nature of our Graph RAG mechanism, the multiple conditions we wanted to\ncompare, and the lack of gold standard answers to our activity-based sensemaking questions, we\ndecided to adopt a head-to-head comparison approach using an LLM evaluator. We selected three\ntarget metrics capturing qualities that are desirable for sensemaking activities, as well as a control\nmetric (directness) used as a indicator of validity. Since directness is effectively in opposition to\ncomprehensiveness and diversity, we would not expect any method to win across all four metrics.\nOur head-to-head measures computed using an LLM evaluator are as follows:\n\u2022Comprehensiveness . How much detail does the answer provide to cover all aspects and\ndetails of the question?\n\u2022Diversity . How varied and rich is the answer in providing different perspectives and insights\non the question?\n\u2022Empowerment . How well does the answer help the reader understand and make informed\njudgements about the topic?\n\u2022Directness . How specifically and clearly does the answer address the question?\nFor our evaluation, the LLM is provided with the question, target metric, and a pair of answers, and\nasked to assess which answer is better according to the metric, as well as why. It returns the winner\nif one exists, otherwise a tie if they are fundamentally similar and the differences are negligible.\nTo account for the stochasticity of LLMs, we run each comparison five times and use mean scores.\nTable 2 shows an example of LLM-generated assessment.\n7Question Which public figures are repeatedly mentioned across various entertainment articles?\nGraph\nRAGOverview of Prominent Public Figures in Entertainment\nThe entertainment industry is vast and diverse, encompassing film, television, music, sports, and\ndigital media. Certain public figures stand out due to their significant contributions and influence\nacross these sectors. The following summary highlights key individuals who are repeatedly\nmentioned in various entertainment articles, reflecting their impact and presence within the industry.\nActors and Directors [...]Public Figures in Controversy [...]Musicians and Executives [...]\nAthletes and Coaches [...]Influencers and Entrepreneurs [...]\nThe repeated mention of these figures in entertainment articles signifies their ongoing relevance and\nthe public\u2019s interest in their work. Their influence spans across various aspects of entertainment, from\nshaping cultural narratives in film and television to driving trends in music and digital media. These\nindividuals not only contribute to their respective fields but also influence the broader cultural","chunk_id":"26b2dad01a219bc034ac7d6a32d07582","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"LLM","type":"TECHNOLOGY","description":"Large Language Model, a type of advanced machine learning model trained on vast amounts of text data to understand and generate human language","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"TECHNOLOGY"},{"name":"USER","type":"ACTOR","description":"An individual who interacts with the dataset, performing various tasks and generating questions","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"ACTOR"},{"name":"TASK","type":"ACTIVITY","description":"A specific activity or job that a user performs with the dataset, such as generating questions","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"ACTIVITY"},{"name":"QUESTION","type":"OUTPUT","description":"A query generated by the LLM based on the dataset and user-task combinations","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"OUTPUT"},{"name":"GRAPH RAG","type":"SYSTEM","description":"A retrieval-augmented generation system that uses graph-based community summaries to answer user queries","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"SYSTEM"},{"name":"TEXT SUMMARIZATION (TS)","type":"TECHNIQUE","description":"A method that applies a map-reduce approach directly to source texts for summarization","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"TECHNIQUE"},{"name":"SEMANTIC SEARCH (SS)","type":"TECHNIQUE","description":"A naive retrieval-augmented generation approach where text chunks are retrieved and added to the context window until the token limit is reached","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"TECHNIQUE"},{"name":"CONTEXT WINDOW","type":"PARAMETER","description":"The size of the text context used for generating answers in the retrieval-augmented generation systems","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"PARAMETER"},{"name":"COMMUNITY SUMMARIES","type":"DATA","description":"Summaries of different levels of graph communities (C0, C1, C2, C3) used to answer user queries in the Graph RAG system","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"DATA"},{"name":"PODCAST DATASET","type":"DATA","description":"A specific dataset used in the evaluation, consisting of podcast-related data","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"DATA"},{"name":"NEWS DATASET","type":"DATA","description":"A specific dataset used in the evaluation, consisting of news-related data","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"DATA"},{"name":"METRIC","type":"PARAMETER","description":"A standard of measurement used to evaluate the quality of generated answers, including comprehensiveness, diversity, empowerment, and directness","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"PARAMETER"},{"name":"COMPREHENSIVENESS","type":"METRIC","description":"A metric that measures how much detail an answer provides to cover all aspects and details of the question","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"METRIC"},{"name":"DIVERSITY","type":"METRIC","description":"A metric that measures how varied and rich an answer is in providing different perspectives and insights on the question","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"METRIC"},{"name":"EMPOWERMENT","type":"METRIC","description":"A metric that measures how well an answer helps the reader understand and make informed judgments about the topic","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"METRIC"},{"name":"DIRECTNESS","type":"METRIC","description":"A control metric that measures how specifically and clearly an answer addresses the question","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"METRIC"},{"name":"LLM EVALUATOR","type":"TECHNOLOGY","description":"A large language model used to evaluate the quality of generated answers based on specific metrics","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"TECHNOLOGY"},{"name":"PUBLIC FIGURES","type":"ENTITY","description":"Individuals who are repeatedly mentioned across various entertainment articles, reflecting their impact and presence within the industry","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"ENTITY"},{"name":"ENTERTAINMENT ARTICLES","type":"DATA","description":"Articles related to the entertainment industry, encompassing film, television, music, sports, and digital media","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"DATA"},{"name":"SOURCE TEXTS","type":"","description":"","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"TEXT CHUNKS","type":"","description":"","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"ANSWER GENERATION","type":"","description":"","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"EVALUATION","type":"","description":"","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"GRAPH INDEX","type":"DATA","description":"An index created using generic prompts for entity and relationship extraction, tailored to the domain of the data","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"GLEANING","type":"TECHNIQUE","description":"A method used in the graph indexing process to extract relevant information from the dataset","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"RAGAS","type":"SYSTEM","description":"A system for automatically evaluating the performance of retrieval-augmented generation systems","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"WANG ET AL.","type":"PERSON","description":"Authors who contributed to the research on evaluating natural language generation using LLMs","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"ZHENG ET AL.","type":"PERSON","description":"Authors who contributed to the research on head-to-head comparison of competing outputs using LLMs","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"ES ET AL.","type":"PERSON","description":"Authors who contributed to the research on automatically evaluating qualities like context relevance, faithfulness, and answer relevance in RAG systems","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"RETRIEVAL-AUGMENTED GENERATION","type":"","description":"","source_id":"26b2dad01a219bc034ac7d6a32d07582"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Large Language Model, a type of advanced machine learning model trained on vast amounts of text data to understand and generate human language<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"USER\">      <data key=\"d0\">ACTOR<\/data>      <data key=\"d1\">An individual who interacts with the dataset, performing various tasks and generating questions<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">ACTOR<\/data>    <\/node>    <node id=\"TASK\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">A specific activity or job that a user performs with the dataset, such as generating questions<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">ACTIVITY<\/data>    <\/node>    <node id=\"QUESTION\">      <data key=\"d0\">OUTPUT<\/data>      <data key=\"d1\">A query generated by the LLM based on the dataset and user-task combinations<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">OUTPUT<\/data>    <\/node>    <node id=\"GRAPH RAG\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A retrieval-augmented generation system that uses graph-based community summaries to answer user queries<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"TEXT SUMMARIZATION (TS)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method that applies a map-reduce approach directly to source texts for summarization<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"SEMANTIC SEARCH (SS)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A naive retrieval-augmented generation approach where text chunks are retrieved and added to the context window until the token limit is reached<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"CONTEXT WINDOW\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">The size of the text context used for generating answers in the retrieval-augmented generation systems<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"COMMUNITY SUMMARIES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Summaries of different levels of graph communities (C0, C1, C2, C3) used to answer user queries in the Graph RAG system<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"PODCAST DATASET\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">A specific dataset used in the evaluation, consisting of podcast-related data<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"NEWS DATASET\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">A specific dataset used in the evaluation, consisting of news-related data<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"METRIC\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">A standard of measurement used to evaluate the quality of generated answers, including comprehensiveness, diversity, empowerment, and directness<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"COMPREHENSIVENESS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A metric that measures how much detail an answer provides to cover all aspects and details of the question<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"DIVERSITY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A metric that measures how varied and rich an answer is in providing different perspectives and insights on the question<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"EMPOWERMENT\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A metric that measures how well an answer helps the reader understand and make informed judgments about the topic<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"DIRECTNESS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A control metric that measures how specifically and clearly an answer addresses the question<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"LLM EVALUATOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A large language model used to evaluate the quality of generated answers based on specific metrics<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"PUBLIC FIGURES\">      <data key=\"d0\">ENTITY<\/data>      <data key=\"d1\">Individuals who are repeatedly mentioned across various entertainment articles, reflecting their impact and presence within the industry<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">ENTITY<\/data>    <\/node>    <node id=\"ENTERTAINMENT ARTICLES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Articles related to the entertainment industry, encompassing film, television, music, sports, and digital media<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"SOURCE TEXTS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"TEXT CHUNKS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"ANSWER GENERATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"EVALUATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"GRAPH INDEX\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">An index created using generic prompts for entity and relationship extraction, tailored to the domain of the data<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"GLEANING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method used in the graph indexing process to extract relevant information from the dataset<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"RAGAS\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system for automatically evaluating the performance of retrieval-augmented generation systems<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"WANG ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Authors who contributed to the research on evaluating natural language generation using LLMs<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"ZHENG ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Authors who contributed to the research on head-to-head comparison of competing outputs using LLMs<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"ES ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Authors who contributed to the research on automatically evaluating qualities like context relevance, faithfulness, and answer relevance in RAG systems<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED GENERATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <edge source=\"LLM\" target=\"QUESTION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The LLM generates questions based on the dataset and user-task combinations<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"USER\" target=\"TASK\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Users perform specific tasks with the dataset<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"TASK\" target=\"QUESTION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Tasks performed by users result in the generation of questions<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"COMMUNITY SUMMARIES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG uses community summaries to answer user queries<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"TEXT SUMMARIZATION (TS)\" target=\"SOURCE TEXTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Text summarization applies a map-reduce approach directly to source texts<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"SEMANTIC SEARCH (SS)\" target=\"TEXT CHUNKS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Semantic search retrieves text chunks and adds them to the context window<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"CONTEXT WINDOW\" target=\"ANSWER GENERATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The size of the context window affects answer generation<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"GRAPH INDEX\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The graph index is created using community summaries<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"PODCAST DATASET\" target=\"EVALUATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The podcast dataset is used for evaluation<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"NEWS DATASET\" target=\"EVALUATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The news dataset is used for evaluation<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"METRIC\" target=\"EVALUATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Metrics are used to evaluate the quality of generated answers<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"METRIC\" target=\"LLM EVALUATOR\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The LLM evaluator uses metrics to assess the quality of answers<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"LLM EVALUATOR\" target=\"WANG ET AL.\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Wang et al. contributed to the research on evaluating natural language generation using LLMs<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"LLM EVALUATOR\" target=\"ZHENG ET AL.\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Zheng et al. contributed to the research on head-to-head comparison of competing outputs using LLMs<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"ENTERTAINMENT ARTICLES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Public figures are repeatedly mentioned in entertainment articles<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"GRAPH INDEX\" target=\"GLEANING\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Gleaning is used in the graph indexing process<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"RAGAS\" target=\"RETRIEVAL-AUGMENTED GENERATION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">RAGAS evaluates the performance of retrieval-augmented generation systems<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"RAGAS\" target=\"ES ET AL.\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Es et al. contributed to the research on automatically evaluating qualities in RAG systems<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"c8e8019de153e439d6a79dcf209b943b","chunk":" Directors [...]Public Figures in Controversy [...]Musicians and Executives [...]\nAthletes and Coaches [...]Influencers and Entrepreneurs [...]\nThe repeated mention of these figures in entertainment articles signifies their ongoing relevance and\nthe public\u2019s interest in their work. Their influence spans across various aspects of entertainment, from\nshaping cultural narratives in film and television to driving trends in music and digital media. These\nindividuals not only contribute to their respective fields but also influence the broader cultural\nlandscape, often becoming central figures in social discussions and public discourse.\nNa\u00a8\u0131ve\nRAGPublic figures who are repeatedly mentioned across various entertainment articles include Taylor\nSwift, Travis Kelce, Britney Spears, and Justin Timberlake. These individuals have been highlighted\nfor various reasons, ranging from their professional achievements to their personal lives.\nTaylor Swift [...]Travis Kelce [...]Britney Spears [...]Justin Timberlake [...]\nThese figures are frequently covered due to their high-profile status and the public\u2019s interest in their\ncareers and personal lives. Their activities, whether in music, sports, or personal relationships, have\nsignificant cultural and economic impacts, as evidenced by the media coverage and public reactions.\nLLM\nDecisionComprehensiveness: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a more comprehensive and detailed list of public figures from a\nwider range of entertainment sectors, including film, television, music, sports, gaming, and digital\nmedia. It also includes specific examples of their contributions and the impact they have on their\nrespective fields, as well as mentions of controversies and their implications. Answer 2, while\ndetailed in its coverage of a few individuals, is limited to a smaller number of public figures and\nfocuses primarily on their personal lives and relationships rather than a broad spectrum of their\nprofessional influence across the entertainment industry.\nDiversity: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a more varied and rich response by covering a wide range of\npublic figures from different sectors of the entertainment industry, including film, television, music,\nsports, gaming, and digital media. It offers insights into the contributions and influence of these\nfigures, as well as controversies and their impact on public discourse. The answer also cites specific\ndata sources for each mentioned figure, indicating a diverse range of evidence to support the claims.\nIn contrast, Answer 2 focuses on a smaller group of public figures, primarily from the music industry\nand sports, and relies heavily on a single source for data, which makes it less diverse in perspectives\nand insights.\nEmpowerment: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a comprehensive and structured overview of public figures\nacross various sectors of the entertainment industry, including film, television, music, sports, and\ndigital media. It lists multiple individuals, providing specific examples of their contributions and the\ncontext in which they are mentioned in entertainment articles, along with references to data reports\nfor each claim. This approach helps the reader understand the breadth of the topic and make informed\njudgments without being misled. In contrast, Answer 2 focuses on a smaller group of public figures\nand primarily discusses their personal lives and relationships, which may not provide as broad an\nunderstanding of the topic. While Answer 2 also cites sources, it does not match the depth and variety\nof Answer 1.\nDirectness: Winner=2 (Na \u00a8\u0131ve RAG)\nAnswer 2 is better because it directly lists specific public figures who are repeatedly mentioned\nacross various entertainment articles, such as Taylor Swift, Travis Kelce, Britney Spears, and Justin\nTimberlake, and provides concise explanations for their frequent mentions. Answer 1, while\ncomprehensive, includes a lot of detailed information about various figures in different sectors of\nentertainment, which, while informative, does not directly answer the question with the same level of\nconciseness and specificity as Answer 2.\nTable 2: Example question for the News article dataset, with generated answers from Graph RAG\n(C2) and Na \u00a8\u0131ve RAG, as well as LLM-generated assessments.\n8Podcast transcripts\n501728252221\n835050484344\n725050535049\n755247505250\n785750485052\n795651504850SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nComprehensiveness501823251919\n825050504346\n775050504644\n755050504445\n815754565048\n815456555250SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nDiversity504257524951\n585059555251\n434150494748\n484551504950\n514853515051\n494952504950SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nEmpowerment505665606060\n445055525152\n354550474848\n404853505050\n404952505050\n404852505050SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nDirectness\nNews articles\n502028252121\n805044413836\n725650525452\n755948505855\n796246425059\n796448454150SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nComprehensiveness503338352931\n675053454440\n624750404141\n655560505050\n715659505051\n696059504950SS\nTS\nC0\nC1\nC","chunk_id":"c8e8019de153e439d6a79dcf209b943b","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"PUBLIC FIGURES IN CONTROVERSY","type":"CATEGORY","description":"Individuals who are frequently involved in public controversies, often covered in entertainment articles","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CATEGORY"},{"name":"MUSICIANS","type":"PROFESSION","description":"Individuals who create, perform, and produce music","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"PROFESSION"},{"name":"EXECUTIVES","type":"PROFESSION","description":"Individuals who hold high-level management positions in companies, particularly in the entertainment industry","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"PROFESSION"},{"name":"ATHLETES","type":"PROFESSION","description":"Individuals who compete in sports at a professional level","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"PROFESSION"},{"name":"COACHES","type":"PROFESSION","description":"Individuals who train and guide athletes or sports teams","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"PROFESSION"},{"name":"INFLUENCERS","type":"PROFESSION","description":"Individuals who have the power to affect the purchasing decisions of others because of their authority, knowledge, position, or relationship with their audience","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"PROFESSION"},{"name":"ENTREPRENEURS","type":"PROFESSION","description":"Individuals who start and run businesses, often in innovative or high-risk sectors","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"PROFESSION"},{"name":"TAYLOR SWIFT","type":"PERSON","description":"A highly influential musician known for her contributions to the music industry and her high-profile personal life","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"PERSON"},{"name":"TRAVIS KELCE","type":"PERSON","description":"A professional athlete known for his achievements in sports and his high-profile personal life","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"PERSON"},{"name":"BRITNEY SPEARS","type":"PERSON","description":"A highly influential musician known for her contributions to the music industry and her high-profile personal life","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"PERSON"},{"name":"JUSTIN TIMBERLAKE","type":"PERSON","description":"A highly influential musician and actor known for his contributions to the entertainment industry and his high-profile personal life","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"PERSON"},{"name":"ENTERTAINMENT ARTICLES","type":"MEDIA","description":"Articles that cover various aspects of the entertainment industry, including news about public figures, trends, and controversies","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"MEDIA"},{"name":"CULTURAL NARRATIVES","type":"CONCEPT","description":"The stories and ideas that shape the cultural understanding and values of a society, often influenced by media and public figures","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"FILM","type":"MEDIA","description":"A form of visual storytelling that involves the production and distribution of movies","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"MEDIA"},{"name":"TELEVISION","type":"MEDIA","description":"A form of visual storytelling that involves the production and distribution of TV shows","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"MEDIA"},{"name":"MUSIC","type":"MEDIA","description":"A form of auditory art that involves the creation, performance, and distribution of songs and compositions","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"MEDIA"},{"name":"DIGITAL MEDIA","type":"MEDIA","description":"Content that is created, distributed, and consumed through digital platforms, including social media, websites, and streaming services","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"MEDIA"},{"name":"SOCIAL DISCUSSIONS","type":"CONCEPT","description":"Public conversations and debates that occur in society, often influenced by media coverage and public figures","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"PUBLIC DISCOURSE","type":"CONCEPT","description":"The exchange of ideas and opinions in the public sphere, often influenced by media and public figures","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"MEDIA COVERAGE","type":"CONCEPT","description":"The reporting and analysis of events, trends, and public figures by various media outlets","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"PROFESSIONAL ACHIEVEMENTS","type":"CONCEPT","description":"The accomplishments and successes that individuals achieve in their professional careers","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"PERSONAL LIVES","type":"CONCEPT","description":"The private aspects of individuals' lives, often covered by media when it involves public figures","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CULTURAL IMPACT","type":"CONCEPT","description":"The influence that individuals, media, and events have on the cultural values, norms, and practices of a society","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"ECONOMIC IMPACT","type":"CONCEPT","description":"The effect that individuals, media, and events have on the economy, including financial markets, consumer behavior, and industry trends","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"GRAPH RAG","type":"SYSTEM","description":"A system that provides comprehensive and detailed lists of public figures from various entertainment sectors, including their contributions and impacts","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"SYSTEM"},{"name":"NAIVE RAG","type":"SYSTEM","description":"A system that provides concise and specific lists of public figures, focusing on their frequent mentions in entertainment articles","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"SYSTEM"},{"name":"LLM","type":"TECHNOLOGY","description":"Large Language Models used for generating assessments and evaluations of different systems","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"TECHNOLOGY"},{"name":"DECISION","type":"CONCEPT","description":"The process of making a choice between different options, often based on evaluations and assessments","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"COMPREHENSIVENESS","type":"CRITERION","description":"A measure of how complete and detailed a response or list is, covering a wide range of aspects and examples","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CRITERION"},{"name":"DIVERSITY","type":"CRITERION","description":"A measure of how varied and rich a response or list is, covering different sectors, perspectives, and sources","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CRITERION"},{"name":"EMPOWERMENT","type":"CRITERION","description":"A measure of how well a response or list helps the reader understand the breadth of a topic and make informed judgments","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CRITERION"},{"name":"DIRECTNESS","type":"CRITERION","description":"A measure of how concise and specific a response or list is, directly addressing the question with clear examples","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CRITERION"},{"name":"NEWS ARTICLES","type":"MEDIA","description":"Articles that cover current events, trends, and public figures, often influencing public opinion and discourse","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"MEDIA"},{"name":"PODCAST TRANSCRIPTS","type":"MEDIA","description":"Written records of spoken content from podcasts, often used for analysis and reference","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"MEDIA"},{"name":"DATA REPORTS","type":"MEDIA","description":"Documents that provide detailed information and analysis on various topics, often used as evidence to support claims","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"MEDIA"},{"name":"ENTERTAINMENT INDUSTRY","type":"SECTOR","description":"The sector that includes film, television, music, sports, gaming, and digital media, involving the production and distribution of entertainment content","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"SECTOR"},{"name":"GAMING","type":"MEDIA","description":"A form of interactive entertainment that involves playing video games","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"MEDIA"},{"name":"TRENDS","type":"CONCEPT","description":"Patterns or movements in society, culture, or industry that indicate a general direction of change or development","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CONTROVERSIES","type":"CONCEPT","description":"Public disputes or debates that arise from differing opinions, often involving public figures and media coverage","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"IMPLICATIONS","type":"CONCEPT","description":"The possible effects or consequences of an event, action, or decision, often discussed in media and public discourse","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"PERSPECTIVES","type":"CONCEPT","description":"Different viewpoints or angles from which a topic can be understood or analyzed","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"INSIGHTS","type":"CONCEPT","description":"Deep understanding or knowledge about a topic, often gained through analysis and evaluation","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"EVIDENCE","type":"CONCEPT","description":"Information or data that supports a claim or argument, often used in media and academic writing","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CLAIMS","type":"CONCEPT","description":"Statements or assertions that are made about a topic, often supported by evidence and analysis","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"JUDGMENTS","type":"CONCEPT","description":"Decisions or conclusions that are made based on evaluations and assessments","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"INFORMED JUDGMENTS","type":"CONCEPT","description":"Decisions or conclusions that are made based on comprehensive and well-supported information","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"MISLED","type":"CONCEPT","description":"Being given incorrect or misleading information, often resulting in incorrect judgments or decisions","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"BREADTH","type":"CONCEPT","description":"The wide scope or range of a topic, covering various aspects and examples","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"DEPTH","type":"CONCEPT","description":"The detailed and thorough coverage of a topic, providing in-depth analysis and understanding","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"VARIETY","type":"CONCEPT","description":"The inclusion of different elements, perspectives, or examples in a response or list","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"SPECIFICITY","type":"CONCEPT","description":"The quality of being clear and precise, providing specific examples and details","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CONCISENESS","type":"CONCEPT","description":"The quality of being brief and to the point, providing information in a clear and succinct manner","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"SECTORS","type":"CONCEPT","description":"Different areas or divisions within an industry or field, such as film, television, music, sports, and digital media","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CONTRIBUTIONS","type":"CONCEPT","description":"The actions or efforts made by individuals that add value or impact to their field or society","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"IMPACT","type":"CONCEPT","description":"The effect or influence that an event, action, or individual has on a field, society, or culture","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CONTEXT","type":"CONCEPT","description":"The circumstances or background information that help to understand a topic or event","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"DATA SOURCES","type":"CONCEPT","description":"The origins of information or data used to support claims and analysis","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"EVIDENCE TO SUPPORT CLAIMS","type":"CONCEPT","description":"Information or data that is used to back up statements or assertions","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"PERSPECTIVES AND INSIGHTS","type":"CONCEPT","description":"Different viewpoints and deep understanding about a topic, often gained through analysis and evaluation","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"SINGLE SOURCE","type":"CONCEPT","description":"Relying on one origin of information or data, which may limit the diversity of perspectives and insights","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"BROAD UNDERSTANDING","type":"CONCEPT","description":"A wide and comprehensive grasp of a topic, covering various aspects and examples","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"SPECIFIC EXAMPLES","type":"CONCEPT","description":"Clear and precise instances or cases that illustrate a point or concept","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CONTRIBUTIONS AND IMPACTS","type":"CONCEPT","description":"The actions or efforts made by individuals and their effects on their field or society","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"FREQUENT MENTIONS","type":"CONCEPT","description":"The repeated appearance or reference to individuals or topics in media or discussions","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CONCISE EXPLANATIONS","type":"CONCEPT","description":"Brief and clear descriptions that provide essential information","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"DETAILED INFORMATION","type":"CONCEPT","description":"Comprehensive and thorough data or descriptions that provide in-depth understanding","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"ENTERTAINMENT SECTORS","type":"CONCEPT","description":"Different areas within the entertainment industry, such as film, television, music, sports, gaming, and digital media","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"SPECIFIC DATA SOURCES","type":"CONCEPT","description":"Particular origins of information or data used to support claims and analysis","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"VARIED AND RICH RESPONSE","type":"CONCEPT","description":"A reply that includes a wide range of elements, perspectives, and examples","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"WIDE RANGE OF PUBLIC FIGURES","type":"CONCEPT","description":"A diverse group of individuals from different sectors of the entertainment industry","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CONTRIBUTIONS AND INFLUENCE","type":"CONCEPT","description":"The actions or efforts made by individuals and their effects on their field or society","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CONTROVERSIES AND IMPACT","type":"CONCEPT","description":"Public disputes or debates and their effects on society and public discourse","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"PUBLIC DISCOURSE AND IMPACT","type":"CONCEPT","description":"The exchange of ideas and opinions in the public sphere and its effects on society","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"BROAD SPECTRUM OF PROFESSIONAL INFLUENCE","type":"CONCEPT","description":"The wide range of effects that individuals have in their professional fields","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"PERSONAL LIVES AND RELATIONSHIPS","type":"CONCEPT","description":"The private aspects of individuals' lives and their interactions with others, often covered by media","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"BREADTH OF THE TOPIC","type":"CONCEPT","description":"The wide scope or range of a subject, covering various aspects and examples","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"INFORMED JUDGMENTS AND UNDERSTANDING","type":"CONCEPT","description":"Decisions or conclusions that are made based on comprehensive and well-supported information\nDecisions or conclusions that are made based on comprehensive and well-supported information, providing a sound basis for the choice and a deeper grasp of the subject","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"MISLED AND INCORRECT JUDGMENTS","type":"CONCEPT","description":"Being given incorrect or misleading information, resulting in incorrect decisions or conclusions","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"BREADTH AND VARIETY","type":"CONCEPT","description":"The wide scope and range of elements, perspectives, or examples in a response or list, providing a comprehensive understanding\nThe wide scope and range of elements, perspectives, or examples in a response or list","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"DEPTH AND VARIETY","type":"CONCEPT","description":"The detailed and thorough coverage of a topic, providing in-depth analysis and understanding, along with a range of elements and perspectives","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"VARIETY AND PERSPECTIVES","type":"CONCEPT","description":"The inclusion of different elements, viewpoints, or angles in a response or list","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"SPECIFICITY AND CONCISENESS","type":"CONCEPT","description":"The quality of being clear and precise, providing specific examples and details in a brief and to-the-point manner","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CONCISENESS AND SPECIFICITY","type":"CONCEPT","description":"The quality of being brief and to the point, providing clear and precise examples and details","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"SECTORS AND CONTRIBUTIONS","type":"CONCEPT","description":"Different areas within an industry or field and the actions or efforts made by individuals that add value or impact","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CONTRIBUTIONS AND IMPACT","type":"CONCEPT","description":"The actions or efforts made by individuals and their effects on their field or society","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CONTEXT AND UNDERSTANDING","type":"CONCEPT","description":"The circumstances or background information that help to understand a topic or event, providing a deeper grasp of the subject","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"DATA SOURCES AND EVIDENCE","type":"CONCEPT","description":"The origins of information or data used to support claims and analysis, providing proof or backing for statements","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"EVIDENCE AND CLAIMS","type":"CONCEPT","description":"Information or data that supports a statement or assertion, providing proof or backing for the argument","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"JUDGMENTS AND DECISIONS","type":"CONCEPT","description":"Decisions or conclusions that are made based on evaluations and assessments, often influenced by evidence and analysis","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"INFORMED JUDGMENTS AND DECISIONS","type":"CONCEPT","description":"Decisions or conclusions that are made based on comprehensive and well-supported information, providing a sound basis for the choice","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"MISLED AND INCORRECT DECISIONS","type":"CONCEPT","description":"Being given incorrect or misleading information, resulting in incorrect judgments or conclusions","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"BREADTH AND DEPTH","type":"CONCEPT","description":"The wide scope and detailed coverage of a topic, providing a comprehensive and thorough understanding","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"VARIETY AND DEPTH","type":"CONCEPT","description":"The inclusion of different elements and detailed coverage of a topic, providing a comprehensive and thorough understanding","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"SPECIFICITY AND EXAMPLES","type":"CONCEPT","description":"The quality of being clear and precise, providing specific instances or cases that illustrate a point or concept","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CONCISENESS AND EXAMPLES","type":"CONCEPT","description":"The quality of being brief and to the point, providing clear and precise instances or cases that illustrate a point or concept","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"SECTORS AND IMPACT","type":"CONCEPT","description":"Different areas within an industry or field and the effects or influence that individuals, media, and events have on the field or society","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"CONTRIBUTIONS AND CONTEXT","type":"CONCEPT","description":"The actions or efforts made by individuals and the circumstances or background information that help to understand their impact","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"IMPACT AND CONTEXT","type":"CONCEPT","description":"The effects or influence that individuals, media, and events have on a field or society, and the circumstances or background information that help to understand these effects","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"DATA SOURCES AND CLAIMS","type":"CONCEPT","description":"The origins of information or data used to support statements or assertions, providing proof or backing for the argument","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"EVIDENCE AND JUDGMENTS","type":"CONCEPT","description":"Information or data that supports a statement or assertion, providing proof or backing for decisions or conclusions","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"JUDGMENTS AND UNDERSTANDING","type":"CONCEPT","description":"Decisions or conclusions that are made based on evaluations and assessments, providing a deeper grasp of the subject","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"},{"name":"MISLED AND INCORRECT UNDERSTANDING","type":"CONCEPT","description":"Being given incorrect or misleading information, resulting in incorrect judgments or conclusions and a flawed grasp of the subject","source_id":"c8e8019de153e439d6a79dcf209b943b","entity_type":"CONCEPT"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"PUBLIC FIGURES IN CONTROVERSY\">      <data key=\"d0\">CATEGORY<\/data>      <data key=\"d1\">Individuals who are frequently involved in public controversies, often covered in entertainment articles<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CATEGORY<\/data>    <\/node>    <node id=\"MUSICIANS\">      <data key=\"d0\">PROFESSION<\/data>      <data key=\"d1\">Individuals who create, perform, and produce music<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">PROFESSION<\/data>    <\/node>    <node id=\"EXECUTIVES\">      <data key=\"d0\">PROFESSION<\/data>      <data key=\"d1\">Individuals who hold high-level management positions in companies, particularly in the entertainment industry<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">PROFESSION<\/data>    <\/node>    <node id=\"ATHLETES\">      <data key=\"d0\">PROFESSION<\/data>      <data key=\"d1\">Individuals who compete in sports at a professional level<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">PROFESSION<\/data>    <\/node>    <node id=\"COACHES\">      <data key=\"d0\">PROFESSION<\/data>      <data key=\"d1\">Individuals who train and guide athletes or sports teams<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">PROFESSION<\/data>    <\/node>    <node id=\"INFLUENCERS\">      <data key=\"d0\">PROFESSION<\/data>      <data key=\"d1\">Individuals who have the power to affect the purchasing decisions of others because of their authority, knowledge, position, or relationship with their audience<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">PROFESSION<\/data>    <\/node>    <node id=\"ENTREPRENEURS\">      <data key=\"d0\">PROFESSION<\/data>      <data key=\"d1\">Individuals who start and run businesses, often in innovative or high-risk sectors<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">PROFESSION<\/data>    <\/node>    <node id=\"TAYLOR SWIFT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A highly influential musician known for her contributions to the music industry and her high-profile personal life<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TRAVIS KELCE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A professional athlete known for his achievements in sports and his high-profile personal life<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BRITNEY SPEARS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A highly influential musician known for her contributions to the music industry and her high-profile personal life<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JUSTIN TIMBERLAKE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A highly influential musician and actor known for his contributions to the entertainment industry and his high-profile personal life<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ENTERTAINMENT ARTICLES\">      <data key=\"d0\">MEDIA<\/data>      <data key=\"d1\">Articles that cover various aspects of the entertainment industry, including news about public figures, trends, and controversies<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">MEDIA<\/data>    <\/node>    <node id=\"CULTURAL NARRATIVES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The stories and ideas that shape the cultural understanding and values of a society, often influenced by media and public figures<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"FILM\">      <data key=\"d0\">MEDIA<\/data>      <data key=\"d1\">A form of visual storytelling that involves the production and distribution of movies<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">MEDIA<\/data>    <\/node>    <node id=\"TELEVISION\">      <data key=\"d0\">MEDIA<\/data>      <data key=\"d1\">A form of visual storytelling that involves the production and distribution of TV shows<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">MEDIA<\/data>    <\/node>    <node id=\"MUSIC\">      <data key=\"d0\">MEDIA<\/data>      <data key=\"d1\">A form of auditory art that involves the creation, performance, and distribution of songs and compositions<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">MEDIA<\/data>    <\/node>    <node id=\"DIGITAL MEDIA\">      <data key=\"d0\">MEDIA<\/data>      <data key=\"d1\">Content that is created, distributed, and consumed through digital platforms, including social media, websites, and streaming services<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">MEDIA<\/data>    <\/node>    <node id=\"SOCIAL DISCUSSIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Public conversations and debates that occur in society, often influenced by media coverage and public figures<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"PUBLIC DISCOURSE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The exchange of ideas and opinions in the public sphere, often influenced by media and public figures<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"MEDIA COVERAGE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The reporting and analysis of events, trends, and public figures by various media outlets<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"PROFESSIONAL ACHIEVEMENTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The accomplishments and successes that individuals achieve in their professional careers<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"PERSONAL LIVES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The private aspects of individuals' lives, often covered by media when it involves public figures<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CULTURAL IMPACT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The influence that individuals, media, and events have on the cultural values, norms, and practices of a society<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"ECONOMIC IMPACT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The effect that individuals, media, and events have on the economy, including financial markets, consumer behavior, and industry trends<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"GRAPH RAG\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system that provides comprehensive and detailed lists of public figures from various entertainment sectors, including their contributions and impacts<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"NAIVE RAG\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system that provides concise and specific lists of public figures, focusing on their frequent mentions in entertainment articles<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Large Language Models used for generating assessments and evaluations of different systems<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"DECISION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The process of making a choice between different options, often based on evaluations and assessments<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"COMPREHENSIVENESS\">      <data key=\"d0\">CRITERION<\/data>      <data key=\"d1\">A measure of how complete and detailed a response or list is, covering a wide range of aspects and examples<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CRITERION<\/data>    <\/node>    <node id=\"DIVERSITY\">      <data key=\"d0\">CRITERION<\/data>      <data key=\"d1\">A measure of how varied and rich a response or list is, covering different sectors, perspectives, and sources<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CRITERION<\/data>    <\/node>    <node id=\"EMPOWERMENT\">      <data key=\"d0\">CRITERION<\/data>      <data key=\"d1\">A measure of how well a response or list helps the reader understand the breadth of a topic and make informed judgments<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CRITERION<\/data>    <\/node>    <node id=\"DIRECTNESS\">      <data key=\"d0\">CRITERION<\/data>      <data key=\"d1\">A measure of how concise and specific a response or list is, directly addressing the question with clear examples<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CRITERION<\/data>    <\/node>    <node id=\"NEWS ARTICLES\">      <data key=\"d0\">MEDIA<\/data>      <data key=\"d1\">Articles that cover current events, trends, and public figures, often influencing public opinion and discourse<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">MEDIA<\/data>    <\/node>    <node id=\"PODCAST TRANSCRIPTS\">      <data key=\"d0\">MEDIA<\/data>      <data key=\"d1\">Written records of spoken content from podcasts, often used for analysis and reference<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">MEDIA<\/data>    <\/node>    <node id=\"DATA REPORTS\">      <data key=\"d0\">MEDIA<\/data>      <data key=\"d1\">Documents that provide detailed information and analysis on various topics, often used as evidence to support claims<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">MEDIA<\/data>    <\/node>    <node id=\"ENTERTAINMENT INDUSTRY\">      <data key=\"d0\">SECTOR<\/data>      <data key=\"d1\">The sector that includes film, television, music, sports, gaming, and digital media, involving the production and distribution of entertainment content<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">SECTOR<\/data>    <\/node>    <node id=\"GAMING\">      <data key=\"d0\">MEDIA<\/data>      <data key=\"d1\">A form of interactive entertainment that involves playing video games<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">MEDIA<\/data>    <\/node>    <node id=\"TRENDS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Patterns or movements in society, culture, or industry that indicate a general direction of change or development<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CONTROVERSIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Public disputes or debates that arise from differing opinions, often involving public figures and media coverage<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"IMPLICATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The possible effects or consequences of an event, action, or decision, often discussed in media and public discourse<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"PERSPECTIVES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Different viewpoints or angles from which a topic can be understood or analyzed<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"INSIGHTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Deep understanding or knowledge about a topic, often gained through analysis and evaluation<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"EVIDENCE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Information or data that supports a claim or argument, often used in media and academic writing<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CLAIMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Statements or assertions that are made about a topic, often supported by evidence and analysis<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"JUDGMENTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Decisions or conclusions that are made based on evaluations and assessments<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"INFORMED JUDGMENTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Decisions or conclusions that are made based on comprehensive and well-supported information<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"MISLED\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Being given incorrect or misleading information, often resulting in incorrect judgments or decisions<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"BREADTH\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The wide scope or range of a topic, covering various aspects and examples<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"DEPTH\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The detailed and thorough coverage of a topic, providing in-depth analysis and understanding<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"VARIETY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The inclusion of different elements, perspectives, or examples in a response or list<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SPECIFICITY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The quality of being clear and precise, providing specific examples and details<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CONCISENESS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The quality of being brief and to the point, providing information in a clear and succinct manner<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SECTORS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Different areas or divisions within an industry or field, such as film, television, music, sports, and digital media<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CONTRIBUTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The actions or efforts made by individuals that add value or impact to their field or society<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"IMPACT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The effect or influence that an event, action, or individual has on a field, society, or culture<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CONTEXT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The circumstances or background information that help to understand a topic or event<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"DATA SOURCES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The origins of information or data used to support claims and analysis<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"EVIDENCE TO SUPPORT CLAIMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Information or data that is used to back up statements or assertions<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"PERSPECTIVES AND INSIGHTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Different viewpoints and deep understanding about a topic, often gained through analysis and evaluation<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SINGLE SOURCE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Relying on one origin of information or data, which may limit the diversity of perspectives and insights<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"BROAD UNDERSTANDING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A wide and comprehensive grasp of a topic, covering various aspects and examples<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SPECIFIC EXAMPLES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Clear and precise instances or cases that illustrate a point or concept<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CONTRIBUTIONS AND IMPACTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The actions or efforts made by individuals and their effects on their field or society<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"FREQUENT MENTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The repeated appearance or reference to individuals or topics in media or discussions<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CONCISE EXPLANATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Brief and clear descriptions that provide essential information<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"DETAILED INFORMATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Comprehensive and thorough data or descriptions that provide in-depth understanding<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"ENTERTAINMENT SECTORS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Different areas within the entertainment industry, such as film, television, music, sports, gaming, and digital media<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SPECIFIC DATA SOURCES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Particular origins of information or data used to support claims and analysis<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"VARIED AND RICH RESPONSE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A reply that includes a wide range of elements, perspectives, and examples<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"WIDE RANGE OF PUBLIC FIGURES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A diverse group of individuals from different sectors of the entertainment industry<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CONTRIBUTIONS AND INFLUENCE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The actions or efforts made by individuals and their effects on their field or society<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CONTROVERSIES AND IMPACT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Public disputes or debates and their effects on society and public discourse<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"PUBLIC DISCOURSE AND IMPACT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The exchange of ideas and opinions in the public sphere and its effects on society<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"BROAD SPECTRUM OF PROFESSIONAL INFLUENCE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The wide range of effects that individuals have in their professional fields<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"PERSONAL LIVES AND RELATIONSHIPS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The private aspects of individuals' lives and their interactions with others, often covered by media<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"BREADTH OF THE TOPIC\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The wide scope or range of a subject, covering various aspects and examples<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"INFORMED JUDGMENTS AND UNDERSTANDING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Decisions or conclusions that are made based on comprehensive and well-supported informationDecisions or conclusions that are made based on comprehensive and well-supported information, providing a sound basis for the choice and a deeper grasp of the subject<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"MISLED AND INCORRECT JUDGMENTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Being given incorrect or misleading information, resulting in incorrect decisions or conclusions<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"BREADTH AND VARIETY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The wide scope and range of elements, perspectives, or examples in a response or list, providing a comprehensive understandingThe wide scope and range of elements, perspectives, or examples in a response or list<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"DEPTH AND VARIETY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The detailed and thorough coverage of a topic, providing in-depth analysis and understanding, along with a range of elements and perspectives<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"VARIETY AND PERSPECTIVES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The inclusion of different elements, viewpoints, or angles in a response or list<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SPECIFICITY AND CONCISENESS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The quality of being clear and precise, providing specific examples and details in a brief and to-the-point manner<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CONCISENESS AND SPECIFICITY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The quality of being brief and to the point, providing clear and precise examples and details<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SECTORS AND CONTRIBUTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Different areas within an industry or field and the actions or efforts made by individuals that add value or impact<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CONTRIBUTIONS AND IMPACT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The actions or efforts made by individuals and their effects on their field or society<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CONTEXT AND UNDERSTANDING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The circumstances or background information that help to understand a topic or event, providing a deeper grasp of the subject<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"DATA SOURCES AND EVIDENCE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The origins of information or data used to support claims and analysis, providing proof or backing for statements<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"EVIDENCE AND CLAIMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Information or data that supports a statement or assertion, providing proof or backing for the argument<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"JUDGMENTS AND DECISIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Decisions or conclusions that are made based on evaluations and assessments, often influenced by evidence and analysis<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"INFORMED JUDGMENTS AND DECISIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Decisions or conclusions that are made based on comprehensive and well-supported information, providing a sound basis for the choice<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"MISLED AND INCORRECT DECISIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Being given incorrect or misleading information, resulting in incorrect judgments or conclusions<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"BREADTH AND DEPTH\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The wide scope and detailed coverage of a topic, providing a comprehensive and thorough understanding<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"VARIETY AND DEPTH\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The inclusion of different elements and detailed coverage of a topic, providing a comprehensive and thorough understanding<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SPECIFICITY AND EXAMPLES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The quality of being clear and precise, providing specific instances or cases that illustrate a point or concept<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CONCISENESS AND EXAMPLES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The quality of being brief and to the point, providing clear and precise instances or cases that illustrate a point or concept<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SECTORS AND IMPACT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Different areas within an industry or field and the effects or influence that individuals, media, and events have on the field or society<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CONTRIBUTIONS AND CONTEXT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The actions or efforts made by individuals and the circumstances or background information that help to understand their impact<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"IMPACT AND CONTEXT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The effects or influence that individuals, media, and events have on a field or society, and the circumstances or background information that help to understand these effects<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"DATA SOURCES AND CLAIMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The origins of information or data used to support statements or assertions, providing proof or backing for the argument<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"EVIDENCE AND JUDGMENTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Information or data that supports a statement or assertion, providing proof or backing for decisions or conclusions<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"JUDGMENTS AND UNDERSTANDING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Decisions or conclusions that are made based on evaluations and assessments, providing a deeper grasp of the subject<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"MISLED AND INCORRECT UNDERSTANDING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Being given incorrect or misleading information, resulting in incorrect judgments or conclusions and a flawed grasp of the subject<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"ede7063998065122cf7a7152979c1909","chunk":"\n502028252121\n805044413836\n725650525452\n755948505855\n796246425059\n796448454150SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nComprehensiveness503338352931\n675053454440\n624750404141\n655560505050\n715659505051\n696059504950SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nDiversity504757495050\n535058505048\n434250424544\n515058505251\n505055485050\n505256495050SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nEmpowerment505459555554\n465055535252\n414550484847\n454752504949\n454852515049\n464853515150SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nDirectness\nFigure 4: Head-to-head win rate percentages of (row condition) over (column condition) across two\ndatasets, four metrics, and 125 questions per comparison (each repeated five times and averaged).\nThe overall winner per dataset and metric is shown in bold. Self-win rates were not computed but\nare shown as the expected 50% for reference. All Graph RAG conditions outperformed na \u00a8\u0131ve RAG\non comprehensiveness and diversity. Conditions C1-C3 also showed slight improvements in answer\ncomprehensiveness and diversity over TS (global text summarization without a graph index).\n3.5 Configuration\nThe effect of context window size on any particular task is unclear, especially for models like\ngpt-4-turbo with a large context size of 128k tokens. Given the potential for information to\nbe \u201clost in the middle\u201d of longer contexts (Kuratov et al., 2024; Liu et al., 2023), we wanted to ex-\nplore the effects of varying the context window size for our combinations of datasets, questions, and\nmetrics. In particular, our goal was to determine the optimum context size for our baseline condition\n(SS) and then use this uniformly for all query-time LLM use. To that end, we tested four context\nwindow sizes: 8k, 16k, 32k and 64k. Surprisingly, the smallest context window size tested (8k)\nwas universally better for all comparisons on comprehensiveness (average win rate of 58.1%), while\nperforming comparably with larger context sizes on diversity (average win rate = 52.4%), and em-\npowerment (average win rate = 51.3%). Given our preference for more comprehensive and diverse\nanswers, we therefore used a fixed context window size of 8k tokens for the final evaluation.\n3.6 Results\nThe indexing process resulted in a graph consisting of 8564 nodes and 20691 edges for the Podcast\ndataset, and a larger graph of 15754 nodes and 19520 edges for the News dataset. Table 3 shows the\nnumber of community summaries at different levels of each graph community hierarchy.\nGlobal approaches vs. na \u00a8\u0131ve RAG . As shown in Figure 4, global approaches consistently out-\nperformed the na \u00a8\u0131ve RAG ( SS) approach in both comprehensiveness and diversity metrics across\ndatasets. Specifically, global approaches achieved comprehensiveness win rates between 72-83%\nfor Podcast transcripts and 72-80% for News articles, while diversity win rates ranged from 75-82%\nand 62-71% respectively. Our use of directness as a validity test also achieved the expected results,\ni.e., that na \u00a8\u0131ve RAG produces the most direct responses across all comparisons.\n9Podcast Transcripts News Articles\nC0 C1 C2 C3 TS C0 C1 C2 C3 TS\nUnits 34 367 969 1310 1669 55 555 1797 2142 3197\nTokens 26657 225756 565720 746100 1014611 39770 352641 980898 1140266 1707694\n% Max 2.6 22.2 55.8 73.5 100 2.3 20.7 57.4 66.8 100\nTable 3: Number of context units (community summaries for C0-C3 and text chunks for TS), corre-\nsponding token counts, and percentage of the maximum token count. Map-reduce summarization of\nsource texts is the most resource-intensive approach requiring the highest number of context tokens.\nRoot-level community summaries ( C0) require dramatically fewer tokens per query (9x-43x).\nCommunity summaries vs. source texts. When comparing community summaries to source texts\nusing Graph RAG, community summaries generally provided a small but consistent improvement\nin answer comprehensiveness and diversity, except for root-level summaries. Intermediate-level\nsummaries in the Podcast dataset and low-level community summaries in the News dataset achieved\ncomprehensiveness win rates of 57% and 64%, respectively. Diversity win rates were 57% for\nPodcast intermediate-level summaries and 60% for News low-level community summaries. Table 3\nalso illustrates the scalability advantages of Graph RAG compared to source text summarization: for\nlow-level community summaries ( C3), Graph RAG required 26-33% fewer context tokens, while\nfor root-level community summaries ( C0), it required over 97% fewer tokens.","chunk_id":"ede7063998065122cf7a7152979c1909","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"NA\u00cfVE RAG","type":"TECHNIQUE","description":"A basic retrieval-augmented generation approach that does not use graph-based methods","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"TECHNIQUE"},{"name":"COMPREHENSIVENESS","type":"METRIC","description":"A measure of how complete and thorough the answers are","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"METRIC"},{"name":"DIVERSITY","type":"METRIC","description":"A measure of how varied and different the answers are","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"METRIC"},{"name":"EMPOWERMENT","type":"METRIC","description":"A measure of how much the answers enable or empower the user","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"METRIC"},{"name":"DIRECTNESS","type":"METRIC","description":"A measure of how straightforward and direct the answers are","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"METRIC"},{"name":"CONTEXT WINDOW SIZE","type":"PARAMETER","description":"The size of the context window used in the model, tested at 8k, 16k, 32k, and 64k tokens","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"PARAMETER"},{"name":"GPT-4-TURBO","type":"MODEL","description":"A large language model with a context size of 128k tokens","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"MODEL"},{"name":"PODCAST DATASET","type":"DATASET","description":"A dataset consisting of podcast transcripts used for testing the models","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"DATASET"},{"name":"NEWS DATASET","type":"DATASET","description":"A dataset consisting of news articles used for testing the models","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"DATASET"},{"name":"COMMUNITY SUMMARIES","type":"TECHNIQUE","description":"Summaries generated at different levels of a graph community hierarchy","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"TECHNIQUE"},{"name":"SOURCE TEXTS","type":"DATA","description":"The original texts from which summaries are generated","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"DATA"},{"name":"KURATOV ET AL., 2024","type":"REFERENCE","description":"A reference to a study by Kuratov and colleagues in 2024","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"REFERENCE"},{"name":"LIU ET AL., 2023","type":"REFERENCE","description":"A reference to a study by Liu and colleagues in 2023","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"REFERENCE"},{"name":"GRAPH RAG","type":"","description":"","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"MAP-REDUCE SUMMARIZATION","type":"TECHNIQUE","description":"A summarization approach that is resource-intensive and requires a high number of context tokens","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"GLOBAL TEXT SUMMARIZATION","type":"TECHNIQUE","description":"A summarization approach that does not use a graph index","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"BASELINE CONDITION (SS)","type":"PARAMETER","description":"The baseline condition used for testing, referred to as SS","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"C0","type":"PARAMETER","description":"Root-level community summaries in the graph community hierarchy","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"C1","type":"PARAMETER","description":"Intermediate-level community summaries in the graph community hierarchy","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"C2","type":"PARAMETER","description":"Intermediate-level community summaries in the graph community hierarchy","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"C3","type":"PARAMETER","description":"Low-level community summaries in the graph community hierarchy","source_id":"ede7063998065122cf7a7152979c1909"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"NA&#207;VE RAG\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A basic retrieval-augmented generation approach that does not use graph-based methods<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"COMPREHENSIVENESS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A measure of how complete and thorough the answers are<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"DIVERSITY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A measure of how varied and different the answers are<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"EMPOWERMENT\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A measure of how much the answers enable or empower the user<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"DIRECTNESS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A measure of how straightforward and direct the answers are<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"CONTEXT WINDOW SIZE\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">The size of the context window used in the model, tested at 8k, 16k, 32k, and 64k tokens<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"GPT-4-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A large language model with a context size of 128k tokens<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"PODCAST DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset consisting of podcast transcripts used for testing the models<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"NEWS DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset consisting of news articles used for testing the models<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"COMMUNITY SUMMARIES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Summaries generated at different levels of a graph community hierarchy<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"SOURCE TEXTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">The original texts from which summaries are generated<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"KURATOV ET AL., 2024\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a study by Kuratov and colleagues in 2024<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"LIU ET AL., 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a study by Liu and colleagues in 2023<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"GRAPH RAG\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"MAP-REDUCE SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A summarization approach that is resource-intensive and requires a high number of context tokens<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"GLOBAL TEXT SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A summarization approach that does not use a graph index<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"BASELINE CONDITION (SS)\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">The baseline condition used for testing, referred to as SS<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"C0\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Root-level community summaries in the graph community hierarchy<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"C1\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Intermediate-level community summaries in the graph community hierarchy<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"C2\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Intermediate-level community summaries in the graph community hierarchy<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"C3\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Low-level community summaries in the graph community hierarchy<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <edge source=\"NA&#207;VE RAG\" target=\"GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG consistently outperformed na&#239;ve RAG in both comprehensiveness and diversity metrics across datasets<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"NA&#207;VE RAG\" target=\"GLOBAL TEXT SUMMARIZATION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Global text summarization without a graph index is compared to na&#239;ve RAG<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"COMPREHENSIVENESS\" target=\"DIVERSITY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Both metrics are used to evaluate the quality of answers<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"CONTEXT WINDOW SIZE\" target=\"GPT-4-TURBO\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The context window size is a parameter tested for models like GPT-4-Turbo<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"CONTEXT WINDOW SIZE\" target=\"BASELINE CONDITION (SS)\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The baseline condition (SS) was used to determine the optimum context window size<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"PODCAST DATASET\" target=\"NEWS DATASET\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Both datasets are used for testing the models<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"SOURCE TEXTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Community summaries are compared to source texts using Graph RAG<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"C0\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">C0 represents root-level community summaries in the graph community hierarchy<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"C1\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">C1 represents intermediate-level community summaries in the graph community hierarchy<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"C2\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">C2 represents intermediate-level community summaries in the graph community hierarchy<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"C3\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">C3 represents low-level community summaries in the graph community hierarchy<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"SOURCE TEXTS\" target=\"MAP-REDUCE SUMMARIZATION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Map-reduce summarization is a resource-intensive approach requiring a high number of context tokens from source texts<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"KURATOV ET AL., 2024\" target=\"LIU ET AL., 2023\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Both references discuss the potential for information to be \"lost in the middle\" of longer contexts<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"edab4014b8f55e5b25bd7f396314be1f","chunk":" win rates of 57% and 64%, respectively. Diversity win rates were 57% for\nPodcast intermediate-level summaries and 60% for News low-level community summaries. Table 3\nalso illustrates the scalability advantages of Graph RAG compared to source text summarization: for\nlow-level community summaries ( C3), Graph RAG required 26-33% fewer context tokens, while\nfor root-level community summaries ( C0), it required over 97% fewer tokens. For a modest drop in\nperformance compared with other global methods, root-level Graph RAG offers a highly efficient\nmethod for the iterative question answering that characterizes sensemaking activity, while retaining\nadvantages in comprehensiveness (72% win rate) and diversity (62% win rate) over na \u00a8\u0131ve RAG.\nEmpowerment . Empowerment comparisons showed mixed results for both global approaches versus\nna\u00a8\u0131ve RAG ( SS) and Graph RAG approaches versus source text summarization ( TS). Ad-hoc LLM\nuse to analyze LLM reasoning for this measure indicated that the ability to provide specific exam-\nples, quotes, and citations was judged to be key to helping users reach an informed understanding.\nTuning element extraction prompts may help to retain more of these details in the Graph RAG index.\n4 Related Work\n4.1 RAG Approaches and Systems\nWhen using LLMs, RAG involves first retrieving relevant information from external data sources,\nthen adding this information to the context window of the LLM along with the original query (Ram\net al., 2023). Na \u00a8\u0131ve RAG approaches (Gao et al., 2023) do this by converting documents to text,\nsplitting text into chunks, and embedding these chunks into a vector space in which similar positions\nrepresent similar semantics. Queries are then embedded into the same vector space, with the text\nchunks of the nearest kvectors used as context. More advanced variations exist, but all solve the\nproblem of what to do when an external dataset of interest exceeds the LLM\u2019s context window.\nAdvanced RAG systems include pre-retrieval, retrieval, post-retrieval strategies designed to over-\ncome the drawbacks of Na \u00a8\u0131ve RAG, while Modular RAG systems include patterns for iterative and\ndynamic cycles of interleaved retrieval and generation (Gao et al., 2023). Our implementation of\nGraph RAG incorporates multiple concepts related to other systems. For example, our community\nsummaries are a kind of self-memory (Selfmem, Cheng et al., 2024) for generation-augmented re-\ntrieval (GAR, Mao et al., 2020) that facilitates future generation cycles, while our parallel generation\nof community answers from these summaries is a kind of iterative (Iter-RetGen, Shao et al., 2023)\nor federated (FeB4RAG, Wang et al., 2024) retrieval-generation strategy. Other systems have also\ncombined these concepts for multi-document summarization (CAiRE-COVID, Su et al., 2020) and\nmulti-hop question answering (ITRG, Feng et al., 2023; IR-CoT, Trivedi et al., 2022; DSP, Khattab\net al., 2022). Our use of a hierarchical index and summarization also bears resemblance to further\napproaches, such as generating a hierarchical index of text chunks by clustering the vectors of text\nembeddings (RAPTOR, Sarthi et al., 2024) or generating a \u201ctree of clarifications\u201d to answer mul-\ntiple interpretations of ambiguous questions (Kim et al., 2023). However, none of these iterative or\nhierarchical approaches use the kind of self-generated graph index that enables Graph RAG.\n104.2 Graphs and LLMs\nUse of graphs in connection with LLMs and RAG is a developing research area, with multiple\ndirections already established. These include using LLMs for knowledge graph creation (Tra-\njanoska et al., 2023) and completion (Yao et al., 2023), as well as for the extraction of causal\ngraphs (Ban et al., 2023; Zhang et al., 2024) from source texts. They also include forms of ad-\nvanced RAG (Gao et al., 2023) where the index is a knowledge graph (KAPING, Baek et al., 2023),\nwhere subsets of the graph structure (G-Retriever, He et al., 2024) or derived graph metrics (Graph-\nToolFormer, Zhang, 2023) are the objects of enquiry, where narrative outputs are strongly grounded\nin the facts of retrieved subgraphs (SURGE, Kang et al., 2023), where retrieved event-plot sub-\ngraphs are serialized using narrative templates (FABULA, Ranade and Joshi, 2023), and where the\nsystem supports both creation and traversal of text-relationship graphs for multi-hop question an-\nswering (Wang et al., 2023b). In terms of open-source software, a variety a graph databases are\nsupported by both the LangChain (LangChain, 2024) and LlamaIndex (LlamaIndex, 2024) libraries,\nwhile a more general class of graph-based RAG applications is also emerging, including systems that\ncan create and reason over knowledge graphs in both Neo4J (NaLLM, Neo4J, 2024) and Nebula-\nGraph (GraphRAG, NebulaGraph, 2024) formats. Unlike our Graph RAG approach, however, none\nof these systems use the natural modularity of graphs to partition data for global summarization","chunk_id":"edab4014b8f55e5b25bd7f396314be1f","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"NA\u00cfVE RAG","type":"SYSTEM","description":"Na\u00efve RAG is a basic retrieval-augmented generation approach that converts documents to text, splits text into chunks, and embeds these chunks into a vector space for retrieval","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"LLM","type":"TECHNOLOGY","description":"Large Language Models (LLMs) are advanced machine learning models trained on vast amounts of text data to understand and generate human language","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"EMPOWERMENT","type":"CONCEPT","description":"Empowerment in this context refers to the ability of a system to help users reach an informed understanding by providing specific examples, quotes, and citations","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"RAG","type":"TECHNIQUE","description":"Retrieval-Augmented Generation (RAG) is a method that involves retrieving relevant information from external data sources and adding it to the context window of an LLM along with the original query","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"MODULAR RAG","type":"SYSTEM","description":"Modular RAG systems include patterns for iterative and dynamic cycles of interleaved retrieval and generation to overcome the drawbacks of Na\u00efve RAG","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"SELF-MEMORY","type":"CONCEPT","description":"Self-memory is a concept related to generation-augmented retrieval, facilitating future generation cycles by retaining information from previous interactions","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"GENERATION-AUGMENTED RETRIEVAL","type":"TECHNIQUE","description":"Generation-Augmented Retrieval (GAR) is a method that combines retrieval and generation processes to improve the quality of responses","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"ITERATIVE RETRIEVAL-GENERATION","type":"TECHNIQUE","description":"Iterative Retrieval-Generation (Iter-RetGen) is a strategy that involves multiple cycles of retrieval and generation to refine responses","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"FEDERATED RETRIEVAL-GENERATION","type":"TECHNIQUE","description":"Federated Retrieval-Generation (FeB4RAG) is a strategy that involves parallel generation of community answers from summaries","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"MULTI-DOCUMENT SUMMARIZATION","type":"TECHNIQUE","description":"Multi-Document Summarization is a method that combines information from multiple documents to create a comprehensive summary","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"MULTI-HOP QUESTION ANSWERING","type":"TECHNIQUE","description":"Multi-Hop Question Answering is a method that involves answering questions by retrieving and integrating information from multiple sources","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"HIERARCHICAL INDEX","type":"TECHNIQUE","description":"Hierarchical Indexing involves generating a hierarchical structure of text chunks by clustering the vectors of text embeddings","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"TREE OF CLARIFICATIONS","type":"TECHNIQUE","description":"Tree of Clarifications is a method for generating multiple interpretations of ambiguous questions and answering them","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"KNOWLEDGE GRAPH","type":"TECHNOLOGY","description":"A Knowledge Graph is a structured representation of knowledge in the form of entities and their relationships","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"CAUSAL GRAPHS","type":"TECHNOLOGY","description":"Causal Graphs are graphical representations that show causal relationships between variables","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"KAPING","type":"SYSTEM","description":"An advanced RAG system where the index is a knowledge graph\nKAPING is an advanced RAG system where the index is a knowledge graph","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"SYSTEM"},{"name":"G-RETRIEVER","type":"SYSTEM","description":"G-Retriever is a system where subsets of the graph structure are the objects of enquiry\nA system where subsets of the graph structure are the objects of enquiry","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"SYSTEM"},{"name":"GRAPH-TOOLFORMER","type":"SYSTEM","description":"A system where derived graph metrics are the objects of enquiry\nGraph-ToolFormer is a system where derived graph metrics are the objects of enquiry","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"SYSTEM"},{"name":"SURGE","type":"SYSTEM","description":"A system where narrative outputs are strongly grounded in the facts of retrieved subgraphs\nSURGE is a system where narrative outputs are strongly grounded in the facts of retrieved subgraphs","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"SYSTEM"},{"name":"FABULA","type":"SYSTEM","description":"A system where retrieved event-plot subgraphs are serialized using narrative templates\nFABULA is a system where retrieved event-plot subgraphs are serialized using narrative templates","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"SYSTEM"},{"name":"ITRG","type":"SYSTEM","description":"ITRG is a system that supports both creation and traversal of text-relationship graphs for multi-hop question answering\nA system that supports both creation and traversal of text-relationship graphs for multi-hop question answering","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"SYSTEM"},{"name":"LANGCHAIN","type":"SOFTWARE","description":"A library that supports various graph databases for graph-based RAG applications\nLangChain is a library that supports various graph databases for graph-based RAG applications","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"SOFTWARE"},{"name":"LLAMAINDEX","type":"SOFTWARE","description":"A library that supports various graph databases for graph-based RAG applications\nLlamaIndex is a library that supports various graph databases for graph-based RAG applications","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"SOFTWARE"},{"name":"NEO4J","type":"SOFTWARE","description":"Neo4J is a graph database supported by LangChain and LlamaIndex for creating and reasoning over knowledge graphs\nA graph database supported by LangChain and LlamaIndex for creating and reasoning over knowledge graphs","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"SOFTWARE"},{"name":"NEBULAGRAPH","type":"SOFTWARE","description":"A graph database supported by LangChain and LlamaIndex for creating and reasoning over knowledge graphs\nNebulaGraph is a graph database supported by LangChain and LlamaIndex for creating and reasoning over knowledge graphs","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"SOFTWARE"},{"name":"GRAPH RAG INDEX","type":"TECHNIQUE","description":"A self-generated graph index used in Graph RAG for partitioning data for global summarization\nGraph RAG Index is a self-generated graph index used in Graph RAG for partitioning data for global summarization","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"TECHNIQUE"},{"name":"RAM ET AL., 2023","type":"REFERENCE","description":"A reference to a work by Ram et al. in 2023 related to RAG approaches","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"GAO ET AL., 2023","type":"REFERENCE","description":"A reference to a work by Gao et al. in 2023 related to Na\u00efve RAG and advanced RAG systems","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"CHENG ET AL., 2024","type":"REFERENCE","description":"A reference to a work by Cheng et al. in 2024 related to self-memory in generation-augmented retrieval","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"MAO ET AL., 2020","type":"REFERENCE","description":"A reference to a work by Mao et al. in 2020 related to generation-augmented retrieval","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"SHAO ET AL., 2023","type":"REFERENCE","description":"A reference to a work by Shao et al. in 2023 related to iterative retrieval-generation strategies","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"WANG ET AL., 2024","type":"REFERENCE","description":"A reference to a work by Wang et al. in 2024 related to federated retrieval-generation strategies","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"SU ET AL., 2020","type":"REFERENCE","description":"A reference to a work by Su et al. in 2020 related to multi-document summarization","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"FENG ET AL., 2023","type":"REFERENCE","description":"A reference to a work by Feng et al. in 2023 related to multi-hop question answering","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"TRIVEDI ET AL., 2022","type":"REFERENCE","description":"A reference to a work by Trivedi et al. in 2022 related to multi-hop question answering","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"KHATTAB ET AL., 2022","type":"REFERENCE","description":"A reference to a work by Khattab et al. in 2022 related to multi-hop question answering","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"SARTHI ET AL., 2024","type":"REFERENCE","description":"A reference to a work by Sarthi et al. in 2024 related to hierarchical indexing","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"KIM ET AL., 2023","type":"REFERENCE","description":"A reference to a work by Kim et al. in 2023 related to generating a tree of clarifications","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"TRAJANOSKA ET AL., 2023","type":"REFERENCE","description":"A reference to a work by Trajanoska et al. in 2023 related to knowledge graph creation","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"YAO ET AL., 2023","type":"REFERENCE","description":"A reference to a work by Yao et al. in 2023 related to knowledge graph completion","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"BAN ET AL., 2023","type":"REFERENCE","description":"A reference to a work by Ban et al. in 2023 related to the extraction of causal graphs","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"ZHANG ET AL., 2024","type":"REFERENCE","description":"A reference to a work by Zhang et al. in 2024 related to the extraction of causal graphs","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"BAEK ET AL., 2023","type":"REFERENCE","description":"A reference to a work by Baek et al. in 2023 related to KAPING","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"HE ET AL., 2024","type":"REFERENCE","description":"A reference to a work by He et al. in 2024 related to G-Retriever","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"ZHANG, 2023","type":"REFERENCE","description":"A reference to a work by Zhang in 2023 related to Graph-ToolFormer","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"KANG ET AL., 2023","type":"REFERENCE","description":"A reference to a work by Kang et al. in 2023 related to SURGE","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"RANADE AND JOSHI, 2023","type":"REFERENCE","description":"A reference to a work by Ranade and Joshi in 2023 related to FABULA","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"WANG ET AL., 2023B","type":"REFERENCE","description":"A reference to a work by Wang et al. in 2023 related to ITRG","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"LANGCHAIN, 2024","type":"REFERENCE","description":"A reference to the LangChain library in 2024","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"LLAMAINDEX, 2024","type":"REFERENCE","description":"A reference to the LlamaIndex library in 2024","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"NEO4J, 2024","type":"REFERENCE","description":"A reference to the Neo4J graph database in 2024","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"NEBULAGRAPH, 2024","type":"REFERENCE","description":"A reference to the NebulaGraph database in 2024","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"REFERENCE"},{"name":"GRAPH RAG","type":"","description":"","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"NEWS LOW-LEVEL COMMUNITY SUMMARIES","type":"SUMMARY","description":"Summaries of news content at a low level of detail, focused on community aspects","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"TABLE 3","type":"REFERENCE","description":"A reference to a table that illustrates the scalability advantages of Graph RAG compared to source text summarization","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"C3","type":"REFERENCE","description":"A reference to low-level community summaries in the context of Graph RAG","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"C0","type":"REFERENCE","description":"A reference to root-level community summaries in the context of Graph RAG","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"SS","type":"REFERENCE","description":"A reference to global approaches versus na\u00efve RAG in the context of empowerment comparisons","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"TS","type":"REFERENCE","description":"A reference to Graph RAG approaches versus source text summarization in the context of empowerment comparisons","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"AD-HOC LLM","type":"TECHNOLOGY","description":"The use of large language models in an ad-hoc manner to analyze reasoning for empowerment measures","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"ELEMENT EXTRACTION PROMPTS","type":"TECHNIQUE","description":"Prompts used to extract specific elements from text, which can be tuned to retain more details in the Graph RAG index","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"RAPTOR","type":"SYSTEM","description":"A system that generates a hierarchical index of text chunks by clustering the vectors of text embeddings","source_id":"edab4014b8f55e5b25bd7f396314be1f"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"NA&#207;VE RAG\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">Na&#239;ve RAG is a basic retrieval-augmented generation approach that converts documents to text, splits text into chunks, and embeds these chunks into a vector space for retrieval<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Large Language Models (LLMs) are advanced machine learning models trained on vast amounts of text data to understand and generate human language<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"EMPOWERMENT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Empowerment in this context refers to the ability of a system to help users reach an informed understanding by providing specific examples, quotes, and citations<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"RAG\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Retrieval-Augmented Generation (RAG) is a method that involves retrieving relevant information from external data sources and adding it to the context window of an LLM along with the original query<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"MODULAR RAG\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">Modular RAG systems include patterns for iterative and dynamic cycles of interleaved retrieval and generation to overcome the drawbacks of Na&#239;ve RAG<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"SELF-MEMORY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Self-memory is a concept related to generation-augmented retrieval, facilitating future generation cycles by retaining information from previous interactions<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"GENERATION-AUGMENTED RETRIEVAL\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Generation-Augmented Retrieval (GAR) is a method that combines retrieval and generation processes to improve the quality of responses<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"ITERATIVE RETRIEVAL-GENERATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Iterative Retrieval-Generation (Iter-RetGen) is a strategy that involves multiple cycles of retrieval and generation to refine responses<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"FEDERATED RETRIEVAL-GENERATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Federated Retrieval-Generation (FeB4RAG) is a strategy that involves parallel generation of community answers from summaries<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"MULTI-DOCUMENT SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Multi-Document Summarization is a method that combines information from multiple documents to create a comprehensive summary<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"MULTI-HOP QUESTION ANSWERING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Multi-Hop Question Answering is a method that involves answering questions by retrieving and integrating information from multiple sources<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"HIERARCHICAL INDEX\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Hierarchical Indexing involves generating a hierarchical structure of text chunks by clustering the vectors of text embeddings<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"TREE OF CLARIFICATIONS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Tree of Clarifications is a method for generating multiple interpretations of ambiguous questions and answering them<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"KNOWLEDGE GRAPH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A Knowledge Graph is a structured representation of knowledge in the form of entities and their relationships<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"CAUSAL GRAPHS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Causal Graphs are graphical representations that show causal relationships between variables<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"KAPING\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">An advanced RAG system where the index is a knowledge graphKAPING is an advanced RAG system where the index is a knowledge graph<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"G-RETRIEVER\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">G-Retriever is a system where subsets of the graph structure are the objects of enquiryA system where subsets of the graph structure are the objects of enquiry<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"GRAPH-TOOLFORMER\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system where derived graph metrics are the objects of enquiryGraph-ToolFormer is a system where derived graph metrics are the objects of enquiry<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"SURGE\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system where narrative outputs are strongly grounded in the facts of retrieved subgraphsSURGE is a system where narrative outputs are strongly grounded in the facts of retrieved subgraphs<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"FABULA\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system where retrieved event-plot subgraphs are serialized using narrative templatesFABULA is a system where retrieved event-plot subgraphs are serialized using narrative templates<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"ITRG\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">ITRG is a system that supports both creation and traversal of text-relationship graphs for multi-hop question answeringA system that supports both creation and traversal of text-relationship graphs for multi-hop question answering<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"LANGCHAIN\">      <data key=\"d0\">SOFTWARE<\/data>      <data key=\"d1\">A library that supports various graph databases for graph-based RAG applicationsLangChain is a library that supports various graph databases for graph-based RAG applications<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">SOFTWARE<\/data>    <\/node>    <node id=\"LLAMAINDEX\">      <data key=\"d0\">SOFTWARE<\/data>      <data key=\"d1\">A library that supports various graph databases for graph-based RAG applicationsLlamaIndex is a library that supports various graph databases for graph-based RAG applications<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">SOFTWARE<\/data>    <\/node>    <node id=\"NEO4J\">      <data key=\"d0\">SOFTWARE<\/data>      <data key=\"d1\">Neo4J is a graph database supported by LangChain and LlamaIndex for creating and reasoning over knowledge graphsA graph database supported by LangChain and LlamaIndex for creating and reasoning over knowledge graphs<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">SOFTWARE<\/data>    <\/node>    <node id=\"NEBULAGRAPH\">      <data key=\"d0\">SOFTWARE<\/data>      <data key=\"d1\">A graph database supported by LangChain and LlamaIndex for creating and reasoning over knowledge graphsNebulaGraph is a graph database supported by LangChain and LlamaIndex for creating and reasoning over knowledge graphs<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">SOFTWARE<\/data>    <\/node>    <node id=\"GRAPH RAG INDEX\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A self-generated graph index used in Graph RAG for partitioning data for global summarizationGraph RAG Index is a self-generated graph index used in Graph RAG for partitioning data for global summarization<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"RAM ET AL., 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Ram et al. in 2023 related to RAG approaches<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"GAO ET AL., 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Gao et al. in 2023 related to Na&#239;ve RAG and advanced RAG systems<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"CHENG ET AL., 2024\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Cheng et al. in 2024 related to self-memory in generation-augmented retrieval<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"MAO ET AL., 2020\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Mao et al. in 2020 related to generation-augmented retrieval<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"SHAO ET AL., 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Shao et al. in 2023 related to iterative retrieval-generation strategies<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"WANG ET AL., 2024\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Wang et al. in 2024 related to federated retrieval-generation strategies<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"SU ET AL., 2020\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Su et al. in 2020 related to multi-document summarization<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"FENG ET AL., 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Feng et al. in 2023 related to multi-hop question answering<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"TRIVEDI ET AL., 2022\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Trivedi et al. in 2022 related to multi-hop question answering<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"KHATTAB ET AL., 2022\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Khattab et al. in 2022 related to multi-hop question answering<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"SARTHI ET AL., 2024\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Sarthi et al. in 2024 related to hierarchical indexing<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"KIM ET AL., 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Kim et al. in 2023 related to generating a tree of clarifications<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"TRAJANOSKA ET AL., 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Trajanoska et al. in 2023 related to knowledge graph creation<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"YAO ET AL., 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Yao et al. in 2023 related to knowledge graph completion<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"BAN ET AL., 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Ban et al. in 2023 related to the extraction of causal graphs<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"ZHANG ET AL., 2024\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Zhang et al. in 2024 related to the extraction of causal graphs<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"BAEK ET AL., 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Baek et al. in 2023 related to KAPING<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"HE ET AL., 2024\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by He et al. in 2024 related to G-Retriever<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"ZHANG, 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Zhang in 2023 related to Graph-ToolFormer<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"KANG ET AL., 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Kang et al. in 2023 related to SURGE<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"RANADE AND JOSHI, 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Ranade and Joshi in 2023 related to FABULA<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"WANG ET AL., 2023B\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a work by Wang et al. in 2023 related to ITRG<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"LANGCHAIN, 2024\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to the LangChain library in 2024<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"LLAMAINDEX, 2024\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to the LlamaIndex library in 2024<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"NEO4J, 2024\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to the Neo4J graph database in 2024<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"NEBULAGRAPH, 2024\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to the NebulaGraph database in 2024<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">REFERENCE<\/data>    <\/node>    <node id=\"GRAPH RAG\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"NEWS LOW-LEVEL COMMUNITY SUMMARIES\">      <data key=\"d0\">SUMMARY<\/data>      <data key=\"d1\">Summaries of news content at a low level of detail, focused on community aspects<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"TABLE 3\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to a table that illustrates the scalability advantages of Graph RAG compared to source text summarization<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"C3\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to low-level community summaries in the context of Graph RAG<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"C0\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to root-level community summaries in the context of Graph RAG<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"SS\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to global approaches versus na&#239;ve RAG in the context of empowerment comparisons<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"TS\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">A reference to Graph RAG approaches versus source text summarization in the context of empowerment comparisons<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"AD-HOC LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The use of large language models in an ad-hoc manner to analyze reasoning for empowerment measures<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"ELEMENT EXTRACTION PROMPTS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Prompts used to extract specific elements from text, which can be tuned to retain more details in the Graph RAG index<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"RAPTOR\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system that generates a hierarchical index of text chunks by clustering the vectors of text embeddings<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <edge source=\"NA&#207;VE RAG\" target=\"GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG is an advanced system that overcomes the drawbacks of Na&#239;ve RAG<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"NA&#207;VE RAG\" target=\"LLM\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Na&#239;ve RAG uses LLMs for basic retrieval-augmented generation<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"NA&#207;VE RAG\" target=\"MODULAR RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Modular RAG systems are designed to overcome the drawbacks of Na&#239;ve RAG<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"NA&#207;VE RAG\" target=\"GAO ET AL., 2023\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Gao et al. (2023) contributed to the research on Na&#239;ve RAG<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"LLM\" target=\"GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG uses LLMs for summarizing and retrieving information<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"LLM\" target=\"RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">RAG involves using LLMs to retrieve and generate relevant information<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"RAG\" target=\"RAM ET AL., 2023\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Ram et al. (2023) contributed to the research on RAG approaches<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"MODULAR RAG\" target=\"GAO ET AL., 2023\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Gao et al. (2023) contributed to the research on advanced RAG systems<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"SELF-MEMORY\" target=\"GENERATION-AUGMENTED RETRIEVAL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Self-memory is a concept related to generation-augmented retrieval<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"SELF-MEMORY\" target=\"CHENG ET AL., 2024\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Cheng et al. (2024) contributed to the research on self-memory<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GENERATION-AUGMENTED RETRIEVAL\" target=\"GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG incorporates concepts related to generation-augmented retrieval<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GENERATION-AUGMENTED RETRIEVAL\" target=\"MAO ET AL., 2020\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Mao et al. (2020) contributed to the research on generation-augmented retrieval<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"ITERATIVE RETRIEVAL-GENERATION\" target=\"GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG uses iterative retrieval-generation strategies<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"ITERATIVE RETRIEVAL-GENERATION\" target=\"SHAO ET AL., 2023\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Shao et al. (2023) contributed to the research on iterative retrieval-generation<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"FEDERATED RETRIEVAL-GENERATION\" target=\"GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG uses federated retrieval-generation strategies<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"FEDERATED RETRIEVAL-GENERATION\" target=\"WANG ET AL., 2024\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Wang et al. (2024) contributed to the research on federated retrieval-generation<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"MULTI-DOCUMENT SUMMARIZATION\" target=\"GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG incorporates concepts related to multi-document summarization<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"MULTI-DOCUMENT SUMMARIZATION\" target=\"SU ET AL., 2020\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Su et al. (2020) contributed to the research on multi-document summarization<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"MULTI-HOP QUESTION ANSWERING\" target=\"GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG incorporates concepts related to multi-hop question answering<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"MULTI-HOP QUESTION ANSWERING\" target=\"FENG ET AL., 2023\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Feng et al. (2023) contributed to the research on multi-hop question answering<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"MULTI-HOP QUESTION ANSWERING\" target=\"TRIVEDI ET AL., 2022\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Trivedi et al. (2022) contributed to the research on multi-hop question answering<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"MULTI-HOP QUESTION ANSWERING\" target=\"KHATTAB ET AL., 2022\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Khattab et al. (2022) contributed to the research on multi-hop question answering<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"HIERARCHICAL INDEX\" target=\"GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG uses hierarchical indexing for summarization<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"HIERARCHICAL INDEX\" target=\"SARTHI ET AL., 2024\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Sarthi et al. (2024) contributed to the research on hierarchical indexing<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"TREE OF CLARIFICATIONS\" target=\"GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG's approach bears resemblance to generating a tree of clarifications<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"TREE OF CLARIFICATIONS\" target=\"KIM ET AL., 2023\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Kim et al. (2023) contributed to the research on generating a tree of clarifications<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG uses a self-generated graph index, which is a type of knowledge graph<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"KAPING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">KAPING uses a knowledge graph as its index<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"G-RETRIEVER\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">G-Retriever uses subsets of the graph structure for retrieval<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"GRAPH-TOOLFORMER\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph-ToolFormer uses derived graph metrics for retrieval<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"SURGE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">SURGE grounds narrative outputs in the facts of retrieved subgraphs<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"FABULA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">FABULA serializes retrieved event-plot subgraphs using narrative templates<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"ITRG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">ITRG supports creation and traversal of text-relationship graphs<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"LANGCHAIN\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">LangChain supports various graph databases for knowledge graph creation and reasoning<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"LLAMAINDEX\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">LlamaIndex supports various graph databases for knowledge graph creation and reasoning<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"NEO4J\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Neo4J is a graph database used for knowledge graph creation and reasoning<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"NEBULAGRAPH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">NebulaGraph is a graph database used for knowledge graph creation and reasoning<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"TRAJANOSKA ET AL., 2023\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Trajanoska et al. (2023) contributed to the research on knowledge graph creation<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH\" target=\"YAO ET AL., 2023\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Yao et al. (2023) contributed to the research on knowledge graph completion<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"CAUSAL GRAPHS\" target=\"GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG's approach is related to the extraction of causal graphs<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"CAUSAL GRAPHS\" target=\"BAN ET AL., 2023\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Ban et al. (2023) contributed to the research on the extraction of causal graphs<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"CAUSAL GRAPHS\" target=\"ZHANG ET AL., 2024\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Zhang et al. (2024) contributed to the research on the extraction of causal graphs<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"KAPING\" target=\"BAEK ET AL., 2023\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Baek et al. (2023) contributed to the research on KAPING<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"G-RETRIEVER\" target=\"HE ET AL., 2024\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">He et al. (2024) contributed to the research on G-Retriever<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH-TOOLFORMER\" target=\"ZHANG, 2023\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Zhang (2023) contributed to the research on Graph-ToolFormer<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"SURGE\" target=\"KANG ET AL., 2023\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Kang et al. (2023) contributed to the research on SURGE<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"FABULA\" target=\"RANADE AND JOSHI, 2023\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Ranade and Joshi (2023) contributed to the research on FABULA<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"ITRG\" target=\"WANG ET AL., 2023B\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Wang et al. (2023) contributed to the research on ITRG<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"LANGCHAIN\" target=\"LANGCHAIN, 2024\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">LangChain (2024) is a reference to the LangChain library<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"LLAMAINDEX\" target=\"LLAMAINDEX, 2024\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">LlamaIndex (2024) is a reference to the LlamaIndex library<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG INDEX\" target=\"GRAPH RAG\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG uses a self-generated graph index for partitioning data<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"ac21ebe9a9d70d691c717f961d3f10c8","chunk":"Index, 2024) libraries,\nwhile a more general class of graph-based RAG applications is also emerging, including systems that\ncan create and reason over knowledge graphs in both Neo4J (NaLLM, Neo4J, 2024) and Nebula-\nGraph (GraphRAG, NebulaGraph, 2024) formats. Unlike our Graph RAG approach, however, none\nof these systems use the natural modularity of graphs to partition data for global summarization.\n5 Discussion\nLimitations of evaluation approach . Our evaluation to date has only examined a certain class of\nsensemaking questions for two corpora in the region of 1 million tokens. More work is needed\nto understand how performance varies across different ranges of question types, data types, and\ndataset sizes, as well as to validate our sensemaking questions and target metrics with end users.\nComparison of fabrication rates, e.g., using approaches like SelfCheckGPT (Manakul et al., 2023),\nwould also improve on the current analysis.\nTrade-offs of building a graph index . We consistently observed Graph RAG achieve the best head-\nto-head results against other methods, but in many cases the graph-free approach to global summa-\nrization of source texts performed competitively. The real-world decision about whether to invest in\nbuilding a graph index depends on multiple factors, including the compute budget, expected number\nof lifetime queries per dataset, and value obtained from other aspects of the graph index (including\nthe generic community summaries and the use of other graph-related RAG approaches).\nFuture work . The graph index, rich text annotations, and hierarchical community structure support-\ning the current Graph RAG approach offer many possibilities for refinement and adaptation. This\nincludes RAG approaches that operate in a more local manner, via embedding-based matching of\nuser queries and graph annotations, as well as the possibility of hybrid RAG schemes that combine\nembedding-based matching against community reports before employing our map-reduce summa-\nrization mechanisms. This \u201croll-up\u201d operation could also be extended across more levels of the\ncommunity hierarchy, as well as implemented as a more exploratory \u201cdrill down\u201d mechanism that\nfollows the information scent contained in higher-level community summaries.\n6 Conclusion\nWe have presented a global approach to Graph RAG, combining knowledge graph generation,\nretrieval-augmented generation (RAG), and query-focused summarization (QFS) to support human\nsensemaking over entire text corpora. Initial evaluations show substantial improvements over a\nna\u00a8\u0131ve RAG baseline for both the comprehensiveness and diversity of answers, as well as favorable\ncomparisons to a global but graph-free approach using map-reduce source text summarization. For\nsituations requiring many global queries over the same dataset, summaries of root-level communi-\nties in the entity-based graph index provide a data index that is both superior to na \u00a8\u0131ve RAG and\nachieves competitive performance to other global methods at a fraction of the token cost.\nAn open-source, Python-based implementation of both global and local Graph RAG approaches is\nforthcoming at https:\/\/aka .ms\/graphrag .\n11Acknowledgements\nWe would also like to thank the following people who contributed to the work: Alonso Guevara\nFern \u00b4andez, Amber Hoak, Andr \u00b4es Morales Esquivel, Ben Cutler, Billie Rinaldi, Chris Sanchez,\nChris Trevino, Christine Caggiano, David Tittsworth, Dayenne de Souza, Douglas Orbaker, Ed\nClark, Gabriel Nieves-Ponce, Gaudy Blanco Meneses, Kate Lytvynets, Katy Smith, M \u00b4onica Carva-\njal, Nathan Evans, Richard Ortega, Rodrigo Racanicci, Sarah Smith, and Shane Solomon.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Al-\ntenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv preprint\narXiv:2303.08774 .\nAnil, R., Borgeaud, S., Wu, Y ., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M.,\nHauth, A., et al. (2023). Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805 .\nBaek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for\nzero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136 .\nBan, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing\nlarge language models for advanced causal discovery from data.\nBaumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorpo-\nrating query relevance, multi-document coverage, and summary length constraints into seq2seq\nmodels. arXiv preprint arXiv:1801.07704 .\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\n2008(10):P10008.\nBrown, T., Mann, B., Ryder, N., Subbia","chunk_id":"ac21ebe9a9d70d691c717f961d3f10c8","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"NEO4J","type":"TECHNOLOGY","description":"A graph database management system that is used for creating and managing knowledge graphs","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"TECHNOLOGY"},{"name":"NEBULAGRAPH","type":"TECHNOLOGY","description":"A graph database management system that supports the creation and management of knowledge graphs","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"TECHNOLOGY"},{"name":"GRAPH RAG","type":"SYSTEM","description":"A specific approach to retrieval-augmented generation that uses the natural modularity of graphs to partition data for global summarization","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"SYSTEM"},{"name":"SELFCHECKGPT","type":"TECHNOLOGY","description":"A method or tool used to compare fabrication rates in generated content, as mentioned in the context of improving analysis","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"TECHNOLOGY"},{"name":"GLOBAL SUMMARIZATION","type":"TECHNIQUE","description":"A method for summarizing source texts without using graph structures, mentioned as performing competitively against graph-based methods","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"TECHNIQUE"},{"name":"GRAPH INDEX","type":"TECHNOLOGY","description":"An index built using graph structures to support various retrieval-augmented generation approaches and global summarization","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"TECHNOLOGY"},{"name":"MAP-REDUCE SUMMARIZATION","type":"TECHNIQUE","description":"A summarization method that uses map-reduce operations to summarize source texts, mentioned as a global but graph-free approach","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"TECHNIQUE"},{"name":"QUERY-FOCUSED SUMMARIZATION (QFS)","type":"TECHNIQUE","description":"A summarization method that focuses on generating summaries based on specific user queries","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"TECHNIQUE"},{"name":"SENSEMAKING","type":"PROCESS","description":"The process of understanding and making sense of large text corpora, supported by Graph RAG and other summarization techniques","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PROCESS"},{"name":"ALONSO GUEVARA FERN\u00c1NDEZ","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"AMBER HOAK","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"ANDR\u00c9S MORALES ESQUIVEL","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"BEN CUTLER","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"BILLIE RINALDI","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"CHRIS SANCHEZ","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"CHRIS TREVINO","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"CHRISTINE CAGGIANO","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"DAVID TITTSWORTH","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"DAYENNE DE SOUZA","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"DOUGLAS ORBAKER","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"ED CLARK","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"GABRIEL NIEVES-PONCE","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"GAUDY BLANCO MENESES","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"KATE LYTVYNETS","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"KATY SMITH","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"M\u00d3NICA CARVAJAL","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"NATHAN EVANS","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"RICHARD ORTEGA","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"RODRIGO RACANICCI","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"SARAH SMITH","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"SHANE SOLOMON","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"GPT-4 TECHNICAL REPORT","type":"DOCUMENT","description":"A technical report on GPT-4, as mentioned in the references","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"DOCUMENT"},{"name":"GEMINI","type":"MODEL","description":"A family of highly capable multimodal models, as mentioned in the references","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"MODEL"},{"name":"KNOWLEDGE-AUGMENTED LANGUAGE MODEL PROMPTING","type":"TECHNIQUE","description":"A method for zero-shot knowledge graph question answering, as mentioned in the references","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"TECHNIQUE"},{"name":"LARGE LANGUAGE MODELS","type":"TECHNOLOGY","description":"Advanced machine learning models that are trained on vast amounts of text data to understand and generate human language","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"TECHNOLOGY"},{"name":"CAUSAL DISCOVERY","type":"TECHNIQUE","description":"The process of identifying causal relationships from data, as mentioned in the references","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"TECHNIQUE"},{"name":"QUERY FOCUSED ABSTRACTIVE SUMMARIZATION","type":"TECHNIQUE","description":"A summarization method that incorporates query relevance, multi-document coverage, and summary length constraints into seq2seq models","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"TECHNIQUE"},{"name":"FAST UNFOLDING OF COMMUNITIES","type":"TECHNIQUE","description":"A method for detecting communities in large networks, as mentioned in the references","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"TECHNIQUE"},{"name":"J. ACHIAM","type":"PERSON","description":"An author of the GPT-4 technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"S. ADLER","type":"PERSON","description":"An author of the GPT-4 technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"S. AGARWAL","type":"PERSON","description":"An author of the GPT-4 technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"L. AHMAD","type":"PERSON","description":"An author of the GPT-4 technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"I. AKKAYA","type":"PERSON","description":"An author of the GPT-4 technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"F. L. ALEMAN","type":"PERSON","description":"An author of the GPT-4 technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"D. ALMEIDA","type":"PERSON","description":"An author of the GPT-4 technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"J. ALTENSCHMIDT","type":"PERSON","description":"An author of the GPT-4 technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"S. ALTMAN","type":"PERSON","description":"An author of the GPT-4 technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"S. ANADKAT","type":"PERSON","description":"An author of the GPT-4 technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"R. ANIL","type":"PERSON","description":"An author of the Gemini technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"S. BORGEAUD","type":"PERSON","description":"An author of the Gemini technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"Y. WU","type":"PERSON","description":"An author of the Gemini technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"J.-B. ALAYRAC","type":"PERSON","description":"An author of the Gemini technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"J. YU","type":"PERSON","description":"An author of the Gemini technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"R. SORICUT","type":"PERSON","description":"An author of the Gemini technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"J. SCHALKWYK","type":"PERSON","description":"An author of the Gemini technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"A. M. DAI","type":"PERSON","description":"An author of the Gemini technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"A. HAUTH","type":"PERSON","description":"An author of the Gemini technical report","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"J. BAEK","type":"PERSON","description":"An author of the knowledge-augmented language model prompting paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"A. F. AJI","type":"PERSON","description":"An author of the knowledge-augmented language model prompting paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"A. SAFFARI","type":"PERSON","description":"An author of the knowledge-augmented language model prompting paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"T. BAN","type":"PERSON","description":"An author of the paper on harnessing large language models for advanced causal discovery","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"L. CHEN","type":"PERSON","description":"An author of the paper on harnessing large language models for advanced causal discovery","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"X. WANG","type":"PERSON","description":"An author of the paper on harnessing large language models for advanced causal discovery","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"H. CHEN","type":"PERSON","description":"An author of the paper on harnessing large language models for advanced causal discovery","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"T. BAUMEL","type":"PERSON","description":"An author of the paper on query focused abstractive summarization","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"M. EYAL","type":"PERSON","description":"An author of the paper on query focused abstractive summarization","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"M. ELHADAD","type":"PERSON","description":"An author of the paper on query focused abstractive summarization","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"V. D. BLONDEL","type":"PERSON","description":"An author of the paper on fast unfolding of communities in large networks","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"J.-L. GUILLAUME","type":"PERSON","description":"An author of the paper on fast unfolding of communities in large networks","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"R. LAMBIOTTE","type":"PERSON","description":"An author of the paper on fast unfolding of communities in large networks","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"E. LEFEBVRE","type":"PERSON","description":"An author of the paper on fast unfolding of communities in large networks","source_id":"ac21ebe9a9d70d691c717f961d3f10c8","entity_type":"PERSON"},{"name":"GRAPH-BASED RAG APPLICATIONS","type":"","description":"","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"FABRICATION RATES","type":"","description":"","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"COMMUNITY DETECTION","type":"","description":"","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"GPT-4 TECHNICAL REPORT<**ANALYSIS:**(\"ENTITY\"","type":"","description":"","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"KATE LYTIVYNETS","type":"PERSON","description":"A contributor to the work on Graph RAG","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"NEO4J\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A graph database management system that is used for creating and managing knowledge graphs<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"NEBULAGRAPH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A graph database management system that supports the creation and management of knowledge graphs<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GRAPH RAG\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A specific approach to retrieval-augmented generation that uses the natural modularity of graphs to partition data for global summarization<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"SELFCHECKGPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A method or tool used to compare fabrication rates in generated content, as mentioned in the context of improving analysis<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GLOBAL SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method for summarizing source texts without using graph structures, mentioned as performing competitively against graph-based methods<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"GRAPH INDEX\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">An index built using graph structures to support various retrieval-augmented generation approaches and global summarization<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MAP-REDUCE SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A summarization method that uses map-reduce operations to summarize source texts, mentioned as a global but graph-free approach<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"QUERY-FOCUSED SUMMARIZATION (QFS)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A summarization method that focuses on generating summaries based on specific user queries<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"SENSEMAKING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The process of understanding and making sense of large text corpora, supported by Graph RAG and other summarization techniques<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"ALONSO GUEVARA FERN&#193;NDEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AMBER HOAK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANDR&#201;S MORALES ESQUIVEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BEN CUTLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BILLIE RINALDI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHRIS SANCHEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHRIS TREVINO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHRISTINE CAGGIANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DAVID TITTSWORTH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DAYENNE DE SOUZA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DOUGLAS ORBAKER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ED CLARK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GABRIEL NIEVES-PONCE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GAUDY BLANCO MENESES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KATE LYTVYNETS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"KATY SMITH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"M&#211;NICA CARVAJAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NATHAN EVANS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RICHARD ORTEGA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RODRIGO RACANICCI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SARAH SMITH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHANE SOLOMON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GPT-4 TECHNICAL REPORT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">A technical report on GPT-4, as mentioned in the references<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"GEMINI\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A family of highly capable multimodal models, as mentioned in the references<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"KNOWLEDGE-AUGMENTED LANGUAGE MODEL PROMPTING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method for zero-shot knowledge graph question answering, as mentioned in the references<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"LARGE LANGUAGE MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Advanced machine learning models that are trained on vast amounts of text data to understand and generate human language<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"CAUSAL DISCOVERY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The process of identifying causal relationships from data, as mentioned in the references<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"QUERY FOCUSED ABSTRACTIVE SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A summarization method that incorporates query relevance, multi-document coverage, and summary length constraints into seq2seq models<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"FAST UNFOLDING OF COMMUNITIES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method for detecting communities in large networks, as mentioned in the references<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"J. ACHIAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the GPT-4 technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"S. ADLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the GPT-4 technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"S. AGARWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the GPT-4 technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"L. AHMAD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the GPT-4 technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"I. AKKAYA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the GPT-4 technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"F. L. ALEMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the GPT-4 technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"D. ALMEIDA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the GPT-4 technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"J. ALTENSCHMIDT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the GPT-4 technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"S. ALTMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the GPT-4 technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"S. ANADKAT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the GPT-4 technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"R. ANIL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the Gemini technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"S. BORGEAUD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the Gemini technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"Y. WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the Gemini technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"J.-B. ALAYRAC\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the Gemini technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"J. YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the Gemini technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"R. SORICUT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the Gemini technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"J. SCHALKWYK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the Gemini technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"A. M. DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the Gemini technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"A. HAUTH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the Gemini technical report<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"J. BAEK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the knowledge-augmented language model prompting paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"A. F. AJI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the knowledge-augmented language model prompting paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"A. SAFFARI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the knowledge-augmented language model prompting paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"T. BAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the paper on harnessing large language models for advanced causal discovery<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"L. CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the paper on harnessing large language models for advanced causal discovery<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"X. WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the paper on harnessing large language models for advanced causal discovery<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"H. CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the paper on harnessing large language models for advanced causal discovery<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"T. BAUMEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the paper on query focused abstractive summarization<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"M. EYAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the paper on query focused abstractive summarization<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"M. ELHADAD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the paper on query focused abstractive summarization<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"V. D. BLONDEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the paper on fast unfolding of communities in large networks<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"J.-L. GUILLAUME\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the paper on fast unfolding of communities in large networks<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"R. LAMBIOTTE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the paper on fast unfolding of communities in large networks<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"E. LEFEBVRE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An author of the paper on fast unfolding of communities in large networks<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GRAPH-BASED RAG APPLICATIONS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"FABRICATION RATES\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"COMMUNITY DETECTION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"GPT-4 TECHNICAL REPORT&lt;**ANALYSIS:**(&quot;ENTITY&quot;\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"KATE LYTIVYNETS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A contributor to the work on Graph RAG<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <edge source=\"NEO4J\" target=\"GRAPH-BASED RAG APPLICATIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Neo4J is a format used by graph-based RAG applications for creating and reasoning over knowledge graphs<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"NEBULAGRAPH\" target=\"GRAPH-BASED RAG APPLICATIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">NebulaGraph is a format used by graph-based RAG applications for creating and reasoning over knowledge graphs<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"GRAPH INDEX\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG uses the graph index to partition data for global summarization<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"GLOBAL SUMMARIZATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG achieves competitive results in global summarization compared to graph-free approaches<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"QUERY-FOCUSED SUMMARIZATION (QFS)\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG combines query-focused summarization with knowledge graph generation and retrieval-augmented generation<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"SENSEMAKING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Graph RAG supports human sensemaking over entire text corpora<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"ALONSO GUEVARA FERN&#193;NDEZ\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Alonso Guevara Fern&#225;ndez contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"AMBER HOAK\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Amber Hoak contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"ANDR&#201;S MORALES ESQUIVEL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Andr&#233;s Morales Esquivel contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"BEN CUTLER\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Ben Cutler contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"BILLIE RINALDI\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Billie Rinaldi contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"CHRIS SANCHEZ\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Chris Sanchez contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"CHRIS TREVINO\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Chris Trevino contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"CHRISTINE CAGGIANO\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Christine Caggiano contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"DAVID TITTSWORTH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">David Tittsworth contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"DAYENNE DE SOUZA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Dayenne de Souza contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"DOUGLAS ORBAKER\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Douglas Orbaker contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"ED CLARK\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Ed Clark contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"GABRIEL NIEVES-PONCE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Gabriel Nieves-Ponce contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"GAUDY BLANCO MENESES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Gaudy Blanco Meneses contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"KATE LYTVYNETS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Kate Lytvynets contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"KATY SMITH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Katy Smith contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"M&#211;NICA CARVAJAL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">M&#243;nica Carvajal contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"NATHAN EVANS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Nathan Evans contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"RICHARD ORTEGA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Richard Ortega contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"RODRIGO RACANICCI\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Rodrigo Racanicci contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"SARAH SMITH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Sarah Smith contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"SHANE SOLOMON\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Shane Solomon contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"KATE LYTIVYNETS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Kate Lytvynets contributed to the work on Graph RAG<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"SELFCHECKGPT\" target=\"FABRICATION RATES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">SelfCheckGPT is used to compare fabrication rates in generated content<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GLOBAL SUMMARIZATION\" target=\"MAP-REDUCE SUMMARIZATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Map-reduce summarization is a global but graph-free approach to summarizing source texts<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"QUERY-FOCUSED SUMMARIZATION (QFS)\" target=\"QUERY FOCUSED ABSTRACTIVE SUMMARIZATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Query focused abstractive summarization is a method that incorporates query relevance into summarization<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GPT-4 TECHNICAL REPORT\" target=\"J. ACHIAM\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">J. Achiam is an author of the GPT-4 technical report<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GPT-4 TECHNICAL REPORT\" target=\"S. ADLER\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">S. Adler is an author of the GPT-4 technical report<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GPT-4 TECHNICAL REPORT\" target=\"S. AGARWAL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">S. Agarwal is an author of the GPT-4 technical report<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GPT-4 TECHNICAL REPORT\" target=\"L. AHMAD\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">L. Ahmad is an author of the GPT-4 technical report<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GPT-4 TECHNICAL REPORT\" target=\"I. AKKAYA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">I. Akkaya is an author of the GPT-4 technical report<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GPT-4 TECHNICAL REPORT\" target=\"F. L. ALEMAN\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">F. L. Aleman is an author of the GPT-4 technical report<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GPT-4 TECHNICAL REPORT\" target=\"D. ALMEIDA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">D. Almeida is an author of the GPT-4 technical report<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"KNOWLEDGE-AUGMENTED LANGUAGE MODEL PROMPTING\" target=\"LARGE LANGUAGE MODELS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Large language models are used in knowledge-augmented language model prompting for zero-shot knowledge graph question answering<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"LARGE LANGUAGE MODELS\" target=\"CAUSAL DISCOVERY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Large language models are harnessed for advanced causal discovery from data<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"FAST UNFOLDING OF COMMUNITIES\" target=\"COMMUNITY DETECTION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Fast unfolding of communities is a method for detecting communities in large networks<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"J. ALTENSCHMIDT\" target=\"GPT-4 TECHNICAL REPORT&lt;**ANALYSIS:**(&quot;ENTITY&quot;\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">GRAPH-BASED RAG APPLICATIONS<\/data>      <data key=\"d6\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"aa79049289e6532592eec17b9e76adfb","chunk":" summary length constraints into seq2seq\nmodels. arXiv preprint arXiv:1801.07704 .\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\n2008(10):P10008.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in\nneural information processing systems , 33:1877\u20131901.\nCheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., and Yan, R. (2024). Lift yourself up: Retrieval-\naugmented text generation with self-memory. Advances in Neural Information Processing Sys-\ntems, 36.\nDang, H. T. (2006). Duc 2005: Evaluation of question-focused summarization systems. In Proceed-\nings of the Workshop on Task-Focused Summarization and Question Answering , pages 48\u201355.\nEs, S., James, J., Espinosa-Anke, L., and Schockaert, S. (2023). Ragas: Automated evaluation of\nretrieval augmented generation. arXiv preprint arXiv:2309.15217 .\nFeng, Z., Feng, X., Zhao, D., Yang, M., and Qin, B. (2023). Retrieval-generation synergy augmented\nlarge language models. arXiv preprint arXiv:2310.05149 .\nFortunato, S. (2010). Community detection in graphs. Physics reports , 486(3-5):75\u2013174.\nGao, Y ., Xiong, Y ., Gao, X., Jia, K., Pan, J., Bi, Y ., Dai, Y ., Sun, J., and Wang, H. (2023). Retrieval-\naugmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 .\nGoodwin, T. R., Savery, M. E., and Demner-Fushman, D. (2020). Flight of the pegasus? comparing\ntransformers on few-shot and zero-shot multi-document abstractive summarization. In Proceed-\nings of COLING. International Conference on Computational Linguistics , volume 2020, page\n5640. NIH Public Access.\nHe, X., Tian, Y ., Sun, Y ., Chawla, N. V ., Laurent, T., LeCun, Y ., Bresson, X., and Hooi, B. (2024).\nG-retriever: Retrieval-augmented generation for textual graph understanding and question an-\nswering. arXiv preprint arXiv:2402.07630 .\n12Jacomy, M., Venturini, T., Heymann, S., and Bastian, M. (2014). Forceatlas2, a continuous graph\nlayout algorithm for handy network visualization designed for the gephi software. PLoS ONE\n9(6): e98679. https:\/\/doi.org\/10.1371\/journal.pone.0098679 .\nJin, D., Yu, Z., Jiao, P., Pan, S., He, D., Wu, J., Philip, S. Y ., and Zhang, W. (2021). A survey of\ncommunity detection approaches: From statistical modeling to deep learning. IEEE Transactions\non Knowledge and Data Engineering , 35(2):1149\u20131170.\nKang, M., Kwak, J. M., Baek, J., and Hwang, S. J. (2023). Knowledge graph-augmented language\nmodels for knowledge-grounded dialogue generation. arXiv preprint arXiv:2305.18846 .\nKhattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. (2022).\nDemonstrate-search-predict: Composing retrieval and language models for knowledge-intensive\nnlp. arXiv preprint arXiv:2212.14024 .\nKim, G., Kim, S., Jeon, B., Park, J., and Kang, J. (2023). Tree of clarifications: Answering ambigu-\nous questions with retrieval-augmented large language models. arXiv preprint arXiv:2310.14696 .\nKlein, G., Moon, B., and Hoffman, R. R. (2006a). Making sense of sensemaking 1: Alternative\nperspectives. IEEE intelligent systems , 21(4):70\u201373.\nKlein, G., Moon, B., and Hoffman, R. R. (2006b). Making sense of sensemaking 2: A macrocogni-\ntive model. IEEE Intelligent systems , 21(5):88\u201392.\nKoesten, L., Gregory, K., Groth, P., and Simperl, E. (2021). Talking datasets\u2013understanding data\nsensemaking behaviours. International journal of human-computer studies , 146:102562.\nKuratov, Y ., Bulatov, A., Anokhin, P., Sorokin, D., Sorokin, A., and Burtsev, M. (202","chunk_id":"aa79049289e6532592eec17b9e76adfb","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"FAST UNFOLDING OF COMMUNITIES","type":"METHOD","description":"A method for detecting communities in large networks, described by Blondel, Guillaume, Lambiotte, and Lefebvre in 2008","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"METHOD"},{"name":"LANGUAGE MODELS","type":"TECHNOLOGY","description":"Advanced machine learning models designed to understand and generate human language, as discussed in the paper \"Language models are few-shot learners\" by Brown et al. in 2020","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"TECHNOLOGY"},{"name":"RETRIEVAL-AUGMENTED TEXT GENERATION","type":"TECHNIQUE","description":"A method that combines information retrieval with text generation to improve the quality and relevance of generated content, as described by Cheng et al. in 2024","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"TECHNIQUE"},{"name":"DUC 2005","type":"EVALUATION","description":"An evaluation of question-focused summarization systems conducted by H. T. Dang in 2006","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"EVALUATION"},{"name":"RAGAS","type":"SYSTEM","description":"Automated evaluation system for retrieval-augmented generation, as described by Es, James, Espinosa-Anke, and Schockaert in 2023","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"SYSTEM"},{"name":"RETRIEVAL-GENERATION SYNERGY","type":"TECHNIQUE","description":"A method that enhances large language models by combining retrieval and generation processes, as described by Feng et al. in 2023","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"TECHNIQUE"},{"name":"COMMUNITY DETECTION","type":"RESEARCH TOPIC","description":"The process of identifying groups of related nodes within a graph, as discussed by Fortunato in 2010","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"RESEARCH TOPIC"},{"name":"RETRIEVAL-AUGMENTED GENERATION","type":"TECHNIQUE","description":"A method that combines information retrieval with text generation to improve the quality and relevance of generated content, as surveyed by Gao et al. in 2023","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"TECHNIQUE"},{"name":"PEGASUS","type":"MODEL","description":"A transformer model used for few-shot and zero-shot multi-document abstractive summarization, as compared by Goodwin, Savery, and Demner-Fushman in 2020","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"MODEL"},{"name":"G-RETRIEVER","type":"SYSTEM","description":"A system for retrieval-augmented generation for textual graph understanding and question answering, as described by He et al. in 2024","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"SYSTEM"},{"name":"FORCEATLAS2","type":"ALGORITHM","description":"A continuous graph layout algorithm designed for network visualization, as described by Jacomy, Venturini, Heymann, and Bastian in 2014","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"ALGORITHM"},{"name":"COMMUNITY DETECTION APPROACHES","type":"RESEARCH TOPIC","description":"Various methods for identifying communities within graphs, ranging from statistical modeling to deep learning, as surveyed by Jin et al. in 2021","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"RESEARCH TOPIC"},{"name":"KNOWLEDGE GRAPH-AUGMENTED LANGUAGE MODELS","type":"TECHNOLOGY","description":"Language models enhanced with knowledge graphs for knowledge-grounded dialogue generation, as described by Kang et al. in 2023","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"TECHNOLOGY"},{"name":"DEMONSTRATE-SEARCH-PREDICT","type":"TECHNIQUE","description":"A method that combines retrieval and language models for knowledge-intensive NLP tasks, as described by Khattab et al. in 2022","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"TECHNIQUE"},{"name":"TREE OF CLARIFICATIONS","type":"SYSTEM","description":"A system for answering ambiguous questions using retrieval-augmented large language models, as described by Kim et al. in 2023","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"SYSTEM"},{"name":"SENSEMAKING","type":"RESEARCH TOPIC","description":"The process of understanding and making sense of complex information, as discussed by Klein, Moon, and Hoffman in 2006","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"RESEARCH TOPIC"},{"name":"DATA SENSEMAKING","type":"RESEARCH TOPIC","description":"The behaviors and processes involved in understanding and interpreting data, as discussed by Koesten et al. in 2021\nThe behaviors and processes involved in understanding and interpreting data, as discussed by Koesten et al. in 2021)(\"relationship\"","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"RESEARCH TOPIC"},{"name":"FEW-SHOT LEARNING","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SELF-MEMORY","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LARGE LANGUAGE MODELS","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"GRAPH VISUALIZATION","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KNOWLEDGE-GROUNDED DIALOGUE GENERATION","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KNOWLEDGE-INTENSIVE NLP","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RETRIEVAL-AUGMENTED LARGE LANGUAGE MODELS","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ALTERNATIVE PERSPECTIVES","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SENSEMAKING BEHAVIORS","type":"","description":"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"BLONDEL, V. D.","type":"PERSON","description":"V. D. Blondel is an author who contributed to the research on fast unfolding of communities in large networks","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"GUILLAUME, J.-L.","type":"PERSON","description":"J.-L. Guillaume is an author who contributed to the research on fast unfolding of communities in large networks","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LAMBIOTTE, R.","type":"PERSON","description":"R. Lambiotte is an author who contributed to the research on fast unfolding of communities in large networks","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LEFEBVRE, E.","type":"PERSON","description":"E. Lefebvre is an author who contributed to the research on fast unfolding of communities in large networks","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"BROWN, T.","type":"PERSON","description":"T. Brown is an author who contributed to the research on language models as few-shot learners","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"MANN, B.","type":"PERSON","description":"B. Mann is an author who contributed to the research on language models as few-shot learners","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RYDER, N.","type":"PERSON","description":"N. Ryder is an author who contributed to the research on language models as few-shot learners","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SUBBIAH, M.","type":"PERSON","description":"M. Subbiah is an author who contributed to the research on language models as few-shot learners","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KAPLAN, J. D.","type":"PERSON","description":"J. D. Kaplan is an author who contributed to the research on language models as few-shot learners","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DHARIWAL, P.","type":"PERSON","description":"P. Dhariwal is an author who contributed to the research on language models as few-shot learners","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"NEELAKANTAN, A.","type":"PERSON","description":"A. Neelakantan is an author who contributed to the research on language models as few-shot learners","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SHYAM, P.","type":"PERSON","description":"P. Shyam is an author who contributed to the research on language models as few-shot learners","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SASTRY, G.","type":"PERSON","description":"G. Sastry is an author who contributed to the research on language models as few-shot learners","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ASKELL, A.","type":"PERSON","description":"A. Askell is an author who contributed to the research on language models as few-shot learners","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"CHENG, X.","type":"PERSON","description":"X. Cheng is an author who contributed to the research on retrieval-augmented text generation with self-memory","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LUO, D.","type":"PERSON","description":"D. Luo is an author who contributed to the research on retrieval-augmented text generation with self-memory","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"CHEN, X.","type":"PERSON","description":"X. Chen is an author who contributed to the research on retrieval-augmented text generation with self-memory","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LIU, L.","type":"PERSON","description":"L. Liu is an author who contributed to the research on retrieval-augmented text generation with self-memory","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ZHAO, D.","type":"PERSON","description":"D. Zhao is an author who contributed to the research on retrieval-augmented text generation with self-memory","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"YAN, R.","type":"PERSON","description":"R. Yan is an author who contributed to the research on retrieval-augmented text generation with self-memory","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ES, S.","type":"PERSON","description":"S. Es is an author who contributed to the research on automated evaluation of retrieval-augmented generation","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JAMES, J.","type":"PERSON","description":"J. James is an author who contributed to the research on automated evaluation of retrieval-augmented generation","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ESPINOSA-ANKE, L.","type":"PERSON","description":"L. Espinosa-Anke is an author who contributed to the research on automated evaluation of retrieval-augmented generation","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SCHOCKAERT, S.","type":"PERSON","description":"S. Schockaert is an author who contributed to the research on automated evaluation of retrieval-augmented generation","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FENG, Z.","type":"PERSON","description":"Z. Feng is an author who contributed to the research on retrieval-generation synergy augmented large language models","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FENG, X.","type":"PERSON","description":"X. Feng is an author who contributed to the research on retrieval-generation synergy augmented large language models","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"YANG, M.","type":"PERSON","description":"M. Yang is an author who contributed to the research on retrieval-generation synergy augmented large language models","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"QIN, B.","type":"PERSON","description":"B. Qin is an author who contributed to the research on retrieval-generation synergy augmented large language models","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"GAO, Y.","type":"PERSON","description":"Y. Gao is an author who contributed to the survey on retrieval-augmented generation for large language models","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"XIONG, Y.","type":"PERSON","description":"Y. Xiong is an author who contributed to the survey on retrieval-augmented generation for large language models","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"GAO, X.","type":"PERSON","description":"X. Gao is an author who contributed to the survey on retrieval-augmented generation for large language models","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JIA, K.","type":"PERSON","description":"K. Jia is an author who contributed to the survey on retrieval-augmented generation for large language models","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PAN, J.","type":"PERSON","description":"J. Pan is an author who contributed to the survey on retrieval-augmented generation for large language models","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"BI, Y.","type":"PERSON","description":"Y. Bi is an author who contributed to the survey on retrieval-augmented generation for large language models","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DAI, Y.","type":"PERSON","description":"Y. Dai is an author who contributed to the survey on retrieval-augmented generation for large language models","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SUN, J.","type":"PERSON","description":"J. Sun is an author who contributed to the survey on retrieval-augmented generation for large language models","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"WANG, H.","type":"PERSON","description":"H. Wang is an author who contributed to the survey on retrieval-augmented generation for large language models","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"GOODWIN, T. R.","type":"PERSON","description":"T. R. Goodwin is an author who contributed to the research on comparing transformers on few-shot and zero-shot multi-document abstractive summarization","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SAVERY, M. E.","type":"PERSON","description":"M. E. Savery is an author who contributed to the research on comparing transformers on few-shot and zero-shot multi-document abstractive summarization","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DEMNER-FUSHMAN, D.","type":"PERSON","description":"D. Demner-Fushman is an author who contributed to the research on comparing transformers on few-shot and zero-shot multi-document abstractive summarization","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"HE, X.","type":"PERSON","description":"X. He is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"TIAN, Y.","type":"PERSON","description":"Y. Tian is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SUN, Y.","type":"PERSON","description":"Y. Sun is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"CHAWLA, N. V.","type":"PERSON","description":"N. V. Chawla is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LAURENT, T.","type":"PERSON","description":"T. Laurent is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LECUN, Y.","type":"PERSON","description":"Y. LeCun is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"BRESSON, X.","type":"PERSON","description":"X. Bresson is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"HOOI, B.","type":"PERSON","description":"B. Hooi is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JACOMY, M.","type":"PERSON","description":"M. Jacomy is an author who contributed to the research on ForceAtlas2, a continuous graph layout algorithm for network visualization","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"VENTURINI, T.","type":"PERSON","description":"T. Venturini is an author who contributed to the research on ForceAtlas2, a continuous graph layout algorithm for network visualization","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"HEYMANN, S.","type":"PERSON","description":"S. Heymann is an author who contributed to the research on ForceAtlas2, a continuous graph layout algorithm for network visualization","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"BASTIAN, M.","type":"PERSON","description":"M. Bastian is an author who contributed to the research on ForceAtlas2, a continuous graph layout algorithm for network visualization","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JIN, D.","type":"PERSON","description":"D. Jin is an author who contributed to the survey of community detection approaches","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"YU, Z.","type":"PERSON","description":"Z. Yu is an author who contributed to the survey of community detection approaches","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JIAO, P.","type":"PERSON","description":"P. Jiao is an author who contributed to the survey of community detection approaches","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PAN, S.","type":"PERSON","description":"S. Pan is an author who contributed to the survey of community detection approaches","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"HE, D.","type":"PERSON","description":"D. He is an author who contributed to the survey of community detection approaches","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"WU, J.","type":"PERSON","description":"J. Wu is an author who contributed to the survey of community detection approaches","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PHILIP, S. Y.","type":"PERSON","description":"S. Y. Philip is an author who contributed to the survey of community detection approaches","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ZHANG, W.","type":"PERSON","description":"W. Zhang is an author who contributed to the survey of community detection approaches","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KANG, M.","type":"PERSON","description":"M. Kang is an author who contributed to the research on knowledge graph-augmented language models for knowledge-grounded dialogue generation","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KWAK, J. M.","type":"PERSON","description":"J. M. Kwak is an author who contributed to the research on knowledge graph-augmented language models for knowledge-grounded dialogue generation","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"BAEK, J.","type":"PERSON","description":"J. Baek is an author who contributed to the research on knowledge graph-augmented language models for knowledge-grounded dialogue generation","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"HWANG, S. J.","type":"PERSON","description":"S. J. Hwang is an author who contributed to the research on knowledge graph-augmented language models for knowledge-grounded dialogue generation","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KHATTAB, O.","type":"PERSON","description":"O. Khattab is an author who contributed to the research on demonstrate-search-predict for knowledge-intensive NLP","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SANTHANAM, K.","type":"PERSON","description":"K. Santhanam is an author who contributed to the research on demonstrate-search-predict for knowledge-intensive NLP","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LI, X. L.","type":"PERSON","description":"X. L. Li is an author who contributed to the research on demonstrate-search-predict for knowledge-intensive NLP","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"HALL, D.","type":"PERSON","description":"D. Hall is an author who contributed to the research on demonstrate-search-predict for knowledge-intensive NLP","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LIANG, P.","type":"PERSON","description":"P. Liang is an author who contributed to the research on demonstrate-search-predict for knowledge-intensive NLP","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"POTTS, C.","type":"PERSON","description":"C. Potts is an author who contributed to the research on demonstrate-search-predict for knowledge-intensive NLP","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ZAHARIA, M.","type":"PERSON","description":"M. Zaharia is an author who contributed to the research on demonstrate-search-predict for knowledge-intensive NLP","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KIM, G.","type":"PERSON","description":"G. Kim is an author who contributed to the research on tree of clarifications for answering ambiguous questions","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KIM, S.","type":"PERSON","description":"S. Kim is an author who contributed to the research on tree of clarifications for answering ambiguous questions","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JEON, B.","type":"PERSON","description":"B. Jeon is an author who contributed to the research on tree of clarifications for answering ambiguous questions","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PARK, J.","type":"PERSON","description":"J. Park is an author who contributed to the research on tree of clarifications for answering ambiguous questions","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KANG, J.","type":"PERSON","description":"J. Kang is an author who contributed to the research on tree of clarifications for answering ambiguous questions","source_id":"aa79049289e6532592eec17b9e76adfb"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"FAST UNFOLDING OF COMMUNITIES\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">A method for detecting communities in large networks, described by Blondel, Guillaume, Lambiotte, and Lefebvre in 2008<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">METHOD<\/data>    <\/node>    <node id=\"LANGUAGE MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Advanced machine learning models designed to understand and generate human language, as discussed in the paper \"Language models are few-shot learners\" by Brown et al. in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED TEXT GENERATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method that combines information retrieval with text generation to improve the quality and relevance of generated content, as described by Cheng et al. in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"DUC 2005\">      <data key=\"d0\">EVALUATION<\/data>      <data key=\"d1\">An evaluation of question-focused summarization systems conducted by H. T. Dang in 2006<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">EVALUATION<\/data>    <\/node>    <node id=\"RAGAS\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">Automated evaluation system for retrieval-augmented generation, as described by Es, James, Espinosa-Anke, and Schockaert in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"RETRIEVAL-GENERATION SYNERGY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method that enhances large language models by combining retrieval and generation processes, as described by Feng et al. in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"COMMUNITY DETECTION\">      <data key=\"d0\">RESEARCH TOPIC<\/data>      <data key=\"d1\">The process of identifying groups of related nodes within a graph, as discussed by Fortunato in 2010<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">RESEARCH TOPIC<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED GENERATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method that combines information retrieval with text generation to improve the quality and relevance of generated content, as surveyed by Gao et al. in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"PEGASUS\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A transformer model used for few-shot and zero-shot multi-document abstractive summarization, as compared by Goodwin, Savery, and Demner-Fushman in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"G-RETRIEVER\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system for retrieval-augmented generation for textual graph understanding and question answering, as described by He et al. in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"FORCEATLAS2\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">A continuous graph layout algorithm designed for network visualization, as described by Jacomy, Venturini, Heymann, and Bastian in 2014<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">ALGORITHM<\/data>    <\/node>    <node id=\"COMMUNITY DETECTION APPROACHES\">      <data key=\"d0\">RESEARCH TOPIC<\/data>      <data key=\"d1\">Various methods for identifying communities within graphs, ranging from statistical modeling to deep learning, as surveyed by Jin et al. in 2021<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">RESEARCH TOPIC<\/data>    <\/node>    <node id=\"KNOWLEDGE GRAPH-AUGMENTED LANGUAGE MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language models enhanced with knowledge graphs for knowledge-grounded dialogue generation, as described by Kang et al. in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"DEMONSTRATE-SEARCH-PREDICT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method that combines retrieval and language models for knowledge-intensive NLP tasks, as described by Khattab et al. in 2022<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"TREE OF CLARIFICATIONS\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system for answering ambiguous questions using retrieval-augmented large language models, as described by Kim et al. in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"SENSEMAKING\">      <data key=\"d0\">RESEARCH TOPIC<\/data>      <data key=\"d1\">The process of understanding and making sense of complex information, as discussed by Klein, Moon, and Hoffman in 2006<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">RESEARCH TOPIC<\/data>    <\/node>    <node id=\"DATA SENSEMAKING\">      <data key=\"d0\">RESEARCH TOPIC<\/data>      <data key=\"d1\">The behaviors and processes involved in understanding and interpreting data, as discussed by Koesten et al. in 2021The behaviors and processes involved in understanding and interpreting data, as discussed by Koesten et al. in 2021)(\"relationship\"<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">RESEARCH TOPIC<\/data>    <\/node>    <node id=\"FEW-SHOT LEARNING\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SELF-MEMORY\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LARGE LANGUAGE MODELS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"GRAPH VISUALIZATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KNOWLEDGE-GROUNDED DIALOGUE GENERATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KNOWLEDGE-INTENSIVE NLP\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED LARGE LANGUAGE MODELS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ALTERNATIVE PERSPECTIVES\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SENSEMAKING BEHAVIORS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"BLONDEL, V. D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">V. D. Blondel is an author who contributed to the research on fast unfolding of communities in large networks<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"GUILLAUME, J.-L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J.-L. Guillaume is an author who contributed to the research on fast unfolding of communities in large networks<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LAMBIOTTE, R.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">R. Lambiotte is an author who contributed to the research on fast unfolding of communities in large networks<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LEFEBVRE, E.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">E. Lefebvre is an author who contributed to the research on fast unfolding of communities in large networks<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"BROWN, T.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">T. Brown is an author who contributed to the research on language models as few-shot learners<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"MANN, B.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">B. Mann is an author who contributed to the research on language models as few-shot learners<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RYDER, N.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">N. Ryder is an author who contributed to the research on language models as few-shot learners<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SUBBIAH, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. Subbiah is an author who contributed to the research on language models as few-shot learners<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KAPLAN, J. D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. D. Kaplan is an author who contributed to the research on language models as few-shot learners<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DHARIWAL, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Dhariwal is an author who contributed to the research on language models as few-shot learners<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"NEELAKANTAN, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Neelakantan is an author who contributed to the research on language models as few-shot learners<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SHYAM, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Shyam is an author who contributed to the research on language models as few-shot learners<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SASTRY, G.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">G. Sastry is an author who contributed to the research on language models as few-shot learners<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ASKELL, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Askell is an author who contributed to the research on language models as few-shot learners<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"CHENG, X.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">X. Cheng is an author who contributed to the research on retrieval-augmented text generation with self-memory<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LUO, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">D. Luo is an author who contributed to the research on retrieval-augmented text generation with self-memory<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"CHEN, X.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">X. Chen is an author who contributed to the research on retrieval-augmented text generation with self-memory<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LIU, L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">L. Liu is an author who contributed to the research on retrieval-augmented text generation with self-memory<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ZHAO, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">D. Zhao is an author who contributed to the research on retrieval-augmented text generation with self-memory<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"YAN, R.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">R. Yan is an author who contributed to the research on retrieval-augmented text generation with self-memory<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ES, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Es is an author who contributed to the research on automated evaluation of retrieval-augmented generation<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JAMES, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. James is an author who contributed to the research on automated evaluation of retrieval-augmented generation<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ESPINOSA-ANKE, L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">L. Espinosa-Anke is an author who contributed to the research on automated evaluation of retrieval-augmented generation<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SCHOCKAERT, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Schockaert is an author who contributed to the research on automated evaluation of retrieval-augmented generation<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FENG, Z.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Z. Feng is an author who contributed to the research on retrieval-generation synergy augmented large language models<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FENG, X.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">X. Feng is an author who contributed to the research on retrieval-generation synergy augmented large language models<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"YANG, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. Yang is an author who contributed to the research on retrieval-generation synergy augmented large language models<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"QIN, B.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">B. Qin is an author who contributed to the research on retrieval-generation synergy augmented large language models<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"GAO, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Gao is an author who contributed to the survey on retrieval-augmented generation for large language models<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"XIONG, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Xiong is an author who contributed to the survey on retrieval-augmented generation for large language models<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"GAO, X.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">X. Gao is an author who contributed to the survey on retrieval-augmented generation for large language models<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JIA, K.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">K. Jia is an author who contributed to the survey on retrieval-augmented generation for large language models<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PAN, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Pan is an author who contributed to the survey on retrieval-augmented generation for large language models<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"BI, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Bi is an author who contributed to the survey on retrieval-augmented generation for large language models<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DAI, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Dai is an author who contributed to the survey on retrieval-augmented generation for large language models<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SUN, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Sun is an author who contributed to the survey on retrieval-augmented generation for large language models<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"WANG, H.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">H. Wang is an author who contributed to the survey on retrieval-augmented generation for large language models<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"GOODWIN, T. R.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">T. R. Goodwin is an author who contributed to the research on comparing transformers on few-shot and zero-shot multi-document abstractive summarization<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SAVERY, M. E.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. E. Savery is an author who contributed to the research on comparing transformers on few-shot and zero-shot multi-document abstractive summarization<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DEMNER-FUSHMAN, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">D. Demner-Fushman is an author who contributed to the research on comparing transformers on few-shot and zero-shot multi-document abstractive summarization<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"HE, X.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">X. He is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"TIAN, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Tian is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SUN, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Sun is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"CHAWLA, N. V.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">N. V. Chawla is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LAURENT, T.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">T. Laurent is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LECUN, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. LeCun is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"BRESSON, X.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">X. Bresson is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"HOOI, B.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">B. Hooi is an author who contributed to the research on G-Retriever for retrieval-augmented generation for textual graph understanding and question answering<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JACOMY, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. Jacomy is an author who contributed to the research on ForceAtlas2, a continuous graph layout algorithm for network visualization<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"VENTURINI, T.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">T. Venturini is an author who contributed to the research on ForceAtlas2, a continuous graph layout algorithm for network visualization<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"HEYMANN, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Heymann is an author who contributed to the research on ForceAtlas2, a continuous graph layout algorithm for network visualization<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"BASTIAN, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. Bastian is an author who contributed to the research on ForceAtlas2, a continuous graph layout algorithm for network visualization<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JIN, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">D. Jin is an author who contributed to the survey of community detection approaches<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"YU, Z.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Z. Yu is an author who contributed to the survey of community detection approaches<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JIAO, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Jiao is an author who contributed to the survey of community detection approaches<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PAN, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Pan is an author who contributed to the survey of community detection approaches<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"HE, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">D. He is an author who contributed to the survey of community detection approaches<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"WU, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Wu is an author who contributed to the survey of community detection approaches<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PHILIP, S. Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Y. Philip is an author who contributed to the survey of community detection approaches<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ZHANG, W.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">W. Zhang is an author who contributed to the survey of community detection approaches<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KANG, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. Kang is an author who contributed to the research on knowledge graph-augmented language models for knowledge-grounded dialogue generation<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KWAK, J. M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. M. Kwak is an author who contributed to the research on knowledge graph-augmented language models for knowledge-grounded dialogue generation<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"BAEK, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Baek is an author who contributed to the research on knowledge graph-augmented language models for knowledge-grounded dialogue generation<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"HWANG, S. J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. J. Hwang is an author who contributed to the research on knowledge graph-augmented language models for knowledge-grounded dialogue generation<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KHATTAB, O.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">O. Khattab is an author who contributed to the research on demonstrate-search-predict for knowledge-intensive NLP<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SANTHANAM, K.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">K. Santhanam is an author who contributed to the research on demonstrate-search-predict for knowledge-intensive NLP<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LI, X. L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">X. L. Li is an author who contributed to the research on demonstrate-search-predict for knowledge-intensive NLP<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"HALL, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">D. Hall is an author who contributed to the research on demonstrate-search-predict for knowledge-intensive NLP<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LIANG, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Liang is an author who contributed to the research on demonstrate-search-predict for knowledge-intensive NLP<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"POTTS, C.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">C. Potts is an author who contributed to the research on demonstrate-search-predict for knowledge-intensive NLP<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ZAHARIA, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. Zaharia is an author who contributed to the research on demonstrate-search-predict for knowledge-intensive NLP<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KIM, G.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">G. Kim is an author who contributed to the research on tree of clarifications for answering ambiguous questions<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KIM, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Kim is an author who contributed to the research on tree of clarifications for answering ambiguous questions<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JEON, B.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">B. Jeon is an author who contributed to the research on tree of clarifications for answering ambiguous questions<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PARK, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Park is an author who contributed to the research on tree of clarifications for answering ambiguous questions<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KANG, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Kang is an author who contributed to the research on tree of clarifications for answering ambiguous questions<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <edge source=\"LANGUAGE MODELS\" target=\"FEW-SHOT LEARNING\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Language models are capable of few-shot learning, as discussed by Brown et al. in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"RETRIEVAL-AUGMENTED TEXT GENERATION\" target=\"SELF-MEMORY\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The retrieval-augmented text generation method incorporates self-memory, as described by Cheng et al. in 2024<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"RAGAS\" target=\"RETRIEVAL-AUGMENTED GENERATION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">RAGAS is an automated evaluation system for retrieval-augmented generation<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"RETRIEVAL-GENERATION SYNERGY\" target=\"LARGE LANGUAGE MODELS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Retrieval-generation synergy is used to enhance large language models<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"COMMUNITY DETECTION\" target=\"COMMUNITY DETECTION APPROACHES\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Community detection approaches are various methods for identifying communities within graphs<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"RETRIEVAL-AUGMENTED GENERATION\" target=\"G-RETRIEVER\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">G-Retriever is a system for retrieval-augmented generation<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"FORCEATLAS2\" target=\"GRAPH VISUALIZATION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">ForceAtlas2 is an algorithm designed for graph visualization<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"KNOWLEDGE GRAPH-AUGMENTED LANGUAGE MODELS\" target=\"KNOWLEDGE-GROUNDED DIALOGUE GENERATION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Knowledge graph-augmented language models are used for knowledge-grounded dialogue generation<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"DEMONSTRATE-SEARCH-PREDICT\" target=\"KNOWLEDGE-INTENSIVE NLP\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Demonstrate-search-predict is a method for knowledge-intensive NLP tasks<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"TREE OF CLARIFICATIONS\" target=\"RETRIEVAL-AUGMENTED LARGE LANGUAGE MODELS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Tree of Clarifications uses retrieval-augmented large language models to answer ambiguous questions<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"SENSEMAKING\" target=\"ALTERNATIVE PERSPECTIVES\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Sensemaking involves understanding complex information from alternative perspectives, as discussed by Klein, Moon, and Hoffman in 2006<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"DATA SENSEMAKING\" target=\"SENSEMAKING BEHAVIORS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Data sensemaking involves understanding and interpreting data through sensemaking behaviors<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"df50c95dff7da074cbb2f68e88686f88","chunk":" , 21(5):88\u201392.\nKoesten, L., Gregory, K., Groth, P., and Simperl, E. (2021). Talking datasets\u2013understanding data\nsensemaking behaviours. International journal of human-computer studies , 146:102562.\nKuratov, Y ., Bulatov, A., Anokhin, P., Sorokin, D., Sorokin, A., and Burtsev, M. (2024). In search\nof needles in a 11m haystack: Recurrent memory finds what llms miss.\nLangChain (2024). Langchain graphs. https:\/\/python .langchain .com\/docs\/use cases\/graph\/.\nLaskar, M. T. R., Hoque, E., and Huang, J. (2020). Query focused abstractive summarization via\nincorporating query relevance and transfer learning with transformer models. In Advances in\nArtificial Intelligence: 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020,\nOttawa, ON, Canada, May 13\u201315, 2020, Proceedings 33 , pages 342\u2013348. Springer.\nLaskar, M. T. R., Hoque, E., and Huang, J. X. (2022). Domain adaptation with pre-trained transform-\ners for query-focused abstractive text summarization. Computational Linguistics , 48(2):279\u2013320.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., K \u00a8uttler, H., Lewis, M., Yih,\nW.-t., Rockt \u00a8aschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp\ntasks. Advances in Neural Information Processing Systems , 33:9459\u20139474.\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. (2023). Lost\nin the middle: How language models use long contexts. arXiv:2307.03172.\nLiu, Y . and Lapata, M. (2019). Hierarchical transformers for multi-document summarization. arXiv\npreprint arXiv:1905.13164 .\nLlamaIndex (2024). LlamaIndex Knowledge Graph Index. https:\/\/docs .llamaindex .ai\/en\/stable\/\nexamples\/index structs\/knowledge graph\/KnowledgeGraphDemo .html.\nManakul, P., Liusie, A., and Gales, M. J. (2023). Selfcheckgpt: Zero-resource black-box hallucina-\ntion detection for generative large language models. arXiv preprint arXiv:2303.08896 .\nMao, Y ., He, P., Liu, X., Shen, Y ., Gao, J., Han, J., and Chen, W. (2020). Generation-augmented\nretrieval for open-domain question answering. arXiv preprint arXiv:2009.08553 .\nMartin, S., Brown, W. M., Klavans, R., and Boyack, K. (2011). Openord: An open-source toolbox\nfor large graph layout. SPIE Conference on Visualization and Data Analysis (VDA) .\nMicrosoft (2023). The impact of large language models on scientific discovery: a preliminary study\nusing gpt-4.\n13NebulaGraph (2024). Nebulagraph launches industry-first graph rag: Retrieval-augmented genera-\ntion with llm based on knowledge graphs. https:\/\/www .nebula-graph .io\/posts\/graph-RAG.\nNeo4J (2024). Project NaLLM. https:\/\/github .com\/neo4j\/NaLLM.\nNewman, M. E. (2006). Modularity and community structure in networks. Proceedings of the\nnational academy of sciences , 103(23):8577\u20138582.\nRam, O., Levine, Y ., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham,\nY . (2023). In-context retrieval-augmented language models. Transactions of the Association for\nComputational Linguistics , 11:1316\u20131331.\nRanade, P. and Joshi, A. (2023). Fabula: Intelligence report generation using retrieval-augmented\nnarrative construction. arXiv preprint arXiv:2310.13848 .\nSarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., and Manning, C. D. (2024). Raptor:\nRecursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059 .\nScott, K. (2024). Behind the Tech. https:\/\/www .microsoft .com\/en-us\/behind-the-tech.\nShao, Z., Gong, Y ., Shen, Y ., Huang, M., Duan, N., and Chen, W. (2023). Enhancing retrieval-\naugmented large language models with iterative retrieval-generation synergy. arXiv preprint\narXiv:2305.15294 .\nSu, D., Xu, Y ., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A ques-\ntion answering and query-focused multi-document summarization","chunk_id":"df50c95dff7da074cbb2f68e88686f88","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"GREGORY, K.","type":"PERSON","description":"K. Gregory is an author who contributed to the research on understanding data sensemaking behaviors","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"GROTH, P.","type":"PERSON","description":"P. Groth is an author who contributed to the research on understanding data sensemaking behaviors","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"SIMPERL, E.","type":"PERSON","description":"E. Simperl is an author who contributed to the research on understanding data sensemaking behaviors","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"DATA SENSEMAKING BEHAVIORS","type":"RESEARCH TOPIC","description":"The study of how individuals interpret and make sense of data, as discussed in the paper by Koesten et al. (2021)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"RESEARCH TOPIC"},{"name":"KURATOV, Y.","type":"PERSON","description":"Y. Kuratov is an author who contributed to the research on recurrent memory in large language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"BULATOV, A.","type":"PERSON","description":"A. Bulatov is an author who contributed to the research on recurrent memory in large language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"ANOKHIN, P.","type":"PERSON","description":"P. Anokhin is an author who contributed to the research on recurrent memory in large language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"SOROKIN, D.","type":"PERSON","description":"D. Sorokin is an author who contributed to the research on recurrent memory in large language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"SOROKIN, A.","type":"PERSON","description":"A. Sorokin is an author who contributed to the research on recurrent memory in large language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"BURTSEV, M.","type":"PERSON","description":"M. Burtsev is an author who contributed to the research on recurrent memory in large language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"RECURRENT MEMORY","type":"TECHNIQUE","description":"A method used in large language models to improve their ability to find relevant information in large datasets, as discussed by Kuratov et al. (2024)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNIQUE"},{"name":"LANGCHAIN","type":"TECHNOLOGY","description":"LangChain is a framework for building applications with large language models, including graph-based use cases","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNOLOGY"},{"name":"LANGCHAIN GRAPHS","type":"TECHNOLOGY","description":"A specific feature of LangChain that involves the use of graphs for various applications","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNOLOGY"},{"name":"LASKAR, M. T. R.","type":"PERSON","description":"M. T. R. Laskar is an author who contributed to the research on query-focused abstractive summarization and domain adaptation with pre-trained transformers","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"HOQUE, E.","type":"PERSON","description":"E. Hoque is an author who contributed to the research on query-focused abstractive summarization and domain adaptation with pre-trained transformers","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"HUANG, J.","type":"PERSON","description":"J. Huang is an author who contributed to the research on query-focused abstractive summarization and domain adaptation with pre-trained transformers","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION","type":"TECHNIQUE","description":"A method for summarizing text by focusing on the relevance of the query, as discussed by Laskar et al. (2020)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNIQUE"},{"name":"DOMAIN ADAPTATION","type":"TECHNIQUE","description":"The process of adapting pre-trained models to new domains, as discussed by Laskar et al. (2022)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNIQUE"},{"name":"TRANSFORMER MODELS","type":"TECHNOLOGY","description":"Advanced machine learning models used for various natural language processing tasks, including summarization and domain adaptation","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNOLOGY"},{"name":"LEWIS, P.","type":"PERSON","description":"P. Lewis is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"PEREZ, E.","type":"PERSON","description":"E. Perez is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"PIKTUS, A.","type":"PERSON","description":"A. Piktus is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"PETRONI, F.","type":"PERSON","description":"F. Petroni is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"KARPUKHIN, V.","type":"PERSON","description":"V. Karpukhin is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"GOYAL, N.","type":"PERSON","description":"N. Goyal is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"K\u00dcTTLER, H.","type":"PERSON","description":"H. K\u00fcttler is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"LEWIS, M.","type":"PERSON","description":"M. Lewis is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"YIH, W.-T.","type":"PERSON","description":"W.-T. Yih is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"ROCKT\u00c4SCHEL, T.","type":"PERSON","description":"T. Rockt\u00e4schel is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"RETRIEVAL-AUGMENTED GENERATION","type":"TECHNIQUE","description":"A method that combines information retrieval with text generation to improve the quality and relevance of generated content, as discussed by Lewis et al. (2020)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNIQUE"},{"name":"LIU, N. F.","type":"PERSON","description":"N. F. Liu is an author who contributed to the research on how language models use long contexts","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"LIN, K.","type":"PERSON","description":"K. Lin is an author who contributed to the research on how language models use long contexts","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"HEWITT, J.","type":"PERSON","description":"J. Hewitt is an author who contributed to the research on how language models use long contexts","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"PARANJAPE, A.","type":"PERSON","description":"A. Paranjape is an author who contributed to the research on how language models use long contexts","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"BEVILACQUA, M.","type":"PERSON","description":"M. Bevilacqua is an author who contributed to the research on how language models use long contexts","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"LIANG, P.","type":"PERSON","description":"P. Liang is an author who contributed to the research on how language models use long contexts","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"LONG CONTEXTS","type":"RESEARCH TOPIC","description":"The study of how language models handle and utilize long sequences of text, as discussed by Liu et al. (2023)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"RESEARCH TOPIC"},{"name":"LLAMAINDEX","type":"TECHNOLOGY","description":"LlamaIndex is a framework for building knowledge graphs and other structured data representations","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNOLOGY"},{"name":"LLAMAINDEX KNOWLEDGE GRAPH INDEX","type":"TECHNOLOGY","description":"A specific feature of LlamaIndex that involves creating and using knowledge graphs","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNOLOGY"},{"name":"MANAKUL, P.","type":"PERSON","description":"P. Manakul is an author who contributed to the research on hallucination detection in generative large language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"LIUSIE, A.","type":"PERSON","description":"A. Liusie is an author who contributed to the research on hallucination detection in generative large language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"GALES, M. J.","type":"PERSON","description":"M. J. Gales is an author who contributed to the research on hallucination detection in generative large language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"SELFCHECKGPT","type":"TECHNOLOGY","description":"A tool for zero-resource black-box hallucination detection in generative large language models, as discussed by Manakul et al. (2023)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNOLOGY"},{"name":"MAO, Y.","type":"PERSON","description":"Y. Mao is an author who contributed to the research on generation-augmented retrieval for open-domain question answering","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"HE, P.","type":"PERSON","description":"P. He is an author who contributed to the research on generation-augmented retrieval for open-domain question answering","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"LIU, X.","type":"PERSON","description":"X. Liu is an author who contributed to the research on generation-augmented retrieval for open-domain question answering","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"SHEN, Y.","type":"PERSON","description":"Y. Shen is an author who contributed to the research on generation-augmented retrieval for open-domain question answering\nY. Shen is an author who contributed to the research on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"GAO, J.","type":"PERSON","description":"J. Gao is an author who contributed to the research on generation-augmented retrieval for open-domain question answering","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"HAN, J.","type":"PERSON","description":"J. Han is an author who contributed to the research on generation-augmented retrieval for open-domain question answering","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"CHEN, W.","type":"PERSON","description":"W. Chen is an author who contributed to the research on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\nW. Chen is an author who contributed to the research on generation-augmented retrieval for open-domain question answering","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"GENERATION-AUGMENTED RETRIEVAL","type":"TECHNIQUE","description":"A method that combines text generation with information retrieval to improve the quality of answers in open-domain question answering, as discussed by Mao et al. (2020)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNIQUE"},{"name":"MARTIN, S.","type":"PERSON","description":"S. Martin is an author who contributed to the development of OpenOrd, an open-source toolbox for large graph layout","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"BROWN, W. M.","type":"PERSON","description":"W. M. Brown is an author who contributed to the development of OpenOrd, an open-source toolbox for large graph layout","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"KLAVANS, R.","type":"PERSON","description":"R. Klavans is an author who contributed to the development of OpenOrd, an open-source toolbox for large graph layout","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"BOYACK, K.","type":"PERSON","description":"K. Boyack is an author who contributed to the development of OpenOrd, an open-source toolbox for large graph layout","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"OPENORD","type":"TECHNOLOGY","description":"An open-source toolbox for large graph layout, as discussed by Martin et al. (2011)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNOLOGY"},{"name":"MICROSOFT","type":"ORGANIZATION","description":"Microsoft is a technology company that conducted a preliminary study on the impact of large language models on scientific discovery using GPT-4","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"ORGANIZATION"},{"name":"GPT-4","type":"TECHNOLOGY","description":"A large language model developed by OpenAI, used in a preliminary study by Microsoft to assess its impact on scientific discovery","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNOLOGY"},{"name":"NEBULAGRAPH","type":"TECHNOLOGY","description":"NebulaGraph is a graph database technology that launched an industry-first graph RAG (retrieval-augmented generation) with LLM based on knowledge graphs","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNOLOGY"},{"name":"NEO4J","type":"TECHNOLOGY","description":"Neo4j is a graph database technology that developed Project NaLLM","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNOLOGY"},{"name":"PROJECT NALLM","type":"TECHNOLOGY","description":"A project by Neo4j involving large language models and graph databases","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNOLOGY"},{"name":"NEWMAN, M. E.","type":"PERSON","description":"M. E. Newman is an author who contributed to the research on modularity and community structure in networks","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"MODULARITY","type":"RESEARCH TOPIC","description":"The study of the degree to which a network can be divided into smaller, tightly-knit communities, as discussed by Newman (2006)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"RESEARCH TOPIC"},{"name":"COMMUNITY STRUCTURE","type":"RESEARCH TOPIC","description":"The study of how networks are organized into communities or clusters, as discussed by Newman (2006)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"RESEARCH TOPIC"},{"name":"RAM, O.","type":"PERSON","description":"O. Ram is an author who contributed to the research on in-context retrieval-augmented language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"LEVINE, Y.","type":"PERSON","description":"Y. Levine is an author who contributed to the research on in-context retrieval-augmented language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"DALMEDIGOS, I.","type":"PERSON","description":"I. Dalmedigos is an author who contributed to the research on in-context retrieval-augmented language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"MUHLGAY, D.","type":"PERSON","description":"D. Muhlgay is an author who contributed to the research on in-context retrieval-augmented language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"SHASHUA, A.","type":"PERSON","description":"A. Shashua is an author who contributed to the research on in-context retrieval-augmented language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"LEYTON-BROWN, K.","type":"PERSON","description":"K. Leyton-Brown is an author who contributed to the research on in-context retrieval-augmented language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"SHOHAM, Y.","type":"PERSON","description":"Y. Shoham is an author who contributed to the research on in-context retrieval-augmented language models","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"IN-CONTEXT RETRIEVAL-AUGMENTED LANGUAGE MODELS","type":"TECHNOLOGY","description":"A method that enhances language models by incorporating retrieval mechanisms within the context of the input, as discussed by Ram et al. (2023)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNOLOGY"},{"name":"RANADE, P.","type":"PERSON","description":"P. Ranade is an author who contributed to the research on intelligence report generation using retrieval-augmented narrative construction","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"JOSHI, A.","type":"PERSON","description":"A. Joshi is an author who contributed to the research on intelligence report generation using retrieval-augmented narrative construction","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"FABULA","type":"TECHNOLOGY","description":"A system for intelligence report generation using retrieval-augmented narrative construction, as discussed by Ranade and Joshi (2023)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNOLOGY"},{"name":"SARTHI, P.","type":"PERSON","description":"P. Sarthi is an author who contributed to the research on recursive abstractive processing for tree-organized retrieval","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"ABDULLAH, S.","type":"PERSON","description":"S. Abdullah is an author who contributed to the research on recursive abstractive processing for tree-organized retrieval","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"TULI, A.","type":"PERSON","description":"A. Tuli is an author who contributed to the research on recursive abstractive processing for tree-organized retrieval","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"KHANNA, S.","type":"PERSON","description":"S. Khanna is an author who contributed to the research on recursive abstractive processing for tree-organized retrieval","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"GOLDIE, A.","type":"PERSON","description":"A. Goldie is an author who contributed to the research on recursive abstractive processing for tree-organized retrieval","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"MANNING, C. D.","type":"PERSON","description":"C. D. Manning is an author who contributed to the research on recursive abstractive processing for tree-organized retrieval","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"RAPTOR","type":"TECHNOLOGY","description":"A system for recursive abstractive processing for tree-organized retrieval, as discussed by Sarthi et al. (2024)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNOLOGY"},{"name":"SCOTT, K.","type":"PERSON","description":"K. Scott is an author who contributed to the \"Behind the Tech\" series by Microsoft","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"BEHIND THE TECH","type":"SERIES","description":"A series by Microsoft that explores the impact of technology and the people behind it","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"SERIES"},{"name":"SHAO, Z.","type":"PERSON","description":"Z. Shao is an author who contributed to the research on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"GONG, Y.","type":"PERSON","description":"Y. Gong is an author who contributed to the research on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"HUANG, M.","type":"PERSON","description":"M. Huang is an author who contributed to the research on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"DUAN, N.","type":"PERSON","description":"N. Duan is an author who contributed to the research on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"ITERATIVE RETRIEVAL-GENERATION SYNERGY","type":"TECHNIQUE","description":"A method that enhances retrieval-augmented large language models by iteratively refining the retrieval and generation processes, as discussed by Shao et al. (2023)","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"TECHNIQUE"},{"name":"SU, D.","type":"PERSON","description":"D. Su is an author who contributed to the research on CAIRE-COVID, a question answering and query-focused multi-document summarization system","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"XU, Y.","type":"PERSON","description":"Y. Xu is an author who contributed to the research on CAIRE-COVID, a question answering and query-focused multi-document summarization system","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"YU, T.","type":"PERSON","description":"T. Yu is an author who contributed to the research on CAIRE-COVID,\nT. Yu is an author who contributed to the research on CAIRE-COVID","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GREGORY, K.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">K. Gregory is an author who contributed to the research on understanding data sensemaking behaviors<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GROTH, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Groth is an author who contributed to the research on understanding data sensemaking behaviors<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SIMPERL, E.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">E. Simperl is an author who contributed to the research on understanding data sensemaking behaviors<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DATA SENSEMAKING BEHAVIORS\">      <data key=\"d0\">RESEARCH TOPIC<\/data>      <data key=\"d1\">The study of how individuals interpret and make sense of data, as discussed in the paper by Koesten et al. (2021)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">RESEARCH TOPIC<\/data>    <\/node>    <node id=\"KURATOV, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Kuratov is an author who contributed to the research on recurrent memory in large language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BULATOV, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Bulatov is an author who contributed to the research on recurrent memory in large language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANOKHIN, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Anokhin is an author who contributed to the research on recurrent memory in large language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SOROKIN, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">D. Sorokin is an author who contributed to the research on recurrent memory in large language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SOROKIN, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Sorokin is an author who contributed to the research on recurrent memory in large language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BURTSEV, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. Burtsev is an author who contributed to the research on recurrent memory in large language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RECURRENT MEMORY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method used in large language models to improve their ability to find relevant information in large datasets, as discussed by Kuratov et al. (2024)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"LANGCHAIN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LangChain is a framework for building applications with large language models, including graph-based use cases<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LANGCHAIN GRAPHS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A specific feature of LangChain that involves the use of graphs for various applications<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LASKAR, M. T. R.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. T. R. Laskar is an author who contributed to the research on query-focused abstractive summarization and domain adaptation with pre-trained transformers<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HOQUE, E.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">E. Hoque is an author who contributed to the research on query-focused abstractive summarization and domain adaptation with pre-trained transformers<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HUANG, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Huang is an author who contributed to the research on query-focused abstractive summarization and domain adaptation with pre-trained transformers<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method for summarizing text by focusing on the relevance of the query, as discussed by Laskar et al. (2020)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"DOMAIN ADAPTATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The process of adapting pre-trained models to new domains, as discussed by Laskar et al. (2022)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"TRANSFORMER MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Advanced machine learning models used for various natural language processing tasks, including summarization and domain adaptation<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LEWIS, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Lewis is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PEREZ, E.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">E. Perez is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PIKTUS, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Piktus is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PETRONI, F.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">F. Petroni is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KARPUKHIN, V.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">V. Karpukhin is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GOYAL, N.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">N. Goyal is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"K&#220;TTLER, H.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">H. K&#252;ttler is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEWIS, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. Lewis is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YIH, W.-T.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">W.-T. Yih is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ROCKT&#196;SCHEL, T.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">T. Rockt&#228;schel is an author who contributed to the research on retrieval-augmented generation for knowledge-intensive NLP tasks<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED GENERATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method that combines information retrieval with text generation to improve the quality and relevance of generated content, as discussed by Lewis et al. (2020)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"LIU, N. F.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">N. F. Liu is an author who contributed to the research on how language models use long contexts<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIN, K.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">K. Lin is an author who contributed to the research on how language models use long contexts<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HEWITT, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Hewitt is an author who contributed to the research on how language models use long contexts<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PARANJAPE, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Paranjape is an author who contributed to the research on how language models use long contexts<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BEVILACQUA, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. Bevilacqua is an author who contributed to the research on how language models use long contexts<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIANG, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Liang is an author who contributed to the research on how language models use long contexts<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LONG CONTEXTS\">      <data key=\"d0\">RESEARCH TOPIC<\/data>      <data key=\"d1\">The study of how language models handle and utilize long sequences of text, as discussed by Liu et al. (2023)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">RESEARCH TOPIC<\/data>    <\/node>    <node id=\"LLAMAINDEX\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LlamaIndex is a framework for building knowledge graphs and other structured data representations<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LLAMAINDEX KNOWLEDGE GRAPH INDEX\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A specific feature of LlamaIndex that involves creating and using knowledge graphs<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MANAKUL, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Manakul is an author who contributed to the research on hallucination detection in generative large language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIUSIE, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Liusie is an author who contributed to the research on hallucination detection in generative large language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GALES, M. J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. J. Gales is an author who contributed to the research on hallucination detection in generative large language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SELFCHECKGPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A tool for zero-resource black-box hallucination detection in generative large language models, as discussed by Manakul et al. (2023)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MAO, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Mao is an author who contributed to the research on generation-augmented retrieval for open-domain question answering<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HE, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. He is an author who contributed to the research on generation-augmented retrieval for open-domain question answering<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIU, X.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">X. Liu is an author who contributed to the research on generation-augmented retrieval for open-domain question answering<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHEN, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Shen is an author who contributed to the research on generation-augmented retrieval for open-domain question answeringY. Shen is an author who contributed to the research on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GAO, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Gao is an author who contributed to the research on generation-augmented retrieval for open-domain question answering<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAN, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Han is an author who contributed to the research on generation-augmented retrieval for open-domain question answering<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHEN, W.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">W. Chen is an author who contributed to the research on enhancing retrieval-augmented large language models with iterative retrieval-generation synergyW. Chen is an author who contributed to the research on generation-augmented retrieval for open-domain question answering<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GENERATION-AUGMENTED RETRIEVAL\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method that combines text generation with information retrieval to improve the quality of answers in open-domain question answering, as discussed by Mao et al. (2020)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"MARTIN, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Martin is an author who contributed to the development of OpenOrd, an open-source toolbox for large graph layout<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BROWN, W. M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">W. M. Brown is an author who contributed to the development of OpenOrd, an open-source toolbox for large graph layout<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KLAVANS, R.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">R. Klavans is an author who contributed to the development of OpenOrd, an open-source toolbox for large graph layout<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BOYACK, K.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">K. Boyack is an author who contributed to the development of OpenOrd, an open-source toolbox for large graph layout<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"OPENORD\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">An open-source toolbox for large graph layout, as discussed by Martin et al. (2011)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MICROSOFT\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Microsoft is a technology company that conducted a preliminary study on the impact of large language models on scientific discovery using GPT-4<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A large language model developed by OpenAI, used in a preliminary study by Microsoft to assess its impact on scientific discovery<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"NEBULAGRAPH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">NebulaGraph is a graph database technology that launched an industry-first graph RAG (retrieval-augmented generation) with LLM based on knowledge graphs<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"NEO4J\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Neo4j is a graph database technology that developed Project NaLLM<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"PROJECT NALLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A project by Neo4j involving large language models and graph databases<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"NEWMAN, M. E.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. E. Newman is an author who contributed to the research on modularity and community structure in networks<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MODULARITY\">      <data key=\"d0\">RESEARCH TOPIC<\/data>      <data key=\"d1\">The study of the degree to which a network can be divided into smaller, tightly-knit communities, as discussed by Newman (2006)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">RESEARCH TOPIC<\/data>    <\/node>    <node id=\"COMMUNITY STRUCTURE\">      <data key=\"d0\">RESEARCH TOPIC<\/data>      <data key=\"d1\">The study of how networks are organized into communities or clusters, as discussed by Newman (2006)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">RESEARCH TOPIC<\/data>    <\/node>    <node id=\"RAM, O.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">O. Ram is an author who contributed to the research on in-context retrieval-augmented language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEVINE, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Levine is an author who contributed to the research on in-context retrieval-augmented language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DALMEDIGOS, I.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">I. Dalmedigos is an author who contributed to the research on in-context retrieval-augmented language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MUHLGAY, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">D. Muhlgay is an author who contributed to the research on in-context retrieval-augmented language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHASHUA, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Shashua is an author who contributed to the research on in-context retrieval-augmented language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEYTON-BROWN, K.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">K. Leyton-Brown is an author who contributed to the research on in-context retrieval-augmented language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHOHAM, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Shoham is an author who contributed to the research on in-context retrieval-augmented language models<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"IN-CONTEXT RETRIEVAL-AUGMENTED LANGUAGE MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A method that enhances language models by incorporating retrieval mechanisms within the context of the input, as discussed by Ram et al. (2023)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"RANADE, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Ranade is an author who contributed to the research on intelligence report generation using retrieval-augmented narrative construction<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOSHI, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Joshi is an author who contributed to the research on intelligence report generation using retrieval-augmented narrative construction<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FABULA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A system for intelligence report generation using retrieval-augmented narrative construction, as discussed by Ranade and Joshi (2023)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"SARTHI, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Sarthi is an author who contributed to the research on recursive abstractive processing for tree-organized retrieval<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ABDULLAH, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Abdullah is an author who contributed to the research on recursive abstractive processing for tree-organized retrieval<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TULI, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Tuli is an author who contributed to the research on recursive abstractive processing for tree-organized retrieval<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KHANNA, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Khanna is an author who contributed to the research on recursive abstractive processing for tree-organized retrieval<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GOLDIE, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Goldie is an author who contributed to the research on recursive abstractive processing for tree-organized retrieval<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MANNING, C. D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">C. D. Manning is an author who contributed to the research on recursive abstractive processing for tree-organized retrieval<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RAPTOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A system for recursive abstractive processing for tree-organized retrieval, as discussed by Sarthi et al. (2024)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"SCOTT, K.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">K. Scott is an author who contributed to the \"Behind the Tech\" series by Microsoft<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BEHIND THE TECH\">      <data key=\"d0\">SERIES<\/data>      <data key=\"d1\">A series by Microsoft that explores the impact of technology and the people behind it<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">SERIES<\/data>    <\/node>    <node id=\"SHAO, Z.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Z. Shao is an author who contributed to the research on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GONG, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Gong is an author who contributed to the research on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HUANG, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. Huang is an author who contributed to the research on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DUAN, N.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">N. Duan is an author who contributed to the research on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ITERATIVE RETRIEVAL-GENERATION SYNERGY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method that enhances retrieval-augmented large language models by iteratively refining the retrieval and generation processes, as discussed by Shao et al. (2023)<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"SU, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">D. Su is an author who contributed to the research on CAIRE-COVID, a question answering and query-focused multi-document summarization system<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XU, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Xu is an author who contributed to the research on CAIRE-COVID, a question answering and query-focused multi-document summarization system<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YU, T.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">T. Yu is an author who contributed to the research on CAIRE-COVID,T. Yu is an author who contributed to the research on CAIRE-COVID<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","chunk":"., Duan, N., and Chen, W. (2023). Enhancing retrieval-\naugmented large language models with iterative retrieval-generation synergy. arXiv preprint\narXiv:2305.15294 .\nSu, D., Xu, Y ., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A ques-\ntion answering and query-focused multi-document summarization system for covid-19 scholarly\ninformation management. arXiv preprint arXiv:2005.03975 .\nTang, Y . and Yang, Y . (2024). MultiHop-RAG: Benchmarking retrieval-augmented generation for\nmulti-hop queries. arXiv preprint arXiv:2401.15391 .\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,\nBhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288 .\nTraag, V . A., Waltman, L., and Van Eck, N. J. (2019). From Louvain to Leiden: guaranteeing\nwell-connected communities. Scientific Reports , 9(1).\nTrajanoska, M., Stojanov, R., and Trajanov, D. (2023). Enhancing knowledge graph construction\nusing large language models. ArXiv , abs\/2305.04676.\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. (2022). Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint\narXiv:2212.10509 .\nWang, J., Liang, Y ., Meng, F., Sun, Z., Shi, H., Li, Z., Xu, J., Qu, J., and Zhou, J. (2023a). Is chatgpt\na good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048 .\nWang, S., Khramtsova, E., Zhuang, S., and Zuccon, G. (2024). Feb4rag: Evaluating federated search\nin the context of retrieval augmented generation. arXiv preprint arXiv:2402.11891 .\nWang, Y ., Lipka, N., Rossi, R. A., Siu, A., Zhang, R., and Derr, T. (2023b). Knowledge graph\nprompting for multi-document question answering.\nXu, Y . and Lapata, M. (2021). Text summarization with latent queries. arXiv preprint\narXiv:2106.00104 .\nYang, Z., Qi, P., Zhang, S., Bengio, Y ., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. (2018).\nHotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) .\nYao, J.-g., Wan, X., and Xiao, J. (2017). Recent advances in document summarization. Knowledge\nand Information Systems , 53:297\u2013336.\n14Yao, L., Peng, J., Mao, C., and Luo, Y . (2023). Exploring large language models for knowledge\ngraph completion.\nZhang, J. (2023). Graph-toolformer: To empower llms with graph reasoning ability via prompt\naugmented by chatgpt. arXiv preprint arXiv:2304.11116 .\nZhang, Y ., Zhang, Y ., Gan, Y ., Yao, L., and Wang, C. (2024). Causal graph discovery with retrieval-\naugmented generation based large language models. arXiv preprint arXiv:2402.15301 .\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,\nE., et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\nInformation Processing Systems , 36.\n15","chunk_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":987,"entities":[{"name":"CHEN, W.","type":"PERSON","description":"W. Chen is an author who contributed to the research on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"RETRIEVAL-AUGMENTED LARGE LANGUAGE MODELS","type":"TECHNOLOGY","description":"Large language models that are enhanced with retrieval-augmented techniques to improve their performance in generating relevant information","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNOLOGY"},{"name":"ITERATIVE RETRIEVAL-GENERATION SYNERGY","type":"TECHNIQUE","description":"A method that involves iterative processes of retrieval and generation to enhance the performance of large language models","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNIQUE"},{"name":"SU, D.","type":"PERSON","description":"D. Su is an author who contributed to the development of Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19 scholarly information management","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"XU, Y.","type":"PERSON","description":"Y. Xu is an author who contributed to the research on text summarization with latent queries\nY. Xu is an author who contributed to the development of Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19 scholarly information management","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"YU, T.","type":"PERSON","description":"T. Yu is an author who contributed to the development of Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19 scholarly information management","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SIDDIQUE, F. B.","type":"PERSON","description":"F. B. Siddique is an author who contributed to the development of Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19 scholarly information management","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BAREZI, E. J.","type":"PERSON","description":"E. J. Barezi is an author who contributed to the development of Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19 scholarly information management","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"FUNG, P.","type":"PERSON","description":"P. Fung is an author who contributed to the development of Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19 scholarly information management","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"CAIRE-COVID","type":"SYSTEM","description":"A question answering and query-focused multi-document summarization system for managing COVID-19 scholarly information","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"SYSTEM"},{"name":"MULTIHOP-RAG","type":"SYSTEM","description":"A system for benchmarking retrieval-augmented generation for multi-hop queries, as described in the arXiv preprint arXiv:2401.15391","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"SYSTEM"},{"name":"TANG, Y.","type":"PERSON","description":"Y. Tang is an author who contributed to the development of MultiHop-RAG","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"YANG, Y.","type":"PERSON","description":"Y. Yang is an author who contributed to the development of MultiHop-RAG","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"LLAMA 2","type":"MODEL","description":"Open foundation and fine-tuned chat models, as described in the arXiv preprint arXiv:2307.09288","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"MODEL"},{"name":"TOUVRON, H.","type":"PERSON","description":"H. Touvron is an author who contributed to the development of Llama 2","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"MARTIN, L.","type":"PERSON","description":"L. Martin is an author who contributed to the development of Llama 2","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"STONE, K.","type":"PERSON","description":"K. Stone is an author who contributed to the development of Llama 2","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ALBERT, P.","type":"PERSON","description":"P. Albert is an author who contributed to the development of Llama 2","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ALMAHAIRI, A.","type":"PERSON","description":"A. Almahairi is an author who contributed to the development of Llama 2","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BABAEI, Y.","type":"PERSON","description":"Y. Babaei is an author who contributed to the development of Llama 2","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BASHLYKOV, N.","type":"PERSON","description":"N. Bashlykov is an author who contributed to the development of Llama 2","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BATRA, S.","type":"PERSON","description":"S. Batra is an author who contributed to the development of Llama 2","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BHARGAVA, P.","type":"PERSON","description":"P. Bhargava is an author who contributed to the development of Llama 2","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BHOSALE, S.","type":"PERSON","description":"S. Bhosale is an author who contributed to the development of Llama 2","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"LOUVAIN TO LEIDEN","type":"METHOD","description":"A method for guaranteeing well-formedness in community detection, as described by Traag, Waltman, and Van Eck in 2019","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"METHOD"},{"name":"TRAAG, V. A.","type":"PERSON","description":"V. A. Traag is an author who contributed to the development of the Louvain to Leiden method","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"WALTMAN, L.","type":"PERSON","description":"L. Waltman is an author who contributed to the development of the Louvain to Leiden method","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"VAN ECK, N. J.","type":"PERSON","description":"N. J. Van Eck is an author who contributed to the development of the Louvain to Leiden method","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SCIENTIFIC REPORTS","type":"PUBLICATION","description":"Scientific Reports is a peer-reviewed open access scientific journal covering all areas of the natural sciences","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PUBLICATION"},{"name":"TRAJANOSKA, M.","type":"PERSON","description":"M. Trajanoska is an author who contributed to the research on enhancing knowledge graph construction using large language models","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"STOJANOV, R.","type":"PERSON","description":"R. Stojanov is an author who contributed to the research on enhancing knowledge graph construction using large language models","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"TRAJANOV, D.","type":"PERSON","description":"D. Trajanov is an author who contributed to the research on enhancing knowledge graph construction using large language models","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"KNOWLEDGE GRAPH CONSTRUCTION","type":"RESEARCH TOPIC","description":"The process of creating a knowledge graph, which is a structured representation of knowledge in the form of entities and their relationships","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"RESEARCH TOPIC"},{"name":"LARGE LANGUAGE MODELS","type":"TECHNOLOGY","description":"Advanced machine learning models that are trained on vast amounts of text data to understand and generate human language","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNOLOGY"},{"name":"ARXIV","type":"PUBLICATION PLATFORM","description":"arXiv is an open-access repository of electronic preprints approved for publication after moderation, but not full peer review","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PUBLICATION PLATFORM"},{"name":"TRIVEDI, H.","type":"PERSON","description":"H. Trivedi is an author who contributed to the research on interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BALASUBRAMANIAN, N.","type":"PERSON","description":"N. Balasubramanian is an author who contributed to the research on interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"KHOT, T.","type":"PERSON","description":"T. Khot is an author who contributed to the research on interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SABHARWAL, A.","type":"PERSON","description":"A. Sabharwal is an author who contributed to the research on interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"RETRIEVAL-AUGMENTED GENERATION","type":"TECHNIQUE","description":"A method that combines information retrieval with text generation to improve the quality and relevance of generated content","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNIQUE"},{"name":"CHAIN-OF-THOUGHT REASONING","type":"TECHNIQUE","description":"A reasoning process that involves breaking down complex problems into a series of simpler, interconnected steps","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNIQUE"},{"name":"WANG, J.","type":"PERSON","description":"J. Wang is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"LIANG, Y.","type":"PERSON","description":"Y. Liang is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"MENG, F.","type":"PERSON","description":"F. Meng is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SUN, Z.","type":"PERSON","description":"Z. Sun is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SHI, H.","type":"PERSON","description":"H. Shi is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"LI, Z.","type":"PERSON","description":"Z. Li is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"XU, J.","type":"PERSON","description":"J. Xu is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"QU, J.","type":"PERSON","description":"J. Qu is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ZHOU, J.","type":"PERSON","description":"J. Zhou is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"CHATGPT","type":"TECHNOLOGY","description":"ChatGPT is a large language model developed by OpenAI, designed to generate human-like text based on the input it receives","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNOLOGY"},{"name":"NATURAL LANGUAGE GENERATION","type":"TECHNOLOGY","description":"The process of generating coherent and contextually relevant text from structured data or other forms of input","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNOLOGY"},{"name":"WANG, S.","type":"PERSON","description":"S. Wang is an author who contributed to the research on evaluating federated search in the context of retrieval augmented generation","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"KHRAMTSOVA, E.","type":"PERSON","description":"E. Khramtsova is an author who contributed to the research on evaluating federated search in the context of retrieval augmented generation","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ZHUANG, S.","type":"PERSON","description":"S. Zhuang is an author who contributed to the research on evaluating federated search in the context of retrieval augmented generation","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ZUCCON, G.","type":"PERSON","description":"G. Zuccon is an author who contributed to the research on evaluating federated search in the context of retrieval augmented generation","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"FEB4RAG","type":"SYSTEM","description":"A system for evaluating federated search in the context of retrieval augmented generation","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"SYSTEM"},{"name":"WANG, Y.","type":"PERSON","description":"Y. Wang is an author who contributed to the research on knowledge graph prompting for multi-document question answering","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"LIPKA, N.","type":"PERSON","description":"N. Lipka is an author who contributed to the research on knowledge graph prompting for multi-document question answering","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ROSSI, R. A.","type":"PERSON","description":"R. A. Rossi is an author who contributed to the research on knowledge graph prompting for multi-document question answering","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SIU, A.","type":"PERSON","description":"A. Siu is an author who contributed to the research on knowledge graph prompting for multi-document question answering","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ZHANG, R.","type":"PERSON","description":"R. Zhang is an author who contributed to the research on knowledge graph prompting for multi-document question answering","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"DERR, T.","type":"PERSON","description":"T. Derr is an author who contributed to the research on knowledge graph prompting for multi-document question answering","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"KNOWLEDGE GRAPH PROMPTING","type":"TECHNIQUE","description":"A method that uses knowledge graphs to enhance the process of multi-document question answering","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNIQUE"},{"name":"LAPATA, M.","type":"PERSON","description":"M. Lapata is an author who contributed to the research on text summarization with latent queries","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"TEXT SUMMARIZATION","type":"TECHNIQUE","description":"The process of creating a concise and coherent summary of a longer text document","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNIQUE"},{"name":"LATENT QUERIES","type":"TECHNIQUE","description":"A method that involves using hidden or implicit queries to improve the process of text summarization","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNIQUE"},{"name":"YANG, Z.","type":"PERSON","description":"Z. Yang is an author who contributed to the development of HotpotQA","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"QI, P.","type":"PERSON","description":"P. Qi is an author who contributed to the development of HotpotQA","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ZHANG, S.","type":"PERSON","description":"S. Zhang is an author who contributed to the development of HotpotQA","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BENGIO, Y.","type":"PERSON","description":"Y. Bengio is an author who contributed to the development of HotpotQA","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"COHEN, W. W.","type":"PERSON","description":"W. W. Cohen is an author who contributed to the development of HotpotQA","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SALAKHUTDINOV, R.","type":"PERSON","description":"R. Salakhutdinov is an author who contributed to the development of HotpotQA","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"MANNING, C. D.","type":"PERSON","description":"C. D. Manning is an author who contributed to the development of HotpotQA","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"HOTPOTQA","type":"DATASET","description":"A dataset for diverse, explainable multi-hop question answering","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"DATASET"},{"name":"EMNLP","type":"CONFERENCE","description":"The Conference on Empirical Methods in Natural Language Processing, where research related to natural language processing is presented","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"CONFERENCE"},{"name":"YAO, J.-G.","type":"PERSON","description":"J.-G. Yao is an author who contributed to the research on recent advances in document summarization","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"WAN, X.","type":"PERSON","description":"X. Wan is an author who contributed to the research on recent advances in document summarization","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"XIAO, J.","type":"PERSON","description":"J. Xiao is an author who contributed to the research on recent advances in document summarization","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"DOCUMENT SUMMARIZATION","type":"TECHNIQUE","description":"The process of creating a concise and coherent summary of a longer text document","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNIQUE"},{"name":"KNOWLEDGE AND INFORMATION SYSTEMS","type":"PUBLICATION","description":"A journal that publishes research on knowledge and information systems","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PUBLICATION"},{"name":"YAO, L.","type":"PERSON","description":"L. Yao is an author who contributed to the research on causal graph discovery with retrieval-augmented generation based large language models\nL. Yao is an author who contributed to the research on exploring large language models for knowledge graph completion","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"PENG, J.","type":"PERSON","description":"J. Peng is an author who contributed to the research on exploring large language models for knowledge graph completion","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"MAO, C.","type":"PERSON","description":"C. Mao is an author who contributed to the research on exploring large language models for knowledge graph completion","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"LUO, Y.","type":"PERSON","description":"Y. Luo is an author who contributed to the research on exploring large language models for knowledge graph completion","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"KNOWLEDGE GRAPH COMPLETION","type":"TECHNIQUE","description":"The process of filling in missing information in a knowledge graph","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNIQUE"},{"name":"ZHANG, J.","type":"PERSON","description":"J. Zhang is an author who contributed to the development of Graph-toolformer","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"GRAPH-TOOLFORMER","type":"SYSTEM","description":"A system designed to empower large language models with graph reasoning ability via prompt augmented by ChatGPT","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"SYSTEM"},{"name":"ZHANG, Y.","type":"PERSON","description":"Y. Zhang is an author who contributed to the research on causal graph discovery with retrieval-augmented generation based large language models","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"GAN, Y.","type":"PERSON","description":"Y. Gan is an author who contributed to the research on causal graph discovery with retrieval-augmented generation based large language models","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"WANG, C.","type":"PERSON","description":"C. Wang is an author who contributed to the research on causal graph discovery with retrieval-augmented generation based large language models","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"CAUSAL GRAPH DISCOVERY","type":"TECHNIQUE","description":"The process of identifying causal relationships between variables using graph-based methods","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNIQUE"},{"name":"ZHENG, L.","type":"PERSON","description":"L. Zheng is an author who contributed to the research on judging large language models as a judge with MT-Bench and Chatbot Arena","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"CHIANG, W.-L.","type":"PERSON","description":"W.-L. Chiang is an author who contributed to the research on judging large language models as a judge with MT-Bench and Chatbot Arena","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SHENG, Y.","type":"PERSON","description":"Y. Sheng is an author who contributed to the research on judging large language models as a judge with MT-Bench and Chatbot Arena","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"CHEN, W.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">W. Chen is an author who contributed to the research on enhancing retrieval-augmented large language models with iterative retrieval-generation synergy<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED LARGE LANGUAGE MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Large language models that are enhanced with retrieval-augmented techniques to improve their performance in generating relevant information<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ITERATIVE RETRIEVAL-GENERATION SYNERGY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method that involves iterative processes of retrieval and generation to enhance the performance of large language models<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"SU, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">D. Su is an author who contributed to the development of Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19 scholarly information management<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XU, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Xu is an author who contributed to the research on text summarization with latent queriesY. Xu is an author who contributed to the development of Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19 scholarly information management<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YU, T.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">T. Yu is an author who contributed to the development of Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19 scholarly information management<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SIDDIQUE, F. B.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">F. B. Siddique is an author who contributed to the development of Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19 scholarly information management<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BAREZI, E. J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">E. J. Barezi is an author who contributed to the development of Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19 scholarly information management<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FUNG, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Fung is an author who contributed to the development of Caire-covid, a question answering and query-focused multi-document summarization system for COVID-19 scholarly information management<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CAIRE-COVID\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A question answering and query-focused multi-document summarization system for managing COVID-19 scholarly information<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"MULTIHOP-RAG\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system for benchmarking retrieval-augmented generation for multi-hop queries, as described in the arXiv preprint arXiv:2401.15391<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"TANG, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Tang is an author who contributed to the development of MultiHop-RAG<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YANG, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Yang is an author who contributed to the development of MultiHop-RAG<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LLAMA 2\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Open foundation and fine-tuned chat models, as described in the arXiv preprint arXiv:2307.09288<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"TOUVRON, H.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">H. Touvron is an author who contributed to the development of Llama 2<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MARTIN, L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">L. Martin is an author who contributed to the development of Llama 2<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"STONE, K.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">K. Stone is an author who contributed to the development of Llama 2<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALBERT, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Albert is an author who contributed to the development of Llama 2<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALMAHAIRI, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Almahairi is an author who contributed to the development of Llama 2<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BABAEI, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Babaei is an author who contributed to the development of Llama 2<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BASHLYKOV, N.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">N. Bashlykov is an author who contributed to the development of Llama 2<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BATRA, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Batra is an author who contributed to the development of Llama 2<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BHARGAVA, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Bhargava is an author who contributed to the development of Llama 2<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BHOSALE, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Bhosale is an author who contributed to the development of Llama 2<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LOUVAIN TO LEIDEN\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">A method for guaranteeing well-formedness in community detection, as described by Traag, Waltman, and Van Eck in 2019<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">METHOD<\/data>    <\/node>    <node id=\"TRAAG, V. A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">V. A. Traag is an author who contributed to the development of the Louvain to Leiden method<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WALTMAN, L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">L. Waltman is an author who contributed to the development of the Louvain to Leiden method<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"VAN ECK, N. J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">N. J. Van Eck is an author who contributed to the development of the Louvain to Leiden method<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SCIENTIFIC REPORTS\">      <data key=\"d0\">PUBLICATION<\/data>      <data key=\"d1\">Scientific Reports is a peer-reviewed open access scientific journal covering all areas of the natural sciences<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PUBLICATION<\/data>    <\/node>    <node id=\"TRAJANOSKA, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. Trajanoska is an author who contributed to the research on enhancing knowledge graph construction using large language models<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"STOJANOV, R.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">R. Stojanov is an author who contributed to the research on enhancing knowledge graph construction using large language models<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TRAJANOV, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">D. Trajanov is an author who contributed to the research on enhancing knowledge graph construction using large language models<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KNOWLEDGE GRAPH CONSTRUCTION\">      <data key=\"d0\">RESEARCH TOPIC<\/data>      <data key=\"d1\">The process of creating a knowledge graph, which is a structured representation of knowledge in the form of entities and their relationships<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">RESEARCH TOPIC<\/data>    <\/node>    <node id=\"LARGE LANGUAGE MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Advanced machine learning models that are trained on vast amounts of text data to understand and generate human language<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ARXIV\">      <data key=\"d0\">PUBLICATION PLATFORM<\/data>      <data key=\"d1\">arXiv is an open-access repository of electronic preprints approved for publication after moderation, but not full peer review<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PUBLICATION PLATFORM<\/data>    <\/node>    <node id=\"TRIVEDI, H.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">H. Trivedi is an author who contributed to the research on interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BALASUBRAMANIAN, N.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">N. Balasubramanian is an author who contributed to the research on interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KHOT, T.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">T. Khot is an author who contributed to the research on interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SABHARWAL, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Sabharwal is an author who contributed to the research on interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED GENERATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method that combines information retrieval with text generation to improve the quality and relevance of generated content<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT REASONING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A reasoning process that involves breaking down complex problems into a series of simpler, interconnected steps<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"WANG, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Wang is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIANG, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Liang is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MENG, F.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">F. Meng is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SUN, Z.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Z. Sun is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHI, H.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">H. Shi is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LI, Z.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Z. Li is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XU, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Xu is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QU, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Qu is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHOU, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Zhou is an author who contributed to the research on evaluating ChatGPT as a natural language generation evaluator<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHATGPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ChatGPT is a large language model developed by OpenAI, designed to generate human-like text based on the input it receives<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"NATURAL LANGUAGE GENERATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The process of generating coherent and contextually relevant text from structured data or other forms of input<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"WANG, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Wang is an author who contributed to the research on evaluating federated search in the context of retrieval augmented generation<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KHRAMTSOVA, E.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">E. Khramtsova is an author who contributed to the research on evaluating federated search in the context of retrieval augmented generation<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHUANG, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Zhuang is an author who contributed to the research on evaluating federated search in the context of retrieval augmented generation<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZUCCON, G.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">G. Zuccon is an author who contributed to the research on evaluating federated search in the context of retrieval augmented generation<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FEB4RAG\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system for evaluating federated search in the context of retrieval augmented generation<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"WANG, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Wang is an author who contributed to the research on knowledge graph prompting for multi-document question answering<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIPKA, N.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">N. Lipka is an author who contributed to the research on knowledge graph prompting for multi-document question answering<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ROSSI, R. A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">R. A. Rossi is an author who contributed to the research on knowledge graph prompting for multi-document question answering<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SIU, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. Siu is an author who contributed to the research on knowledge graph prompting for multi-document question answering<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHANG, R.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">R. Zhang is an author who contributed to the research on knowledge graph prompting for multi-document question answering<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DERR, T.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">T. Derr is an author who contributed to the research on knowledge graph prompting for multi-document question answering<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KNOWLEDGE GRAPH PROMPTING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method that uses knowledge graphs to enhance the process of multi-document question answering<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"LAPATA, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M. Lapata is an author who contributed to the research on text summarization with latent queries<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TEXT SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The process of creating a concise and coherent summary of a longer text document<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"LATENT QUERIES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method that involves using hidden or implicit queries to improve the process of text summarization<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"YANG, Z.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Z. Yang is an author who contributed to the development of HotpotQA<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QI, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">P. Qi is an author who contributed to the development of HotpotQA<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHANG, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S. Zhang is an author who contributed to the development of HotpotQA<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BENGIO, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Bengio is an author who contributed to the development of HotpotQA<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"COHEN, W. W.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">W. W. Cohen is an author who contributed to the development of HotpotQA<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SALAKHUTDINOV, R.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">R. Salakhutdinov is an author who contributed to the development of HotpotQA<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MANNING, C. D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">C. D. Manning is an author who contributed to the development of HotpotQA<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset for diverse, explainable multi-hop question answering<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"EMNLP\">      <data key=\"d0\">CONFERENCE<\/data>      <data key=\"d1\">The Conference on Empirical Methods in Natural Language Processing, where research related to natural language processing is presented<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">CONFERENCE<\/data>    <\/node>    <node id=\"YAO, J.-G.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J.-G. Yao is an author who contributed to the research on recent advances in document summarization<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WAN, X.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">X. Wan is an author who contributed to the research on recent advances in document summarization<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIAO, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Xiao is an author who contributed to the research on recent advances in document summarization<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DOCUMENT SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The process of creating a concise and coherent summary of a longer text document<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"KNOWLEDGE AND INFORMATION SYSTEMS\">      <data key=\"d0\">PUBLICATION<\/data>      <data key=\"d1\">A journal that publishes research on knowledge and information systems<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PUBLICATION<\/data>    <\/node>    <node id=\"YAO, L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">L. Yao is an author who contributed to the research on causal graph discovery with retrieval-augmented generation based large language modelsL. Yao is an author who contributed to the research on exploring large language models for knowledge graph completion<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PENG, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Peng is an author who contributed to the research on exploring large language models for knowledge graph completion<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MAO, C.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">C. Mao is an author who contributed to the research on exploring large language models for knowledge graph completion<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LUO, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Luo is an author who contributed to the research on exploring large language models for knowledge graph completion<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KNOWLEDGE GRAPH COMPLETION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The process of filling in missing information in a knowledge graph<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"ZHANG, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J. Zhang is an author who contributed to the development of Graph-toolformer<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GRAPH-TOOLFORMER\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system designed to empower large language models with graph reasoning ability via prompt augmented by ChatGPT<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"ZHANG, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Zhang is an author who contributed to the research on causal graph discovery with retrieval-augmented generation based large language models<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GAN, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Gan is an author who contributed to the research on causal graph discovery with retrieval-augmented generation based large language models<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WANG, C.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">C. Wang is an author who contributed to the research on causal graph discovery with retrieval-augmented generation based large language models<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CAUSAL GRAPH DISCOVERY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The process of identifying causal relationships between variables using graph-based methods<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"ZHENG, L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">L. Zheng is an author who contributed to the research on judging large language models as a judge with MT-Bench and Chatbot Arena<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHIANG, W.-L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">W.-L. Chiang is an author who contributed to the research on judging large language models as a judge with MT-Bench and Chatbot Arena<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHENG, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Y. Sheng is an author who contributed to the research on judging large language models as a judge with MT-Bench and Chatbot Arena<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"6fe27f9eb76cf2ddf712a2cee5783d1c","chunk":"AgentInstruct:\nToward Generative Teaching with Agentic\nFlows\nArindam Mitra, Luciano Del Corro, Guoqing Zheng, Shweti Mahajan,\nDany Rouhana, Andres Codas, Yadong Lu, Wei-ge Chen, Olga Vrousgos,\nCorby Rosset, Fillipe Silva, Hamed Khanpour, Yash Lara, Ahmed Awadallah\nMicrosoft Research\nAbstract\nSynthetic data is becoming increasingly important for accelerating the development of\nlanguage models, both large and small. Despite several successful use cases, researchers\nalso raised concerns around model collapse and drawbacks of imitating other models. This\ndiscrepancy can be attributed to the fact that synthetic data varies in quality and diversity.\nEffective use of synthetic data usually requires significant human effort in curating the data.\nWe focus on using synthetic data for post-training, specifically creating data by powerful\nmodels to teach a new skill or behavior to another model, we refer to this setting as Generative\nTeaching . We introduce AgentInstruct, an extensible agentic framework for automatically\ncreating large amounts of diverse and high-quality synthetic data. AgentInstruct can create\nboth the prompts and responses, using only raw data sources like text documents and code\nfiles as seeds. We demonstrate the utility of AgentInstruct by creating a post training dataset\nof 25M pairs to teach language models different skills, such as text editing, creative writing,\ntool usage, coding, reading comprehension, etc. The dataset can be used for instruction\ntuning of any base model. We post-train Mistral-7b with the data. When comparing\nthe resulting model (Orca-3) to Mistral-7b-Instruct (which uses the same base model), we\nobserve significant improvements across many benchmarks. For example, 40% improvement\non AGIEval, 19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement\non BBH and 45% improvement on AlpacaEval. Additionally, it consistently outperforms\nother models such as LLAMA-8B-instruct and GPT-3.5-turbo.\n \n 0102030405060AGIEVAL\n010203040506070MMLU\n010203040506070BBH\n0102030405060708090GSM8K\n0510152025ALPACA\nEVAL\n0102030405060708090FOFO\n010203040506070MIRAGE -\nRAG\n+40.2%  +19.4 % \n+38.3 % +53.7%  \n+45.0% +38.3%  \n+46.6% \nMistral -Instruct -7B Mistra l-7B + AgentInstruct (Orca -3) \nFigure 1: Effect of using AgentInstruct data for post-training Mistral-7BarXiv:2407.03502v1  [cs.AI]  3 Jul 20241 Introduction\nSynthetic data accelerated the development of LLMS : The rise of synthetic data in\nthe training of Large Language Models (LLMs) has been a significant development of the\nlast year. Synthetic data was used to significantly accelerate the progress of model training\n(especially SLMs) in all stages of training from pre-training (e.g., [ 1]), to instruction-tuning\n(e.g., [21, 36]) and RLHF(e.g., [12, 28]).\nGenerating high quality synthetic data is hard : On the other hand, research has also\nshown that pre-training models on synthetic data generated by other models can lead to\nmodel collapse [ 29], leading to models gradually degenerating as a result. Similar arguments\nhave been made against using synthetic data for pos-training, which could amount to an\nimitation process that could teach the trained model to pick only stylistic characteristics\nand not real capabilities [ 8]. This discrepancy could be explained by the observation that\ncreating high-quality and diverse synthetic data is hard [ 17]. Successful use of synthetic data\ninvolved significant human effort in curating and filtering the data to ensure high quality. If\nwe focus on post-training synthetic data, we will see the most widely used approach includes\nstarting with a set of prompts and using a powerful model such as GPT-4 [ 22] to generate\nresponses to these prompts [ 24] or of an expanded set of the prompts [ 36]. This recipe\nwas further improved by eliciting explanations or step-by-step instructions from the teacher\nmodel [20] or using more complex prompting techniques to elicit higher quality answers [ 18].\nSynthetic data meets Agents : Another major development we witnessed last year is the\nrise of Agentic (especially multiagent) workflows [ 33,13]. Agentic workflows can generate\nhigh quality data, that surpasses the capabilities of the underlying LLMs, by using flows\nwith reflection and iteration, where agents can look back at solutions, generate critiques and\nimprove solutions. They can also use tools (e.g. search apis, calculator, code interpreters)\naddressing limitations of LLMs. Multi-agent workflows bring in additional benefits such\nas simulating scenarios where we can generate both new prompts and the corresponding\nresponses. They also enable automation of the data generation workflows reducing or\neliminating need for human intervention on some tasks.\nGenerative Teaching & Orca AgentInstruct : Generating synthetic data for post-\ntraining often relies on an existing prompt set that is used as is or used as seeds for\ngenerating more instructions. In this work, we generalize the problem settings to a broader\nobjective of generating abundant amounts of diverse, challenging and high-quality data to\nte","chunk_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"GENERATIVE TEACHING","type":"CONCEPT","description":"Generative Teaching refers to the process of using synthetic data created by powerful models to teach a new skill or behavior to another model.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"CONCEPT"},{"name":"SYNTHETIC DATA","type":"DATA TYPE","description":"Synthetic data is artificially generated data used to accelerate the development of language models. It varies in quality and diversity and often requires significant human effort in curation.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"DATA TYPE"},{"name":"MISTRAL-7B","type":"MODEL","description":"Mistral-7b is a base language model that was post-trained using synthetic data generated by AgentInstruct.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"MODEL"},{"name":"ORCA-3","type":"MODEL","description":"Orca-3 is the resulting model from post-training Mistral-7b with synthetic data generated by AgentInstruct, showing significant improvements across various benchmarks.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"MODEL"},{"name":"MICROSOFT RESEARCH","type":"ORGANIZATION","description":"Microsoft Research is the research division of Microsoft, where the authors of the paper are affiliated.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"ORGANIZATION"},{"name":"AGIEVAL","type":"BENCHMARK","description":"AGIEval is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 40% improvement over Mistral-7b-Instruct.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"BENCHMARK"},{"name":"MMLU","type":"BENCHMARK","description":"MMLU is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 19% improvement over Mistral-7b-Instruct.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"BENCHMARK"},{"name":"GSM8K","type":"BENCHMARK","description":"GSM8K is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 54% improvement over Mistral-7b-Instruct.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"BENCHMARK"},{"name":"BBH","type":"BENCHMARK","description":"BBH is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 38% improvement over Mistral-7b-Instruct.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"BENCHMARK"},{"name":"ALPACAEVAL","type":"BENCHMARK","description":"AlpacaEval is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 45% improvement over Mistral-7b-Instruct.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"BENCHMARK"},{"name":"LLAMA-8B-INSTRUCT","type":"MODEL","description":"LLAMA-8B-Instruct is another language model that Orca-3 consistently outperforms.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"MODEL"},{"name":"GPT-3.5-TURBO","type":"MODEL","description":"GPT-3.5-turbo is another language model that Orca-3 consistently outperforms.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"MODEL"},{"name":"LARGE LANGUAGE MODELS (LLMS)","type":"TECHNOLOGY","description":"Large Language Models (LLMs) are advanced machine learning models trained on vast amounts of text data to understand and generate human language.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"TECHNOLOGY"},{"name":"INSTRUCTION-TUNING","type":"TECHNIQUE","description":"Instruction-tuning is a technique used in the training of language models, involving the use of synthetic data to improve model performance.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"TECHNIQUE"},{"name":"RLHF","type":"TECHNIQUE","description":"Reinforcement Learning from Human Feedback (RLHF) is a technique used in the training of language models, involving the use of human feedback to improve model performance.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"TECHNIQUE"},{"name":"MODEL COLLAPSE","type":"CONCEPT","description":"Model collapse refers to the phenomenon where models gradually degenerate as a result of being trained on synthetic data generated by other models.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"CONCEPT"},{"name":"MULTIAGENT WORKFLOWS","type":"TECHNIQUE","description":"Multiagent workflows involve the use of multiple agents to generate high-quality data through reflection, iteration, and the use of tools, surpassing the capabilities of the underlying LLMs.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"TECHNIQUE"},{"name":"PROMPTS","type":"DATA TYPE","description":"Prompts are initial inputs used to generate responses in the process of creating synthetic data for training language models.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"DATA TYPE"},{"name":"RESPONSES","type":"DATA TYPE","description":"Responses are the outputs generated in response to prompts in the process of creating synthetic data for training language models.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"DATA TYPE"},{"name":"TOOLS","type":"TECHNOLOGY","description":"Tools such as search APIs, calculators, and code interpreters are used in multiagent workflows to address limitations of LLMs and improve data generation.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"TECHNOLOGY"},{"name":"POST-TRAINING","type":"TECHNIQUE","description":"Post-training is a stage in the training of language models where additional data is used to teach new skills or behaviors to an already trained model.)(\"relationship\"\nPost-training is a stage in the training of language models where additional data is used to teach new skills or behaviors to an already trained model.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","entity_type":"TECHNIQUE"},{"name":"AGENTINSTRUCT","type":"","description":"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"TEXT EDITING","type":"SKILL","description":"Text editing is one of the skills that the dataset created by AgentInstruct aims to teach language models.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"CREATIVE WRITING","type":"SKILL","description":"Creative writing is one of the skills that the dataset created by AgentInstruct aims to teach language models.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"TOOL USAGE","type":"SKILL","description":"Tool usage is one of the skills that the dataset created by AgentInstruct aims to teach language models.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"CODING","type":"SKILL","description":"Coding is one of the skills that the dataset created by AgentInstruct aims to teach language models.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"READING COMPREHENSION","type":"SKILL","description":"Reading comprehension is one of the skills that the dataset created by AgentInstruct aims to teach language models.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is a powerful model often used to generate responses to prompts in the process of creating synthetic data for training language models.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"SEARCH APIS","type":"TECHNOLOGY","description":"Search APIs are tools used in multiagent workflows to enhance the data generation process by providing access to external information.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"CALCULATORS","type":"TECHNOLOGY","description":"Calculators are tools used in multiagent workflows to enhance the data generation process by performing mathematical computations.","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"CODE INTERPRETERS","type":"TECHNOLOGY","description":"Code interpreters are tools used in multiagent workflows to enhance the data generation process by executing and interpreting code.)(\"relationship\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GENERATIVE TEACHING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Generative Teaching refers to the process of using synthetic data created by powerful models to teach a new skill or behavior to another model.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SYNTHETIC DATA\">      <data key=\"d0\">DATA TYPE<\/data>      <data key=\"d1\">Synthetic data is artificially generated data used to accelerate the development of language models. It varies in quality and diversity and often requires significant human effort in curation.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">DATA TYPE<\/data>    <\/node>    <node id=\"MISTRAL-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-7b is a base language model that was post-trained using synthetic data generated by AgentInstruct.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 is the resulting model from post-training Mistral-7b with synthetic data generated by AgentInstruct, showing significant improvements across various benchmarks.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"MICROSOFT RESEARCH\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Microsoft Research is the research division of Microsoft, where the authors of the paper are affiliated.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"AGIEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AGIEval is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 40% improvement over Mistral-7b-Instruct.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MMLU is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 19% improvement over Mistral-7b-Instruct.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">GSM8K is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 54% improvement over Mistral-7b-Instruct.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"BBH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">BBH is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 38% improvement over Mistral-7b-Instruct.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"ALPACAEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AlpacaEval is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 45% improvement over Mistral-7b-Instruct.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"LLAMA-8B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">LLAMA-8B-Instruct is another language model that Orca-3 consistently outperforms.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5-turbo is another language model that Orca-3 consistently outperforms.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"LARGE LANGUAGE MODELS (LLMS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Large Language Models (LLMs) are advanced machine learning models trained on vast amounts of text data to understand and generate human language.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"INSTRUCTION-TUNING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Instruction-tuning is a technique used in the training of language models, involving the use of synthetic data to improve model performance.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"RLHF\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Reinforcement Learning from Human Feedback (RLHF) is a technique used in the training of language models, involving the use of human feedback to improve model performance.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"MODEL COLLAPSE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Model collapse refers to the phenomenon where models gradually degenerate as a result of being trained on synthetic data generated by other models.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"MULTIAGENT WORKFLOWS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Multiagent workflows involve the use of multiple agents to generate high-quality data through reflection, iteration, and the use of tools, surpassing the capabilities of the underlying LLMs.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"PROMPTS\">      <data key=\"d0\">DATA TYPE<\/data>      <data key=\"d1\">Prompts are initial inputs used to generate responses in the process of creating synthetic data for training language models.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">DATA TYPE<\/data>    <\/node>    <node id=\"RESPONSES\">      <data key=\"d0\">DATA TYPE<\/data>      <data key=\"d1\">Responses are the outputs generated in response to prompts in the process of creating synthetic data for training language models.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">DATA TYPE<\/data>    <\/node>    <node id=\"TOOLS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Tools such as search APIs, calculators, and code interpreters are used in multiagent workflows to address limitations of LLMs and improve data generation.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"POST-TRAINING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Post-training is a stage in the training of language models where additional data is used to teach new skills or behaviors to an already trained model.)(\"relationship\"Post-training is a stage in the training of language models where additional data is used to teach new skills or behaviors to an already trained model.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"TEXT EDITING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Text editing is one of the skills that the dataset created by AgentInstruct aims to teach language models.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"CREATIVE WRITING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Creative writing is one of the skills that the dataset created by AgentInstruct aims to teach language models.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"TOOL USAGE\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Tool usage is one of the skills that the dataset created by AgentInstruct aims to teach language models.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"CODING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Coding is one of the skills that the dataset created by AgentInstruct aims to teach language models.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"READING COMPREHENSION\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Reading comprehension is one of the skills that the dataset created by AgentInstruct aims to teach language models.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is a powerful model often used to generate responses to prompts in the process of creating synthetic data for training language models.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"SEARCH APIS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Search APIs are tools used in multiagent workflows to enhance the data generation process by providing access to external information.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"CALCULATORS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Calculators are tools used in multiagent workflows to enhance the data generation process by performing mathematical computations.<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"CODE INTERPRETERS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Code interpreters are tools used in multiagent workflows to enhance the data generation process by executing and interpreting code.)(\"relationship\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <edge source=\"SYNTHETIC DATA\" target=\"AGENTINSTRUCT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct generates synthetic data for post-training language models.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"SYNTHETIC DATA\" target=\"LARGE LANGUAGE MODELS (LLMS)\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Synthetic data is used to accelerate the development of Large Language Models (LLMs).<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"SYNTHETIC DATA\" target=\"INSTRUCTION-TUNING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Synthetic data is used in instruction-tuning to improve the performance of language models.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"SYNTHETIC DATA\" target=\"RLHF\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Synthetic data is used in Reinforcement Learning from Human Feedback (RLHF) to improve the performance of language models.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"SYNTHETIC DATA\" target=\"MODEL COLLAPSE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Model collapse can occur when models are pre-trained on synthetic data generated by other models.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"SYNTHETIC DATA\" target=\"POST-TRAINING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Synthetic data is used in post-training to teach new skills or behaviors to an already trained model.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"SYNTHETIC DATA\" target=\"GPT-4\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">GPT-4 is often used to generate responses to prompts in the process of creating synthetic data.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"MISTRAL-7B\" target=\"AGENTINSTRUCT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct was used to generate synthetic data for post-training Mistral-7b.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"MISTRAL-7B\" target=\"ORCA-3\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 is the result of post-training Mistral-7b with synthetic data generated by AgentInstruct.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AGIEVAL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 showed a 40% improvement over Mistral-7b-Instruct on the AGIEval benchmark.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MMLU\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 showed a 19% improvement over Mistral-7b-Instruct on the MMLU benchmark.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GSM8K\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 showed a 54% improvement over Mistral-7b-Instruct on the GSM8K benchmark.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"BBH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 showed a 38% improvement over Mistral-7b-Instruct on the BBH benchmark.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ALPACAEVAL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 showed a 45% improvement over Mistral-7b-Instruct on the AlpacaEval benchmark.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"LLAMA-8B-INSTRUCT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 consistently outperforms LLAMA-8B-Instruct.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GPT-3.5-TURBO\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 consistently outperforms GPT-3.5-turbo.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"MULTIAGENT WORKFLOWS\" target=\"AGENTINSTRUCT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct utilizes multiagent workflows to generate high-quality synthetic data.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"MULTIAGENT WORKFLOWS\" target=\"TOOLS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Tools are used in multiagent workflows to address limitations of LLMs and improve data generation.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"MULTIAGENT WORKFLOWS\" target=\"SEARCH APIS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Search APIs are used in multiagent workflows to enhance the data generation process.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"MULTIAGENT WORKFLOWS\" target=\"CALCULATORS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Calculators are used in multiagent workflows to enhance the data generation process.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"MULTIAGENT WORKFLOWS\" target=\"CODE INTERPRETERS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Code interpreters are used in multiagent workflows to enhance the data generation process.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"PROMPTS\" target=\"RESPONSES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Prompts are used to generate responses in the process of creating synthetic data.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"TEXT EDITING\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct generates synthetic data to teach language models the skill of text editing.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"CREATIVE WRITING\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct generates synthetic data to teach language models the skill of creative writing.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"TOOL USAGE\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct generates synthetic data to teach language models the skill of tool usage.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"CODING\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct generates synthetic data to teach language models the skill of coding.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"READING COMPREHENSION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct generates synthetic data to teach language models the skill of reading comprehension.<\/data>      <data key=\"d6\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"b88745a13b69cecbc0ee9c3af41389bf","chunk":" both new prompts and the corresponding\nresponses. They also enable automation of the data generation workflows reducing or\neliminating need for human intervention on some tasks.\nGenerative Teaching & Orca AgentInstruct : Generating synthetic data for post-\ntraining often relies on an existing prompt set that is used as is or used as seeds for\ngenerating more instructions. In this work, we generalize the problem settings to a broader\nobjective of generating abundant amounts of diverse, challenging and high-quality data to\nteach a particular skill to an AI model, we refer to this setting as Generative Teaching.\nAgentInstruct is an agentic solution for Generative Teaching. AgentInstruct focuses on\ncreating demonstration and feedback data and requires only raw documents as input. When\ngeneric data is used as seeds, AgentInstruct can be used to teach an LLM a general capability\n(e.g. Math, Reasoning, RAG, etc.). Domain specific data (e.g. gaming, finance) can also be\nused as seeds to improve the model in a certain specialization. AgentInstruct can create:\n1.High-quality data: using powerful models like GPT-4, coupled with tools like search\nand code interpreters.\n2.Diverse data: AgentInstruct generates both prompts and responses. It uses a large\nnumber of agents (equipped with powerful LLMs, tools and reflection flows) and a\ntaxonomy (of over 100 subcategories) to create diverse and high quality prompts\nand responses,\n3.Large quantities of data: AgentInstruct can run autonomously and can apply flows\nfor verification and data filtering. It does not require seed prompts and uses raw\ndocuments for seeding.\nUsing raw data (unstructured text documents or source code) as seeds has two benefits.\nFirst, this data is available in abundance enabling the use of AgentInstruct to create large\namounts of diverse data. Additionally, using raw data as seeds, and hence, avoiding using\nexisting prompts, as is or after paraphrasing, can promote learning more general capabilities\nas opposed to benchmark-specific ones.\nWe demonstrate the utility of AgentInstruct by creating a comprehensive synthetic post-\ntraining dataset of 25 million prompt and response pairs. The dataset covers a wide array\n2of skills including creative writing, reasoning, math, RAG, tool use, etc. To assess the\nvalue of the data, we use it to finetune Mistral-7B[11] model. The finetuned Mistral model\n(Orca-3) shows significant improvement over other instruction-tuned models using the same\nbase model. For example, compared to Mistral-Instruct-7B, it shows 40% improvement on\nAGIEval, 19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement\non BBH, 45% improvement on AlpacaEval and 31.34% reduction on hallucination across\nmultiple summarization benchmarks. Additionally, it outperforms other models such as\nLLAMA-8B-instruct and GPT-3.5 on multiple benchmarks. Note that the only seed data\nused is publicly available raw materials and no task-specific or benchmark data has been\nused as seeds.\nWhilewedemonstratetheutilityofAgentInstructbycreatingagenericpost-trainingsynthetic\ndataset, we believe that agents can enable the creation of Synthetic-Data-Generation-As-A-\nService where we start with raw materials (e.g. web data for general model training or domain\nspecific data for specialized models), and we generate data for post-training and finetuning,\nhence enabling continual learning and improvement of any base LLM. Additionally, we\nbelieve that the AgentInstruct approach can be used for self-improvement of larger, more\ncapable models because of: (1) the ability to generate new prompts and (2) the ability to\ngenerate responses that exceed the quality of the LLM used in the agentic flow (because of\nthe use of tools, reflection, etc.).\n2 Generative Teaching: AgentInstruct\nCreating synthetic datasets for supervised fine-tuning and instruction-tuning has seen\nsignificant progress over the last year. The quality of these datasets has been steadily\nimproving. High quality can be achieved by using powerful frontier models (or agenetic flows\nbased on these models) to generate responses. However, when creating synthetic data, in\naddition to quality, we also need to consider several other fundamental questions:\n1. How can we create a vast amount of data?\n2. How can we ensure that the generated data is diverse?\n3. How can we generate complex or nuanced data points?\nIn the AgentInstruct methodology, we outline a structured approach to tackle these challenges\nas follows:\nFigure 2: Concise Summary of the AgentInstruct Methodology\n1. Assemble a collection of raw seeds (e.g., textbook chapters, web articles, code\nsnippets).\n2.foreach seed in the collection do\n3. Transform the seed with the aid of one or more content transformation Agents (\nContent Transformation Flow).\n4. Route it through a series of instruction creation Agents to create a diverse set of\ninstructions (Seed Instruction Creation Flow).\n5. Utilize another group of Refinement Agents to iteratively refine the complexity\nand quality of the seed instructions (Refinement Flow).\n6.end for\nWe use agentic flows to automate the generation process and leverage raw articles as seeds to\nfoster diversity and ensure that problems generated in different iterations (line 2 in Figure 1)\nare distinct and of broad coverage. This enables us to create data at scale (benefiting from\nautomation of agentic flows), with high diversity (based on the broad and diverse seeds) and\nof varying complexity (benefiting from the iterative and","chunk_id":"b88745a13b69cecbc0ee9c3af41389bf","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"GENERATIVE TEACHING","type":"METHODOLOGY","description":"A methodology for generating abundant amounts of diverse, challenging, and high-quality data to teach a particular skill to an AI model","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"METHODOLOGY"},{"name":"AGENTINSTRUCT","type":"SYSTEM","description":"An agentic solution for Generative Teaching that focuses on creating demonstration and feedback data using raw documents as input","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"SYSTEM"},{"name":"GPT-4","type":"MODEL","description":"A powerful language model used by AgentInstruct to generate high-quality data","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"MODEL"},{"name":"MISTRAL-7B","type":"MODEL","description":"A base model that was fine-tuned using the synthetic dataset created by AgentInstruct","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"MODEL"},{"name":"ORCA-3","type":"MODEL","description":"The fine-tuned version of the Mistral-7B model, which shows significant improvement over other instruction-tuned models","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"MODEL"},{"name":"RAW DOCUMENTS","type":"DATA SOURCE","description":"Unstructured text documents or source code used as seeds for generating synthetic data","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"DATA SOURCE"},{"name":"PROMPTS","type":"DATA","description":"Initial inputs used to generate responses in the data generation process","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"DATA"},{"name":"RESPONSES","type":"DATA","description":"Outputs generated in response to prompts during the data generation process","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"DATA"},{"name":"AGIEVAL","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of the fine-tuned Mistral model (Orca-3)","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"BENCHMARK"},{"name":"MMLU","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of the fine-tuned Mistral model (Orca-3)","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"BENCHMARK"},{"name":"GSM8K","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of the fine-tuned Mistral model (Orca-3)","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"BENCHMARK"},{"name":"BBH","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of the fine-tuned Mistral model (Orca-3)","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"BENCHMARK"},{"name":"ALPACAEVAL","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of the fine-tuned Mistral model (Orca-3)","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"BENCHMARK"},{"name":"LLAMA-8B-INSTRUCT","type":"MODEL","description":"Another model that was outperformed by the fine-tuned Mistral model (Orca-3)","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"MODEL"},{"name":"GPT-3.5","type":"MODEL","description":"Another model that was outperformed by the fine-tuned Mistral model (Orca-3)","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"MODEL"},{"name":"SYNTHETIC-DATA-GENERATION-AS-A-SERVICE","type":"SERVICE","description":"A proposed service for generating data for post-training and fine-tuning using raw materials","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"SERVICE"},{"name":"CONTENT TRANSFORMATION AGENTS","type":"AGENTS","description":"Agents used to transform raw seeds into content during the data generation process","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"AGENTS"},{"name":"INSTRUCTION CREATION AGENTS","type":"AGENTS","description":"Agents used to create a diverse set of instructions from transformed seeds","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"AGENTS"},{"name":"REFINEMENT AGENTS","type":"AGENTS","description":"Agents used to iteratively refine the complexity and quality of seed instructions","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"AGENTS"},{"name":"AGENTIC FLOWS","type":"TECHNIQUE","description":"Automated processes used to generate data at scale, ensuring diversity and complexity","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"TECHNIQUE"},{"name":"CREATIVE WRITING","type":"SKILL","description":"A skill covered in the synthetic post-training dataset created by AgentInstruct","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"REASONING","type":"SKILL","description":"A skill covered in the synthetic post-training dataset created by AgentInstruct","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"MATH","type":"SKILL","description":"A skill covered in the synthetic post-training dataset created by AgentInstruct","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"RAG","type":"SKILL","description":"A skill covered in the synthetic post-training dataset created by AgentInstruct","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"TOOL USE","type":"SKILL","description":"A skill covered in the synthetic post-training dataset created by AgentInstruct","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"DATA GENERATION WORKFLOWS","type":"PROCESS","description":"Workflows that enable automation of data generation, reducing or eliminating the need for human intervention","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"DATA FILTERING","type":"PROCESS","description":"A process applied by AgentInstruct to ensure the quality of generated data","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"VERIFICATION","type":"PROCESS","description":"A process applied by AgentInstruct to ensure the accuracy of generated data","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"REFLECTION FLOWS","type":"TECHNIQUE","description":"Flows used by agents in AgentInstruct to improve the quality of generated responses","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"SEARCH","type":"TOOL","description":"A tool used by AgentInstruct to generate high-quality data","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"CODE INTERPRETERS","type":"TOOL","description":"Tools used by AgentInstruct to generate high-quality data","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"TAXONOMY","type":"TECHNIQUE","description":"A classification system used by AgentInstruct to create diverse and high-quality prompts and responses","source_id":"b88745a13b69cecbc0ee9c3af41389bf"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GENERATIVE TEACHING\">      <data key=\"d0\">METHODOLOGY<\/data>      <data key=\"d1\">A methodology for generating abundant amounts of diverse, challenging, and high-quality data to teach a particular skill to an AI model<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">METHODOLOGY<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">An agentic solution for Generative Teaching that focuses on creating demonstration and feedback data using raw documents as input<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A powerful language model used by AgentInstruct to generate high-quality data<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"MISTRAL-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A base model that was fine-tuned using the synthetic dataset created by AgentInstruct<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">The fine-tuned version of the Mistral-7B model, which shows significant improvement over other instruction-tuned models<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"RAW DOCUMENTS\">      <data key=\"d0\">DATA SOURCE<\/data>      <data key=\"d1\">Unstructured text documents or source code used as seeds for generating synthetic data<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">DATA SOURCE<\/data>    <\/node>    <node id=\"PROMPTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Initial inputs used to generate responses in the data generation process<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"RESPONSES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Outputs generated in response to prompts during the data generation process<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"AGIEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of the fine-tuned Mistral model (Orca-3)<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of the fine-tuned Mistral model (Orca-3)<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of the fine-tuned Mistral model (Orca-3)<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"BBH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of the fine-tuned Mistral model (Orca-3)<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"ALPACAEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of the fine-tuned Mistral model (Orca-3)<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"LLAMA-8B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Another model that was outperformed by the fine-tuned Mistral model (Orca-3)<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Another model that was outperformed by the fine-tuned Mistral model (Orca-3)<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"SYNTHETIC-DATA-GENERATION-AS-A-SERVICE\">      <data key=\"d0\">SERVICE<\/data>      <data key=\"d1\">A proposed service for generating data for post-training and fine-tuning using raw materials<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">SERVICE<\/data>    <\/node>    <node id=\"CONTENT TRANSFORMATION AGENTS\">      <data key=\"d0\">AGENTS<\/data>      <data key=\"d1\">Agents used to transform raw seeds into content during the data generation process<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">AGENTS<\/data>    <\/node>    <node id=\"INSTRUCTION CREATION AGENTS\">      <data key=\"d0\">AGENTS<\/data>      <data key=\"d1\">Agents used to create a diverse set of instructions from transformed seeds<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">AGENTS<\/data>    <\/node>    <node id=\"REFINEMENT AGENTS\">      <data key=\"d0\">AGENTS<\/data>      <data key=\"d1\">Agents used to iteratively refine the complexity and quality of seed instructions<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">AGENTS<\/data>    <\/node>    <node id=\"AGENTIC FLOWS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Automated processes used to generate data at scale, ensuring diversity and complexity<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"CREATIVE WRITING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">A skill covered in the synthetic post-training dataset created by AgentInstruct<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"REASONING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">A skill covered in the synthetic post-training dataset created by AgentInstruct<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"MATH\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">A skill covered in the synthetic post-training dataset created by AgentInstruct<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"RAG\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">A skill covered in the synthetic post-training dataset created by AgentInstruct<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">A skill covered in the synthetic post-training dataset created by AgentInstruct<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"DATA GENERATION WORKFLOWS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Workflows that enable automation of data generation, reducing or eliminating the need for human intervention<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"DATA FILTERING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">A process applied by AgentInstruct to ensure the quality of generated data<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"VERIFICATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">A process applied by AgentInstruct to ensure the accuracy of generated data<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"REFLECTION FLOWS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Flows used by agents in AgentInstruct to improve the quality of generated responses<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"SEARCH\">      <data key=\"d0\">TOOL<\/data>      <data key=\"d1\">A tool used by AgentInstruct to generate high-quality data<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"CODE INTERPRETERS\">      <data key=\"d0\">TOOL<\/data>      <data key=\"d1\">Tools used by AgentInstruct to generate high-quality data<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"TAXONOMY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A classification system used by AgentInstruct to create diverse and high-quality prompts and responses<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <edge source=\"GENERATIVE TEACHING\" target=\"AGENTINSTRUCT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct is an agentic solution for Generative Teaching<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"GPT-4\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct uses GPT-4 to generate high-quality data<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"RAW DOCUMENTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct uses raw documents as input for generating synthetic data<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"PROMPTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct generates both prompts and responses<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"RESPONSES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct generates both prompts and responses<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"SYNTHETIC-DATA-GENERATION-AS-A-SERVICE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct can enable the creation of Synthetic-Data-Generation-As-A-Service<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"CONTENT TRANSFORMATION AGENTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct uses Content Transformation Agents to transform raw seeds<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"INSTRUCTION CREATION AGENTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct uses Instruction Creation Agents to create diverse instructions<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"REFINEMENT AGENTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct uses Refinement Agents to refine seed instructions<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"AGENTIC FLOWS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct uses agentic flows to automate the data generation process<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"CREATIVE WRITING\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct generates data covering the skill of creative writing<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"REASONING\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct generates data covering the skill of reasoning<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"MATH\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct generates data covering the skill of math<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"RAG\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct generates data covering the skill of RAG<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"TOOL USE\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct generates data covering the skill of tool use<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"DATA GENERATION WORKFLOWS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct enables automation of data generation workflows<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"DATA FILTERING\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct applies data filtering to ensure the quality of generated data<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"VERIFICATION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct applies verification to ensure the accuracy of generated data<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"REFLECTION FLOWS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct uses reflection flows to improve the quality of generated responses<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"SEARCH\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct uses search as a tool to generate high-quality data<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"CODE INTERPRETERS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct uses code interpreters as tools to generate high-quality data<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"TAXONOMY\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct uses a taxonomy to create diverse and high-quality prompts and responses<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"MISTRAL-7B\" target=\"ORCA-3\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 is the fine-tuned version of the Mistral-7B model<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AGIEVAL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 shows significant improvement on the AGIEval benchmark<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MMLU\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 shows significant improvement on the MMLU benchmark<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GSM8K\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 shows significant improvement on the GSM8K benchmark<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"BBH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 shows significant improvement on the BBH benchmark<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ALPACAEVAL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 shows significant improvement on the AlpacaEval benchmark<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"LLAMA-8B-INSTRUCT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 outperforms LLAMA-8B-instruct on multiple benchmarks<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GPT-3.5\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 outperforms GPT-3.5 on multiple benchmarks<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"f7eb89a70f544664546a510e46d5febd","chunk":" the seed instructions (Refinement Flow).\n6.end for\nWe use agentic flows to automate the generation process and leverage raw articles as seeds to\nfoster diversity and ensure that problems generated in different iterations (line 2 in Figure 1)\nare distinct and of broad coverage. This enables us to create data at scale (benefiting from\nautomation of agentic flows), with high diversity (based on the broad and diverse seeds) and\nof varying complexity (benefiting from the iterative and refinement patterns supported by\nagentic flows). AgentInstruct defines three different flows:\nContent Transformation Flow converts the raw seed into an intermediate representation\nthat simplifies the creation of instructions tailored to specific objectives. It comprises of\n31\nContent Transformation Flow\u2026...\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026...\u2026...\u2026...\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026Raw text \ndocuments or \nsource code \nfiles are used \nas seedsTransformed \ncontent ( e.g.  \nargument passage, \nmeeting transcript, \nlist of APIs,\u2026.)\nSeed Instruction Generation Flow\n\u2026\u2026Transformed \ncontent provide \ndiversity is easier \nto use to generate \ninstructions Seed instructions \nare generated \nfollowing  a \ncomprehensive \ntaxonomy\u2026...\u2026...\u2026...\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026...\u2026...\u2026...\n3\nInstruction Refinement Flow\nIteratively refine the instructions \nto boost quality, diversity and \ncomplexity\n2Figure 3: Figure provides a thematic overview of the roles played by different groups of\nagents. Content Transformation Flow converts the seed into an intermediate representation\nthat makes it easier to create high quality and diverse data. Seed Instruction Generation\nFlow creates instances of the target tasks following a taxonomy. Refinement Flow explores\nthe space further by starting from these initial data points and exploring the neighborhood.\nThe expectation is that by picking a random seed we will be able to cover the entire region\nof data points.\nmultiple agents and is often instrumental in the generation of high-quality data and serve as\nan additional means to introduce diversity.\nSeed Instruction Generation Flow comprising of multiple agents takes as input the\ntransformed seed from the Content Transformation Flow and generates a set of diverse\ninstructions. The only goal of the Seed Instruction Flow is to introduce diversity for which\nit often relies on a pre-defined, but extensible, taxonomy.\nInstruction Refinement Flow takes as input the instructions from the Seed Instruction\nFlow and iteratively enhances their complexity and quality. Towards this we use the concept\nof Suggester-Editor Agents[ 19]. Suggester agents initially propose various approaches to\nincrease the intricacy of the initial instructions (making them more complex, unsolvable,\nor tricky), after which the Editor agents modify the instructions in accordance with these\nsuggestions.\nEach flow consists of a number of agents. We use a generic definition of an agent, where an\nagent is powered by an LLM and can optionally have the ability to use tools such as search\nAPIs, code interpreter or a calculator. Each agent has a specific role and set of instructions\nspecified as part of the underlying LLM system message.\nWe implemented these flows for 17 different skills, each having multiple subcategories.\nThe skills include reading comprehension, question answering, coding, retrieval augmented\ngeneration, creative writing, tool\/API use and Web control. The full list is provided in\nTable 1\nWe explain how the workflows work with case studies of generating data for the following\nthree skills:\nReading Comprehension: The ability to understand, process, and interpret written text.\nText Modification: The process of altering text to suit different purposes.\nTool Use: The employment of functions or APIs to perform tasks or solve problems.\n4Reading Comprehension: Reading comprehension is a critical skill involving processing and un-\nderstanding text, which is necessary for learning and encompasses decoding, fluency, and vocabulary\nknowledge. Reading comprehension tests typically present text passages of varying lengths and\nsubjects, followed by questions that assess the reader\u2019s understanding.\nOpen Domain Question Answering: Open domain question answering involves generating\nresponses to questions over a wide range of topics, without being restricted to a specific domain.\nText Modification: Text modification involves changing existing text to improve its quality,\nmodify its tone, or to fit a specific context or audience. It is a common task in content creation and\nediting.\nWeb Agent: A web agent is a software program that autonomously performs tasks on the web,\nsuch as where to click, how much to scroll.\nBrain Teaser A brain teaser is a problem or puzzle, typically requiring thought to solve, often for\namusement but also used for training logical thinking and problem-solving skills.\nAnalytical Reasoning Analytical reasoning involves the ability to look at information, be it\nqualitative or quantitative in nature, and discern patterns within the information. It\u2019s a process\nthat includes understanding a system of relationships and draw logical conclusions about those\nrelationships.\nMultiple Choice Questions Multiple choice questions are a form of assessment where respondents\nare asked to select the best possible answer (or answers) out of the choices from a list. They are\ncommon in standardized tests, quizzes, and surveys.\nData To Text Data-to-text refers to human-readable textual summaries from source data. These\ncould be used for generating reports, explanations, or narratives from structured data.\nFermiFermi problems are estimation problems which seek quick, rough estimates of quantities\nwhich can be difficult to measure. Named after physicist Enrico Fermi, these problems often require\nmaking justified guesses or assumptions to reach a solution.\nCoding Coding involves writing code following instructions, understanding code, debugging code,\ntracing or writing test cases.\nText Extraction Text extraction is the process of retrieving relevant information from a larger text\ndocument. This can include tasks like named entity recognition, keyword extraction, or extracting\nspecific data fields from unstructured text.\nText","chunk_id":"f7eb89a70f544664546a510e46d5febd","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"CONTENT TRANSFORMATION FLOW","type":"PROCESS","description":"Content Transformation Flow converts the raw seed into an intermediate representation that simplifies the creation of instructions tailored to specific objectives","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"PROCESS"},{"name":"SEED INSTRUCTION GENERATION FLOW","type":"PROCESS","description":"Seed Instruction Generation Flow creates instances of the target tasks following a taxonomy and introduces diversity by generating a set of diverse instructions","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"PROCESS"},{"name":"INSTRUCTION REFINEMENT FLOW","type":"PROCESS","description":"Instruction Refinement Flow iteratively enhances the complexity and quality of instructions by using Suggester-Editor Agents","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"PROCESS"},{"name":"SUGGESTER-EDITOR AGENTS","type":"AGENTS","description":"Suggester-Editor Agents are used in the Instruction Refinement Flow to propose and modify instructions to increase their intricacy, making them more complex, unsolvable, or tricky","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"AGENTS"},{"name":"AGENT","type":"AGENT","description":"An agent is powered by an LLM and can optionally use tools such as search APIs, code interpreter, or a calculator, with a specific role and set of instructions","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"AGENT"},{"name":"LLM","type":"TECHNOLOGY","description":"Large Language Models (LLMs) are advanced machine learning models trained on vast amounts of text data to understand and generate human language","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"TECHNOLOGY"},{"name":"READING COMPREHENSION","type":"SKILL","description":"Reading comprehension is the ability to understand, process, and interpret written text, necessary for learning and encompassing decoding, fluency, and vocabulary knowledge","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"SKILL"},{"name":"QUESTION ANSWERING","type":"SKILL","description":"Open domain question answering involves generating responses to questions over a wide range of topics, without being restricted to a specific domain","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"SKILL"},{"name":"TEXT MODIFICATION","type":"SKILL","description":"Text modification involves changing existing text to improve its quality, modify its tone, or fit a specific context or audience","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"SKILL"},{"name":"TOOL USE","type":"SKILL","description":"Tool use involves the employment of functions or APIs to perform tasks or solve problems","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"SKILL"},{"name":"WEB AGENT","type":"AGENT","description":"A web agent is a software program that autonomously performs tasks on the web, such as where to click and how much to scroll","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"AGENT"},{"name":"BRAIN TEASER","type":"TASK","description":"A brain teaser is a problem or puzzle requiring thought to solve, often used for amusement or training logical thinking and problem-solving skills","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"TASK"},{"name":"ANALYTICAL REASONING","type":"SKILL","description":"Analytical reasoning involves the ability to look at information, discern patterns, and draw logical conclusions about relationships within the information","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"SKILL"},{"name":"MULTIPLE CHOICE QUESTIONS","type":"TASK","description":"Multiple choice questions are a form of assessment where respondents select the best possible answer from a list of choices","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"TASK"},{"name":"DATA TO TEXT","type":"TASK","description":"Data-to-text refers to generating human-readable textual summaries from source data, used for reports, explanations, or narratives","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"TASK"},{"name":"FERMI PROBLEMS","type":"TASK","description":"Fermi problems are estimation problems that seek quick, rough estimates of quantities, often requiring justified guesses or assumptions","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"TASK"},{"name":"CODING","type":"SKILL","description":"Coding involves writing, understanding, debugging code, and writing test cases","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"SKILL"},{"name":"TEXT EXTRACTION","type":"TASK","description":"Text extraction is the process of retrieving relevant information from a larger text document, including tasks like named entity recognition and keyword extraction","source_id":"f7eb89a70f544664546a510e46d5febd","entity_type":"TASK"},{"name":"AGENTIC FLOWS","type":"","description":"","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"CREATIVE WRITING","type":"SKILL","description":"Creative writing involves crafting original content, often with a focus on narrative, character development, and imaginative expression","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"RETRIEVAL AUGMENTED GENERATION","type":"SKILL","description":"Retrieval augmented generation combines information retrieval with text generation to improve the quality and relevance of generated content","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"WEB CONTROL","type":"SKILL","description":"Web control involves managing and automating interactions with web interfaces, such as navigating websites and performing online tasks","source_id":"f7eb89a70f544664546a510e46d5febd"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Content Transformation Flow converts the raw seed into an intermediate representation that simplifies the creation of instructions tailored to specific objectives<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"SEED INSTRUCTION GENERATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Seed Instruction Generation Flow creates instances of the target tasks following a taxonomy and introduces diversity by generating a set of diverse instructions<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"INSTRUCTION REFINEMENT FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Instruction Refinement Flow iteratively enhances the complexity and quality of instructions by using Suggester-Editor Agents<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"SUGGESTER-EDITOR AGENTS\">      <data key=\"d0\">AGENTS<\/data>      <data key=\"d1\">Suggester-Editor Agents are used in the Instruction Refinement Flow to propose and modify instructions to increase their intricacy, making them more complex, unsolvable, or tricky<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">AGENTS<\/data>    <\/node>    <node id=\"AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">An agent is powered by an LLM and can optionally use tools such as search APIs, code interpreter, or a calculator, with a specific role and set of instructions<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">AGENT<\/data>    <\/node>    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Large Language Models (LLMs) are advanced machine learning models trained on vast amounts of text data to understand and generate human language<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"READING COMPREHENSION\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Reading comprehension is the ability to understand, process, and interpret written text, necessary for learning and encompassing decoding, fluency, and vocabulary knowledge<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">SKILL<\/data>    <\/node>    <node id=\"QUESTION ANSWERING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Open domain question answering involves generating responses to questions over a wide range of topics, without being restricted to a specific domain<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">SKILL<\/data>    <\/node>    <node id=\"TEXT MODIFICATION\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Text modification involves changing existing text to improve its quality, modify its tone, or fit a specific context or audience<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">SKILL<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Tool use involves the employment of functions or APIs to perform tasks or solve problems<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">SKILL<\/data>    <\/node>    <node id=\"WEB AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">A web agent is a software program that autonomously performs tasks on the web, such as where to click and how much to scroll<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">AGENT<\/data>    <\/node>    <node id=\"BRAIN TEASER\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">A brain teaser is a problem or puzzle requiring thought to solve, often used for amusement or training logical thinking and problem-solving skills<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"ANALYTICAL REASONING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Analytical reasoning involves the ability to look at information, discern patterns, and draw logical conclusions about relationships within the information<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">SKILL<\/data>    <\/node>    <node id=\"MULTIPLE CHOICE QUESTIONS\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Multiple choice questions are a form of assessment where respondents select the best possible answer from a list of choices<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"DATA TO TEXT\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Data-to-text refers to generating human-readable textual summaries from source data, used for reports, explanations, or narratives<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"FERMI PROBLEMS\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Fermi problems are estimation problems that seek quick, rough estimates of quantities, often requiring justified guesses or assumptions<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"CODING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Coding involves writing, understanding, debugging code, and writing test cases<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">SKILL<\/data>    <\/node>    <node id=\"TEXT EXTRACTION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Text extraction is the process of retrieving relevant information from a larger text document, including tasks like named entity recognition and keyword extraction<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"AGENTIC FLOWS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"CREATIVE WRITING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Creative writing involves crafting original content, often with a focus on narrative, character development, and imaginative expression<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"RETRIEVAL AUGMENTED GENERATION\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Retrieval augmented generation combines information retrieval with text generation to improve the quality and relevance of generated content<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"WEB CONTROL\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Web control involves managing and automating interactions with web interfaces, such as navigating websites and performing online tasks<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"AGENTIC FLOWS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Content Transformation Flow is a part of agentic flows that converts raw seeds into intermediate representations<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"AGENTIC FLOWS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Seed Instruction Generation Flow is a part of agentic flows that generates diverse instructions<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"AGENTIC FLOWS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Instruction Refinement Flow is a part of agentic flows that enhances the complexity and quality of instructions<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"SUGGESTER-EDITOR AGENTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Suggester-Editor Agents are used in the Instruction Refinement Flow to propose and modify instructions<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"AGENT\" target=\"LLM\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">An agent is powered by an LLM<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"QUESTION ANSWERING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Both are skills related to understanding and processing text<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"MULTIPLE CHOICE QUESTIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Multiple choice questions are often used to assess reading comprehension<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"TEXT MODIFICATION\" target=\"TOOL USE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Text modification can involve the use of tools or APIs<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"TOOL USE\" target=\"WEB AGENT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">A web agent is a type of tool used to perform tasks on the web<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"TOOL USE\" target=\"CODING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Coding often involves the use of various tools and APIs<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"WEB AGENT\" target=\"WEB CONTROL\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Web agents are used to perform web control tasks<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"BRAIN TEASER\" target=\"ANALYTICAL REASONING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Brain teasers are used to train analytical reasoning skills<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"ANALYTICAL REASONING\" target=\"FERMI PROBLEMS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Fermi problems require analytical reasoning to solve<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"DATA TO TEXT\" target=\"TEXT EXTRACTION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Text extraction can be a part of the data-to-text process<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"CREATIVE WRITING\" target=\"RETRIEVAL AUGMENTED GENERATION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Both skills involve generating content, with retrieval augmented generation focusing on improving relevance<\/data>      <data key=\"d6\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"0c212c1467564ad33330b1f655a8e27e","chunk":" quick, rough estimates of quantities\nwhich can be difficult to measure. Named after physicist Enrico Fermi, these problems often require\nmaking justified guesses or assumptions to reach a solution.\nCoding Coding involves writing code following instructions, understanding code, debugging code,\ntracing or writing test cases.\nText Extraction Text extraction is the process of retrieving relevant information from a larger text\ndocument. This can include tasks like named entity recognition, keyword extraction, or extracting\nspecific data fields from unstructured text.\nText Classification Text classification is a type of machine learning task where text documents are\nautomatically classified into predefined categories. This can be used for spam detection, sentiment\nanalysis, and topic labelling among others.\nRetrieval Augmented Generation Retrieval Augmented Generation (RAG) is a method used\nin natural language processing that combines retrieval-based and generative models to generate\nresponses. It first retrieves relevant documents and then uses these documents to generate a response.\nTool Use Tool use involves the manipulation of tools to achieve goals. In AI, this refers to the\nability of an AI system to use available resources or auxiliary systems to solve complex tasks.\nCreative Content Generation Creative content generation involves the creation of original\ncontent, often involving elements of novelty, value, and surprise. In AI, this could refer to generating\ntext, music, or images that are not only new but also meaningful and interesting.\nFew Shot Reasoning Few-shot reasoning refers to the ability of a machine learning model to\nunderstand new concepts, patterns, or tasks with minimal examples or guidance. It\u2019s a desired trait\nin AI, mimicking the human ability to learn quickly from few examples.\nConversation Conversation refers to conversational agents or chatbots that interact with humans\nin a natural, human-like manner.\nTable 1: List of 17 capabilities for which we implemented AgentInstruct Flows\n52.1 AgentInstruct Flow for Reading Comprehension\nReading comprehension is a critical skill involving processing and understanding text, which\nis necessary for learning and encompasses decoding, fluency, and vocabulary knowledge.\nReading comprehension questions range from asking for explicit information (literal com-\nprehension) to requiring inferences, understanding vocabulary in context, analyzing text\nstructure and argumentation, critically evaluating the content, and synthesizing informa-\ntion from different parts of or multiple texts. Reading comprehension is a very important\ncapability and can enable scenarios like question answering, search, grounded reasoning, etc.\nContent Transformation Flow Web crawls encompass an extensive collection of human-\ngenerated text, which holds potential for generating reading comprehension materials.\nHowever, these sources are not inherently structured to facilitate the teaching of read-\ning comprehension skills. Consequently, they do not support the consistent generation of\ndiverse question types required for comprehensive reading comprehension evaluation. For\ninstance, the LSAT Logical Reasoning test features specialized question categories, including\nassumption, strengthening\/weakening, flaw, and inference questions. Crafting such questions\nnecessitates passages with a particular stylistic and logical framework. The objective of\nContent Transformation Flow is to transform arbitrary articles into well-crafted pieces that\nare conducive to the formulation of a wide array of reading comprehension question types.\nOur current flow for Reading Comprehension encompasses a suite of nine content transfor-\nmation agents for generating argument passages, debates and conversations, long passages,\nmeeting transcripts, poems, satarical content, etc. Detailed description is provided in Ap-\npendix A. Given a seed article, the flow will randomly pick one of the Content Transformation\nAgents to assess the seed article and attempt to generate the text passages. The following\nprovides an example for the Argument Passage Generator .\nEXAMPLE: Content Transformation Flow\nRandom Seed\nUric acid is a substance produced naturally by the breakdown of purine (a type of dietary\nprotein). When it is in excess in the body, crystals composed of these substances are formed.\nThese crystals are deposited in various parts of the body, mainly in the joints and kidneys,\ncausing pain and other aggravations. The lack or excess of uric acid in the body is caused by\nsome diseases (such as leukemia, obesity, kidney diseases, and anemia) and factors related\nto lifestyle (consumption of alcohol and processed foods, for example).\nContents\n\u2022Where does purine come from?\n\u2022Where is uric acid found?\n\u2022What is high uric acid?\n\u2022What can high uric acid cause?\n\u2022What is low uric acid?\n\u2022Tests: how to know uric acid levels?\nWhere does purine come from?\nPurine is produced and released into the bloodstream when an amino acid is broken down\nby digestion. Therefore, it is produced naturally in the body. ... (Truncated)\n6Transformed Seed (Argument Passage Generator)\nRecent studies have indicated that high levels of uric acid in the blood, known as hyper-\nuricemia, may increase the risk of cardiovascular disease. Uric acid is a byproduct of purine\nmetabolism, and while it is naturally produced in the body, excessive amounts can lead to\nhealth complications. Hyperuricemia is typically defined as levels above 6 mg\/dL in women\nand 7 mg\/dL in men. This condition can result from increased production of uric acid or\ninsufficient elimination through urine. Factors contributing to high uric acid include a diet\nrich in purines, found in red meat and seafood, as well as lifestyle choices such as alcohol\nconsumption and physical inactivity. Conversely, low levels of uric acid, or hypouricemia,\nare less common and usually do not present symptoms. Nonetheless, they can be indicative\nof underlying kidney or liver issues. Diagnosing either condition requires laboratory blood\nand urine tests. It is important to note that while high uric acid levels are associated with\nan increased risk of cardiovascular disease, the causal","chunk_id":"0c212c1467564ad33330b1f655a8e27e","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"CODING","type":"ACTIVITY","description":"Involves writing code following instructions, understanding code, debugging code, tracing or writing test cases","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"TEXT EXTRACTION","type":"PROCESS","description":"The process of retrieving relevant information from a larger text document, including tasks like named entity recognition, keyword extraction, or extracting specific data fields from unstructured text","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"TEXT CLASSIFICATION","type":"PROCESS","description":"A type of machine learning task where text documents are automatically classified into predefined categories, used for spam detection, sentiment analysis, and topic labeling among others","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"RETRIEVAL AUGMENTED GENERATION","type":"METHOD","description":"A method used in natural language processing that combines retrieval-based and generative models to generate responses by first retrieving relevant documents and then using these documents to generate a response","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"TOOL USE","type":"ACTIVITY","description":"Involves the manipulation of tools to achieve goals, referring to the ability of an AI system to use available resources or auxiliary systems to solve complex tasks","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"CREATIVE CONTENT GENERATION","type":"ACTIVITY","description":"Involves the creation of original content, often involving elements of novelty, value, and surprise, such as generating text, music, or images that are new, meaningful, and interesting","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"FEW SHOT REASONING","type":"CONCEPT","description":"Refers to the ability of a machine learning model to understand new concepts, patterns, or tasks with minimal examples or guidance, mimicking the human ability to learn quickly from few examples","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"CONVERSATION","type":"ACTIVITY","description":"Refers to conversational agents or chatbots that interact with humans in a natural, human-like manner","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"AGENTINSTRUCT FLOW","type":"SYSTEM","description":"A system implemented for various capabilities, including reading comprehension","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"READING COMPREHENSION","type":"SKILL","description":"A critical skill involving processing and understanding text, necessary for learning and encompassing decoding, fluency, and vocabulary knowledge","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"CONTENT TRANSFORMATION FLOW","type":"SYSTEM","description":"A system designed to transform arbitrary articles into well-crafted pieces conducive to the formulation of a wide array of reading comprehension question types","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"ARGUMENT PASSAGE GENERATOR","type":"TOOL","description":"A tool within the Content Transformation Flow that generates argument passages from seed articles","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"URIC ACID","type":"SUBSTANCE","description":"A substance produced naturally by the breakdown of purine, which can form crystals in the body causing pain and other issues when in excess","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"HYPERURICEMIA","type":"CONDITION","description":"A condition characterized by high levels of uric acid in the blood, which may increase the risk of cardiovascular disease","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"HYPOURICEMIA","type":"CONDITION","description":"A condition characterized by low levels of uric acid, which is less common and usually does not present symptoms but can indicate underlying kidney or liver issues","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"PURINE","type":"SUBSTANCE","description":"A type of dietary protein that, when broken down, produces uric acid","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"CARDIOVASCULAR DISEASE","type":"CONDITION","description":"A class of diseases that involve the heart or blood vessels, which may be associated with high levels of uric acid","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"LSAT LOGICAL REASONING TEST","type":"TEST","description":"A test featuring specialized question categories, including assumption, strengthening\/weakening, flaw, and inference questions","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"LABORATORY BLOOD AND URINE TESTS","type":"TEST","description":"Tests required to diagnose conditions like hyperuricemia and hypouricemia","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"NAMED ENTITY RECOGNITION","type":"","description":"\nA task within text extraction that involves identifying and classifying named entities in text","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"PROCESS"},{"name":"KEYWORD EXTRACTION","type":"","description":"\nA task within text extraction that involves identifying and extracting important keywords from text","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"PROCESS"},{"name":"DATA FIELDS","type":"","description":"\nSpecific pieces of information extracted from unstructured text during text extraction","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"PROCESS"},{"name":"SPAM DETECTION","type":"","description":"\nAn application of text classification used to identify and filter out spam messages","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"PROCESS"},{"name":"SENTIMENT ANALYSIS","type":"","description":"\nAn application of text classification used to determine the sentiment expressed in a text","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"PROCESS"},{"name":"TOPIC LABELING","type":"","description":"\nAn application of text classification used to assign topics to text documents","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"PROCESS"},{"name":"NATURAL LANGUAGE PROCESSING","type":"","description":"","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"RETRIEVAL-BASED MODELS","type":"","description":"\nModels used in retrieval augmented generation to retrieve relevant documents","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"TECHNOLOGY"},{"name":"GENERATIVE MODELS","type":"","description":"\nModels used in retrieval augmented generation to generate responses based on retrieved documents","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"TECHNOLOGY"},{"name":"DECODING","type":"","description":"\nA component of reading comprehension involving the ability to interpret and make sense of text","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"SKILL"},{"name":"FLUENCY","type":"","description":"\nA component of reading comprehension involving the ability to read text smoothly and accurately","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"SKILL"},{"name":"VOCABULARY KNOWLEDGE","type":"","description":"\nA component of reading comprehension involving the understanding of word meanings","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"SKILL"},{"name":"QUESTION ANSWERING","type":"","description":"\nA scenario enabled by reading comprehension where questions are answered based on text","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"ACTIVITY"},{"name":"SEARCH","type":"","description":"\nA scenario enabled by reading comprehension where information is sought and found in text","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"ACTIVITY"},{"name":"GROUNDED REASONING","type":"","description":"\nA scenario enabled by reading comprehension involving reasoning based on text","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"ACTIVITY"},{"name":"ASSUMPTION QUESTIONS","type":"","description":"\nA type of question featured in the LSAT Logical Reasoning test that involves identifying assumptions","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"PROCESS"},{"name":"STRENGTHENING\/WEAKENING QUESTIONS","type":"","description":"\nA type of question featured in the LSAT Logical Reasoning test that involves strengthening or weakening arguments","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"PROCESS"},{"name":"FLAW QUESTIONS","type":"","description":"\nA type of question featured in the LSAT Logical Reasoning test that involves identifying flaws in arguments","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"PROCESS"},{"name":"INFERENCE QUESTIONS","type":"","description":"\nA type of question featured in the LSAT Logical Reasoning test that involves making inferences from text","source_id":"0c212c1467564ad33330b1f655a8e27e","entity_type":"PROCESS"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"CODING\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Involves writing code following instructions, understanding code, debugging code, tracing or writing test cases<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"TEXT EXTRACTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The process of retrieving relevant information from a larger text document, including tasks like named entity recognition, keyword extraction, or extracting specific data fields from unstructured text<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"TEXT CLASSIFICATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">A type of machine learning task where text documents are automatically classified into predefined categories, used for spam detection, sentiment analysis, and topic labeling among others<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"RETRIEVAL AUGMENTED GENERATION\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">A method used in natural language processing that combines retrieval-based and generative models to generate responses by first retrieving relevant documents and then using these documents to generate a response<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Involves the manipulation of tools to achieve goals, referring to the ability of an AI system to use available resources or auxiliary systems to solve complex tasks<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"CREATIVE CONTENT GENERATION\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Involves the creation of original content, often involving elements of novelty, value, and surprise, such as generating text, music, or images that are new, meaningful, and interesting<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"FEW SHOT REASONING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Refers to the ability of a machine learning model to understand new concepts, patterns, or tasks with minimal examples or guidance, mimicking the human ability to learn quickly from few examples<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"CONVERSATION\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Refers to conversational agents or chatbots that interact with humans in a natural, human-like manner<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"AGENTINSTRUCT FLOW\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system implemented for various capabilities, including reading comprehension<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"READING COMPREHENSION\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">A critical skill involving processing and understanding text, necessary for learning and encompassing decoding, fluency, and vocabulary knowledge<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system designed to transform arbitrary articles into well-crafted pieces conducive to the formulation of a wide array of reading comprehension question types<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"ARGUMENT PASSAGE GENERATOR\">      <data key=\"d0\">TOOL<\/data>      <data key=\"d1\">A tool within the Content Transformation Flow that generates argument passages from seed articles<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"URIC ACID\">      <data key=\"d0\">SUBSTANCE<\/data>      <data key=\"d1\">A substance produced naturally by the breakdown of purine, which can form crystals in the body causing pain and other issues when in excess<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"HYPERURICEMIA\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">A condition characterized by high levels of uric acid in the blood, which may increase the risk of cardiovascular disease<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"HYPOURICEMIA\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">A condition characterized by low levels of uric acid, which is less common and usually does not present symptoms but can indicate underlying kidney or liver issues<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"PURINE\">      <data key=\"d0\">SUBSTANCE<\/data>      <data key=\"d1\">A type of dietary protein that, when broken down, produces uric acid<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"CARDIOVASCULAR DISEASE\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">A class of diseases that involve the heart or blood vessels, which may be associated with high levels of uric acid<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"LSAT LOGICAL REASONING TEST\">      <data key=\"d0\">TEST<\/data>      <data key=\"d1\">A test featuring specialized question categories, including assumption, strengthening\/weakening, flaw, and inference questions<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"LABORATORY BLOOD AND URINE TESTS\">      <data key=\"d0\">TEST<\/data>      <data key=\"d1\">Tests required to diagnose conditions like hyperuricemia and hypouricemia<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"NAMED ENTITY RECOGNITION\">      <data key=\"d0\" \/>      <data key=\"d1\">A task within text extraction that involves identifying and classifying named entities in text<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"KEYWORD EXTRACTION\">      <data key=\"d0\" \/>      <data key=\"d1\">A task within text extraction that involves identifying and extracting important keywords from text<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"DATA FIELDS\">      <data key=\"d0\" \/>      <data key=\"d1\">Specific pieces of information extracted from unstructured text during text extraction<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"SPAM DETECTION\">      <data key=\"d0\" \/>      <data key=\"d1\">An application of text classification used to identify and filter out spam messages<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"SENTIMENT ANALYSIS\">      <data key=\"d0\" \/>      <data key=\"d1\">An application of text classification used to determine the sentiment expressed in a text<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"TOPIC LABELING\">      <data key=\"d0\" \/>      <data key=\"d1\">An application of text classification used to assign topics to text documents<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"NATURAL LANGUAGE PROCESSING\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"RETRIEVAL-BASED MODELS\">      <data key=\"d0\" \/>      <data key=\"d1\">Models used in retrieval augmented generation to retrieve relevant documents<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GENERATIVE MODELS\">      <data key=\"d0\" \/>      <data key=\"d1\">Models used in retrieval augmented generation to generate responses based on retrieved documents<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"DECODING\">      <data key=\"d0\" \/>      <data key=\"d1\">A component of reading comprehension involving the ability to interpret and make sense of text<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">SKILL<\/data>    <\/node>    <node id=\"FLUENCY\">      <data key=\"d0\" \/>      <data key=\"d1\">A component of reading comprehension involving the ability to read text smoothly and accurately<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">SKILL<\/data>    <\/node>    <node id=\"VOCABULARY KNOWLEDGE\">      <data key=\"d0\" \/>      <data key=\"d1\">A component of reading comprehension involving the understanding of word meanings<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">SKILL<\/data>    <\/node>    <node id=\"QUESTION ANSWERING\">      <data key=\"d0\" \/>      <data key=\"d1\">A scenario enabled by reading comprehension where questions are answered based on text<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">ACTIVITY<\/data>    <\/node>    <node id=\"SEARCH\">      <data key=\"d0\" \/>      <data key=\"d1\">A scenario enabled by reading comprehension where information is sought and found in text<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">ACTIVITY<\/data>    <\/node>    <node id=\"GROUNDED REASONING\">      <data key=\"d0\" \/>      <data key=\"d1\">A scenario enabled by reading comprehension involving reasoning based on text<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">ACTIVITY<\/data>    <\/node>    <node id=\"ASSUMPTION QUESTIONS\">      <data key=\"d0\" \/>      <data key=\"d1\">A type of question featured in the LSAT Logical Reasoning test that involves identifying assumptions<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"STRENGTHENING\/WEAKENING QUESTIONS\">      <data key=\"d0\" \/>      <data key=\"d1\">A type of question featured in the LSAT Logical Reasoning test that involves strengthening or weakening arguments<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"FLAW QUESTIONS\">      <data key=\"d0\" \/>      <data key=\"d1\">A type of question featured in the LSAT Logical Reasoning test that involves identifying flaws in arguments<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"INFERENCE QUESTIONS\">      <data key=\"d0\" \/>      <data key=\"d1\">A type of question featured in the LSAT Logical Reasoning test that involves making inferences from text<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <edge source=\"TEXT EXTRACTION\" target=\"NAMED ENTITY RECOGNITION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Named entity recognition is a task within text extraction<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"TEXT EXTRACTION\" target=\"KEYWORD EXTRACTION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Keyword extraction is a task within text extraction<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"TEXT EXTRACTION\" target=\"DATA FIELDS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Extracting specific data fields from unstructured text is a task within text extraction<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"TEXT CLASSIFICATION\" target=\"SPAM DETECTION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Spam detection is an application of text classification<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"TEXT CLASSIFICATION\" target=\"SENTIMENT ANALYSIS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Sentiment analysis is an application of text classification<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"TEXT CLASSIFICATION\" target=\"TOPIC LABELING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Topic labeling is an application of text classification<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"RETRIEVAL AUGMENTED GENERATION\" target=\"NATURAL LANGUAGE PROCESSING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Retrieval Augmented Generation is a method used in natural language processing<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"RETRIEVAL AUGMENTED GENERATION\" target=\"RETRIEVAL-BASED MODELS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Retrieval Augmented Generation combines retrieval-based models<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"RETRIEVAL AUGMENTED GENERATION\" target=\"GENERATIVE MODELS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Retrieval Augmented Generation combines generative models<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT FLOW\" target=\"READING COMPREHENSION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct Flow includes a flow for reading comprehension<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"DECODING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Decoding is a component of reading comprehension<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"FLUENCY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Fluency is a component of reading comprehension<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"VOCABULARY KNOWLEDGE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Vocabulary knowledge is a component of reading comprehension<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"QUESTION ANSWERING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Reading comprehension enables scenarios like question answering<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"SEARCH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Reading comprehension enables scenarios like search<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"GROUNDED REASONING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Reading comprehension enables scenarios like grounded reasoning<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Content Transformation Flow is designed to generate materials for reading comprehension<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"ARGUMENT PASSAGE GENERATOR\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Argument Passage Generator is a tool within the Content Transformation Flow<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"URIC ACID\" target=\"PURINE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Uric acid is a byproduct of purine metabolism<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"URIC ACID\" target=\"HYPERURICEMIA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Hyperuricemia is characterized by high levels of uric acid<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"URIC ACID\" target=\"HYPOURICEMIA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Hypouricemia is characterized by low levels of uric acid<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"HYPERURICEMIA\" target=\"CARDIOVASCULAR DISEASE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Hyperuricemia may increase the risk of cardiovascular disease<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"HYPERURICEMIA\" target=\"LABORATORY BLOOD AND URINE TESTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Laboratory blood and urine tests are required to diagnose hyperuricemia<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"HYPOURICEMIA\" target=\"LABORATORY BLOOD AND URINE TESTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Laboratory blood and urine tests are required to diagnose hypouricemia<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"LSAT LOGICAL REASONING TEST\" target=\"ASSUMPTION QUESTIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The LSAT Logical Reasoning test features assumption questions<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"LSAT LOGICAL REASONING TEST\" target=\"STRENGTHENING\/WEAKENING QUESTIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The LSAT Logical Reasoning test features strengthening\/weakening questions<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"LSAT LOGICAL REASONING TEST\" target=\"FLAW QUESTIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The LSAT Logical Reasoning test features flaw questions<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"LSAT LOGICAL REASONING TEST\" target=\"INFERENCE QUESTIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The LSAT Logical Reasoning test features inference questions<\/data>      <data key=\"d6\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"1d8835c0ce90e56be22873bcf2740a5d","chunk":" found in red meat and seafood, as well as lifestyle choices such as alcohol\nconsumption and physical inactivity. Conversely, low levels of uric acid, or hypouricemia,\nare less common and usually do not present symptoms. Nonetheless, they can be indicative\nof underlying kidney or liver issues. Diagnosing either condition requires laboratory blood\nand urine tests. It is important to note that while high uric acid levels are associated with\nan increased risk of cardiovascular disease, the causal relationship remains to be conclusively\nestablished.\nSeed Instruction Generation Flow We have currently compiled a collection of 43\nreading comprehension questions types. The list includes literal comprehension questions,\ncritical comprehension questions, evaluative comprehension questions, reasoning, identifying\nassumptions, identifying information that strengthens\/weakens an argument, ordering events,\netc.\nA list some of these types is in Appendix A.\nWe have defined multiple Agents targeting these categories. Each of these agent receives as\ninput a piece of text and generates a list of questions based on a predefined question type.\nThe orchestration process will engage a subset of these agents, determined by the content\ntransformation agent in the preceding step. Upon completion of this phase, we will obtain a\ncollection of (passage, question) pairs that will constitute the input for the subsequent stage.\nThe following figure provides an example of Strengthen type question that is generated from\nthe running example.\nEXAMPLE: Seed Instruction\nWhich one of the following, if true, most strengthens the argument that high lev-\nels of uric acid increase the risk of cardiovascular disease?\n(A) Some individuals with hyperuricemia do not develop cardiovascular disease.\n(B) Physical activity has been found to have no significant impact on uric acid levels in the\nblood.\n(C) Dietary supplements have been shown to reduce uric acid levels in the blood.\n(D) A significant number of patients with cardiovascular disease have been found to have\nuric acid levels above the defined thresholds for hyperuricemia.\n(E) Individuals with a family history of cardiovascular disease are more likely to have high\nuric acid levels.\nInstruction Refinement Flow The refinement flow contains multiple suggester-editor\nagents that will go through each of the (passage, question) pairs and create more such pairs\nwith the following goals: (1) Modify the passage to make the question unanswerable, (2)\nModify the passage to alter the answer, if possible, in opposite direction, or (3) Modify the\nquestions or answer choices (if applicable) to make them complex.\nHere is an example where the suggester agent that tries to add a difficult distractor to\nconfuse the test taker.\n7EXAMPLE: Refinement Flow\nSUGGESTER\n\u2022Suggestion 1 Introduce a hypothetical study or finding that could potentially\nstrengthen the argument, requiring the test-taker to infer its impact on the rela-\ntionship between uric acid levels and cardiovascular disease.\n\u2022Suggestion 2 Add a layer of complexity by suggesting a genetic predisposition\nto hyperuricemia and its correlation with increased cardiovascular events, which\nwould require the test-taker to consider both genetic and physiological factors.\n\u2022Suggestion 3 Include a distractor option that seems to strengthen the argument\nbut upon closer examination, it does not directly relate to the causal relationship\nbetween uric acid levels and cardiovascular disease, thus testing the test-taker\u2019s\nability to discern relevant from irrelevant information.\nEDITOR\n\u2022Modification 1,2 (omitted for brevity)\n\u2022Modification 3 Which one of the following, if true, most strengthens the argument\nthat high levels of uric acid increase the risk of cardiovascular disease?\n(A) Some individuals with hyperuricemia do not develop cardiovascular disease.\n(B) Physical activity has been found to have no significant impact on uric acid\nlevels in the blood.\n(C) A placebo-controlled trial shows that individuals taking uric acid-lowering\nmedication do not exhibit a reduced incidence of cardiovascular disease compared\nto the placebo group.\n(D) A significant number of patients with cardiovascular disease have been found\nto have uric acid levels above the defined thresholds for hyperuricemia.\n(E) A cross-sectional study finds that high uric acid levels are associated with other\nrisk factors for cardiovascular disease, such as hypertension and diabetes, but does\nnot establish a direct causal link.\n2.2 AgentInstruct Flow for Text Modification\nText modification is the process of editing and refining written content to enhance its quality\nand effectiveness or alter its attributes. This involves correcting spelling and grammar,\nclarifyingideas, reorganizingcontentforbetterflow, adjustingtone, ensuringstyleconsistency,\nfact-checking, removingredundancies, formatting, developingcontent, andadaptingtospecific\naudiences. It is a useful skill for LLMs that help in content writing. While several content\ntransformation agents can be introduced to intentionally modify the text as described earlier,\nwe focus here on showing the how the instructions are created and refined.\nSeed Instruction Generation Flow We have currently compiled a collection of 18types\nof text modification tasks including paraphrasing, expansion, simplification, redacting or\nremoving content, styling, code switching, etc. The full list is in Appendix A.\nWe define an Agent for each of the task type. Each agent takes as input a piece of text and\ncreates several text modification tasks of the associated type. Here we provide an example\ninput and a task created by the Paraphrasing Agent.\n8EXAMPLE: Seed Instruction\nRandom Seed\nApril 6-8, 2017, University of Iowa, Iowa City, USA. Abstracts due December 1, 2016.\nFinance is hard to escape. In recent years, the increasing social impact and interconnection\nof financial discourses, markets, actors, and institutions have","chunk_id":"1d8835c0ce90e56be22873bcf2740a5d","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"HYPERURICEMIA","type":"CONDITION","description":"A condition characterized by high levels of uric acid in the blood, which can increase the risk of cardiovascular disease.","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"CONDITION"},{"name":"HYPOURICEMIA","type":"CONDITION","description":"A condition characterized by low levels of uric acid in the blood, usually asymptomatic but can indicate underlying kidney or liver issues.","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"CONDITION"},{"name":"CARDIOVASCULAR DISEASE","type":"DISEASE","description":"A class of diseases that involve the heart or blood vessels, potentially linked to high levels of uric acid.","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"DISEASE"},{"name":"LABORATORY BLOOD AND URINE TESTS","type":"DIAGNOSTIC METHOD","description":"Tests required to diagnose conditions related to uric acid levels, such as hyperuricemia and hypouricemia.","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"DIAGNOSTIC METHOD"},{"name":"SEED INSTRUCTION GENERATION FLOW","type":"PROCESS","description":"A process involving the compilation of 43 types of reading comprehension questions, targeting various comprehension skills.","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"PROCESS"},{"name":"READING COMPREHENSION QUESTIONS","type":"QUESTION TYPE","description":"Questions designed to assess different levels of comprehension, including literal, critical, and evaluative comprehension.","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"QUESTION TYPE"},{"name":"AGENTS","type":"SYSTEM COMPONENT","description":"Defined entities targeting specific categories of comprehension questions, generating questions based on predefined types.","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"SYSTEM COMPONENT"},{"name":"CONTENT TRANSFORMATION AGENT","type":"SYSTEM COMPONENT","description":"An agent that determines which subset of question-generating agents to engage based on the content.","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"SYSTEM COMPONENT"},{"name":"PASSAGE-QUESTION PAIRS","type":"DATA","description":"Pairs of text passages and corresponding questions generated for use in subsequent stages of the process.","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"DATA"},{"name":"INSTRUCTION REFINEMENT FLOW","type":"PROCESS","description":"A process involving suggester-editor agents to refine (passage, question) pairs by modifying passages, questions, or answer choices.","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"PROCESS"},{"name":"SUGGESTER-EDITOR AGENTS","type":"SYSTEM COMPONENT","description":"Agents that refine (passage, question) pairs by making them unanswerable, altering answers, or adding complexity.","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"SYSTEM COMPONENT"},{"name":"TEXT MODIFICATION","type":"PROCESS","description":"The process of editing and refining written content to enhance quality, effectiveness, or alter attributes.","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"PROCESS"},{"name":"TEXT MODIFICATION TASKS","type":"TASK","description":"Tasks such as paraphrasing, expansion, simplification, redacting, styling, and code switching, aimed at modifying text.","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"TASK"},{"name":"PARAPHRASING AGENT","type":"SYSTEM COMPONENT","description":"An agent that takes a piece of text and creates paraphrased versions as part of text modification tasks.\nAn agent that takes a piece of text and creates paraphrased versions as part of text modification tasks.)(\"relationship\"","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"SYSTEM COMPONENT"},{"name":"URIC ACID","type":"","description":"","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"KIDNEY OR LIVER ISSUES","type":"","description":"\nUnderlying health problems that can be indicated by low levels of uric acid.","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"CONDITION"},{"name":"LIFESTYLE CHOICES","type":"FACTOR","description":"Choices such as alcohol consumption and physical inactivity that can affect uric acid levels.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"APPENDIX A","type":"DOCUMENT SECTION","description":"A section in the document that lists types of reading comprehension questions and text modification tasks.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"STRENGTHEN TYPE QUESTION","type":"QUESTION TYPE","description":"A type of question designed to assess the ability to identify information that strengthens an argument.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"HYPOTHETICAL STUDY","type":"DATA","description":"A suggested study or finding used to add complexity to a question by requiring inference.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"GENETIC PREDISPOSITION","type":"FACTOR","description":"A suggested genetic factor that could be correlated with increased cardiovascular events and hyperuricemia.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"DISTRACTOR OPTION","type":"QUESTION COMPONENT","description":"An answer choice designed to confuse the test-taker by appearing relevant but not directly related to the question.","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"EDITOR AGENT","type":"SYSTEM COMPONENT","description":"An agent that modifies passages, questions, or answer choices to refine the (passage, question) pairs.)(\"relationship\"","source_id":"1d8835c0ce90e56be22873bcf2740a5d"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"HYPERURICEMIA\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">A condition characterized by high levels of uric acid in the blood, which can increase the risk of cardiovascular disease.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">CONDITION<\/data>    <\/node>    <node id=\"HYPOURICEMIA\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">A condition characterized by low levels of uric acid in the blood, usually asymptomatic but can indicate underlying kidney or liver issues.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">CONDITION<\/data>    <\/node>    <node id=\"CARDIOVASCULAR DISEASE\">      <data key=\"d0\">DISEASE<\/data>      <data key=\"d1\">A class of diseases that involve the heart or blood vessels, potentially linked to high levels of uric acid.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">DISEASE<\/data>    <\/node>    <node id=\"LABORATORY BLOOD AND URINE TESTS\">      <data key=\"d0\">DIAGNOSTIC METHOD<\/data>      <data key=\"d1\">Tests required to diagnose conditions related to uric acid levels, such as hyperuricemia and hypouricemia.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">DIAGNOSTIC METHOD<\/data>    <\/node>    <node id=\"SEED INSTRUCTION GENERATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">A process involving the compilation of 43 types of reading comprehension questions, targeting various comprehension skills.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"READING COMPREHENSION QUESTIONS\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Questions designed to assess different levels of comprehension, including literal, critical, and evaluative comprehension.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"AGENTS\">      <data key=\"d0\">SYSTEM COMPONENT<\/data>      <data key=\"d1\">Defined entities targeting specific categories of comprehension questions, generating questions based on predefined types.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">SYSTEM COMPONENT<\/data>    <\/node>    <node id=\"CONTENT TRANSFORMATION AGENT\">      <data key=\"d0\">SYSTEM COMPONENT<\/data>      <data key=\"d1\">An agent that determines which subset of question-generating agents to engage based on the content.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">SYSTEM COMPONENT<\/data>    <\/node>    <node id=\"PASSAGE-QUESTION PAIRS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Pairs of text passages and corresponding questions generated for use in subsequent stages of the process.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"INSTRUCTION REFINEMENT FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">A process involving suggester-editor agents to refine (passage, question) pairs by modifying passages, questions, or answer choices.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"SUGGESTER-EDITOR AGENTS\">      <data key=\"d0\">SYSTEM COMPONENT<\/data>      <data key=\"d1\">Agents that refine (passage, question) pairs by making them unanswerable, altering answers, or adding complexity.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">SYSTEM COMPONENT<\/data>    <\/node>    <node id=\"TEXT MODIFICATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The process of editing and refining written content to enhance quality, effectiveness, or alter attributes.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"TEXT MODIFICATION TASKS\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Tasks such as paraphrasing, expansion, simplification, redacting, styling, and code switching, aimed at modifying text.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"PARAPHRASING AGENT\">      <data key=\"d0\">SYSTEM COMPONENT<\/data>      <data key=\"d1\">An agent that takes a piece of text and creates paraphrased versions as part of text modification tasks.An agent that takes a piece of text and creates paraphrased versions as part of text modification tasks.)(\"relationship\"<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">SYSTEM COMPONENT<\/data>    <\/node>    <node id=\"URIC ACID\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"KIDNEY OR LIVER ISSUES\">      <data key=\"d0\" \/>      <data key=\"d1\">Underlying health problems that can be indicated by low levels of uric acid.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">CONDITION<\/data>    <\/node>    <node id=\"LIFESTYLE CHOICES\">      <data key=\"d0\">FACTOR<\/data>      <data key=\"d1\">Choices such as alcohol consumption and physical inactivity that can affect uric acid levels.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"APPENDIX A\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">A section in the document that lists types of reading comprehension questions and text modification tasks.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"STRENGTHEN TYPE QUESTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">A type of question designed to assess the ability to identify information that strengthens an argument.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"HYPOTHETICAL STUDY\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">A suggested study or finding used to add complexity to a question by requiring inference.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"GENETIC PREDISPOSITION\">      <data key=\"d0\">FACTOR<\/data>      <data key=\"d1\">A suggested genetic factor that could be correlated with increased cardiovascular events and hyperuricemia.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"DISTRACTOR OPTION\">      <data key=\"d0\">QUESTION COMPONENT<\/data>      <data key=\"d1\">An answer choice designed to confuse the test-taker by appearing relevant but not directly related to the question.<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"EDITOR AGENT\">      <data key=\"d0\">SYSTEM COMPONENT<\/data>      <data key=\"d1\">An agent that modifies passages, questions, or answer choices to refine the (passage, question) pairs.)(\"relationship\"<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <edge source=\"HYPERURICEMIA\" target=\"CARDIOVASCULAR DISEASE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Hyperuricemia is associated with an increased risk of cardiovascular disease.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"HYPERURICEMIA\" target=\"GENETIC PREDISPOSITION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Genetic predisposition can be correlated with hyperuricemia.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"HYPOURICEMIA\" target=\"URIC ACID\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Low levels of uric acid in the blood are indicative of hypouricemia.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"HYPOURICEMIA\" target=\"KIDNEY OR LIVER ISSUES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Hypouricemia can indicate underlying kidney or liver issues.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"CARDIOVASCULAR DISEASE\" target=\"GENETIC PREDISPOSITION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Genetic predisposition can be correlated with increased cardiovascular events.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"LABORATORY BLOOD AND URINE TESTS\" target=\"URIC ACID\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Laboratory blood and urine tests are required to diagnose conditions related to uric acid levels.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"READING COMPREHENSION QUESTIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The Seed Instruction Generation Flow involves compiling various types of reading comprehension questions.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"PASSAGE-QUESTION PAIRS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The Seed Instruction Generation Flow results in the creation of passage-question pairs.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION QUESTIONS\" target=\"AGENTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Agents are defined to target specific categories of reading comprehension questions.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION QUESTIONS\" target=\"APPENDIX A\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Appendix A lists types of reading comprehension questions.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION QUESTIONS\" target=\"STRENGTHEN TYPE QUESTION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Strengthen type questions are a category of reading comprehension questions.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"AGENTS\" target=\"CONTENT TRANSFORMATION AGENT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The Content Transformation Agent determines which subset of question-generating agents to engage.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"SUGGESTER-EDITOR AGENTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The Instruction Refinement Flow involves suggester-editor agents to refine passage-question pairs.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"EDITOR AGENT\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The Editor Agent is part of the Instruction Refinement Flow, modifying passages, questions, or answer choices.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"TEXT MODIFICATION\" target=\"TEXT MODIFICATION TASKS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Text modification involves various tasks such as paraphrasing, expansion, and simplification.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"TEXT MODIFICATION TASKS\" target=\"PARAPHRASING AGENT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The Paraphrasing Agent creates paraphrased versions of text as part of text modification tasks.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"TEXT MODIFICATION TASKS\" target=\"APPENDIX A\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Appendix A lists types of text modification tasks.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"URIC ACID\" target=\"LIFESTYLE CHOICES\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Lifestyle choices such as alcohol consumption and physical inactivity can affect uric acid levels.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"STRENGTHEN TYPE QUESTION\" target=\"HYPOTHETICAL STUDY\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">A hypothetical study can be used to add complexity to a strengthen type question.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"STRENGTHEN TYPE QUESTION\" target=\"DISTRACTOR OPTION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">A distractor option is used in strengthen type questions to test the ability to discern relevant from irrelevant information.<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"427e98b00e49b6a8f8649054122dd45b","chunk":" text and\ncreates several text modification tasks of the associated type. Here we provide an example\ninput and a task created by the Paraphrasing Agent.\n8EXAMPLE: Seed Instruction\nRandom Seed\nApril 6-8, 2017, University of Iowa, Iowa City, USA. Abstracts due December 1, 2016.\nFinance is hard to escape. In recent years, the increasing social impact and interconnection\nof financial discourses, markets, actors, and institutions have been understood under the\nbroad concept of financialization. Natascha van der Zwan identifies three distinct research\nstreams that have approached financialization as 1) a regime of accumulation, 2) the influence\nof financial markets and instruments on non-financial corporations as well as the banking\nand finance industry, and 3) a discourse of risk-taking, self-management and self-fulfillment\nthat is transforming people into investing subjects. Some anthropological skeptics, however,\nargue that finance has a far longer genealogy than the financialization literature has to date\nrecognized. For example, in the context of a lengthy human history of creating hierarchy,\nfinancialization may simply be a new technology serving an old purpose.\n... (omitted for brevity)... \u00b7The supply chains of financial products connect different places\nand political projects across the globe. How do such financial instruments transform social\nlife?\nAbstract deadline is December 1, 2016.\n1. Go to americananthro.org and log in. If you don\u2019t have a login id and password, create\none (you do not need to join the American Anthropological Association).\n2. Once you are logged in, look to the left hand column, click on Meeting registration.\n3. Click on register under the SEA 2017 Annual Meeting then follow online prompts to\nregister for the meeting (if we do not accept your abstract and you decide not to attend,\nyou may request that your registration fee be refunded and we would be happy to do so).\n4. Once you are registered, AAA will automatically send you an email inviting you to submit\nan abstract. Click the link and follow the instructions.\nTask:\nRewrite the event details (date, location, abstract deadline) in a more casual tone.\nGiven a random seed, the seed instruction creation flow randomly picks one of the 18agents\nand uses that to obtain a seed (text, text modification instruction) pair.\nInstruction Refinement Flow The refinement flow consists of only one Suggester-Editor\npair. The Suggester-Editor duo increase the complexity of the generated instructions while\ntaking as input the text and the task modification instruction as input. Here we show three\nsuggestions and edits as proposed by the suggester-editor pair.\nEXAMPLE: IRefinement Flow\nSuggestions\nSuggestion 1: Incorporate a fictional narrative. Use a conversational style with colloquial\nlanguage and include a humorous element.\nSuggestion 2: Translate the event details into a poetic format. Maintain accurate infor-\nmation while using rhyming couplets and ensure the tone remains light and\nengaging.\nSuggestion 3: Frame the event details as a social media post. Use internet slang and\nemojis. Keep the message within 280 characters.\nModified Instructions\nInstruction 1: Rewrite the event details (date, location, abstract deadline) as if you\u2019re\ntelling a funny story to a friend using casual and colloquial language,\nwhile incorporating a fictional narrative that still conveys the necessary\ninformation.\nInstruction 2: Transform the event details (date, location, abstract deadline) into a light-\nheartedpoemwithrhymingcouplets, ensuringthattheessentialinformation\nis accurately conveyed in a poetic format.\nInstruction 3: Craft a social media post that includes the event details (date, location,\nabstract deadline) using internet slang, emojis, and a casual tone, while\nkeeping the message concise and within 280 characters.\n92.3 AgentInstruct Flow for Tool Use\nThe task of tool use or API use for LLMs involves enabling models to interact with external\ntools or services; via APIs. This capability allows AI systems to extend their functionality,\naccess external data, and perform actions beyond their native capabilities.\nContent Transformation Flow We use source code snippets or an API description [ 26] as\nthe random seed. If source code snippets has been used as the seed, a content transformation\nagent is used to synthesize an API description from the code snippet. The goal of the\nContent Transformation Flow is to synthesize list of APIs from the random seed. API lists\nare created by either: (1) using an API retrieval agent that iteratively searches for similar\ncode to expand the API list or (2) the agent uses the LLM to hypothesize other APIs present\nin the library.\nThe following figure provides an example of the library reconstruction scenario.\nEXAMPLE: Content Transformation Flow [Library Reconstruction]\nSeed\n{\n\"name\": \"View All Food Items\",\n\"description\": \"The request enables clients to obtain a detailed list of food items,\ncomplete with nutritional profiles such as calorie count, protein, fat, ...\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"limit\": {\n\"type\": \"NUMBER\",\n\"description\": \"limit the length of response\"\n}\n},\n\"required\": []\n}\n}\nReconstructed Remaining APIs from the Library\n[\n{\n\"name\": \"Search Food Items\",\n\"description\": \"Allows clients to search for food items by name and retrieve a list of\nmatching items ...\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"query\": {\n\"type\": \"string\",\n\"description\": \"The name or partial name of the food item to search for.\"\n},\n\"limit\": {\n\"type\": \"number\",\n\"description\": \"Optional parameter to limit the number of search results\nreturned.\"\n","chunk_id":"427e98b00e49b6a8f8649054122dd45b","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"SEED INSTRUCTION","type":"INSTRUCTION","description":"An initial instruction used to generate tasks, such as rewriting event details in a more casual tone","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"INSTRUCTION"},{"name":"RANDOM SEED","type":"CONCEPT","description":"A randomly selected element used to initiate a process, such as generating a seed instruction","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"CONCEPT"},{"name":"UNIVERSITY OF IOWA","type":"LOCATION","description":"The location of the event mentioned in the text, specifically in Iowa City, USA","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"LOCATION"},{"name":"FINANCIALIZATION","type":"CONCEPT","description":"A broad concept describing the increasing social impact and interconnection of financial discourses, markets, actors, and institutions","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"CONCEPT"},{"name":"NATASCHA VAN DER ZWAN","type":"PERSON","description":"A researcher who identifies three distinct research streams that approach financialization","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"PERSON"},{"name":"ANTHROPOLOGICAL SKEPTICS","type":"GROUP","description":"A group of anthropologists who argue that finance has a longer genealogy than recognized by financialization literature","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"GROUP"},{"name":"SUPPLY CHAINS OF FINANCIAL PRODUCTS","type":"CONCEPT","description":"The networks that connect different places and political projects across the globe through financial instruments","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"CONCEPT"},{"name":"AMERICAN ANTHROPOLOGICAL ASSOCIATION","type":"ORGANIZATION","description":"An organization that hosts the SEA 2017 Annual Meeting and provides a platform for submitting abstracts","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"ORGANIZATION"},{"name":"SEA 2017 ANNUAL MEETING","type":"EVENT","description":"An annual meeting organized by the American Anthropological Association, held in April 2017 at the University of Iowa","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"EVENT"},{"name":"SUGGESTER-EDITOR PAIR","type":"AGENT","description":"A duo responsible for increasing the complexity of generated instructions by providing suggestions and edits","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"AGENT"},{"name":"INSTRUCTION REFINEMENT FLOW","type":"PROCESS","description":"A process that involves refining instructions by incorporating suggestions and edits from a suggester-editor pair","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"PROCESS"},{"name":"TOOL USE","type":"TASK","description":"The task of enabling models to interact with external tools or services via APIs","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"TASK"},{"name":"CONTENT TRANSFORMATION FLOW","type":"PROCESS","description":"A process that involves synthesizing API descriptions from source code snippets or other seeds","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"PROCESS"},{"name":"API RETRIEVAL AGENT","type":"AGENT","description":"An agent that searches for similar code to expand the list of APIs","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"AGENT"},{"name":"LLM","type":"TECHNOLOGY","description":"Large Language Models used to hypothesize other APIs present in a library","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"TECHNOLOGY"},{"name":"VIEW ALL FOOD ITEMS","type":"API","description":"An API that enables clients to obtain a detailed list of food items, complete with nutritional profiles","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"API"},{"name":"SEARCH FOOD ITEMS","type":"API","description":"An API that allows clients to search for food items by name and retrieve a list of matching items","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"API"},{"name":"LIBRARY RECONSTRUCTION","type":"SCENARIO","description":"A scenario in which a content transformation flow is used to reconstruct a list of APIs from a given seed\nA scenario in which a content transformation flow is used to reconstruct a list of APIs from a given seed)(\"relationship\"","source_id":"427e98b00e49b6a8f8649054122dd45b","entity_type":"SCENARIO"},{"name":"APRIL 6-8, 2017","type":"DATE","description":"The dates on which the SEA 2017 Annual Meeting was held","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"DECEMBER 1, 2016","type":"DATE","description":"The deadline for submitting abstracts for the SEA 2017 Annual Meeting","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"MEETING REGISTRATION","type":"TASK","description":"The process of registering for the SEA 2017 Annual Meeting through the American Anthropological Association's website","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"INSTRUCTION 1","type":"INSTRUCTION","description":"An instruction to rewrite event details in a casual and colloquial language, incorporating a fictional narrative","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"INSTRUCTION 2","type":"INSTRUCTION","description":"An instruction to transform event details into a light-hearted poem with rhyming couplets","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"INSTRUCTION 3","type":"INSTRUCTION","description":"An instruction to craft a social media post with event details using internet slang, emojis, and a casual tone","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"SUGGESTION 1","type":"SUGGESTION","description":"A suggestion to incorporate a fictional narrative and use a conversational style with colloquial language and humor","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"SUGGESTION 2","type":"SUGGESTION","description":"A suggestion to translate event details into a poetic format with rhyming couplets","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"SUGGESTION 3","type":"SUGGESTION","description":"A suggestion to frame event details as a social media post using internet slang and emojis","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"API DESCRIPTION","type":"DESCRIPTION","description":"A detailed description of an API, including its name, purpose, and parameters","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"NUTRITIONAL PROFILES","type":"DATA","description":"Information about the nutritional content of food items, such as calorie count, protein, and fat)(\"relationship\"","source_id":"427e98b00e49b6a8f8649054122dd45b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"SEED INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">An initial instruction used to generate tasks, such as rewriting event details in a more casual tone<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">INSTRUCTION<\/data>    <\/node>    <node id=\"RANDOM SEED\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A randomly selected element used to initiate a process, such as generating a seed instruction<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"UNIVERSITY OF IOWA\">      <data key=\"d0\">LOCATION<\/data>      <data key=\"d1\">The location of the event mentioned in the text, specifically in Iowa City, USA<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">LOCATION<\/data>    <\/node>    <node id=\"FINANCIALIZATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A broad concept describing the increasing social impact and interconnection of financial discourses, markets, actors, and institutions<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"NATASCHA VAN DER ZWAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A researcher who identifies three distinct research streams that approach financialization<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANTHROPOLOGICAL SKEPTICS\">      <data key=\"d0\">GROUP<\/data>      <data key=\"d1\">A group of anthropologists who argue that finance has a longer genealogy than recognized by financialization literature<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">GROUP<\/data>    <\/node>    <node id=\"SUPPLY CHAINS OF FINANCIAL PRODUCTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The networks that connect different places and political projects across the globe through financial instruments<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"AMERICAN ANTHROPOLOGICAL ASSOCIATION\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">An organization that hosts the SEA 2017 Annual Meeting and provides a platform for submitting abstracts<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"SEA 2017 ANNUAL MEETING\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">An annual meeting organized by the American Anthropological Association, held in April 2017 at the University of Iowa<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"SUGGESTER-EDITOR PAIR\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">A duo responsible for increasing the complexity of generated instructions by providing suggestions and edits<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">AGENT<\/data>    <\/node>    <node id=\"INSTRUCTION REFINEMENT FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">A process that involves refining instructions by incorporating suggestions and edits from a suggester-editor pair<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">The task of enabling models to interact with external tools or services via APIs<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">A process that involves synthesizing API descriptions from source code snippets or other seeds<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"API RETRIEVAL AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">An agent that searches for similar code to expand the list of APIs<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">AGENT<\/data>    <\/node>    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Large Language Models used to hypothesize other APIs present in a library<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"VIEW ALL FOOD ITEMS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">An API that enables clients to obtain a detailed list of food items, complete with nutritional profiles<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">API<\/data>    <\/node>    <node id=\"SEARCH FOOD ITEMS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">An API that allows clients to search for food items by name and retrieve a list of matching items<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">API<\/data>    <\/node>    <node id=\"LIBRARY RECONSTRUCTION\">      <data key=\"d0\">SCENARIO<\/data>      <data key=\"d1\">A scenario in which a content transformation flow is used to reconstruct a list of APIs from a given seedA scenario in which a content transformation flow is used to reconstruct a list of APIs from a given seed)(\"relationship\"<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>      <data key=\"d3\">SCENARIO<\/data>    <\/node>    <node id=\"APRIL 6-8, 2017\">      <data key=\"d0\">DATE<\/data>      <data key=\"d1\">The dates on which the SEA 2017 Annual Meeting was held<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"DECEMBER 1, 2016\">      <data key=\"d0\">DATE<\/data>      <data key=\"d1\">The deadline for submitting abstracts for the SEA 2017 Annual Meeting<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"MEETING REGISTRATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">The process of registering for the SEA 2017 Annual Meeting through the American Anthropological Association's website<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"INSTRUCTION 1\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">An instruction to rewrite event details in a casual and colloquial language, incorporating a fictional narrative<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"INSTRUCTION 2\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">An instruction to transform event details into a light-hearted poem with rhyming couplets<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"INSTRUCTION 3\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">An instruction to craft a social media post with event details using internet slang, emojis, and a casual tone<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"SUGGESTION 1\">      <data key=\"d0\">SUGGESTION<\/data>      <data key=\"d1\">A suggestion to incorporate a fictional narrative and use a conversational style with colloquial language and humor<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"SUGGESTION 2\">      <data key=\"d0\">SUGGESTION<\/data>      <data key=\"d1\">A suggestion to translate event details into a poetic format with rhyming couplets<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"SUGGESTION 3\">      <data key=\"d0\">SUGGESTION<\/data>      <data key=\"d1\">A suggestion to frame event details as a social media post using internet slang and emojis<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"API DESCRIPTION\">      <data key=\"d0\">DESCRIPTION<\/data>      <data key=\"d1\">A detailed description of an API, including its name, purpose, and parameters<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"NUTRITIONAL PROFILES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Information about the nutritional content of food items, such as calorie count, protein, and fat)(\"relationship\"<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <edge source=\"SEED INSTRUCTION\" target=\"RANDOM SEED\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The Seed Instruction is generated using a Random Seed<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"UNIVERSITY OF IOWA\" target=\"SEA 2017 ANNUAL MEETING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The SEA 2017 Annual Meeting was held at the University of Iowa<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"FINANCIALIZATION\" target=\"NATASCHA VAN DER ZWAN\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Natascha van der Zwan identifies three research streams related to financialization<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"FINANCIALIZATION\" target=\"ANTHROPOLOGICAL SKEPTICS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Anthropological skeptics argue against the current understanding of financialization<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"FINANCIALIZATION\" target=\"SUPPLY CHAINS OF FINANCIAL PRODUCTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Supply chains of financial products are part of the broader concept of financialization<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"AMERICAN ANTHROPOLOGICAL ASSOCIATION\" target=\"SEA 2017 ANNUAL MEETING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The American Anthropological Association organizes the SEA 2017 Annual Meeting<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"SEA 2017 ANNUAL MEETING\" target=\"APRIL 6-8, 2017\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The SEA 2017 Annual Meeting was held on April 6-8, 2017<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"SEA 2017 ANNUAL MEETING\" target=\"DECEMBER 1, 2016\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The abstract deadline for the SEA 2017 Annual Meeting was December 1, 2016<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"SEA 2017 ANNUAL MEETING\" target=\"MEETING REGISTRATION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Meeting registration is required to attend the SEA 2017 Annual Meeting<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"SUGGESTER-EDITOR PAIR\" target=\"INSTRUCTION REFINEMENT FLOW\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The Suggester-Editor Pair is part of the Instruction Refinement Flow<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"TOOL USE\" target=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Tool use is a task within the Content Transformation Flow<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"API RETRIEVAL AGENT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The API Retrieval Agent is used in the Content Transformation Flow to expand the list of APIs<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"LLM\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">LLMs are used in the Content Transformation Flow to hypothesize other APIs<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"VIEW ALL FOOD ITEMS\" target=\"LIBRARY RECONSTRUCTION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The \"View All Food Items\" API is part of the Library Reconstruction scenario<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"VIEW ALL FOOD ITEMS\" target=\"API DESCRIPTION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The \"View All Food Items\" API includes a detailed description<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"VIEW ALL FOOD ITEMS\" target=\"NUTRITIONAL PROFILES\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The \"View All Food Items\" API provides nutritional profiles<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"SEARCH FOOD ITEMS\" target=\"LIBRARY RECONSTRUCTION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The \"Search Food Items\" API is part of the Library Reconstruction scenario<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"SEARCH FOOD ITEMS\" target=\"API DESCRIPTION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The \"Search Food Items\" API includes a detailed description<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"INSTRUCTION 1\" target=\"SUGGESTION 1\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Instruction 1 is based on Suggestion 1<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"INSTRUCTION 2\" target=\"SUGGESTION 2\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Instruction 2 is based on Suggestion 2<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"INSTRUCTION 3\" target=\"SUGGESTION 3\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Instruction 3 is based on Suggestion 3<\/data>      <data key=\"d6\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"0922646b93a124514ce2a267d961d229","chunk":" Remaining APIs from the Library\n[\n{\n\"name\": \"Search Food Items\",\n\"description\": \"Allows clients to search for food items by name and retrieve a list of\nmatching items ...\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"query\": {\n\"type\": \"string\",\n\"description\": \"The name or partial name of the food item to search for.\"\n},\n\"limit\": {\n\"type\": \"number\",\n\"description\": \"Optional parameter to limit the number of search results\nreturned.\"\n}\n},\n\"required\": [\n\"query\"\n]\n}\n},\n10{\n\"name\": \"Get Food Item Details\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Create Meal Plan\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Update Food Item\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Track User Meal\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Get Dietary Recommendations\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Add New Food Item\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Delete Food Item\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Get User Nutritional Stats\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n}\n]\nSeed Instruction Creation Flow The seed instruction creation flow, consume the list of\nAPIs and employs variety of agents to create several tasks of the following types:\n1. Tasks that require the use of a single API:\n11(a) Tasks where the input supplies all necessary parameters.\n(b) Tasks where the input includes superfluous parameters.\n(c) Tasks where the input is missing some required parameters.\n2. Tasks that necessitate the use of multiple APIs:\n(a) Tasks where the input provides all necessary parameters.\n(b) Tasks where the input is missing some required parameters.\n3. Tasks that require a single API which is not listed among the available APIs.\n4.Tasks that require multiple APIs but lack some of the essential APIs in the provided\nlist.\nRefinement Flow The goal of the Refinement flow is to increase the complexity, for e.g.,\nlooking at the task and the conversation and to suggest refinements to increase the number\nof steps required to solve the task.\nThe following figure provides an example of a multi-turn conversation created by the Agent-\nInstruct flow.\nEXAMPLE: Instruction Data Created by AgentInstruct]\nSystem Message\nAs an AI assistant, your role is to assist users in achieving their desired outcomes. You have\nthe capability to utilize a variety of APIs, and at each step of the process, you are required\nto produce a markdown output.\nAt any given moment, you have two choices to proceed:\nChoice 1: Initiate an API Call If you choose to call an API, you must generate a makdown\nincluding the \"name\" of the API and the necessary \"parameters\" for the call. After creating\nthe response, you must wait for the system to execute the API with the provided parameters\nand return the results to you. The markdown you output should follow this structure:\nACTION_TYPE : API_CALL\nACTION: name(parameter_name=value, another_parameter_name=value, ....)\nChoice 2: Conclude the Process There are times when you may decide to end the process,\neither because a) you have successfully obtained the answer\nb) you have determined that the appropriate APIs are not available.\nc) you discovered that user did not provide values for some required parameter\nIn case you successfully obtained the answer use FINAL_ANSWER as the action name\nwith only one parameter being the final answer. In case you decided to stop due to other\nreasons, use FAILED as the action with one parameter describing the reason you decided to\nstop. Overall you should follow this structure:\nACTION_TYPE : STOP\nACTION: [ACTION_NAME]([VALUE])\nBelow is a list of APIs that you have access to:\n[\nAPIs from the Library construction example both the seed and the\ngenerated ones\n]\nUSER:\nI want to start a new diet plan for the next week. My goal is to consume around 1500\ncalories per day, and I prefer vegetarian meals. I\u2019ll be eating three meals a day. Can you\ncreate a meal plan for me? Also, I\u2019d like to track what I eat every day and get some new\nfood recommendations. At the end of the week, I want to see my nutritional summary. Oh,\nand I found this great recipe for a Quinoa Salad that I want to add to your database. I also\nnoticed that the calorie count for \u2018Chana Masala\u2019 seems off; it should be lower. And please\nremove \u2019Butter Chicken\u2019 from my list; I\u2019ve stopped eating it.\n12ASSISTANT:\nTo assist you with your request, I will perform the following actions:\n1. Create a meal plan based on your dietary preferences (vegetarian) and caloric goal (1500\ncalories per day) for three meals a day.\n2. Provide you with a way to track your daily meals.\n3. Offer you new food recommendations.\n4. Generate a nutritional summary at the end of the week. 5. Add the Quinoa Salad recipe\nto the database.\n6. Update the nutritional information for \u2019Chana Masala\u2019.\n7. Remove \u2019Butter Chicken\u2019 from the database.\nLet\u2019s start by creating a vegetarian meal plan for you with a caloric goal of 1500 calories\nper day and three meals a day.\n\u2018 \u2018 \u2018markdown\nACTION_TYPE : API_CALL\nACTION: Create Meal Plan(dietary_preferences=[\"vegetarian \"] ,\ncaloric_goal=1500, num_meals=3)\n\u2018 \u2018 \u2018\nUSER:\n{\n\"status\": \"success\",\n\"meal_plan\": {\n\"day_1\": {\n\"breakfast\": {\n\"","chunk_id":"0922646b93a124514ce2a267d961d229","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"GET FOOD ITEM DETAILS","type":"API","description":"Provides detailed information about a specific food item","source_id":"0922646b93a124514ce2a267d961d229","entity_type":"API"},{"name":"CREATE MEAL PLAN","type":"API","description":"Enables the creation of a meal plan based on specified dietary preferences and caloric goals","source_id":"0922646b93a124514ce2a267d961d229","entity_type":"API"},{"name":"UPDATE FOOD ITEM","type":"API","description":"Allows updating the details of an existing food item","source_id":"0922646b93a124514ce2a267d961d229","entity_type":"API"},{"name":"TRACK USER MEAL","type":"API","description":"Enables tracking of user meals","source_id":"0922646b93a124514ce2a267d961d229","entity_type":"API"},{"name":"GET DIETARY RECOMMENDATIONS","type":"API","description":"Provides dietary recommendations based on user preferences and goals","source_id":"0922646b93a124514ce2a267d961d229","entity_type":"API"},{"name":"ADD NEW FOOD ITEM","type":"API","description":"Allows adding a new food item to the database","source_id":"0922646b93a124514ce2a267d961d229","entity_type":"API"},{"name":"DELETE FOOD ITEM","type":"API","description":"Enables the deletion of a food item from the database","source_id":"0922646b93a124514ce2a267d961d229","entity_type":"API"},{"name":"GET USER NUTRITIONAL STATS","type":"API","description":"Provides nutritional statistics for a user","source_id":"0922646b93a124514ce2a267d961d229","entity_type":"API"},{"name":"SEED INSTRUCTION CREATION FLOW","type":"PROCESS","description":"A process that consumes a list of APIs and employs various agents to create several types of tasks","source_id":"0922646b93a124514ce2a267d961d229","entity_type":"PROCESS"},{"name":"REFINEMENT FLOW","type":"PROCESS","description":"A process aimed at increasing the complexity of tasks by suggesting refinements to increase the number of steps required to solve the task","source_id":"0922646b93a124514ce2a267d961d229","entity_type":"PROCESS"},{"name":"AGENT-INSTRUCT FLOW","type":"PROCESS","description":"A process that creates multi-turn conversations for task completion","source_id":"0922646b93a124514ce2a267d961d229","entity_type":"PROCESS"},{"name":"SYSTEM MESSAGE","type":"INSTRUCTION","description":"A message that provides guidelines for an AI assistant to assist users in achieving their desired outcomes using various APIs","source_id":"0922646b93a124514ce2a267d961d229","entity_type":"INSTRUCTION"},{"name":"USER","type":"ACTOR","description":"The individual who interacts with the AI assistant to achieve specific goals, such as creating a meal plan, tracking meals, and updating food items","source_id":"0922646b93a124514ce2a267d961d229","entity_type":"ACTOR"},{"name":"ASSISTANT","type":"ACTOR","description":"The AI assistant responsible for helping the user achieve their goals by utilizing various APIs","source_id":"0922646b93a124514ce2a267d961d229","entity_type":"ACTOR"},{"name":"SEARCH FOOD ITEMS","type":"","description":"","source_id":"0922646b93a124514ce2a267d961d229"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GET FOOD ITEM DETAILS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Provides detailed information about a specific food item<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>      <data key=\"d3\">API<\/data>    <\/node>    <node id=\"CREATE MEAL PLAN\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Enables the creation of a meal plan based on specified dietary preferences and caloric goals<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>      <data key=\"d3\">API<\/data>    <\/node>    <node id=\"UPDATE FOOD ITEM\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Allows updating the details of an existing food item<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>      <data key=\"d3\">API<\/data>    <\/node>    <node id=\"TRACK USER MEAL\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Enables tracking of user meals<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>      <data key=\"d3\">API<\/data>    <\/node>    <node id=\"GET DIETARY RECOMMENDATIONS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Provides dietary recommendations based on user preferences and goals<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>      <data key=\"d3\">API<\/data>    <\/node>    <node id=\"ADD NEW FOOD ITEM\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Allows adding a new food item to the database<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>      <data key=\"d3\">API<\/data>    <\/node>    <node id=\"DELETE FOOD ITEM\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Enables the deletion of a food item from the database<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>      <data key=\"d3\">API<\/data>    <\/node>    <node id=\"GET USER NUTRITIONAL STATS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Provides nutritional statistics for a user<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>      <data key=\"d3\">API<\/data>    <\/node>    <node id=\"SEED INSTRUCTION CREATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">A process that consumes a list of APIs and employs various agents to create several types of tasks<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"REFINEMENT FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">A process aimed at increasing the complexity of tasks by suggesting refinements to increase the number of steps required to solve the task<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"AGENT-INSTRUCT FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">A process that creates multi-turn conversations for task completion<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"SYSTEM MESSAGE\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">A message that provides guidelines for an AI assistant to assist users in achieving their desired outcomes using various APIs<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>      <data key=\"d3\">INSTRUCTION<\/data>    <\/node>    <node id=\"USER\">      <data key=\"d0\">ACTOR<\/data>      <data key=\"d1\">The individual who interacts with the AI assistant to achieve specific goals, such as creating a meal plan, tracking meals, and updating food items<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>      <data key=\"d3\">ACTOR<\/data>    <\/node>    <node id=\"ASSISTANT\">      <data key=\"d0\">ACTOR<\/data>      <data key=\"d1\">The AI assistant responsible for helping the user achieve their goals by utilizing various APIs<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>      <data key=\"d3\">ACTOR<\/data>    <\/node>    <node id=\"SEARCH FOOD ITEMS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <edge source=\"GET FOOD ITEM DETAILS\" target=\"SEARCH FOOD ITEMS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Search Food Items API can be used to find food items, and Get Food Item Details API can provide detailed information about those items<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"GET FOOD ITEM DETAILS\" target=\"ASSISTANT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The assistant can use the Get Food Item Details API to provide detailed information about food items<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"CREATE MEAL PLAN\" target=\"TRACK USER MEAL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Create Meal Plan API can be used to create a meal plan, and Track User Meal API can be used to track the meals in that plan<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"CREATE MEAL PLAN\" target=\"GET DIETARY RECOMMENDATIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Create Meal Plan API can be used to create a meal plan, and Get Dietary Recommendations API can provide recommendations for that plan<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"CREATE MEAL PLAN\" target=\"ASSISTANT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The assistant can use the Create Meal Plan API to create a meal plan for the user<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"UPDATE FOOD ITEM\" target=\"ADD NEW FOOD ITEM\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Add New Food Item API can be used to add new items, and Update Food Item API can be used to update those items<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"UPDATE FOOD ITEM\" target=\"DELETE FOOD ITEM\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Delete Food Item API can be used to remove items, and Update Food Item API can be used to modify existing items<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"UPDATE FOOD ITEM\" target=\"ASSISTANT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The assistant can use the Update Food Item API to update food item details<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"TRACK USER MEAL\" target=\"GET USER NUTRITIONAL STATS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Get User Nutritional Stats API can provide nutritional statistics based on the meals tracked using Track User Meal API<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"TRACK USER MEAL\" target=\"ASSISTANT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The assistant can use the Track User Meal API to track the user's meals<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"GET DIETARY RECOMMENDATIONS\" target=\"ASSISTANT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The assistant can use the Get Dietary Recommendations API to provide dietary recommendations<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"ADD NEW FOOD ITEM\" target=\"ASSISTANT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The assistant can use the Add New Food Item API to add new food items<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"DELETE FOOD ITEM\" target=\"ASSISTANT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The assistant can use the Delete Food Item API to delete food items<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"GET USER NUTRITIONAL STATS\" target=\"ASSISTANT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The assistant can use the Get User Nutritional Stats API to provide nutritional statistics<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION CREATION FLOW\" target=\"REFINEMENT FLOW\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Seed Instruction Creation Flow generates initial tasks, and Refinement Flow increases the complexity of these tasks<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"AGENT-INSTRUCT FLOW\" target=\"SYSTEM MESSAGE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Agent-Instruct Flow creates multi-turn conversations, and System Message provides guidelines for these conversations<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"USER\" target=\"ASSISTANT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The user interacts with the assistant to achieve specific goals<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"ASSISTANT\" target=\"SEARCH FOOD ITEMS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The assistant can use the Search Food Items API to help the user find food items<\/data>      <data key=\"d6\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"09cb89de3b77d765983cff25b7d74a1a","chunk":" database.\nLet\u2019s start by creating a vegetarian meal plan for you with a caloric goal of 1500 calories\nper day and three meals a day.\n\u2018 \u2018 \u2018markdown\nACTION_TYPE : API_CALL\nACTION: Create Meal Plan(dietary_preferences=[\"vegetarian \"] ,\ncaloric_goal=1500, num_meals=3)\n\u2018 \u2018 \u2018\nUSER:\n{\n\"status\": \"success\",\n\"meal_plan\": {\n\"day_1\": {\n\"breakfast\": {\n\"food_items\": [\"Oatmeal with Fruits\", \"Almond Milk\"],\n\"total_calories\": 350\n},\n\"lunch\": {\n\"food_items\": [\"Chickpea Salad\", \"Whole Wheat Bread\"],\n\"total_calories\": 500\n},\n\"dinner\": {\n\"food_items\": [\"Mixed Vegetable Stir Fry\", \"Brown Rice\"],\n\"total_calories\": 650\n}\n},\n\"day_2\": {\n...\n},\n...\n}\n}\nASSISTANT:\nYour meal plan for the next week has been successfully created. Here\u2019s a brief overview:\n- **Day 1**: Oatmeal with fruits and almond milk for breakfast, chickpea salad with whole\nwheat bread for lunch, and mixed vegetable stir fry with brown rice for dinner.\n- **Day 2**: .... ....\nNext, let\u2019s proceed to add the Quinoa Salad recipe to the database. Please provide the\nnutritional information for the Quinoa Salad so I can proceed with the addition.\nAs for updating the \u2019Chana Masala\u2019 and removing \u2019Butter Chicken\u2019, I will need\ntheir unique identifiers (food_id) to make those changes. If you have those identi-\nfiers, please provide them. If not, I can search for these items in the database to find\nthe correct identifiers. Would you like me to search for \u2019Chana Masala\u2019 and \u2019Butter Chicken\u2019?\nUSER:\n...(truncated)\n133 Orca-3\n3.1 Dataset Description\nWe implemented an AgentInstruct Flow for 17 different capabilities as described in Table 1.\nWe created a collection of approximately 22 million instructions aimed at teaching the\naforementioned skills. We have used unstructured text and code files sampled from\nKnowledgePile[ 7], AutoMathText[ 38], a subset of openstax and a subset of apache-2.0\nlicensed source code files from [ 4]. The dataset covers variety of skills, as detailed in Table 1.\nUsing unstructured content as seeds for instruction data generation has several benefits.\nFirst, there is abundance of such data enabling the generation of large-scale and diverse\ninstruction data. Additionally, it enables us to avoid using any benchmark-specific data as\nseeds and hence focus on optimizing for a capability, not for a specific benchmark.\nIn addition to the 22 million instructions, we have incorporated approximately 3.8 million\npaired instructions sourced from Orca-1[ 21], Orca-2[ 18], Orca-Math[ 19] and samples from\nother publicly available sources such as [ 5,37,10,30]. We refer to this data as Orca-2.5-\ndataset.\nThe culmination of these datasets results in approximately 25.8 million paired instructions,\nall of which are incorporated into the training of Orca-3. Furthermore, we have trained a\nseparate model, referred to as Orca-2.5, using the 3.8 million instructions (Orca-2.5-dataset).\nThe purpose of this is to compare and evaluate the impact of the 22 million instructions\ncurated through AgentInstruct.\n3.2 Training Details\nWe use the 25.8 million paired instructions described earlier to finetune Mistral-7b-v0.1.\nWe choose this model because the it makes the weights publicly available for the base\n(no-instruction-tuned) version, with a permissive license allowing easy redistribution. We\nrefer to the finetuned model ( Mistral-7b-v0.1 finetuned on AgentInstruct dataset) as Orca-3.\nEach pair in the dataset undergoes a tokenization process using the Mistral tokenizer,\nensuring a maximum sequence length of 8192 with packing. To guarantee that the training\nloss is calculated based only on the response conditioned on the prompt, label masking is\napplied. Weight decay was set at 0.1\nThe finetuneing used 19 NVIDIA A100 nodes, or 152 NVIDIA A100 GPUs, each with a\nbatch size of 10. We used AdamW optimizer with an initial learning rate of 8e-6 and a a\ncosine learning rate schedule. We also used a linear learning rate warm-up during the initial\n500 steps. The model was trained for three epochs and the training process concluded after\napproximately 200 hours.\n4 Evaluation Results\n4.1 Orca-Bench\nThe Orca-Bench dataset serves as a held-out test set, consisting of 100 samples from each of\nthe 17 skills for which data was curated using AgentInstruct, except for the Open Domain\nQuestion Answering (ODQA) category, where we created two test sets. The first subset,\nreferred to as ODQA, consists of 100 questions originated from the initial seed instruction\nphase. The second subset, termed Complex ODQA, includes more intricate questions\ndeveloped during the refinement phase.\nWe evaluated the performance of all baselines using the Orca-Bench dataset. These were\nscored relative to GPT-4 on a scale ranging from 0 to 10. It is worth noting that some of\nthe entries within Orca-Bench involve multiple exchanges. To illustrate, let a multi-turn\ninteraction in Orca-Bench be denoted by","chunk_id":"09cb89de3b77d765983cff25b7d74a1a","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"CALORIC GOAL","type":"GOAL","description":"A target of 1500 calories per day for the meal plan","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"GOAL"},{"name":"MEAL PLAN","type":"PLAN","description":"A structured plan that includes specific meals for each day, designed to meet dietary preferences and caloric goals","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"PLAN"},{"name":"DAY 1","type":"DAY","description":"The first day of the meal plan, which includes specific meals for breakfast, lunch, and dinner","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"DAY"},{"name":"BREAKFAST","type":"MEAL","description":"The first meal of the day, which includes oatmeal with fruits and almond milk, totaling 350 calories","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"MEAL"},{"name":"LUNCH","type":"MEAL","description":"The second meal of the day, which includes chickpea salad and whole wheat bread, totaling 500 calories","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"MEAL"},{"name":"DINNER","type":"MEAL","description":"The third meal of the day, which includes mixed vegetable stir fry and brown rice, totaling 650 calories","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"MEAL"},{"name":"QUINOA SALAD","type":"FOOD ITEM","description":"A salad made with quinoa, for which nutritional information is requested to be added to the database","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"FOOD ITEM"},{"name":"CHANA MASALA","type":"FOOD ITEM","description":"A dish made with chickpeas, for which the unique identifier (food_id) is requested for updating in the database","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"FOOD ITEM"},{"name":"BUTTER CHICKEN","type":"FOOD ITEM","description":"A dish made with chicken, for which the unique identifier (food_id) is requested for removal from the database","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"FOOD ITEM"},{"name":"ORCA-3","type":"MODEL","description":"A model trained using approximately 25.8 million paired instructions, including data from Orca-1, Orca-2, Orca-Math, and other sources","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"MODEL"},{"name":"AGENTINSTRUCT","type":"TECHNIQUE","description":"A method used to generate instruction data from unstructured content, aimed at teaching various skills","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"TECHNIQUE"},{"name":"KNOWLEDGEPILE","type":"DATASET","description":"A dataset used as a source for unstructured text and code files in the creation of instruction data","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"DATASET"},{"name":"AUTOMATHTEXT","type":"DATASET","description":"A dataset used as a source for unstructured text and code files in the creation of instruction data","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"DATASET"},{"name":"OPENSTAX","type":"DATASET","description":"A subset of openstax content used as a source for unstructured text and code files in the creation of instruction data","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"DATASET"},{"name":"APACHE-2.0 LICENSED SOURCE CODE","type":"DATASET","description":"A subset of source code files licensed under Apache-2.0, used as a source for unstructured text and code files in the creation of instruction data","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"DATASET"},{"name":"ORCA-2.5-DATASET","type":"DATASET","description":"A dataset consisting of approximately 3.8 million paired instructions sourced from Orca-1, Orca-2, Orca-Math, and other publicly available sources","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"DATASET"},{"name":"MISTRAL-7B-V0.1","type":"MODEL","description":"A base model with publicly available weights, finetuned using the AgentInstruct dataset to create Orca-3","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"MODEL"},{"name":"NVIDIA A100","type":"HARDWARE","description":"A type of GPU used in the training process of the model, with 19 nodes or 152 GPUs in total","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"HARDWARE"},{"name":"ADAMW OPTIMIZER","type":"OPTIMIZER","description":"An optimization algorithm used in the training process, with an initial learning rate of 8e-6 and a cosine learning rate schedule","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"OPTIMIZER"},{"name":"ORCA-BENCH","type":"DATASET","description":"A held-out test set consisting of 100 samples from each of the 17 skills curated using AgentInstruct, used for evaluation","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"DATASET"},{"name":"GPT-4","type":"MODEL","description":"A model used as a baseline for scoring the performance of other models on the Orca-Bench dataset","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"MODEL"},{"name":"ODQA","type":"CATEGORY","description":"Open Domain Question Answering, a category within the Orca-Bench dataset with two test sets: ODQA and Complex ODQA","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"CATEGORY"},{"name":"COMPLEX ODQA","type":"CATEGORY","description":"A subset of the ODQA category within the Orca-Bench dataset, consisting of more intricate questions developed during the refinement phase\nA subset of the ODQA category within the Orca-Bench dataset, consisting of more intricate questions developed during the refinement phase)(\"relationship\"","source_id":"09cb89de3b77d765983cff25b7d74a1a","entity_type":"CATEGORY"},{"name":"VEGETARIAN MEAL PLAN","type":"","description":"","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"DATABASE","type":"","description":"","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"INSTRUCTION DATA","type":"DATA","description":"Data generated from unstructured content to teach various skills, used in the training of models like Orca-3","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"INSTRUCTION TUNING","type":"TECHNIQUE","description":"A method of finetuning models using instruction data to improve their performance on specific tasks","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"TOKENIZATION","type":"PROCESS","description":"The process of converting text into tokens, used in the preparation of data for model training","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"LABEL MASKING","type":"TECHNIQUE","description":"A method used during training to ensure that the loss is calculated based only on the response conditioned on the prompt","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"WEIGHT DECAY","type":"HYPERPARAMETER","description":"A regularization technique used during training to prevent overfitting, set at 0.1 in the training of Orca-3","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"COSINE LEARNING RATE SCHEDULE","type":"TECHNIQUE","description":"A method of adjusting the learning rate during training, used in the training of Orca-3","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"LINEAR LEARNING RATE WARM-UP","type":"TECHNIQUE","description":"A method of gradually increasing the learning rate during the initial steps of training, used in the training of Orca-3","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"EPOCH","type":"TERM","description":"A complete pass through the training dataset, with Orca-3 being trained for three epochs","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"TRAINING LOSS","type":"METRIC","description":"A measure of the error during training, calculated based on the response conditioned on the prompt","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"MULTI-TURN INTERACTION","type":"TERM","description":"A type of interaction involving multiple exchanges, used in some entries within the Orca-Bench dataset)(\"relationship\"","source_id":"09cb89de3b77d765983cff25b7d74a1a"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"CALORIC GOAL\">      <data key=\"d0\">GOAL<\/data>      <data key=\"d1\">A target of 1500 calories per day for the meal plan<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">GOAL<\/data>    <\/node>    <node id=\"MEAL PLAN\">      <data key=\"d0\">PLAN<\/data>      <data key=\"d1\">A structured plan that includes specific meals for each day, designed to meet dietary preferences and caloric goals<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">PLAN<\/data>    <\/node>    <node id=\"DAY 1\">      <data key=\"d0\">DAY<\/data>      <data key=\"d1\">The first day of the meal plan, which includes specific meals for breakfast, lunch, and dinner<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">DAY<\/data>    <\/node>    <node id=\"BREAKFAST\">      <data key=\"d0\">MEAL<\/data>      <data key=\"d1\">The first meal of the day, which includes oatmeal with fruits and almond milk, totaling 350 calories<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">MEAL<\/data>    <\/node>    <node id=\"LUNCH\">      <data key=\"d0\">MEAL<\/data>      <data key=\"d1\">The second meal of the day, which includes chickpea salad and whole wheat bread, totaling 500 calories<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">MEAL<\/data>    <\/node>    <node id=\"DINNER\">      <data key=\"d0\">MEAL<\/data>      <data key=\"d1\">The third meal of the day, which includes mixed vegetable stir fry and brown rice, totaling 650 calories<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">MEAL<\/data>    <\/node>    <node id=\"QUINOA SALAD\">      <data key=\"d0\">FOOD ITEM<\/data>      <data key=\"d1\">A salad made with quinoa, for which nutritional information is requested to be added to the database<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">FOOD ITEM<\/data>    <\/node>    <node id=\"CHANA MASALA\">      <data key=\"d0\">FOOD ITEM<\/data>      <data key=\"d1\">A dish made with chickpeas, for which the unique identifier (food_id) is requested for updating in the database<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">FOOD ITEM<\/data>    <\/node>    <node id=\"BUTTER CHICKEN\">      <data key=\"d0\">FOOD ITEM<\/data>      <data key=\"d1\">A dish made with chicken, for which the unique identifier (food_id) is requested for removal from the database<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">FOOD ITEM<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A model trained using approximately 25.8 million paired instructions, including data from Orca-1, Orca-2, Orca-Math, and other sources<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method used to generate instruction data from unstructured content, aimed at teaching various skills<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"KNOWLEDGEPILE\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset used as a source for unstructured text and code files in the creation of instruction data<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"AUTOMATHTEXT\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset used as a source for unstructured text and code files in the creation of instruction data<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"OPENSTAX\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A subset of openstax content used as a source for unstructured text and code files in the creation of instruction data<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"APACHE-2.0 LICENSED SOURCE CODE\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A subset of source code files licensed under Apache-2.0, used as a source for unstructured text and code files in the creation of instruction data<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"ORCA-2.5-DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset consisting of approximately 3.8 million paired instructions sourced from Orca-1, Orca-2, Orca-Math, and other publicly available sources<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"MISTRAL-7B-V0.1\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A base model with publicly available weights, finetuned using the AgentInstruct dataset to create Orca-3<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"NVIDIA A100\">      <data key=\"d0\">HARDWARE<\/data>      <data key=\"d1\">A type of GPU used in the training process of the model, with 19 nodes or 152 GPUs in total<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">HARDWARE<\/data>    <\/node>    <node id=\"ADAMW OPTIMIZER\">      <data key=\"d0\">OPTIMIZER<\/data>      <data key=\"d1\">An optimization algorithm used in the training process, with an initial learning rate of 8e-6 and a cosine learning rate schedule<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">OPTIMIZER<\/data>    <\/node>    <node id=\"ORCA-BENCH\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A held-out test set consisting of 100 samples from each of the 17 skills curated using AgentInstruct, used for evaluation<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A model used as a baseline for scoring the performance of other models on the Orca-Bench dataset<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"ODQA\">      <data key=\"d0\">CATEGORY<\/data>      <data key=\"d1\">Open Domain Question Answering, a category within the Orca-Bench dataset with two test sets: ODQA and Complex ODQA<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">CATEGORY<\/data>    <\/node>    <node id=\"COMPLEX ODQA\">      <data key=\"d0\">CATEGORY<\/data>      <data key=\"d1\">A subset of the ODQA category within the Orca-Bench dataset, consisting of more intricate questions developed during the refinement phaseA subset of the ODQA category within the Orca-Bench dataset, consisting of more intricate questions developed during the refinement phase)(\"relationship\"<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>      <data key=\"d3\">CATEGORY<\/data>    <\/node>    <node id=\"VEGETARIAN MEAL PLAN\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"DATABASE\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"INSTRUCTION DATA\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Data generated from unstructured content to teach various skills, used in the training of models like Orca-3<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"INSTRUCTION TUNING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method of finetuning models using instruction data to improve their performance on specific tasks<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"TOKENIZATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The process of converting text into tokens, used in the preparation of data for model training<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"LABEL MASKING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method used during training to ensure that the loss is calculated based only on the response conditioned on the prompt<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"WEIGHT DECAY\">      <data key=\"d0\">HYPERPARAMETER<\/data>      <data key=\"d1\">A regularization technique used during training to prevent overfitting, set at 0.1 in the training of Orca-3<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"COSINE LEARNING RATE SCHEDULE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method of adjusting the learning rate during training, used in the training of Orca-3<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"LINEAR LEARNING RATE WARM-UP\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method of gradually increasing the learning rate during the initial steps of training, used in the training of Orca-3<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"EPOCH\">      <data key=\"d0\">TERM<\/data>      <data key=\"d1\">A complete pass through the training dataset, with Orca-3 being trained for three epochs<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"TRAINING LOSS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A measure of the error during training, calculated based on the response conditioned on the prompt<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"MULTI-TURN INTERACTION\">      <data key=\"d0\">TERM<\/data>      <data key=\"d1\">A type of interaction involving multiple exchanges, used in some entries within the Orca-Bench dataset)(\"relationship\"<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <edge source=\"MEAL PLAN\" target=\"VEGETARIAN MEAL PLAN\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The vegetarian meal plan is a specific type of meal plan<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"DAY 1\" target=\"BREAKFAST\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Day 1 of the meal plan includes oatmeal with fruits and almond milk for breakfast<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"DAY 1\" target=\"LUNCH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Day 1 of the meal plan includes chickpea salad and whole wheat bread for lunch<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"DAY 1\" target=\"DINNER\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Day 1 of the meal plan includes mixed vegetable stir fry and brown rice for dinner<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"QUINOA SALAD\" target=\"DATABASE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The Quinoa Salad recipe is to be added to the database<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"CHANA MASALA\" target=\"DATABASE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The Chana Masala dish is to be updated in the database<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"BUTTER CHICKEN\" target=\"DATABASE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The Butter Chicken dish is to be removed from the database<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AGENTINSTRUCT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 is trained using data generated through the AgentInstruct method<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"KNOWLEDGEPILE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3's training data includes unstructured text and code files from KnowledgePile<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AUTOMATHTEXT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3's training data includes unstructured text and code files from AutoMathText<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"OPENSTAX\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3's training data includes a subset of openstax content<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"APACHE-2.0 LICENSED SOURCE CODE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3's training data includes a subset of Apache-2.0 licensed source code files<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ORCA-2.5-DATASET\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3's training data includes approximately 3.8 million paired instructions from the Orca-2.5-dataset<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MISTRAL-7B-V0.1\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 is a finetuned version of the Mistral-7b-v0.1 model<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"NVIDIA A100\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The training of Orca-3 used 152 NVIDIA A100 GPUs<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ADAMW OPTIMIZER\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The training of Orca-3 used the AdamW optimizer<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ORCA-BENCH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3's performance is evaluated using the Orca-Bench dataset<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"INSTRUCTION DATA\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3 is trained using instruction data generated from unstructured content<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"INSTRUCTION TUNING\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3 is finetuned using instruction tuning<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"TOKENIZATION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The training data for Orca-3 undergoes tokenization<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"LABEL MASKING\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Label masking is applied during the training of Orca-3<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"WEIGHT DECAY\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Weight decay is used during the training of Orca-3<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"COSINE LEARNING RATE SCHEDULE\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">A cosine learning rate schedule is used during the training of Orca-3<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"LINEAR LEARNING RATE WARM-UP\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">A linear learning rate warm-up is used during the initial steps of training Orca-3<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"EPOCH\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3 is trained for three epochs<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"TRAINING LOSS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Training loss is calculated during the training of Orca-3<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"MISTRAL-7B-V0.1\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Mistral-7b-v0.1 is finetuned using the AgentInstruct dataset<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"GPT-4\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">GPT-4 is used as a baseline for scoring the performance on the Orca-Bench dataset<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"ODQA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">ODQA is a category within the Orca-Bench dataset<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"COMPLEX ODQA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Complex ODQA is a subset of the ODQA category within the Orca-Bench dataset<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"MULTI-TURN INTERACTION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Some entries within the Orca-Bench dataset involve multi-turn interactions<\/data>      <data key=\"d6\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"bd4eb9459bc29b4c2da4658914fd4635","chunk":" instruction\nphase. The second subset, termed Complex ODQA, includes more intricate questions\ndeveloped during the refinement phase.\nWe evaluated the performance of all baselines using the Orca-Bench dataset. These were\nscored relative to GPT-4 on a scale ranging from 0 to 10. It is worth noting that some of\nthe entries within Orca-Bench involve multiple exchanges. To illustrate, let a multi-turn\ninteraction in Orca-Bench be denoted by the sequence (system message, user 1, assistant,\nuser 2, assistant, ...), where each turn is crafted by GPT4 (teacher). For every user iinput, we\ngenerateacorresponding studentresponse, which isconditionedontheprecedingconversation\nhistory as established by the teacher. We then evaluate the student\u2019s generated response\n14Figure 4: Performance Comparison between Baselines and Orca3 Checkpoints. Scores are\nrelative to GPT-4 with the outer circle denoting GPT-4\u2019s score of 10.\nagainst the original teacher\u2019s response, rating each on a scale from 0 to 10. To calculate a\nstudent\u2019s overall score, we sum the student\u2019s individual scores and divide this total by the\nsum of the teacher\u2019s scores. This ratio is then multiplied by 10 to normalize the student\u2019s\nfinal score to a 0 to 10 scale.\nAgentInstruct\u2019s objective is to synthesize a large and diverse corpus of data with varying\ndegrees of difficulty. An efficacious execution of this strategy should yield a dataset against\nwhich baseline models like Orca-2.5, Mistral-Instruct-7b, and ChatGPT score substantially\nbelow 10, demonstrating their relative inferiority to GPT-4\u2014a model that is designated as\nthe benchmark with a score of 10. The performance comparison, as depicted in Figure 4,\nillustrates the comparative analysis between baseline models and Orca-3 . This figure shows\nthe notable enhancement in a broad spectrum of capabilities during post-training, enabled\nby the AgentInstruct data.\nTable 2 encapsulates the average (macro) scores across all assessed dimensions. On average,\nincluding orca-3 after each training epoch, the inclusion of AgentInstruct data has led to\na performance augmentation of 33.94% over the Orca 2.5 baseline and an enhancement of\n14.92% over Mistral-Instruct-7B.\n4.2 Benchmark Results\nWe evaluate Orca-3 against 5 baseline models including Orca-2.5, Mistral-7B-Instruct-v0.3,\nLLAMA3-8B-Instruct, GPT-3.5-turbo and GPT-4 on the following benchmarks:\n15Model Orca-Bench Score\nOrca-2.5 7.13\nMistral-Instruct-7B 8.31\nChatGPT 8.13\nOrca-3 (checkpoint epoch 1) 9.35\nOrca-3 (checkpoint epoch 2) 9.49\nOrca-3 9.55\nTable 2: Average Performance of Different Models on Orca-Bench. Scores are computed on\na scale of 0 to 10; 10 being the score of GPT4.\nModelOrca-3\n-7BOrca-2.5\n-7BMistral-\n7B-\nInstructLLAMA3-\n8B\ninstructGPT-3.5-\nturboGPT-4\nAGIEval 56.80 (+40%) 42.71 40.52 47.17 50.91 61.99\nMMLU 69.95 (+19%) 60.34 58.61 63.44 68.26 67.07\nARC 92.47 (+12%) 86.39 82.72 85.74 92.0 93.35\nBBH 61.83 (+38%) 48.63 44.71 54.97 54.17 76.06\nGPQA 28.12 (-4%) 27.68 29.46 28.12 27.9 33.93\nDROP 71.14 (+22%) 65.19 58.12 68.44 67.15 67.36\nGSM8K 83.09 (+54%) 74.3 54.06 77.48 78.1\u221786.88\nFOFO 84.01 (+12%) 66.19 75.3 79.35 76.92 87.45\nIFEval 49.54 (+2%) 45.29 48.61 - 58.6 79.3\nMT-Bench 8.20 (+9%) 7.15 7.53 7.99 8.01 9.04\nAlpacaEval 24.80 (+45%) 13.47 17.1 22.9 22.7 55\nInfoBench 84.30 (+4%) 79.6 81 - 86.7 89.4\nEQBench\nMetric-v2 91.36(+4%) 88.03 87.75 88.67 88.95 93.32\nMetric-v1 50.28 (+28%) 38.8 39.27 42.13 42.05 55.98\nTable 3: Performance of Orca-3 and other baseline models on all the benchmarks. Note:\nGPT-3.5-turbo scores for GSM8","chunk_id":"bd4eb9459bc29b4c2da4658914fd4635","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"ORCA-BENCH","type":"DATASET","description":"A dataset used to evaluate the performance of various models, scored relative to GPT-4 on a scale from 0 to 10","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"DATASET"},{"name":"GPT-4","type":"MODEL","description":"A state-of-the-art language model used as a benchmark with a score of 10 in the Orca-Bench dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"MODEL"},{"name":"AGENTINSTRUCT","type":"SYSTEM","description":"A system designed to synthesize a large and diverse corpus of data with varying degrees of difficulty, aiming to create a challenging dataset for baseline models","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"SYSTEM"},{"name":"ORCA-2.5","type":"MODEL","description":"A baseline model evaluated using the Orca-Bench dataset, scoring 7.13 on average","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"MODEL"},{"name":"MISTRAL-INSTRUCT-7B","type":"MODEL","description":"A baseline model evaluated using the Orca-Bench dataset, scoring 8.31 on average","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"MODEL"},{"name":"CHATGPT","type":"MODEL","description":"A baseline model evaluated using the Orca-Bench dataset, scoring 8.13 on average","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"MODEL"},{"name":"ORCA-3","type":"MODEL","description":"An advanced model evaluated using the Orca-Bench dataset, showing notable enhancement in capabilities and scoring 9.55 on average","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"MODEL"},{"name":"LLAMA3-8B-INSTRUCT","type":"MODEL","description":"A baseline model evaluated using the Orca-Bench dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"MODEL"},{"name":"GPT-3.5-TURBO","type":"MODEL","description":"A baseline model evaluated using the Orca-Bench dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"MODEL"},{"name":"AGIEVAL","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of various models, with Orca-3 scoring 56.80","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"MMLU","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of various models, with Orca-3 scoring 69.95","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"ARC","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of various models, with Orca-3 scoring 92.47","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"BBH","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of various models, with Orca-3 scoring 61.83","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"GPQA","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of various models, with Orca-3 scoring 28.12","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"DROP","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of various models, with Orca-3 scoring 71.14","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"GSM8K","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of various models, with Orca-3 scoring 83.09","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"FOFO","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of various models, with Orca-3 scoring 84.01","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"IFEVAL","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of various models, with Orca-3 scoring 49.54","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"MT-BENCH","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of various models, with Orca-3 scoring 8.20","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"ALPACAEVAL","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of various models, with Orca-3 scoring 24.80","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"INFOBENCH","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of various models, with Orca-3 scoring 84.30","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"EQBENCH","type":"BENCHMARK","description":"A benchmark used to evaluate the performance of various models, with Orca-3 scoring 91.36","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"COMPLEX ODQA","type":"","description":"","source_id":"bd4eb9459bc29b4c2da4658914fd4635"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ORCA-BENCH\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset used to evaluate the performance of various models, scored relative to GPT-4 on a scale from 0 to 10<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A state-of-the-art language model used as a benchmark with a score of 10 in the Orca-Bench dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system designed to synthesize a large and diverse corpus of data with varying degrees of difficulty, aiming to create a challenging dataset for baseline models<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"ORCA-2.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A baseline model evaluated using the Orca-Bench dataset, scoring 7.13 on average<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"MISTRAL-INSTRUCT-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A baseline model evaluated using the Orca-Bench dataset, scoring 8.31 on average<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"CHATGPT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A baseline model evaluated using the Orca-Bench dataset, scoring 8.13 on average<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">An advanced model evaluated using the Orca-Bench dataset, showing notable enhancement in capabilities and scoring 9.55 on average<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"LLAMA3-8B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A baseline model evaluated using the Orca-Bench dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A baseline model evaluated using the Orca-Bench dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"AGIEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of various models, with Orca-3 scoring 56.80<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of various models, with Orca-3 scoring 69.95<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of various models, with Orca-3 scoring 92.47<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"BBH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of various models, with Orca-3 scoring 61.83<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"GPQA\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of various models, with Orca-3 scoring 28.12<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of various models, with Orca-3 scoring 71.14<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of various models, with Orca-3 scoring 83.09<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"FOFO\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of various models, with Orca-3 scoring 84.01<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"IFEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of various models, with Orca-3 scoring 49.54<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"MT-BENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of various models, with Orca-3 scoring 8.20<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"ALPACAEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of various models, with Orca-3 scoring 24.80<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"INFOBENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of various models, with Orca-3 scoring 84.30<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"EQBENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate the performance of various models, with Orca-3 scoring 91.36<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"COMPLEX ODQA\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <edge source=\"ORCA-BENCH\" target=\"COMPLEX ODQA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Complex ODQA is a subset of the Orca-Bench dataset<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"GPT-4\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">GPT-4 is used as a benchmark model in the Orca-Bench dataset<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"ORCA-2.5\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct data led to a performance augmentation of 33.94% over the Orca-2.5 baseline<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"MISTRAL-INSTRUCT-7B\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct data led to a performance enhancement of 14.92% over Mistral-Instruct-7B<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-2.5\" target=\"ORCA-3\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 shows a performance improvement over Orca-2.5<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"MISTRAL-INSTRUCT-7B\" target=\"ORCA-3\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 shows a performance improvement over Mistral-Instruct-7B<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"CHATGPT\" target=\"ORCA-3\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 shows a performance improvement over ChatGPT<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"LLAMA3-8B-INSTRUCT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 is evaluated against LLAMA3-8B-Instruct<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GPT-3.5-TURBO\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 is evaluated against GPT-3.5-turbo<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AGIEVAL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 scored 56.80 on the AGIEval benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MMLU\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 scored 69.95 on the MMLU benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ARC\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 scored 92.47 on the ARC benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"BBH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 scored 61.83 on the BBH benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GPQA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 scored 28.12 on the GPQA benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"DROP\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 scored 71.14 on the DROP benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GSM8K\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 scored 83.09 on the GSM8K benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"FOFO\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 scored 84.01 on the FOFO benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"IFEVAL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 scored 49.54 on the IFEval benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MT-BENCH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 scored 8.20 on the MT-Bench benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ALPACAEVAL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 scored 24.80 on the AlpacaEval benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"INFOBENCH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 scored 84.30 on the InfoBench benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"EQBENCH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 scored 91.36 on the EQBench benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"86f77e15d41cbd0cb33f635ccb2cb66b","chunk":"EQBench\nMetric-v2 91.36(+4%) 88.03 87.75 88.67 88.95 93.32\nMetric-v1 50.28 (+28%) 38.8 39.27 42.13 42.05 55.98\nTable 3: Performance of Orca-3 and other baseline models on all the benchmarks. Note:\nGPT-3.5-turbo scores for GSM8K are taken from [ 1]. We show in (+x%) the relative\nimprovement over Mistral-7b-Instruct.\n\u2022AGIEval : AGIEval [ 39] is a human-centric benchmark that evaluates a model\u2019s\nabilities in tasks pertinent to human-cognition and problem-solving. It evaluates\nhow well models perform in answering questions from human-centric standardized\nexams such as SAT, LSAT and math competitions.\n\u2022MMLU : Massive Multitask Language Understanding (MMLU) [ 9] benchmark\nmeasures a model\u2019s multitask understanding. The benchmark includes approximately\n16000 multiple choice questions covering a wide range of 57 academic subjects such\nas maths, philosphy, medicine, psychology, computer-science, law etc. testing both\ngeneral and specialized knowledge of the model being tested.\n\u2022ARC: The AI2 Reasoning Challenge (ARC) [ 2] benchmark, developed by AllenAI,\nmeasures the reasoning, commonsense knowledge and deep comprehension abilities\nof language models. The test set contains 3548 multiple-choice questions that are\ndivided into 2 sets : Easy(2376) and Challenge(1172).\n\u2022BBH: Big Bench Hard [ 31] consists of a set of 23 tasks selected from the broader\nBig-Bench benchmark spanning a wide array of academic subjects requiring complex,\nmulti-step reasoning.\n\u2022GPQA: Graduate-level Google-Proof Q&A [ 27] is a challenging benchmark of 448\nhigh-quality and extremely difficult multiple-choice questions created by domain\nexperts(pursuing PhDs in their domains) in biology, chemistry and physics.\n\u2022DROP: Discrete Reasoning over Paragraphs [ 6] is a Reading Comprehension bench-\nmark requiring the models to resolve references in questions and perform discrete\noperations over them such as sorting, counting, addition etc.\n16\u2022GSM8K : Grade School Math 8K [ 3] is a dataset of high quality diverse grade\nschool math word problems. The test split of the dataset consists of 1.32K problems\nrequiring between 2 and 8 steps to solve primarily involving sequence of elementary\ncalculations using basic arithmetic operations.\n\u2022FoFo: Format Following [ 34] is a benchmark that evaluates a model\u2019s ability to\nfollow complex, domain-specific formats. The benchmark tests format following on\na diverse range of real-world formats and instructions from domains like Healthcare,\nFinance, Marketing etc. created using AI-Human collaboration.\n\u2022IFEval: Instruction-Following Evaluation [ 40] is a benchmark measuring a model\u2019s\nability to follow natural language instructions using a set of 500 prompts covering\n25 types of \u2019verifiable instructions\u2019 where each prompt can contain one or more of\nthese instructions.\n\u2022MT-Bench : MT-Bench [ 16] benchmark is specifically designed to assess the com-\npetence of chat assistants in multi-turn conversations using GPT-4 as the evaluator.\n\u2022AlpacaEval : AlpacaEval [ 14] is a benchmark specifically designed for chat-based\nlanguage models to assess their abilities in the context of instruction-following tasks.\nIt is a single-turn benchmark consisting of 805 instructions representative of user\ninteractions on Alpaca web demo.\n\u2022InFoBench : The InFoBench [ 25] benchmark evaluates models instruction fol-\nlowing capability using a new metric called Decomposed Requirements Following\nRatio(DRFR). DRFR breaks complex instructions down into simpler criteria and\nfacilitates analysis of an LLM\u2019s compliance to these decomposed tasks in detail.\nThe benchmark has 500 diverse instructions and 2250 decomposed questions across\nmultiple constraint categories.\n\u2022EQBench : This Emotional Intelligence benchmark [ 23] evaluates aspects of emo-\ntional intelligence in language models. It tests models capabilities to comprehend\nintricate emotions and social interactions by providing a conversation between char-\nacters and then asking the model to predict intensity of emotional states of those\ncharacters. The authors discovered a strong correlation (r=0.97) between EQ-Bench\nand comprehensive multi-domain benchmarks like MMLU.\nThe results for all the baselines on each benchmark are given in table 3. All of the evaluations\nfor Orca-3 and other baselines was done in a zero-shot setting unless mentioned otherwise in\nthe text.\nThe types of tasks\/benchmarks and the corresponding method used to extract answer and\ngenerate metrics is specified in Appendix B.\n4.3 Evaluation: Reading Comprehension\nReading comprehension is a crucial capability for LLMs. It is arguably even more important\nfor Small Language Models (SLMs), as they are better suited as reasoning engines than mere\nretrieval systems. Through targeted training with AgentInstruct, we observe substantial\nimprovement in Mistral\u2019s reading comprehension capabilities (Table ??)\u2014showcasing an 18%\nimprovement over Orca 2.5 and a 21% gain relative to Mistral-Instruct-7b. Furthermore, by\nleveraging this data-driven approach, we have elevated the performance of a 7B model to\nmatch that of GPT-4 on the reading comprehension sections of the Law School Admission\nTests (LSATs), which are considered difficult for human test-takers.\n4.4 Evaluation: Math","chunk_id":"86f77e15d41cbd0cb33f635ccb2cb66b","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"METRIC-V2","type":"METRIC","description":"Metric-v2 is a performance metric used to evaluate models, showing a score of 91.36 with a 4% improvement over a previous version.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"METRIC"},{"name":"METRIC-V1","type":"METRIC","description":"Metric-v1 is a performance metric used to evaluate models, showing a score of 50.28 with a 28% improvement over a previous version.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"METRIC"},{"name":"ORCA-3","type":"MODEL","description":"Orca-3 is a model evaluated on various benchmarks, showing performance improvements over other baseline models.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"MODEL"},{"name":"GPT-3.5-TURBO","type":"MODEL","description":"GPT-3.5-turbo is a model whose scores for GSM8K are referenced in the text.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"MODEL"},{"name":"MISTRAL-7B-INSTRUCT","type":"MODEL","description":"Mistral-7b-Instruct is a model used as a baseline for comparison in performance metrics.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"MODEL"},{"name":"AGIEVAL","type":"BENCHMARK","description":"AGIEval is a human-centric benchmark that evaluates a model\u2019s abilities in tasks pertinent to human cognition and problem-solving, including standardized exams like SAT and LSAT.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"BENCHMARK"},{"name":"MMLU","type":"BENCHMARK","description":"Massive Multitask Language Understanding (MMLU) is a benchmark that measures a model\u2019s multitask understanding across 57 academic subjects, including general and specialized knowledge.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"BENCHMARK"},{"name":"ARC","type":"BENCHMARK","description":"The AI2 Reasoning Challenge (ARC) is a benchmark developed by AllenAI to measure reasoning, commonsense knowledge, and deep comprehension abilities of language models.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"BENCHMARK"},{"name":"BBH","type":"BENCHMARK","description":"Big Bench Hard (BBH) is a set of 23 tasks selected from the broader Big-Bench benchmark, requiring complex, multi-step reasoning.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"BENCHMARK"},{"name":"GPQA","type":"BENCHMARK","description":"Graduate-level Google-Proof Q&A (GPQA) is a challenging benchmark of 448 high-quality, difficult multiple-choice questions created by domain experts in biology, chemistry, and physics.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"BENCHMARK"},{"name":"DROP","type":"BENCHMARK","description":"Discrete Reasoning over Paragraphs (DROP) is a reading comprehension benchmark requiring models to resolve references in questions and perform discrete operations like sorting, counting, and addition.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"BENCHMARK"},{"name":"GSM8K","type":"BENCHMARK","description":"Grade School Math 8K (GSM8K) is a dataset of high-quality, diverse grade school math word problems requiring between 2 and 8 steps to solve.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"BENCHMARK"},{"name":"FOFO","type":"BENCHMARK","description":"Format Following (FoFo) is a benchmark that evaluates a model\u2019s ability to follow complex, domain-specific formats across various real-world domains.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"BENCHMARK"},{"name":"IFEVAL","type":"BENCHMARK","description":"Instruction-Following Evaluation (IFEval) is a benchmark measuring a model\u2019s ability to follow natural language instructions using a set of 500 prompts covering 25 types of verifiable instructions.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"BENCHMARK"},{"name":"MT-BENCH","type":"BENCHMARK","description":"MT-Bench is a benchmark designed to assess the competence of chat assistants in multi-turn conversations using GPT-4 as the evaluator.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"BENCHMARK"},{"name":"ALPACAEVAL","type":"BENCHMARK","description":"AlpacaEval is a benchmark specifically designed for chat-based language models to assess their abilities in instruction-following tasks.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"BENCHMARK"},{"name":"INFOBENCH","type":"BENCHMARK","description":"InFoBench is a benchmark that evaluates models' instruction-following capability using a metric called Decomposed Requirements Following Ratio (DRFR).","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"BENCHMARK"},{"name":"AGENTINSTRUCT","type":"TECHNIQUE","description":"AgentInstruct is a targeted training technique used to improve reading comprehension capabilities in models like Mistral.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"TECHNIQUE"},{"name":"LSAT","type":"EXAM","description":"The Law School Admission Test (LSAT) is a standardized test considered difficult for human test-takers, used to evaluate reading comprehension in models.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"EXAM"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is a model used as a benchmark for comparison in reading comprehension evaluations.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"MODEL"},{"name":"SAT","type":"","description":"\nThe SAT is a standardized test widely used for college admissions in the United States, referenced in the AGIEval benchmark.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"EXAM"},{"name":"MATH COMPETITIONS","type":"","description":"\nMath competitions are referenced in the AGIEval benchmark as part of the tasks used to evaluate models.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"EXAM"},{"name":"ACADEMIC SUBJECTS","type":"","description":"","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"ALLENAI","type":"","description":"\nAllenAI is the organization that developed the AI2 Reasoning Challenge (ARC) benchmark.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"ORGANIZATION"},{"name":"BIG-BENCH","type":"","description":"\nBig-Bench is a broader benchmark from which the Big Bench Hard (BBH) tasks are selected.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"BENCHMARK"},{"name":"DOMAIN EXPERTS","type":"","description":"\nDomain experts, who are pursuing PhDs in their fields, created the questions for the GPQA benchmark.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"PERSON"},{"name":"READING COMPREHENSION","type":"","description":"\nReading comprehension is a crucial capability for language models, especially for Small Language Models (SLMs), and is evaluated using various benchmarks like DROP and LSAT.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"CAPABILITY"},{"name":"GRADE SCHOOL MATH","type":"","description":"","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"DOMAIN-SPECIFIC FORMATS","type":"","description":"","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"NATURAL LANGUAGE INSTRUCTIONS","type":"","description":"","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"INSTRUCTION-FOLLOWING TASKS","type":"","description":"","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"DRFR","type":"","description":"","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"MISTRAL","type":"","description":"\nMistral is a model whose reading comprehension capabilities are improved using the AgentInstruct technique.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b","entity_type":"MODEL"},{"name":"SMALL LANGUAGE MODELS (SLMS)","type":"MODEL","description":"Small Language Models (SLMs) are better suited as reasoning engines than mere retrieval systems, and their reading comprehension capabilities are evaluated in the text.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"ORCA 2.5","type":"MODEL","description":"Orca 2.5 is a model referenced in the text, showing a performance improvement in reading comprehension when compared to Mistral.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"MISTRAL-INSTRUCT-7B","type":"MODEL","description":"Mistral-Instruct-7b is a model referenced in the text, showing a performance improvement in reading comprehension when compared to Mistral.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"LAW SCHOOL ADMISSION TESTS (LSATS)","type":"EXAM","description":"The Law School Admission Tests (LSATs) are used to evaluate reading comprehension in models, and are considered difficult for human test-takers.","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"METRIC-V2\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Metric-v2 is a performance metric used to evaluate models, showing a score of 91.36 with a 4% improvement over a previous version.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"METRIC-V1\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Metric-v1 is a performance metric used to evaluate models, showing a score of 50.28 with a 28% improvement over a previous version.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 is a model evaluated on various benchmarks, showing performance improvements over other baseline models.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5-turbo is a model whose scores for GSM8K are referenced in the text.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"MISTRAL-7B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-7b-Instruct is a model used as a baseline for comparison in performance metrics.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"AGIEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AGIEval is a human-centric benchmark that evaluates a model&#8217;s abilities in tasks pertinent to human cognition and problem-solving, including standardized exams like SAT and LSAT.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Massive Multitask Language Understanding (MMLU) is a benchmark that measures a model&#8217;s multitask understanding across 57 academic subjects, including general and specialized knowledge.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">The AI2 Reasoning Challenge (ARC) is a benchmark developed by AllenAI to measure reasoning, commonsense knowledge, and deep comprehension abilities of language models.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"BBH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Big Bench Hard (BBH) is a set of 23 tasks selected from the broader Big-Bench benchmark, requiring complex, multi-step reasoning.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"GPQA\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Graduate-level Google-Proof Q&amp;A (GPQA) is a challenging benchmark of 448 high-quality, difficult multiple-choice questions created by domain experts in biology, chemistry, and physics.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Discrete Reasoning over Paragraphs (DROP) is a reading comprehension benchmark requiring models to resolve references in questions and perform discrete operations like sorting, counting, and addition.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Grade School Math 8K (GSM8K) is a dataset of high-quality, diverse grade school math word problems requiring between 2 and 8 steps to solve.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"FOFO\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Format Following (FoFo) is a benchmark that evaluates a model&#8217;s ability to follow complex, domain-specific formats across various real-world domains.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"IFEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Instruction-Following Evaluation (IFEval) is a benchmark measuring a model&#8217;s ability to follow natural language instructions using a set of 500 prompts covering 25 types of verifiable instructions.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"MT-BENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MT-Bench is a benchmark designed to assess the competence of chat assistants in multi-turn conversations using GPT-4 as the evaluator.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"ALPACAEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AlpacaEval is a benchmark specifically designed for chat-based language models to assess their abilities in instruction-following tasks.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"INFOBENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">InFoBench is a benchmark that evaluates models' instruction-following capability using a metric called Decomposed Requirements Following Ratio (DRFR).<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">AgentInstruct is a targeted training technique used to improve reading comprehension capabilities in models like Mistral.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"LSAT\">      <data key=\"d0\">EXAM<\/data>      <data key=\"d1\">The Law School Admission Test (LSAT) is a standardized test considered difficult for human test-takers, used to evaluate reading comprehension in models.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">EXAM<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is a model used as a benchmark for comparison in reading comprehension evaluations.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"SAT\">      <data key=\"d0\" \/>      <data key=\"d1\">The SAT is a standardized test widely used for college admissions in the United States, referenced in the AGIEval benchmark.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">EXAM<\/data>    <\/node>    <node id=\"MATH COMPETITIONS\">      <data key=\"d0\" \/>      <data key=\"d1\">Math competitions are referenced in the AGIEval benchmark as part of the tasks used to evaluate models.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">EXAM<\/data>    <\/node>    <node id=\"ACADEMIC SUBJECTS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"ALLENAI\">      <data key=\"d0\" \/>      <data key=\"d1\">AllenAI is the organization that developed the AI2 Reasoning Challenge (ARC) benchmark.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"BIG-BENCH\">      <data key=\"d0\" \/>      <data key=\"d1\">Big-Bench is a broader benchmark from which the Big Bench Hard (BBH) tasks are selected.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"DOMAIN EXPERTS\">      <data key=\"d0\" \/>      <data key=\"d1\">Domain experts, who are pursuing PhDs in their fields, created the questions for the GPQA benchmark.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"READING COMPREHENSION\">      <data key=\"d0\" \/>      <data key=\"d1\">Reading comprehension is a crucial capability for language models, especially for Small Language Models (SLMs), and is evaluated using various benchmarks like DROP and LSAT.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">CAPABILITY<\/data>    <\/node>    <node id=\"GRADE SCHOOL MATH\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"DOMAIN-SPECIFIC FORMATS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"NATURAL LANGUAGE INSTRUCTIONS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"INSTRUCTION-FOLLOWING TASKS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"DRFR\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"MISTRAL\">      <data key=\"d0\" \/>      <data key=\"d1\">Mistral is a model whose reading comprehension capabilities are improved using the AgentInstruct technique.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"SMALL LANGUAGE MODELS (SLMS)\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Small Language Models (SLMs) are better suited as reasoning engines than mere retrieval systems, and their reading comprehension capabilities are evaluated in the text.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"ORCA 2.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca 2.5 is a model referenced in the text, showing a performance improvement in reading comprehension when compared to Mistral.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"MISTRAL-INSTRUCT-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-Instruct-7b is a model referenced in the text, showing a performance improvement in reading comprehension when compared to Mistral.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"LAW SCHOOL ADMISSION TESTS (LSATS)\">      <data key=\"d0\">EXAM<\/data>      <data key=\"d1\">The Law School Admission Tests (LSATs) are used to evaluate reading comprehension in models, and are considered difficult for human test-takers.<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <edge source=\"ORCA-3\" target=\"MISTRAL-7B-INSTRUCT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 shows performance improvements over Mistral-7b-Instruct in various benchmarks.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ORCA 2.5\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3 shows an 18% improvement over Orca 2.5 in reading comprehension.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO\" target=\"GSM8K\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">GPT-3.5-turbo scores for GSM8K are referenced in the text.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"AGIEVAL\" target=\"SAT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AGIEval evaluates models' performance in answering questions from standardized exams like SAT.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"AGIEVAL\" target=\"LSAT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AGIEval evaluates models' performance in answering questions from standardized exams like LSAT.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"AGIEVAL\" target=\"MATH COMPETITIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AGIEval evaluates models' performance in answering questions from math competitions.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"ACADEMIC SUBJECTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">MMLU includes approximately 16000 multiple-choice questions covering 57 academic subjects.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"ARC\" target=\"ALLENAI\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">ARC is a benchmark developed by AllenAI.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"BBH\" target=\"BIG-BENCH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">BBH consists of tasks selected from the broader Big-Bench benchmark.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"GPQA\" target=\"DOMAIN EXPERTS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">GPQA questions are created by domain experts pursuing PhDs in their fields.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"DROP\" target=\"READING COMPREHENSION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">DROP is a reading comprehension benchmark.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"GSM8K\" target=\"GRADE SCHOOL MATH\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">GSM8K is a dataset of grade school math word problems.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"FOFO\" target=\"DOMAIN-SPECIFIC FORMATS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">FoFo evaluates a model&#8217;s ability to follow complex, domain-specific formats.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"IFEVAL\" target=\"NATURAL LANGUAGE INSTRUCTIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">IFEval measures a model&#8217;s ability to follow natural language instructions.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MT-BENCH\" target=\"GPT-4\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">MT-Bench uses GPT-4 as the evaluator for assessing chat assistants.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"ALPACAEVAL\" target=\"INSTRUCTION-FOLLOWING TASKS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AlpacaEval assesses chat-based language models in instruction-following tasks.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"INFOBENCH\" target=\"DRFR\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">InFoBench uses the Decomposed Requirements Following Ratio (DRFR) to evaluate models.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"MISTRAL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct is used to improve Mistral&#8217;s reading comprehension capabilities.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"LSAT\" target=\"READING COMPREHENSION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">LSAT is used to evaluate reading comprehension in models.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"READING COMPREHENSION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">GPT-4 is used as a benchmark for reading comprehension evaluations.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"LAW SCHOOL ADMISSION TESTS (LSATS)\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The Law School Admission Tests (LSATs) are used to evaluate reading comprehension in models.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MISTRAL\" target=\"MISTRAL-INSTRUCT-7B\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Mistral shows a 21% gain relative to Mistral-Instruct-7b in reading comprehension.<\/data>      <data key=\"d6\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"bb87f82e6a9f1d4da6480ec78a0e3701","chunk":"Table ??)\u2014showcasing an 18%\nimprovement over Orca 2.5 and a 21% gain relative to Mistral-Instruct-7b. Furthermore, by\nleveraging this data-driven approach, we have elevated the performance of a 7B model to\nmatch that of GPT-4 on the reading comprehension sections of the Law School Admission\nTests (LSATs), which are considered difficult for human test-takers.\n4.4 Evaluation: Math\nAssessing the reasoning capabilities of AI models can be effectively accomplished through\nmath problem solving. While SLMs have shown considerable improvement in elementary\nmath, their performance typically falters with more complex high school and college-level\nmathematics. Math problems are generated by the Open Domain Question Answering\nand Multiple-Choice Questions Flows. With AgentInstruct, we have managed to enhance\nMistral\u2019s proficiency across a spectrum of difficulties, ranging from elementary to college-level\nmath (Table 5. This has led to a signficant performance boost, with improvements ranging\n17Model Orca-3\n-7BOrca-2.5\n-7BMistral-\n7B-InstructGPT-3.5-\nturboGPT-4\nAGIEval lsat-rc75.84\n(+21%,+20%)62.45 63.2 63.57 72.86\nAGIEval sat-en87.38\n(+13%,+15%)77.18 75.73 82.04 82.52\nAGIEval\ngaokao-english87.25\n(+13%,+17%)77.45 74.84 83.01 87.25\nAGIEval lsat-lr63.14\n(+45%,+36%)43.53 46.27 54.9 68.82\nDROP71.14\n(+9%,+22%)65.19 58.12 67.15 67.36\nAverage76.95\n(+18%,+21%)65.16 63.63 70.13 75.76\nTable 4: Performance of models on reading comprehension based sub-tasks and benchmarks.\nThe figures (x%, y%) adjacent to the Orca-3 results signify the percentage of improvement\ncompared to Orca 2.5 and Mistral-7B-Instruct, respectively.\nModel Orca-3\n-7BOrca-2.5\n-7BMistral-\n7B-InstructGPT-3.5-\nturboGPT-4\nAGIEval math42.90\n(+73%,+168%)24.8 16.0 38.0 57.9\nAGIEval sat-math80.91\n(+34%,+50%)60.45 54.09 67.73 90.0\nBBH multistep\n-arithmetic-two66.80\n(+1418%,+882%)4.4 6.8 46.4 77.2\nMMLU abstract\nalgebra55.00\n(+129%,+104%)24.0 27.0 47.0 70.0\nMMLU college\nmathematics44.00\n(+63%,+44%)30.0 34.0 39.0 62.0\nMMLU high-school\nmathematics66.67\n(+41%,+94%)47.41 34.44 57.04 66.67\nGSM8K83.09\n(+12%,+54%)74.3 54.06 78.1\u221786.88\nTable 5: Performance scores of models on Math benchmarks. Note: GPT-3.5-turbo accuracy\nscores reported for GSM8K are taken from Phi3 paper[ 1]. The figures (x%, y%) adjacent\nto the Orca-3 results signify the percentage of improvement compared to Orca 2.5 and\nMistral-7B-Instruct, respectively.\nfrom 44% to 168% on various popular mathematical benchmarks. It should be emphasized\nthat the objective of Generative Teaching is to teach a skill than generating data to meet a\nspecific benchmark. The effectiveness of AgentInstruct for Generative Teaching is evidenced\nby marked enhancements across a variety of mathematical datasets.\n4.5 Evaluation: Format Following\nFollowing formatting guidelines is essential for language models to be applicable in real-world\nsituations. In all AgentInstruct flows, we ensure that format-following is taught for each\nparticular scenario, by synthesizing, through agents, several formatting guidelines. By doing\nso, we are able to significantly improve (11.5%) Mistral\u2019s ability to follow formats, surpassing\neven the capabilities of Gemini Pro.\n18Model FoFo\nOpen-sourceOrca-3-7B 84.01 (+26.92%,+11.5%)\nOrca-2.5-7B 66.19\nMistral-7B-Instruct 75.3\nClosed-sourceGPT-3.5-turbo 76.92\nGemini Pro 80.25\u2217\nGPT-4 87.45\nTable 6: Performance of Orca-3-7B model and other open and closed-source baselines on\nFoFo benchmark. The figures (x%, y%) adjacent to the Orca-3 results signify the percentage\nof improvement compared to Orca 2.5 and Mistral-7B-Instruct, respectively. Note: The\nscores for Gemini Pro are taken from the original paper [34]\n4.6 Evaluation: Abstractive","chunk_id":"bb87f82e6a9f1d4da6480ec78a0e3701","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"MISTRAL-INSTRUCT-7B","type":"MODEL","description":"Mistral-Instruct-7B is a model used as a baseline for comparison in various performance evaluations, including reading comprehension and math problem-solving tasks","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"MODEL"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is a state-of-the-art language model developed by OpenAI, used as a benchmark for evaluating the performance of other models","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"MODEL"},{"name":"LSAT","type":"EXAM","description":"The Law School Admission Test (LSAT) is a standardized test used for law school admissions, known for its challenging reading comprehension sections","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"EXAM"},{"name":"AGENTINSTRUCT","type":"TECHNIQUE","description":"AgentInstruct is a technique used to enhance the proficiency of models like Mistral across various difficulties, including elementary to college-level math","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"TECHNIQUE"},{"name":"ORCA-3","type":"MODEL","description":"Orca-3 is a model that has shown significant improvements over Orca 2.5 and Mistral-Instruct-7B in various benchmarks, including reading comprehension and math problem-solving tasks","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"MODEL"},{"name":"GPT-3.5-TURBO","type":"MODEL","description":"GPT-3.5-turbo is a version of the GPT-3 model, used as a baseline for comparison in various performance evaluations","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"MODEL"},{"name":"AGIEVAL","type":"BENCHMARK","description":"AGIEval is a benchmark used to evaluate the performance of models on various tasks, including reading comprehension and math problem-solving","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"BENCHMARK"},{"name":"DROPM","type":"BENCHMARK","description":"DROP is a benchmark used to evaluate the performance of models on reading comprehension tasks","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"BENCHMARK"},{"name":"BBH MULTISTEP-ARITHMETIC-TWO","type":"BENCHMARK","description":"BBH multistep-arithmetic-two is a benchmark used to evaluate the performance of models on multi-step arithmetic problems","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"BENCHMARK"},{"name":"MMLU","type":"BENCHMARK","description":"MMLU is a benchmark used to evaluate the performance of models on various mathematical tasks, including abstract algebra and college-level mathematics","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"BENCHMARK"},{"name":"GSM8K","type":"BENCHMARK","description":"GSM8K is a benchmark used to evaluate the performance of models on math problem-solving tasks","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"BENCHMARK"},{"name":"GENERATIVE TEACHING","type":"TECHNIQUE","description":"Generative Teaching is a technique aimed at teaching skills rather than generating data to meet specific benchmarks, evidenced by enhancements across various mathematical datasets","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"TECHNIQUE"},{"name":"FORMAT FOLLOWING","type":"TECHNIQUE","description":"Format Following is a technique used to ensure that language models adhere to specific formatting guidelines, improving their applicability in real-world situations","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"TECHNIQUE"},{"name":"GEMINI PRO","type":"MODEL","description":"Gemini Pro is a model used as a baseline for comparison in format-following tasks","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"MODEL"},{"name":"FOFO","type":"BENCHMARK","description":"FoFo is a benchmark used to evaluate the performance of models on format-following tasks","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"BENCHMARK"},{"name":"ORCA 2.5","type":"","description":"","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"MISTRAL-INSTRUCT-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-Instruct-7B is a model used as a baseline for comparison in various performance evaluations, including reading comprehension and math problem-solving tasks<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is a state-of-the-art language model developed by OpenAI, used as a benchmark for evaluating the performance of other models<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"LSAT\">      <data key=\"d0\">EXAM<\/data>      <data key=\"d1\">The Law School Admission Test (LSAT) is a standardized test used for law school admissions, known for its challenging reading comprehension sections<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">EXAM<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">AgentInstruct is a technique used to enhance the proficiency of models like Mistral across various difficulties, including elementary to college-level math<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 is a model that has shown significant improvements over Orca 2.5 and Mistral-Instruct-7B in various benchmarks, including reading comprehension and math problem-solving tasks<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5-turbo is a version of the GPT-3 model, used as a baseline for comparison in various performance evaluations<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"AGIEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AGIEval is a benchmark used to evaluate the performance of models on various tasks, including reading comprehension and math problem-solving<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"DROPM\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">DROP is a benchmark used to evaluate the performance of models on reading comprehension tasks<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"BBH MULTISTEP-ARITHMETIC-TWO\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">BBH multistep-arithmetic-two is a benchmark used to evaluate the performance of models on multi-step arithmetic problems<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MMLU is a benchmark used to evaluate the performance of models on various mathematical tasks, including abstract algebra and college-level mathematics<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">GSM8K is a benchmark used to evaluate the performance of models on math problem-solving tasks<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"GENERATIVE TEACHING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Generative Teaching is a technique aimed at teaching skills rather than generating data to meet specific benchmarks, evidenced by enhancements across various mathematical datasets<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"FORMAT FOLLOWING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Format Following is a technique used to ensure that language models adhere to specific formatting guidelines, improving their applicability in real-world situations<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"GEMINI PRO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Gemini Pro is a model used as a baseline for comparison in format-following tasks<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"FOFO\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">FoFo is a benchmark used to evaluate the performance of models on format-following tasks<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"ORCA 2.5\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <edge source=\"MISTRAL-INSTRUCT-7B\" target=\"ORCA-3\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 shows significant improvements over Mistral-Instruct-7B in various benchmarks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"MISTRAL-INSTRUCT-7B\" target=\"AGENTINSTRUCT\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct is used to enhance Mistral's proficiency across various difficulties<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"ORCA-3\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3's performance is compared to GPT-4 in various benchmarks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"GENERATIVE TEACHING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct is effective for Generative Teaching, evidenced by enhancements across various mathematical datasets<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"FORMAT FOLLOWING\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Format Following is taught in all AgentInstruct flows to improve model performance<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ORCA 2.5\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 shows significant improvements over Orca 2.5 in various benchmarks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"FORMAT FOLLOWING\" target=\"FOFO\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">FoFo is a benchmark used to evaluate the performance of models on format-following tasks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"FORMAT FOLLOWING\" target=\"GEMINI PRO\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Gemini Pro is used as a baseline for comparison in format-following tasks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"8ee9617c145e19fa95f1f9349bfbe69b","chunk":"-4 87.45\nTable 6: Performance of Orca-3-7B model and other open and closed-source baselines on\nFoFo benchmark. The figures (x%, y%) adjacent to the Orca-3 results signify the percentage\nof improvement compared to Orca 2.5 and Mistral-7B-Instruct, respectively. Note: The\nscores for Gemini Pro are taken from the original paper [34]\n4.6 Evaluation: Abstractive Summarization\nSummarization is an important capability for Language Models, with many models achieving\nhigh quality summarization performance, yet struggling with hallucination. We assessed\nsummarization ability using two key metrics: hallucinations and quality. For this purpose,\nwe utilized GPT4 as our evaluator. The prompts utilized in these evaluations can be found\nin Appendix B. We used the following benchmarks for evaluating summarization abilities:\n\u2022ACI-Bench: The Ambient Clinical Intelligence Benchmark (ACI-Bench) [ 32] is a\ndataset designed for benchmarking automatic report generation from doctor-patient\nconversations. The test set comprises 120 data points.\n\u2022InstruSum: A dataset [ 15] for evaluating the generation capabilities LLMs for\ninstruction-controllable summarization. It consists of 100 datapoints.\n\u2022Orca-Sum: A newly created benchmark to evaluate LLMs\u2019 ability to follow summa-\nrization and grounded data transformation instructions. To construct this test set,\nwe sampled data from 45 summarization datasets collected from Hugging Face across\nmultiple domains such as news, conversations, science, health, social, e-mails, code,\netc. for a total of 458 datapoints. We randomly collected, up to 1000 datapoints\nwhich then we carefully deduplicated to avoid overlapping with the training set. We\nthen used GPT-4 to generate a set of 40 prompts for each dataset out of each we\nrandomly sampled one for each selected datapoint. The prompts are dataset-specific\nand focus on summarization, grounding, and data transformation. For instance, a\nprompt may ask the model to generate a TikTok video out of a scientific paper or\na legal contract from a Wikipedia page. This allows us to measure not only the\nquality of the response but also hallucination in a challenging scenario, as the model\nis forced to move between formats and domains.\nThe results are presented in Table 7. With the AgentInstruct approach, we successfully\nachieved a reduction in hallucinations by 31.34%, while attaining a quality level comparable\nto GPT4 (Teacher).\n4.7 Evaluation: RAG\nThe RAG (Retrieval Augmented Generation) skill significantly boosts the capacity of Lan-\nguage Models to generate informed, contextually precise responses, hence upgrading their\noverall performance and usefulness. It is arguably more effective to test the RAG proficiency\nof language models in areas where the models have limited knowledge. For this study, we\nselected MIRAGE[ 35], a benchmark that focuses on answering medical questions by referring\nto information retrieved from a medical corpus. Since the medical domain is not typically a\nprimary focus of the models evaluated in this study, MIRAGE provides an effective platform\nfor assessing their RAG capabilities. Additionally, AgentInstruct RAG data used generic,\nnon medical data seed, enabling us to test how well can the skill (RAG) be applied to new\ndomains.\n19ModelOrca-3\n-7BOrca-2.5\n-7BMistral-\n7B-InstructLLAMA3-\n8B-instructGPT-3.5-\nturboGPT-4\nHallucination Rate (%) - Smaller is better\nAll (micro) 21.09\n(-26.12%,\n-31.34%)28.55 30.72 34.22 21.13 15.07\nOrca-Sum 28.17 36.84 39.61 38.43 28.60 21.66\nInstruSum 9.00 12.00 17.00 25.00 12.00 1.00\nACI-Bench 4.20 10.83 8.30 25.83 1.70 1.70\nQuality Score (1-10) - Higher is better\nAll (micro) 9.14\n(+7.91%,\n+3.28%)8.47 8.85 9.00 8.69 9.08\nOrca-Sum 8.95 8.27 8.61 8.90 8.32 8.61\nInstruSum 9.17 8.30 9.16 9.21 9.27 9.31\nACI-Bench 9.72 9.39 9.48 9.23 9.60 9.70\nTable 7: Hallucination rates and quality scores evaluated by GPT4. The figures (x%, y%)\nadjacent to the Orca-3 results signify the percentage of improvement compared to Orca v2.5\nand Mistral-7B-Instruct, respectively.\nMIRAGE Datasets\nMMLU-\nMedMedQA-\nUSMedMCQA PubMedQA BioASQ Avg.\nGPT-4\n(0613)CoT 89.44 83.97 69.88 39.6 84.3 73.44\nRAG87.24 82.8 66.65 70.6 92.56 79.97\nGPT-3.5-turbo\n(","chunk_id":"8ee9617c145e19fa95f1f9349bfbe69b","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"ORCA-2.5","type":"MODEL","description":"Orca-2.5 is a previous version of the Orca language model, used as a baseline for comparison in performance evaluations","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"MODEL"},{"name":"MISTRAL-7B-INSTRUCT","type":"MODEL","description":"Mistral-7B-Instruct is a language model used as a baseline for comparison in performance evaluations","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"MODEL"},{"name":"GEMINI PRO","type":"MODEL","description":"Gemini Pro is a model whose scores are referenced from its original paper for comparison purposes","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"MODEL"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is a language model used as an evaluator for summarization abilities and other performance metrics","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"MODEL"},{"name":"GPT-3.5-TURBO","type":"MODEL","description":"GPT-3.5-turbo is a language model used for comparison in performance evaluations","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"MODEL"},{"name":"FOFO BENCHMARK","type":"BENCHMARK","description":"FoFo benchmark is used to evaluate the performance of language models, including Orca-3-7B, in various tasks","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"BENCHMARK"},{"name":"ABSTRACTIVE SUMMARIZATION","type":"TASK","description":"Abstractive summarization is the process of generating a concise and coherent summary of a longer text, focusing on the main points and ideas","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"TASK"},{"name":"HALLUCINATIONS","type":"METRIC","description":"Hallucinations refer to the generation of incorrect or nonsensical information by language models during summarization tasks","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"METRIC"},{"name":"QUALITY","type":"METRIC","description":"Quality is a metric used to evaluate the coherence and relevance of the generated summaries by language models","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"METRIC"},{"name":"GPT4","type":"EVALUATOR","description":"GPT4 is used as an evaluator for assessing the summarization abilities of other language models","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"EVALUATOR"},{"name":"ACI-BENCH","type":"BENCHMARK","description":"The Ambient Clinical Intelligence Benchmark (ACI-Bench) is a dataset designed for benchmarking automatic report generation from doctor-patient conversations","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"BENCHMARK"},{"name":"INSTRUSUM","type":"BENCHMARK","description":"InstruSum is a dataset for evaluating the generation capabilities of language models for instruction-controllable summarization","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"BENCHMARK"},{"name":"ORCA-SUM","type":"BENCHMARK","description":"Orca-Sum is a newly created benchmark to evaluate language models' ability to follow summarization and grounded data transformation instructions","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"BENCHMARK"},{"name":"AGENTINSTRUCT","type":"APPROACH","description":"AgentInstruct is an approach that successfully reduced hallucinations by 31.34% while maintaining a quality level comparable to GPT4","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"APPROACH"},{"name":"RAG","type":"TECHNIQUE","description":"Retrieval Augmented Generation (RAG) is a technique that enhances the ability of language models to generate informed and contextually precise responses","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"TECHNIQUE"},{"name":"MIRAGE","type":"BENCHMARK","description":"MIRAGE is a benchmark focusing on answering medical questions by referring to information retrieved from a medical corpus","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"BENCHMARK"},{"name":"MEDICAL CORPUS","type":"DATASET","description":"A collection of medical information used to evaluate the RAG capabilities of language models","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"DATASET"},{"name":"MMLU-MED","type":"DATASET","description":"A dataset used in the MIRAGE benchmark for evaluating language models on medical questions","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"DATASET"},{"name":"MEDQA-US","type":"DATASET","description":"A dataset used in the MIRAGE benchmark for evaluating language models on medical questions","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"DATASET"},{"name":"MEDMCQA","type":"DATASET","description":"A dataset used in the MIRAGE benchmark for evaluating language models on medical questions","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"DATASET"},{"name":"PUBMEDQA","type":"DATASET","description":"A dataset used in the MIRAGE benchmark for evaluating language models on medical questions","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"DATASET"},{"name":"BIOASQ","type":"DATASET","description":"A dataset used in the MIRAGE benchmark for evaluating language models on medical questions","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"DATASET"},{"name":"COT","type":"TECHNIQUE","description":"Chain-of-Thought (CoT) is a reasoning process used by language models to break down complex problems into simpler, interconnected steps","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"TECHNIQUE"},{"name":"ORCA-3-7B","type":"","description":"","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"LANGUAGE MODELS","type":"","description":"","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"LLAMA3-8B-INSTRUCT","type":"MODEL","description":"LLAMA3-8B-Instruct is a language model used for comparison in performance evaluations","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"MODELORCA-3","type":"MODEL","description":"ModelOrca-3 is a language model evaluated on various benchmarks, including FoFo, for its performance in summarization and hallucination rates","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"MODELORCA-2.5","type":"MODEL","description":"ModelOrca-2.5 is a previous version of the Orca language model, used as a baseline for comparison in performance evaluations","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"MODELORCA-7B-INSTRUCT","type":"MODEL","description":"ModelOrca-7B-Instruct is a language model used as a baseline for comparison in performance evaluations","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"MODELORCA-8B-INSTRUCT","type":"MODEL","description":"ModelOrca-8B-Instruct is a language model used as a baseline for comparison in performance evaluations","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"MODELORCA-3.5-TURBO","type":"MODEL","description":"ModelOrca-3.5-turbo is a language model used for comparison in performance evaluations\nModelOrca-3.5-turbo is a language model used","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"MODEL"},{"name":"MODELORCA-4","type":"MODEL","description":"ModelOrca-4 is a language model used as an evaluator for summarization abilities and other performance metrics","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"MODEL"},{"name":"MODELORCA-3.5","type":"MODEL","description":"ModelOrca-3.5 is a language model used for comparison in performance evaluations","source_id":"8ee9617c145e19fa95f1f9349bfbe69b","entity_type":"MODEL"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ORCA-2.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-2.5 is a previous version of the Orca language model, used as a baseline for comparison in performance evaluations<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"MISTRAL-7B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-7B-Instruct is a language model used as a baseline for comparison in performance evaluations<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"GEMINI PRO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Gemini Pro is a model whose scores are referenced from its original paper for comparison purposes<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is a language model used as an evaluator for summarization abilities and other performance metrics<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5-turbo is a language model used for comparison in performance evaluations<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"FOFO BENCHMARK\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">FoFo benchmark is used to evaluate the performance of language models, including Orca-3-7B, in various tasks<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"ABSTRACTIVE SUMMARIZATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Abstractive summarization is the process of generating a concise and coherent summary of a longer text, focusing on the main points and ideas<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"HALLUCINATIONS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Hallucinations refer to the generation of incorrect or nonsensical information by language models during summarization tasks<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"QUALITY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Quality is a metric used to evaluate the coherence and relevance of the generated summaries by language models<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"GPT4\">      <data key=\"d0\">EVALUATOR<\/data>      <data key=\"d1\">GPT4 is used as an evaluator for assessing the summarization abilities of other language models<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">EVALUATOR<\/data>    <\/node>    <node id=\"ACI-BENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">The Ambient Clinical Intelligence Benchmark (ACI-Bench) is a dataset designed for benchmarking automatic report generation from doctor-patient conversations<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"INSTRUSUM\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">InstruSum is a dataset for evaluating the generation capabilities of language models for instruction-controllable summarization<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"ORCA-SUM\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Orca-Sum is a newly created benchmark to evaluate language models' ability to follow summarization and grounded data transformation instructions<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">APPROACH<\/data>      <data key=\"d1\">AgentInstruct is an approach that successfully reduced hallucinations by 31.34% while maintaining a quality level comparable to GPT4<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">APPROACH<\/data>    <\/node>    <node id=\"RAG\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Retrieval Augmented Generation (RAG) is a technique that enhances the ability of language models to generate informed and contextually precise responses<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"MIRAGE\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MIRAGE is a benchmark focusing on answering medical questions by referring to information retrieved from a medical corpus<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"MEDICAL CORPUS\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A collection of medical information used to evaluate the RAG capabilities of language models<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"MMLU-MED\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset used in the MIRAGE benchmark for evaluating language models on medical questions<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"MEDQA-US\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset used in the MIRAGE benchmark for evaluating language models on medical questions<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"MEDMCQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset used in the MIRAGE benchmark for evaluating language models on medical questions<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"PUBMEDQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset used in the MIRAGE benchmark for evaluating language models on medical questions<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"BIOASQ\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset used in the MIRAGE benchmark for evaluating language models on medical questions<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"COT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain-of-Thought (CoT) is a reasoning process used by language models to break down complex problems into simpler, interconnected steps<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"ORCA-3-7B\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"LANGUAGE MODELS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"LLAMA3-8B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">LLAMA3-8B-Instruct is a language model used for comparison in performance evaluations<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"MODELORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">ModelOrca-3 is a language model evaluated on various benchmarks, including FoFo, for its performance in summarization and hallucination rates<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"MODELORCA-2.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">ModelOrca-2.5 is a previous version of the Orca language model, used as a baseline for comparison in performance evaluations<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"MODELORCA-7B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">ModelOrca-7B-Instruct is a language model used as a baseline for comparison in performance evaluations<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"MODELORCA-8B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">ModelOrca-8B-Instruct is a language model used as a baseline for comparison in performance evaluations<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"MODELORCA-3.5-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">ModelOrca-3.5-turbo is a language model used for comparison in performance evaluationsModelOrca-3.5-turbo is a language model used<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"MODELORCA-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">ModelOrca-4 is a language model used as an evaluator for summarization abilities and other performance metrics<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"MODELORCA-3.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">ModelOrca-3.5 is a language model used for comparison in performance evaluations<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <edge source=\"ORCA-2.5\" target=\"ORCA-3-7B\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3-7B's performance was compared to Orca-2.5<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT\" target=\"ORCA-3-7B\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3-7B's performance was compared to Mistral-7B-Instruct<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GEMINI PRO\" target=\"ORCA-3-7B\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3-7B's performance was compared to Gemini Pro<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"ORCA-3-7B\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3-7B's performance was evaluated by GPT-4<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO\" target=\"ORCA-3-7B\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3-7B's performance was compared to GPT-3.5-turbo<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"FOFO BENCHMARK\" target=\"ORCA-3-7B\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3-7B was evaluated on the FoFo benchmark<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ABSTRACTIVE SUMMARIZATION\" target=\"HALLUCINATIONS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Hallucinations are a key metric in evaluating abstractive summarization<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ABSTRACTIVE SUMMARIZATION\" target=\"QUALITY\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Quality is a key metric in evaluating abstractive summarization<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ABSTRACTIVE SUMMARIZATION\" target=\"GPT4\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">GPT4 was used to evaluate abstractive summarization<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ABSTRACTIVE SUMMARIZATION\" target=\"ACI-BENCH\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">ACI-Bench is used to evaluate abstractive summarization<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ABSTRACTIVE SUMMARIZATION\" target=\"INSTRUSUM\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">InstruSum is used to evaluate abstractive summarization<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ABSTRACTIVE SUMMARIZATION\" target=\"ORCA-SUM\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-Sum is used to evaluate abstractive summarization<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"HALLUCINATIONS\" target=\"AGENTINSTRUCT\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct approach reduced hallucinations by 31.34%<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"RAG\" target=\"LANGUAGE MODELS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">RAG enhances the ability of language models to generate informed and contextually precise responses<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"RAG\" target=\"MIRAGE\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">MIRAGE is used to evaluate the RAG capabilities of language models<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MIRAGE\" target=\"MEDICAL CORPUS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">MIRAGE uses a medical corpus to evaluate language models<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MIRAGE\" target=\"MMLU-MED\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">MMLU-MED is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MIRAGE\" target=\"MEDQA-US\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">MEDQA-US is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MIRAGE\" target=\"MEDMCQA\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">MEDMCQA is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MIRAGE\" target=\"PUBMEDQA\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">PUBMEDQA is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MIRAGE\" target=\"BIOASQ\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">BIOASQ is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"COT\" target=\"LANGUAGE MODELS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Chain-of-Thought reasoning is used by language models to break down complex problems<\/data>      <data key=\"d6\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"ab04427ae0415a1c812a35cf8d3ee1a2","chunk":" respectively.\nMIRAGE Datasets\nMMLU-\nMedMedQA-\nUSMedMCQA PubMedQA BioASQ Avg.\nGPT-4\n(0613)CoT 89.44 83.97 69.88 39.6 84.3 73.44\nRAG87.24 82.8 66.65 70.6 92.56 79.97\nGPT-3.5-turbo\n(0613)CoT 72.91 65.04 55.25 36 74.27 60.69\nRAG75.48 66.61 58.04 67.4 90.29 71.57\nOrca-2.5-7BCoT 63.91 51.37 43.65 29.6 71.04 51.92\nRAG53.72 37.08 39.23 19 69.09 43.62\nMistral-7B-\nInstruct-v0.1CoT 50.96 42.73 34.9 27.6 47.57 40.75\nRAG54.64 35.35 43.41 30.2 68.77 46.47\nOrca-3-7BCoT 71.35 55.38 51.33 27.8 75.24 56.22\nRAG71.17\n(+30.25%)51.85\n(+46.68%)57.95\n(+33.49%)58.2\n(+92.71%)82.2\n(+19.52%)64.27\n(+38.30%)\nTable 8: Evaluation results of RAG skill on MIRAGE. The figures (x%) adjacent to the Orca-\n3 results signify the percentage of improvement compared to Mistral-7B-Instruct respectively.\nCoT shows the performance of the same models when answering directly without using RAG\nWe use the same retrieval mechanism across all models on MIRAGE [ 35], using MedRAG,\nthe corpus accompanying the benchmark. This involves using the same retrieval function\nand the same number of retrieved documents for all models. As all models are presented\nwith the same set of retrieved documents, the comparison accurately reflects the ability of\ndifferent models to incorporate retrieved results into their responses.\nTable 8 shows the results of all models on MIRAGE with and without leveraging RAG1.\nOverall, we observe that\n\u2022Models with a deeper understanding of the task (CoT scores) tend to have higher\nRAG scores. If we restrict our focus to only RAG performance, applying post-\ntraining, we\u2019ve managed to enhance Mistral\u2019s performance by an average of 38.30%.\n1Results of GPT-4 and GPT-3.5-Turbo are from [35].\n20\u2022Of the five datasets in MIRAGE, PubMedQA arguably offers the most effective\ntestbed for assessing models ability to do RAG. In PubMedQA, all models have\nlimited prior knowledge, and the retrieved context provides essential information, as\ndemonstrated by GPT4\u2019s performance leap. All Mistral fine-tunes exhibit similar\nperformance, but only Orca-3 (Mistral trained with AgentInstruct RAG flow data)\nshows a substantial improvement, resulting in a relative improvement of 92.71% over\nMistral-Instruct.\n5 Limitations\nAgentInstruct reduces human expertise required for data generation significantly and enables\ncreating of high-quality synthetic data at scale. However, this is till an early step in this\ndirection and could suffer from many limitations associated with synthetic data generation,\nincluding but not limited to:\nExtensibility: Creating the agentic flows for different skills depends on human effort for\nthe construction of the flows. Future work should consider how to automate the construction\nof the agentic flow from the user specification.\nAccuracy: Synthetic data may not perfectly replicate the complexity and nuances of real-\nworld data, leading to potential inaccuracies. Additional work is needed to better assess the\nquality of the data.\nCost: Generating synthetic data with multiple agents using LLMs and tools can be resource-\nintensive.\nBias:If the original seed data used to generate synthetic data contains biases, these biases\ncan be reflected and even amplified in the synthetic data.\nValidation : It can be difficult to validate synthetic data to ensure it accurately represents\nthe desired scenarios.\nDependency on Seed Data : The quality of synthetic data is dependent on the quality of\nthe real data used as seeds. Poor quality input data could result in poor quality synthetic\ndata.\nOrca-3 is fine-tuned with the AgentInstruct data based on the Mistral model family, and\nretains many of its limitations, as well as the common limitations of other large language\nmodels and limitations originating from its training process, including:\nData Biases: Large language models, trained on extensive data, can inadvertently carry\nbiases present in the source data. Consequently, the models may generate outputs that could\nbe potentially biased or unfair.\nLack of Transparency: Due to the complexity and size, large language models can act\nas \u201cblack boxes\u201d, making it difficult to comprehend the rationale behind specific outputs or\ndecisions. We recommend reviewing transparency notes from Azure for more information2.\nContent Harms: There are various types of content harms that large language models\ncan cause. It is important to be aware of them when using these models, and to take\nactions to prevent them. It is recommended to leverage various content moderation services\nprovided by different companies and institutions. On an important note, we hope for better\nregulations and standards from","chunk_id":"ab04427ae0415a1c812a35cf8d3ee1a2","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"GPT-4","type":"MODEL","description":"A large language model developed by OpenAI, evaluated on the MIRAGE datasets using CoT and RAG techniques","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"GPT-3.5-TURBO","type":"MODEL","description":"A variant of the GPT-3 model developed by OpenAI, evaluated on the MIRAGE datasets using CoT and RAG techniques","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"ORCA-2.5-7B","type":"MODEL","description":"A model evaluated on the MIRAGE datasets using CoT and RAG techniques","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"MISTRAL-7B-INSTRUCT-V0.1","type":"MODEL","description":"A model evaluated on the MIRAGE datasets using CoT and RAG techniques","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"ORCA-3-7B","type":"MODEL","description":"A model fine-tuned with AgentInstruct data, evaluated on the MIRAGE datasets using CoT and RAG techniques","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"COT (CHAIN-OF-THOUGHT)","type":"TECHNIQUE","description":"A technique that shows the performance of models when answering directly without using RAG","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"RAG (RETRIEVAL-AUGMENTED GENERATION)","type":"TECHNIQUE","description":"A technique that involves using a retrieval mechanism to incorporate retrieved results into model responses","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"MEDRAG","type":"TECHNIQUE","description":"A retrieval mechanism used across all models on the MIRAGE datasets","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"AGENTINSTRUCT","type":"TECHNIQUE","description":"A method that reduces human expertise required for data generation and enables creating high-quality synthetic data at scale","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"PUBMEDQA","type":"DATASET","description":"A dataset within the MIRAGE collection, considered an effective testbed for assessing models' ability to do RAG","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"AZURE","type":"PLATFORM","description":"A cloud computing service that provides transparency notes for large language models","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2","entity_type":"PLATFORM"},{"name":"LARGE LANGUAGE MODELS","type":"TECHNOLOGY","description":"Advanced machine learning models trained on extensive data to understand and generate human language","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2","entity_type":"TECHNOLOGY"},{"name":"SYNTHETIC DATA","type":"DATA","description":"Data generated artificially using techniques like AgentInstruct, which may have limitations such as extensibility, accuracy, cost, bias, validation, and dependency on seed data","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2","entity_type":"DATA"},{"name":"DATA BIASES","type":"ISSUE","description":"Biases present in the source data that can be inadvertently carried by large language models, leading to potentially biased or unfair outputs","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2","entity_type":"ISSUE"},{"name":"LACK OF TRANSPARENCY","type":"ISSUE","description":"The difficulty in comprehending the rationale behind specific outputs or decisions made by large language models due to their complexity and size","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2","entity_type":"ISSUE"},{"name":"CONTENT HARMS","type":"ISSUE","description":"Various types of harmful content that large language models can generate, necessitating the use of content moderation services","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2","entity_type":"ISSUE"},{"name":"TRANSPARENCY NOTES","type":"DOCUMENT","description":"Documents provided by platforms like Azure to offer more information on the transparency of large language models)(\"relationship\"\nDocuments provided by platforms like Azure to offer more information on the transparency of large language models","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2","entity_type":"DOCUMENT"},{"name":"MIRAGE DATASETS","type":"","description":"","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"VALIDATION","type":"","description":"","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"DEPENDENCY ON SEED DATA","type":"","description":"","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"MEDMEDQA","type":"DATASET","description":"A dataset within the MIRAGE collection used for evaluating the performance of various models","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"USMEDMCQA","type":"DATASET","description":"A dataset within the MIRAGE collection used for evaluating the performance of various models","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"BIOASQ","type":"DATASET","description":"A dataset within the MIRAGE collection used for evaluating the performance of various models","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"MISTRAL","type":"MODEL","description":"A family of models used as a base for fine-tuning with AgentInstruct data","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"AGENTIC FLOWS","type":"TECHNIQUE","description":"Flows created to enable different skills in models, requiring human effort for construction","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A large language model developed by OpenAI, evaluated on the MIRAGE datasets using CoT and RAG techniques<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A variant of the GPT-3 model developed by OpenAI, evaluated on the MIRAGE datasets using CoT and RAG techniques<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"ORCA-2.5-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A model evaluated on the MIRAGE datasets using CoT and RAG techniques<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"MISTRAL-7B-INSTRUCT-V0.1\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A model evaluated on the MIRAGE datasets using CoT and RAG techniques<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"ORCA-3-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A model fine-tuned with AgentInstruct data, evaluated on the MIRAGE datasets using CoT and RAG techniques<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"COT (CHAIN-OF-THOUGHT)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A technique that shows the performance of models when answering directly without using RAG<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"RAG (RETRIEVAL-AUGMENTED GENERATION)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A technique that involves using a retrieval mechanism to incorporate retrieved results into model responses<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"MEDRAG\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A retrieval mechanism used across all models on the MIRAGE datasets<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method that reduces human expertise required for data generation and enables creating high-quality synthetic data at scale<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"PUBMEDQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset within the MIRAGE collection, considered an effective testbed for assessing models' ability to do RAG<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"AZURE\">      <data key=\"d0\">PLATFORM<\/data>      <data key=\"d1\">A cloud computing service that provides transparency notes for large language models<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>      <data key=\"d3\">PLATFORM<\/data>    <\/node>    <node id=\"LARGE LANGUAGE MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Advanced machine learning models trained on extensive data to understand and generate human language<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"SYNTHETIC DATA\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Data generated artificially using techniques like AgentInstruct, which may have limitations such as extensibility, accuracy, cost, bias, validation, and dependency on seed data<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"DATA BIASES\">      <data key=\"d0\">ISSUE<\/data>      <data key=\"d1\">Biases present in the source data that can be inadvertently carried by large language models, leading to potentially biased or unfair outputs<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>      <data key=\"d3\">ISSUE<\/data>    <\/node>    <node id=\"LACK OF TRANSPARENCY\">      <data key=\"d0\">ISSUE<\/data>      <data key=\"d1\">The difficulty in comprehending the rationale behind specific outputs or decisions made by large language models due to their complexity and size<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>      <data key=\"d3\">ISSUE<\/data>    <\/node>    <node id=\"CONTENT HARMS\">      <data key=\"d0\">ISSUE<\/data>      <data key=\"d1\">Various types of harmful content that large language models can generate, necessitating the use of content moderation services<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>      <data key=\"d3\">ISSUE<\/data>    <\/node>    <node id=\"TRANSPARENCY NOTES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Documents provided by platforms like Azure to offer more information on the transparency of large language models)(\"relationship\"Documents provided by platforms like Azure to offer more information on the transparency of large language models<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"MIRAGE DATASETS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"VALIDATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"DEPENDENCY ON SEED DATA\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"MEDMEDQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset within the MIRAGE collection used for evaluating the performance of various models<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"USMEDMCQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset within the MIRAGE collection used for evaluating the performance of various models<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"BIOASQ\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset within the MIRAGE collection used for evaluating the performance of various models<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"MISTRAL\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A family of models used as a base for fine-tuning with AgentInstruct data<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"AGENTIC FLOWS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Flows created to enable different skills in models, requiring human effort for construction<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <edge source=\"GPT-4\" target=\"COT (CHAIN-OF-THOUGHT)\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">GPT-4's performance is measured using CoT<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"RAG (RETRIEVAL-AUGMENTED GENERATION)\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">GPT-4's performance is measured using RAG<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO\" target=\"MIRAGE DATASETS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">GPT-3.5-Turbo is evaluated on the MIRAGE datasets<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO\" target=\"COT (CHAIN-OF-THOUGHT)\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">GPT-3.5-Turbo's performance is measured using CoT<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO\" target=\"RAG (RETRIEVAL-AUGMENTED GENERATION)\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">GPT-3.5-Turbo's performance is measured using RAG<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"ORCA-2.5-7B\" target=\"MIRAGE DATASETS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-2.5-7B is evaluated on the MIRAGE datasets<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"ORCA-2.5-7B\" target=\"COT (CHAIN-OF-THOUGHT)\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-2.5-7B's performance is measured using CoT<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"ORCA-2.5-7B\" target=\"RAG (RETRIEVAL-AUGMENTED GENERATION)\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-2.5-7B's performance is measured using RAG<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT-V0.1\" target=\"MIRAGE DATASETS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Mistral-7B-Instruct-v0.1 is evaluated on the MIRAGE datasets<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT-V0.1\" target=\"COT (CHAIN-OF-THOUGHT)\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Mistral-7B-Instruct-v0.1's performance is measured using CoT<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT-V0.1\" target=\"RAG (RETRIEVAL-AUGMENTED GENERATION)\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Mistral-7B-Instruct-v0.1's performance is measured using RAG<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"MIRAGE DATASETS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3-7B is evaluated on the MIRAGE datasets<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"COT (CHAIN-OF-THOUGHT)\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3-7B's performance is measured using CoT<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"RAG (RETRIEVAL-AUGMENTED GENERATION)\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3-7B's performance is measured using RAG<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"AGENTINSTRUCT\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3-7B is fine-tuned with AgentInstruct data<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"MISTRAL\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Orca-3-7B is fine-tuned with AgentInstruct data based on the Mistral model family<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MEDRAG\" target=\"MIRAGE DATASETS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">MedRAG is used as the retrieval mechanism across all models on the MIRAGE datasets<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"SYNTHETIC DATA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">AgentInstruct is used to generate synthetic data<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"AGENTIC FLOWS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">AgentInstruct involves creating agentic flows for different skills<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"PUBMEDQA\" target=\"MIRAGE DATASETS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">PubMedQA is one of the datasets within the MIRAGE collection<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"AZURE\" target=\"LACK OF TRANSPARENCY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Azure provides transparency notes to address the lack of transparency in large language models<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"LARGE LANGUAGE MODELS\" target=\"DATA BIASES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Large language models can carry biases present in the source data<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"LARGE LANGUAGE MODELS\" target=\"LACK OF TRANSPARENCY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Large language models can act as \"black boxes\" due to their complexity and size<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"LARGE LANGUAGE MODELS\" target=\"CONTENT HARMS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Large language models can generate various types of harmful content<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"SYNTHETIC DATA\" target=\"DATA BIASES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Synthetic data can reflect and amplify biases present in the original seed data<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"SYNTHETIC DATA\" target=\"VALIDATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">It can be difficult to validate synthetic data to ensure it accurately represents the desired scenarios<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"SYNTHETIC DATA\" target=\"DEPENDENCY ON SEED DATA\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The quality of synthetic data is dependent on the quality of the real data used as seeds<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MIRAGE DATASETS\" target=\"MEDMEDQA\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">MedMedQA is one of the datasets within the MIRAGE collection<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MIRAGE DATASETS\" target=\"USMEDMCQA\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">USMedMCQA is one of the datasets within the MIRAGE collection<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MIRAGE DATASETS\" target=\"BIOASQ\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">BioASQ is one of the datasets within the MIRAGE collection<\/data>      <data key=\"d6\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"dd9a46950237e49ef9b1c7ef08e08d42","chunk":" making it difficult to comprehend the rationale behind specific outputs or\ndecisions. We recommend reviewing transparency notes from Azure for more information2.\nContent Harms: There are various types of content harms that large language models\ncan cause. It is important to be aware of them when using these models, and to take\nactions to prevent them. It is recommended to leverage various content moderation services\nprovided by different companies and institutions. On an important note, we hope for better\nregulations and standards from government and technology leaders around content harms\nfor AI technologies in future. We value and acknowledge the important role that research\nand open source community can play in this direction.\nHallucination: It is important to be aware and cautious not to entirely rely on a given\nlanguage model for critical decisions or information that might have deep impact as it is\nnot obvious how to prevent these models from fabricating content. Moreover, it is not clear\nwhether small models may be more susceptible to hallucination in ungrounded generation\nuse cases due to their smaller sizes and hence reduced memorization capacities. This is an\n2https:\/\/learn.microsoft.com\/en-us\/legal\/cognitive-services\/openai\/\ntransparency-note\n21active research topic and we hope there will be more rigorous measurement, understanding\nand mitigations around this topic.\nPotential for Misuse: Without suitable safeguards, there is a risk that these models could\nbe maliciously used for generating disinformation or harmful content.\nData Distribution: Orca-3\u2019s performance is likely to correlate strongly with the distribution\nof the tuning data. This correlation might limit its accuracy in areas underrepresented in\nthe training dataset.\n6 Conclusions\nThe AgentInstruct approach to Generative Teaching offers a promising solution to the\nchallenge of generating large amount of diverse and high-quality data for model post-training.\nThis method stands out by using agentic flows for synthetic data generation, thus addressing\nkey concerns associated with the use of synthetic data in model training, such as the lack of\ndiversity and the need for intensive human curation and intervention during the data creation\nprocess. By leveraging an agentic framework, AgentInstruct can generate tailored datasets\ncomprising both prompts and responses from unstructured data sources, facilitating the\npost-training of models and teaching them variety of skills. The efficacy of this approach is\nexemplifiedbythesubstantialimprovementobserved intheOrca-3 model, which, post-trained\nwith a 25M pair dataset generated by AgentInstruct, showcased a notable performance gain\nacross multiple benchmarks. We believe using agentic flows for creating synthetic data can\nshow significant value for all stages of model training, including pre-training, post-training\nand domain\/task specialization. The ability to use unstructured content to generate diverse\nand high-quality instruction data given any specifications could pave the way for creating\n(semi) automated pipelines using synthetic data for model customization (using domain\nspecific content as seeds) and continual improvement (generating higher quality data than\nthe base model with agentic flows).\nReferences\n[1]Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah,\nHany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl,\nAlon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Qin Cai, Martin Cai, Caio\nC\u00e9sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Dong Chen, Dongdong Chen,\nYen-Chun Chen, Yi-Ling Chen, Parul Chopra, Xiyang Dai, Allie Del Giorno, Gustavo de Rosa,\nMatthew Dixon, Ronen Eldan, Victor Fragoso, Dan Iter, Mei Gao, Min Gao, Jianfeng Gao,\nAmit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J.\nHewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis,\nDongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li,\nYunsheng Li, Chen Liang, Lars Liden, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin, Zeqi\nLin, Chong Luo, Piyush Madan, Matt Mazzola, Arindam Mitra, Hardik Modi, Anh Nguyen,\nBrandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin,\nMarko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied,\nAdil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Swadheen Shukla,\nXia Song, Masahiro Tanaka, Andrea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang, Yu Wang,\nRachel Ward, Guanhua Wang, Philipp Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can\nXu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang,\nDonghan Yu, Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang,\nYi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report","chunk_id":"dd9a46950237e49ef9b1c7ef08e08d42","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"AZURE","type":"PLATFORM","description":"Azure is a cloud computing service created by Microsoft for building, testing, deploying, and managing applications and services","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PLATFORM"},{"name":"CONTENT HARMS","type":"ISSUE","description":"Various types of negative impacts that large language models can cause, such as generating harmful or inappropriate content","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"ISSUE"},{"name":"CONTENT MODERATION SERVICES","type":"SERVICE","description":"Services provided by companies and institutions to monitor and manage the content generated by large language models to prevent harm","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"SERVICE"},{"name":"GOVERNMENT AND TECHNOLOGY LEADERS","type":"STAKEHOLDERS","description":"Authorities and industry leaders responsible for creating regulations and standards around content harms for AI technologies","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"STAKEHOLDERS"},{"name":"RESEARCH AND OPEN SOURCE COMMUNITY","type":"STAKEHOLDERS","description":"Groups and individuals involved in research and open-source projects that can contribute to addressing content harms in AI technologies","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"STAKEHOLDERS"},{"name":"HALLUCINATION","type":"ISSUE","description":"The phenomenon where language models generate content that is not based on real data, leading to fabricated information","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"ISSUE"},{"name":"UNGROUNDED GENERATION","type":"ISSUE","description":"The generation of content by language models without a solid basis in factual data, which can lead to hallucinations","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"ISSUE"},{"name":"POTENTIAL FOR MISUSE","type":"ISSUE","description":"The risk that large language models could be used maliciously to generate disinformation or harmful content","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"ISSUE"},{"name":"DATA DISTRIBUTION","type":"CONCEPT","description":"The distribution of data used to train a model, which can affect the model's performance and accuracy","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"CONCEPT"},{"name":"ORCA-3","type":"MODEL","description":"A model whose performance is influenced by the distribution of its tuning data, potentially limiting its accuracy in underrepresented areas","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"MODEL"},{"name":"AGENTINSTRUCT","type":"METHOD","description":"A generative teaching approach that uses agentic flows for synthetic data generation to improve model post-training","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"METHOD"},{"name":"SYNTHETIC DATA GENERATION","type":"TECHNIQUE","description":"The process of creating artificial data to be used in training machine learning models","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"TECHNIQUE"},{"name":"POST-TRAINING","type":"PROCESS","description":"The phase of training a model after its initial training, often involving fine-tuning with additional data","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PROCESS"},{"name":"UNSTRUCTURED DATA SOURCES","type":"DATA TYPE","description":"Data that is not organized in a pre-defined manner, which can be used by AgentInstruct to generate synthetic datasets","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"DATA TYPE"},{"name":"25M PAIR DATASET","type":"DATASET","description":"A dataset consisting of 25 million pairs of prompts and responses generated by AgentInstruct for post-training the Orca-3 model","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"DATASET"},{"name":"PRE-TRAINING","type":"PROCESS","description":"The initial phase of training a machine learning model using a large dataset","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PROCESS"},{"name":"DOMAIN\/TASK SPECIALIZATION","type":"PROCESS","description":"The process of customizing a model to perform well in specific domains or tasks","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PROCESS"},{"name":"PHI-3 TECHNICAL REPORT","type":"DOCUMENT","description":"A technical report authored by multiple researchers, detailing the Phi-3 model and its attributes","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"DOCUMENT"},{"name":"MARAH ABDIN","type":"PERSON","description":"Marah Abdin is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"SAM ADE JACOBS","type":"PERSON","description":"Sam Ade Jacobs is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"AMMAR AHMAD AWAN","type":"PERSON","description":"Ammar Ahmad Awan is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"JYOTI ANEJA","type":"PERSON","description":"Jyoti Aneja is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"AHMED AWADALLAH","type":"PERSON","description":"Ahmed Awadallah is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"HANY AWADALLA","type":"PERSON","description":"Hany Awadalla is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"NGUYEN BACH","type":"PERSON","description":"Nguyen Bach is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"AMIT BAHREE","type":"PERSON","description":"Amit Bahree is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"ARASH BAKHTIARI","type":"PERSON","description":"Arash Bakhtiari is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"JIANMIN BAO","type":"PERSON","description":"Jianmin Bao is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"HARKIRAT BEHL","type":"PERSON","description":"Harkirat Behl is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"ALON BENHAIM","type":"PERSON","description":"Alon Benhaim is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"MISHA BILENKO","type":"PERSON","description":"Misha Bilenko is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"JOHAN BJORCK","type":"PERSON","description":"Johan Bjorck is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"S\u00c9BASTIEN BUBECK","type":"PERSON","description":"S\u00e9bastien Bubeck is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"QIN CAI","type":"PERSON","description":"Qin Cai is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"MARTIN CAI","type":"PERSON","description":"Martin Cai is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"CAIO C\u00c9SAR TEODORO MENDES","type":"PERSON","description":"Caio C\u00e9sar Teodoro Mendes is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"WEIZHU CHEN","type":"PERSON","description":"Weizhu Chen is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"VISHRAV CHAUDHARY","type":"PERSON","description":"Vishrav Chaudhary is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"DONG CHEN","type":"PERSON","description":"Dong Chen is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"DONGDONG CHEN","type":"PERSON","description":"Dongdong Chen is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"YEN-CHUN CHEN","type":"PERSON","description":"Yen-Chun Chen is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"YI-LING CHEN","type":"PERSON","description":"Yi-Ling Chen is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"PARUL CHOPRA","type":"PERSON","description":"Parul Chopra is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"XIYANG DAI","type":"PERSON","description":"Xiyang Dai is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"ALLIE DEL GIORNO","type":"PERSON","description":"Allie Del Giorno is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"GUSTAVO DE ROSA","type":"PERSON","description":"Gustavo de Rosa is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"MATTHEW DIXON","type":"PERSON","description":"Matthew Dixon is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"RONEN ELDAN","type":"PERSON","description":"Ronen Eldan is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"VICTOR FRAGOSO","type":"PERSON","description":"Victor Fragoso is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"DAN ITER","type":"PERSON","description":"Dan Iter is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"MEI GAO","type":"PERSON","description":"Mei Gao is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"MIN GAO","type":"PERSON","description":"Min Gao is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"JIANFENG GAO","type":"PERSON","description":"Jianfeng Gao is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"AMIT GARG","type":"PERSON","description":"Amit Garg is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"ABHISHEK GOSWAMI","type":"PERSON","description":"Abhishek Goswami is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"SURIYA GUNASEKAR","type":"PERSON","description":"Suriya Gunasekar is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"EMMAN HAIDER","type":"PERSON","description":"Emman Haider is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"JUNHENG HAO","type":"PERSON","description":"Junheng Hao is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"RUSSELL J. HEWETT","type":"PERSON","description":"Russell J. Hewett is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"JAMIE HUYNH","type":"PERSON","description":"Jamie Huynh is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"MOJAN JAVAHERIPI","type":"PERSON","description":"Mojan Javaheripi is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"XIN JIN","type":"PERSON","description":"Xin Jin is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"PIERO KAUFFMANN","type":"PERSON","description":"Piero Kauffmann is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"NIKOS KARAMPATZIAKIS","type":"PERSON","description":"Nikos Karampatziakis is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"DONGWOO KIM","type":"PERSON","description":"Dongwoo Kim is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"MAHOUD KHADEMI","type":"PERSON","description":"Mahoud Khademi is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"LEV KURILENKO","type":"PERSON","description":"Lev Kurilenko is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"JAMES R. LEE","type":"PERSON","description":"James R. Lee is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"YIN TAT LEE","type":"PERSON","description":"Yin Tat Lee is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"YUANZHI LI","type":"PERSON","description":"Yuanzhi Li is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"YUNSHENG LI","type":"PERSON","description":"Yunsheng Li is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"CHEN LIANG","type":"PERSON","description":"Chen Liang is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"LARS LIDEN","type":"PERSON","description":"Lars Liden is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"CE LIU","type":"PERSON","description":"Ce Liu is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"MENGCHEN LIU","type":"PERSON","description":"Mengchen Liu is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"WEISHUNG LIU","type":"PERSON","description":"Weishung Liu is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"ERIC LIN","type":"PERSON","description":"Eric Lin is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"ZEQI LIN","type":"PERSON","description":"Zeqi Lin is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"CHONG LUO","type":"PERSON","description":"Chong Luo is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"PIYUSH MADAN","type":"PERSON","description":"Piyush Madan is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"MATT MAZZOLA","type":"PERSON","description":"Matt Mazzola is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"ARINDAM MITRA","type":"PERSON","description":"Arindam Mitra is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"HARDIK MODI","type":"PERSON","description":"Hardik Modi is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"ANH NGUYEN","type":"PERSON","description":"Anh Nguyen is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"BRANDON NORICK","type":"PERSON","description":"Brandon Norick is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"BARUN PATRA","type":"PERSON","description":"Barun Patra is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"DANIEL PEREZ-BECKER","type":"PERSON","description":"Daniel Perez-Becker is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"THOMAS PORTET","type":"PERSON","description":"Thomas Portet is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"REID PRYZANT","type":"PERSON","description":"Reid Pryzant is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"HEYANG QIN","type":"PERSON","description":"Heyang Qin is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"MARKO RADMILAC","type":"PERSON","description":"Marko Radmilac is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"CORBY ROSSET","type":"PERSON","description":"Corby Rosset is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"SAMBUDHA ROY","type":"PERSON","description":"Sambudha Roy is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"OLATUNJI RUWASE","type":"PERSON","description":"Olatunji Ruwase is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"OLLI SAARIKIVI","type":"PERSON","description":"Olli Saarikivi is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"AMIN SAIED","type":"PERSON","description":"Amin Saied is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"ADIL SALIM","type":"PERSON","description":"Adil Salim is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"MICHAEL SANTACROCE","type":"PERSON","description":"Michael Santacroce is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"SHITAL SHAH","type":"PERSON","description":"Shital Shah is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"NING SHANG","type":"PERSON","description":"Ning Shang is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"HITESHI SHARMA","type":"PERSON","description":"Hiteshi Sharma is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"SWADHEEN SHUKLA","type":"PERSON","description":"Swadheen Shukla is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"XIA SONG","type":"PERSON","description":"Xia Song is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"MASAHIRO TANAKA","type":"PERSON","description":"Masahiro Tanaka is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"ANDREA TUPINI","type":"PERSON","description":"Andrea Tupini is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"XIN WANG","type":"PERSON","description":"Xin Wang is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"LIJUAN WANG","type":"PERSON","description":"Lijuan Wang is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"CHUNYU WANG","type":"PERSON","description":"Chunyu Wang is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"YU WANG","type":"PERSON","description":"Yu Wang is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"},{"name":"RACHEL WARD","type":"PERSON","description":"Rachel Ward is one of the authors of the Phi-3 technical","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"PERSON"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AZURE\">      <data key=\"d0\">PLATFORM<\/data>      <data key=\"d1\">Azure is a cloud computing service created by Microsoft for building, testing, deploying, and managing applications and services<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PLATFORM<\/data>    <\/node>    <node id=\"CONTENT HARMS\">      <data key=\"d0\">ISSUE<\/data>      <data key=\"d1\">Various types of negative impacts that large language models can cause, such as generating harmful or inappropriate content<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">ISSUE<\/data>    <\/node>    <node id=\"CONTENT MODERATION SERVICES\">      <data key=\"d0\">SERVICE<\/data>      <data key=\"d1\">Services provided by companies and institutions to monitor and manage the content generated by large language models to prevent harm<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">SERVICE<\/data>    <\/node>    <node id=\"GOVERNMENT AND TECHNOLOGY LEADERS\">      <data key=\"d0\">STAKEHOLDERS<\/data>      <data key=\"d1\">Authorities and industry leaders responsible for creating regulations and standards around content harms for AI technologies<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">STAKEHOLDERS<\/data>    <\/node>    <node id=\"RESEARCH AND OPEN SOURCE COMMUNITY\">      <data key=\"d0\">STAKEHOLDERS<\/data>      <data key=\"d1\">Groups and individuals involved in research and open-source projects that can contribute to addressing content harms in AI technologies<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">STAKEHOLDERS<\/data>    <\/node>    <node id=\"HALLUCINATION\">      <data key=\"d0\">ISSUE<\/data>      <data key=\"d1\">The phenomenon where language models generate content that is not based on real data, leading to fabricated information<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">ISSUE<\/data>    <\/node>    <node id=\"UNGROUNDED GENERATION\">      <data key=\"d0\">ISSUE<\/data>      <data key=\"d1\">The generation of content by language models without a solid basis in factual data, which can lead to hallucinations<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">ISSUE<\/data>    <\/node>    <node id=\"POTENTIAL FOR MISUSE\">      <data key=\"d0\">ISSUE<\/data>      <data key=\"d1\">The risk that large language models could be used maliciously to generate disinformation or harmful content<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">ISSUE<\/data>    <\/node>    <node id=\"DATA DISTRIBUTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The distribution of data used to train a model, which can affect the model's performance and accuracy<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A model whose performance is influenced by the distribution of its tuning data, potentially limiting its accuracy in underrepresented areas<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">A generative teaching approach that uses agentic flows for synthetic data generation to improve model post-training<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">METHOD<\/data>    <\/node>    <node id=\"SYNTHETIC DATA GENERATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">The process of creating artificial data to be used in training machine learning models<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"POST-TRAINING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The phase of training a model after its initial training, often involving fine-tuning with additional data<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"UNSTRUCTURED DATA SOURCES\">      <data key=\"d0\">DATA TYPE<\/data>      <data key=\"d1\">Data that is not organized in a pre-defined manner, which can be used by AgentInstruct to generate synthetic datasets<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">DATA TYPE<\/data>    <\/node>    <node id=\"25M PAIR DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset consisting of 25 million pairs of prompts and responses generated by AgentInstruct for post-training the Orca-3 model<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"PRE-TRAINING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The initial phase of training a machine learning model using a large dataset<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"DOMAIN\/TASK SPECIALIZATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The process of customizing a model to perform well in specific domains or tasks<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PROCESS<\/data>    <\/node>    <node id=\"PHI-3 TECHNICAL REPORT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">A technical report authored by multiple researchers, detailing the Phi-3 model and its attributes<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"MARAH ABDIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marah Abdin is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SAM ADE JACOBS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sam Ade Jacobs is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AMMAR AHMAD AWAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ammar Ahmad Awan is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JYOTI ANEJA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jyoti Aneja is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AHMED AWADALLAH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ahmed Awadallah is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HANY AWADALLA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hany Awadalla is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NGUYEN BACH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nguyen Bach is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AMIT BAHREE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amit Bahree is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ARASH BAKHTIARI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arash Bakhtiari is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIANMIN BAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jianmin Bao is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HARKIRAT BEHL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Harkirat Behl is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALON BENHAIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alon Benhaim is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MISHA BILENKO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Misha Bilenko is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOHAN BJORCK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Johan Bjorck is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"S&#201;BASTIEN BUBECK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S&#233;bastien Bubeck is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QIN CAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qin Cai is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MARTIN CAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Martin Cai is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CAIO C&#201;SAR TEODORO MENDES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Caio C&#233;sar Teodoro Mendes is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WEIZHU CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weizhu Chen is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"VISHRAV CHAUDHARY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vishrav Chaudhary is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DONG CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dong Chen is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DONGDONG CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dongdong Chen is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YEN-CHUN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yen-Chun Chen is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YI-LING CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi-Ling Chen is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PARUL CHOPRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Parul Chopra is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIYANG DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiyang Dai is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALLIE DEL GIORNO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Allie Del Giorno is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GUSTAVO DE ROSA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gustavo de Rosa is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MATTHEW DIXON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthew Dixon is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RONEN ELDAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ronen Eldan is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"VICTOR FRAGOSO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Victor Fragoso is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DAN ITER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dan Iter is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MEI GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mei Gao is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MIN GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Min Gao is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIANFENG GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jianfeng Gao is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AMIT GARG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amit Garg is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ABHISHEK GOSWAMI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Abhishek Goswami is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SURIYA GUNASEKAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Suriya Gunasekar is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"EMMAN HAIDER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Emman Haider is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JUNHENG HAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Junheng Hao is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RUSSELL J. HEWETT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Russell J. Hewett is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JAMIE HUYNH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jamie Huynh is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MOJAN JAVAHERIPI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mojan Javaheripi is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIN JIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xin Jin is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PIERO KAUFFMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Piero Kauffmann is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NIKOS KARAMPATZIAKIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nikos Karampatziakis is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DONGWOO KIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dongwoo Kim is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MAHOUD KHADEMI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mahoud Khademi is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEV KURILENKO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lev Kurilenko is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JAMES R. LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">James R. Lee is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YIN TAT LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yin Tat Lee is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUANZHI LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuanzhi Li is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUNSHENG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yunsheng Li is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHEN LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Liang is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LARS LIDEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lars Liden is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CE LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ce Liu is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MENGCHEN LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mengchen Liu is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WEISHUNG LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weishung Liu is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ERIC LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Lin is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZEQI LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zeqi Lin is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHONG LUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chong Luo is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PIYUSH MADAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Piyush Madan is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MATT MAZZOLA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matt Mazzola is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ARINDAM MITRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arindam Mitra is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HARDIK MODI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hardik Modi is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANH NGUYEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anh Nguyen is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BRANDON NORICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brandon Norick is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BARUN PATRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Barun Patra is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DANIEL PEREZ-BECKER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel Perez-Becker is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"THOMAS PORTET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Portet is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"REID PRYZANT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Reid Pryzant is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HEYANG QIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heyang Qin is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MARKO RADMILAC\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marko Radmilac is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CORBY ROSSET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Corby Rosset is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SAMBUDHA ROY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sambudha Roy is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"OLATUNJI RUWASE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Olatunji Ruwase is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"OLLI SAARIKIVI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Olli Saarikivi is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AMIN SAIED\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amin Saied is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ADIL SALIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adil Salim is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MICHAEL SANTACROCE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Santacroce is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHITAL SHAH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shital Shah is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NING SHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ning Shang is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HITESHI SHARMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hiteshi Sharma is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SWADHEEN SHUKLA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Swadheen Shukla is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIA SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xia Song is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MASAHIRO TANAKA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Masahiro Tanaka is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANDREA TUPINI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrea Tupini is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xin Wang is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIJUAN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lijuan Wang is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHUNYU WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chunyu Wang is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YU WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu Wang is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RACHEL WARD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rachel Ward is one of the authors of the Phi-3 technical<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"cc20c99cad8edecc66b82ac751ff7172","chunk":" Wang, Philipp Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can\nXu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang,\nDonghan Yu, Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang,\nYi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: A highly capable\nlanguage model locally on your phone, 2024. URL https:\/\/arxiv.org\/abs\/2404.14219 .\n[2]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\nchallenge. arXiv:1803.05457v1, 2018.\n[3]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXivpreprint arXiv:2110.14168, 2021.\n[4]CodeParrot. Github-code clean dataset, 2022. https:\/\/huggingface.co\/datasets\/\ncodeparrot\/github-code-clean [Accessed: (06\/15\/2024)].\n22[5]Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong\nSun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional\nconversations. arXivpreprint arXiv:2305.14233, 2023.\n[6]Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt\nGardner. DROP: A reading comprehension benchmark requiring discrete reasoning over\nparagraphs. In Proceedings ofthe2019Conference oftheNorthAmerican Chapter ofthe\nAssociation forComputational Linguistics: HumanLanguage Technologies, Volume1(Long\nandShortPapers), pages 2368\u20132378, Minneapolis, Minnesota, June 2019. Association for\nComputational Linguistics. doi: 10.18653\/v1\/N19-1246. URL https:\/\/aclanthology.org\/\nN19-1246 .\n[7]Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Hang Yan, Xipeng Qiu, and Dahua Lin.\nQuery of cc: Unearthing large scale domain-specific knowledge from public corpora. arXiv\npreprint arXiv:2401.14624, 2024.\n[8]Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey\nLevine, and Dawn Song. The false promise of imitating proprietary llms, 2023. URL https:\n\/\/arxiv.org\/abs\/2305.15717 .\n[9]Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.\narXivpreprint arXiv:2103.03874, 2021.\n[10]Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep\nDasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.\nCamels in a changing climate: Enhancing lm adaptation with tulu 2, 2023. URL https:\n\/\/arxiv.org\/abs\/2311.10702 .\n[11]Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023.\n[12]Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie\nLu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash.\nRlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2309.00267 .\n[13]Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\nGhanem. Camel: Communicative agents for \"mind\" exploration of large language model society,\n2023. URL https:\/\/arxiv.org\/abs\/2303.17760 .\n[14]Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, andTatsunoriB.Hashimoto. Alpacae","chunk_id":"cc20c99cad8edecc66b82ac751ff7172","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"AI2 REASONING CHALLENGE","type":"CHALLENGE","description":"A challenge designed to test the capabilities of AI systems in question answering, as described by Clark et al., 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"CHALLENGE"},{"name":"VERIFIERS","type":"TECHNIQUE","description":"A method used to solve math word problems, as described by Cobbe et al., 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"TECHNIQUE"},{"name":"GITHUB-CODE CLEAN DATASET","type":"DATASET","description":"A dataset of cleaned code from GitHub, as described by CodeParrot, 2022","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"DATASET"},{"name":"CHAT LANGUAGE MODELS","type":"MODEL","description":"Language models enhanced by scaling high-quality instructional conversations, as described by Ding et al., 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"MODEL"},{"name":"DROP","type":"BENCHMARK","description":"A reading comprehension benchmark requiring discrete reasoning over paragraphs, as described by Dua et al., 2019","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"BENCHMARK"},{"name":"QUERY OF CC","type":"TECHNIQUE","description":"A method for unearthing large-scale domain-specific knowledge from public corpora, as described by Fei et al., 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"TECHNIQUE"},{"name":"IMITATING PROPRIETARY LLMS","type":"TECHNIQUE","description":"A method critiqued for its false promise in imitating proprietary large language models, as described by Gudibande et al., 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"TECHNIQUE"},{"name":"MATH DATASET","type":"DATASET","description":"A dataset used for measuring mathematical problem solving, as described by Hendrycks et al., 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"DATASET"},{"name":"TULU 2","type":"MODEL","description":"A model used for enhancing language model adaptation, as described by Ivison et al., 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"MODEL"},{"name":"MISTRAL 7B","type":"MODEL","description":"A language model, as described by Jiang et al., 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"MODEL"},{"name":"RLAIF","type":"TECHNIQUE","description":"A method for scaling reinforcement learning from human feedback with AI feedback, as described by Lee et al., 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"TECHNIQUE"},{"name":"CAMEL","type":"MODEL","description":"Communicative agents for \"mind\" exploration of large language model society, as described by Li et al., 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"MODEL"},{"name":"WANG","type":"PERSON","description":"Wang is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"PERSON"},{"name":"PETER CLARK","type":"PERSON","description":"Peter Clark is an author who contributed to the AI2 reasoning challenge, 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"PERSON"},{"name":"KARL COBBE","type":"PERSON","description":"Karl Cobbe is an author who contributed to the research on training verifiers to solve math word problems, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"PERSON"},{"name":"CODEPARROT","type":"ORGANIZATION","description":"CodeParrot is an organization that created the GitHub-code clean dataset, 2022","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"ORGANIZATION"},{"name":"NING DING","type":"PERSON","description":"Ning Ding is an author who contributed to the research on enhancing chat language models, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"PERSON"},{"name":"DHEERU DUA","type":"PERSON","description":"Dheeru Dua is an author who contributed to the DROP benchmark, 2019","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"PERSON"},{"name":"ZHAOYE FEI","type":"PERSON","description":"Zhaoye Fei is an author who contributed to the research on Query of CC, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"PERSON"},{"name":"ARNAV GUDIBANDE","type":"PERSON","description":"Arnav Gudibande is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"PERSON"},{"name":"DAN HENDRYCKS","type":"PERSON","description":"Dan Hendrycks is an author who contributed to the research on the math dataset, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"PERSON"},{"name":"HAMISH IVISON","type":"PERSON","description":"Hamish Ivison is an author who contributed to the research on Tulu 2, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"PERSON"},{"name":"ALBERT Q. JIANG","type":"PERSON","description":"Albert Q. Jiang is an author who contributed to the research on Mistral 7B, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"PERSON"},{"name":"HARRISON LEE","type":"PERSON","description":"Harrison Lee is an author who contributed to the research on RLAIF, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"PERSON"},{"name":"GUOHAO LI","type":"PERSON","description":"Guohao Li is an author who contributed to the research on CAMEL, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172","entity_type":"PERSON"},{"name":"PHI-3","type":"","description":"","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"PHILIPP WITTE","type":"PERSON","description":"Philipp Witte is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HAIPING WU","type":"PERSON","description":"Haiping Wu is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MICHAEL WYATT","type":"PERSON","description":"Michael Wyatt is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"BIN XIAO","type":"PERSON","description":"Bin Xiao is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CAN XU","type":"PERSON","description":"Can Xu is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JIAHANG XU","type":"PERSON","description":"Jiahang Xu is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"WEIJIAN XU","type":"PERSON","description":"Weijian Xu is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SONALI YADAV","type":"PERSON","description":"Sonali Yadav is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"FAN YANG","type":"PERSON","description":"Fan Yang is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JIANWEI YANG","type":"PERSON","description":"Jianwei Yang is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ZIYI YANG","type":"PERSON","description":"Ziyi Yang is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YIFAN YANG","type":"PERSON","description":"Yifan Yang is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DONGHAN YU","type":"PERSON","description":"Donghan Yu is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"LU YUAN","type":"PERSON","description":"Lu Yuan is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CHENGRUIDONG ZHANG","type":"PERSON","description":"Chengruidong Zhang is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CYRIL ZHANG","type":"PERSON","description":"Cyril Zhang is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JIANWEN ZHANG","type":"PERSON","description":"Jianwen Zhang is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"LI LYNA ZHANG","type":"PERSON","description":"Li Lyna Zhang is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YI ZHANG","type":"PERSON","description":"Yi Zhang is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YUE ZHANG","type":"PERSON","description":"Yue Zhang is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YUNAN ZHANG","type":"PERSON","description":"Yunan Zhang is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"XIREN ZHOU","type":"PERSON","description":"Xiren Zhou is an author who contributed to the technical report on Phi-3, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ISAAC COWHEY","type":"PERSON","description":"Isaac Cowhey is an author who contributed to the AI2 reasoning challenge, 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"OREN ETZIONI","type":"PERSON","description":"Oren Etzioni is an author who contributed to the AI2 reasoning challenge, 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"TUSHAR KHOT","type":"PERSON","description":"Tushar Khot is an author who contributed to the AI2 reasoning challenge, 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ASHISH SABHARWAL","type":"PERSON","description":"Ashish Sabharwal is an author who contributed to the AI2 reasoning challenge, 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CARISSA SCHOENICK","type":"PERSON","description":"Carissa Schoenick is an author who contributed to the AI2 reasoning challenge, 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"OYVIND TAFJORD","type":"PERSON","description":"Oyvind Tafjord is an author who contributed to the AI2 reasoning challenge, 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"VINEET KOSARAJU","type":"PERSON","description":"Vineet Kosaraju is an author who contributed to the research on training verifiers to solve math word problems, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MOHAMMAD BAVARIAN","type":"PERSON","description":"Mohammad Bavarian is an author who contributed to the research on training verifiers to solve math word problems, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MARK CHEN","type":"PERSON","description":"Mark Chen is an author who contributed to the research on training verifiers to solve math word problems, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HEEWOO JUN","type":"PERSON","description":"Heewoo Jun is an author who contributed to the research on training verifiers to solve math word problems, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"LUKASZ KAISER","type":"PERSON","description":"Lukasz Kaiser is an author who contributed to the research on training verifiers to solve math word problems, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MATTHIAS PLAPPERT","type":"PERSON","description":"Matthias Plappert is an author who contributed to the research on training verifiers to solve math word problems, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JERRY TWOREK","type":"PERSON","description":"Jerry Tworek is an author who contributed to the research on training verifiers to solve math word problems, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JACOB HILTON","type":"PERSON","description":"Jacob Hilton is an author who contributed to the research on training verifiers to solve math word problems, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"REIICHIRO NAKANO","type":"PERSON","description":"Reiichiro Nakano is an author who contributed to the research on training verifiers to solve math word problems, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YULIN CHEN","type":"PERSON","description":"Yulin Chen is an author who contributed to the research on enhancing chat language models, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"BOKAI XU","type":"PERSON","description":"Bokai Xu is an author who contributed to the research on enhancing chat language models, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YUJIA QIN","type":"PERSON","description":"Yujia Qin is an author who contributed to the research on enhancing chat language models, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ZHI ZHENG","type":"PERSON","description":"Zhi Zheng is an author who contributed to the research on enhancing chat language models, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SHENGDING HU","type":"PERSON","description":"Shengding Hu is an author who contributed to the research on enhancing chat language models, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ZHIYUAN LIU","type":"PERSON","description":"Zhiyuan Liu is an author who contributed to the research on enhancing chat language models, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MAOSONG SUN","type":"PERSON","description":"Maosong Sun is an author who contributed to the research on enhancing chat language models, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"BOWEN ZHOU","type":"PERSON","description":"Bowen Zhou is an author who contributed to the research on enhancing chat language models, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YIZHONG WANG","type":"PERSON","description":"Yizhong Wang is an author who contributed to the DROP benchmark, 2019","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"PRADEEP DASIGI","type":"PERSON","description":"Pradeep Dasigi is an author who contributed to the DROP benchmark, 2019","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"GABRIEL STANOVSKY","type":"PERSON","description":"Gabriel Stanovsky is an author who contributed to the DROP benchmark, 2019","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SAMEER SINGH","type":"PERSON","description":"Sameer Singh is an author who contributed to the DROP benchmark, 2019","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MATT GARDNER","type":"PERSON","description":"Matt Gardner is an author who contributed to the DROP benchmark, 2019","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YUNFAN SHAO","type":"PERSON","description":"Yunfan Shao is an author who contributed to the research on Query of CC, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"LINYANG LI","type":"PERSON","description":"Linyang Li is an author who contributed to the research on Query of CC, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ZHIYUAN ZENG","type":"PERSON","description":"Zhiyuan Zeng is an author who contributed to the research on Query of CC, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HANG YAN","type":"PERSON","description":"Hang Yan is an author who contributed to the research on Query of CC, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"XI PENG QIU","type":"PERSON","description":"Xi Peng Qiu is an author who contributed to the research on Query of CC, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DAHUA LIN","type":"PERSON","description":"Dahua Lin is an author who contributed to the research on Query of CC, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ERIC WALLACE","type":"PERSON","description":"Eric Wallace is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CHARLIE SNELL","type":"PERSON","description":"Charlie Snell is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"XINYANG GENG","type":"PERSON","description":"Xinyang Geng is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HAO LIU","type":"PERSON","description":"Hao Liu is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"PIETER ABBEEL","type":"PERSON","description":"Pieter Abbeel is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SERGEY LEVINE","type":"PERSON","description":"Sergey Levine is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DAWN SONG","type":"PERSON","description":"Dawn Song is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"COLLIN BURNS","type":"PERSON","description":"Collin Burns is an author who contributed to the research on the math dataset, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SAURAV KADAVATH","type":"PERSON","description":"Saurav Kadavath is an author who contributed to the research on the math dataset, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"AKUL ARORA","type":"PERSON","description":"Akul Arora is an author who contributed to the research on the math dataset, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"STEVEN BASART","type":"PERSON","description":"Steven Basart is an author who contributed to the research on the math dataset, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ERIC TANG","type":"PERSON","description":"Eric Tang is an author who contributed to the research on the math dataset, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JACOB STEINHARDT","type":"PERSON","description":"Jacob Steinhardt is an author who contributed to the research on the math dataset, 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"VALENTINA PYATKIN","type":"PERSON","description":"Valentina Pyatkin is an author who contributed to the research on Tulu 2, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"NATHAN LAMBERT","type":"PERSON","description":"Nathan Lambert is an author who contributed to the research on Tulu 2, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MATTHEW PETERS","type":"PERSON","description":"Matthew Peters is an author who contributed to the research on Tulu 2, 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JOEL JANG","type":"PERSON","description":"Joel Jang is an author who contributed","source_id":"cc20c99cad8edecc66b82ac751ff7172"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AI2 REASONING CHALLENGE\">      <data key=\"d0\">CHALLENGE<\/data>      <data key=\"d1\">A challenge designed to test the capabilities of AI systems in question answering, as described by Clark et al., 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">CHALLENGE<\/data>    <\/node>    <node id=\"VERIFIERS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method used to solve math word problems, as described by Cobbe et al., 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"GITHUB-CODE CLEAN DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset of cleaned code from GitHub, as described by CodeParrot, 2022<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"CHAT LANGUAGE MODELS\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Language models enhanced by scaling high-quality instructional conversations, as described by Ding et al., 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A reading comprehension benchmark requiring discrete reasoning over paragraphs, as described by Dua et al., 2019<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"QUERY OF CC\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method for unearthing large-scale domain-specific knowledge from public corpora, as described by Fei et al., 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"IMITATING PROPRIETARY LLMS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method critiqued for its false promise in imitating proprietary large language models, as described by Gudibande et al., 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"MATH DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset used for measuring mathematical problem solving, as described by Hendrycks et al., 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"TULU 2\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A model used for enhancing language model adaptation, as described by Ivison et al., 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"MISTRAL 7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A language model, as described by Jiang et al., 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"RLAIF\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method for scaling reinforcement learning from human feedback with AI feedback, as described by Lee et al., 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"CAMEL\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Communicative agents for \"mind\" exploration of large language model society, as described by Li et al., 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PETER CLARK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter Clark is an author who contributed to the AI2 reasoning challenge, 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KARL COBBE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karl Cobbe is an author who contributed to the research on training verifiers to solve math word problems, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CODEPARROT\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">CodeParrot is an organization that created the GitHub-code clean dataset, 2022<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"NING DING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ning Ding is an author who contributed to the research on enhancing chat language models, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DHEERU DUA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dheeru Dua is an author who contributed to the DROP benchmark, 2019<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHAOYE FEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhaoye Fei is an author who contributed to the research on Query of CC, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ARNAV GUDIBANDE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arnav Gudibande is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DAN HENDRYCKS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dan Hendrycks is an author who contributed to the research on the math dataset, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAMISH IVISON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hamish Ivison is an author who contributed to the research on Tulu 2, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALBERT Q. JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Albert Q. Jiang is an author who contributed to the research on Mistral 7B, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HARRISON LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Harrison Lee is an author who contributed to the research on RLAIF, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GUOHAO LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guohao Li is an author who contributed to the research on CAMEL, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PHI-3\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"PHILIPP WITTE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Philipp Witte is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HAIPING WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Haiping Wu is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MICHAEL WYATT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Wyatt is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"BIN XIAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bin Xiao is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CAN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Can Xu is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JIAHANG XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiahang Xu is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"WEIJIAN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weijian Xu is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SONALI YADAV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sonali Yadav is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"FAN YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fan Yang is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JIANWEI YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jianwei Yang is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ZIYI YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ziyi Yang is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YIFAN YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yifan Yang is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DONGHAN YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Donghan Yu is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"LU YUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lu Yuan is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CHENGRUIDONG ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chengruidong Zhang is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CYRIL ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cyril Zhang is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JIANWEN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jianwen Zhang is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"LI LYNA ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Li Lyna Zhang is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YI ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi Zhang is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YUE ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yue Zhang is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YUNAN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yunan Zhang is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"XIREN ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiren Zhou is an author who contributed to the technical report on Phi-3, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ISAAC COWHEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Isaac Cowhey is an author who contributed to the AI2 reasoning challenge, 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"OREN ETZIONI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Oren Etzioni is an author who contributed to the AI2 reasoning challenge, 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"TUSHAR KHOT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tushar Khot is an author who contributed to the AI2 reasoning challenge, 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ASHISH SABHARWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ashish Sabharwal is an author who contributed to the AI2 reasoning challenge, 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CARISSA SCHOENICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carissa Schoenick is an author who contributed to the AI2 reasoning challenge, 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"OYVIND TAFJORD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Oyvind Tafjord is an author who contributed to the AI2 reasoning challenge, 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"VINEET KOSARAJU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vineet Kosaraju is an author who contributed to the research on training verifiers to solve math word problems, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MOHAMMAD BAVARIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mohammad Bavarian is an author who contributed to the research on training verifiers to solve math word problems, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MARK CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mark Chen is an author who contributed to the research on training verifiers to solve math word problems, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HEEWOO JUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heewoo Jun is an author who contributed to the research on training verifiers to solve math word problems, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"LUKASZ KAISER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lukasz Kaiser is an author who contributed to the research on training verifiers to solve math word problems, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MATTHIAS PLAPPERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthias Plappert is an author who contributed to the research on training verifiers to solve math word problems, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JERRY TWOREK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jerry Tworek is an author who contributed to the research on training verifiers to solve math word problems, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JACOB HILTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Hilton is an author who contributed to the research on training verifiers to solve math word problems, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"REIICHIRO NAKANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Reiichiro Nakano is an author who contributed to the research on training verifiers to solve math word problems, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YULIN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yulin Chen is an author who contributed to the research on enhancing chat language models, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"BOKAI XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bokai Xu is an author who contributed to the research on enhancing chat language models, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YUJIA QIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yujia Qin is an author who contributed to the research on enhancing chat language models, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ZHI ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhi Zheng is an author who contributed to the research on enhancing chat language models, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SHENGDING HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengding Hu is an author who contributed to the research on enhancing chat language models, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ZHIYUAN LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyuan Liu is an author who contributed to the research on enhancing chat language models, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MAOSONG SUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maosong Sun is an author who contributed to the research on enhancing chat language models, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"BOWEN ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bowen Zhou is an author who contributed to the research on enhancing chat language models, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YIZHONG WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yizhong Wang is an author who contributed to the DROP benchmark, 2019<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"PRADEEP DASIGI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pradeep Dasigi is an author who contributed to the DROP benchmark, 2019<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"GABRIEL STANOVSKY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gabriel Stanovsky is an author who contributed to the DROP benchmark, 2019<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SAMEER SINGH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sameer Singh is an author who contributed to the DROP benchmark, 2019<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MATT GARDNER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matt Gardner is an author who contributed to the DROP benchmark, 2019<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YUNFAN SHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yunfan Shao is an author who contributed to the research on Query of CC, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"LINYANG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linyang Li is an author who contributed to the research on Query of CC, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ZHIYUAN ZENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyuan Zeng is an author who contributed to the research on Query of CC, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HANG YAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hang Yan is an author who contributed to the research on Query of CC, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"XI PENG QIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xi Peng Qiu is an author who contributed to the research on Query of CC, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DAHUA LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dahua Lin is an author who contributed to the research on Query of CC, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ERIC WALLACE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Wallace is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CHARLIE SNELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Charlie Snell is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"XINYANG GENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xinyang Geng is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HAO LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao Liu is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"PIETER ABBEEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pieter Abbeel is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SERGEY LEVINE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sergey Levine is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DAWN SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dawn Song is an author who contributed to the research on the false promise of imitating proprietary LLMs, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"COLLIN BURNS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Collin Burns is an author who contributed to the research on the math dataset, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SAURAV KADAVATH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Saurav Kadavath is an author who contributed to the research on the math dataset, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"AKUL ARORA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Akul Arora is an author who contributed to the research on the math dataset, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"STEVEN BASART\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Steven Basart is an author who contributed to the research on the math dataset, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ERIC TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Tang is an author who contributed to the research on the math dataset, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JACOB STEINHARDT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Steinhardt is an author who contributed to the research on the math dataset, 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"VALENTINA PYATKIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Valentina Pyatkin is an author who contributed to the research on Tulu 2, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"NATHAN LAMBERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nathan Lambert is an author who contributed to the research on Tulu 2, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MATTHEW PETERS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthew Peters is an author who contributed to the research on Tulu 2, 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JOEL JANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Jang is an author who contributed<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <edge source=\"AI2 REASONING CHALLENGE\" target=\"PETER CLARK\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Peter Clark is an author who contributed to the AI2 reasoning challenge<\/data>      <data key=\"d6\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"VERIFIERS\" target=\"KARL COBBE\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Karl Cobbe is an author who contributed to the research on training verifiers to solve math word problems<\/data>      <data key=\"d6\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"GITHUB-CODE CLEAN DATASET\" target=\"CODEPARROT\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">CodeParrot is the organization that created the GitHub-code clean dataset<\/data>      <data key=\"d6\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"CHAT LANGUAGE MODELS\" target=\"NING DING\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Ning Ding is an author who contributed to the research on enhancing chat language models<\/data>      <data key=\"d6\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"DROP\" target=\"DHEERU DUA\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Dheeru Dua is an author who contributed to the DROP benchmark<\/data>      <data key=\"d6\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"QUERY OF CC\" target=\"ZHAOYE FEI\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Zhaoye Fei is an author who contributed to the research on Query of CC<\/data>      <data key=\"d6\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"IMITATING PROPRIETARY LLMS\" target=\"ARNAV GUDIBANDE\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Arnav Gudibande is an author who contributed to the research on the false promise of imitating proprietary LLMs<\/data>      <data key=\"d6\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"MATH DATASET\" target=\"DAN HENDRYCKS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Dan Hendrycks is an author who contributed to the research on the math dataset<\/data>      <data key=\"d6\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"TULU 2\" target=\"HAMISH IVISON\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Hamish Ivison is an author who contributed to the research on Tulu 2<\/data>      <data key=\"d6\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"MISTRAL 7B\" target=\"ALBERT Q. JIANG\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Albert Q. Jiang is an author who contributed to the research on Mistral 7B<\/data>      <data key=\"d6\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"RLAIF\" target=\"HARRISON LEE\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Harrison Lee is an author who contributed to the research on RLAIF<\/data>      <data key=\"d6\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"CAMEL\" target=\"GUOHAO LI\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Guohao Li is an author who contributed to the research on CAMEL<\/data>      <data key=\"d6\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"WANG\" target=\"PHI-3\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Wang is an author who contributed to the technical report on Phi-3<\/data>      <data key=\"d6\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"3d1f6634f93f8a4c296dc8df7e59859e","chunk":"rii Khizbullin, and Bernard\nGhanem. Camel: Communicative agents for \"mind\" exploration of large language model society,\n2023. URL https:\/\/arxiv.org\/abs\/2303.17760 .\n[14]Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, andTatsunoriB.Hashimoto. Alpacaeval: Anautomaticevaluatorofinstruction-following\nmodels. https:\/\/github.com\/tatsu-lab\/alpaca_eval , 2023.\n[15]Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei\nLiu, Dragomir Radev, Chien-Sheng Wu, and Arman Cohan. Benchmarking generation and\nevaluation capabilities of large language models for instruction controllable summarization,\n2023. URL https:\/\/arxiv.org\/abs\/2311.09184 .\n[16]Lm-sys. Mt-Bench, 2023. URL https:\/\/huggingface.co\/spaces\/lmsys\/mt-bench\/tree\/\ncf27f9f9da48f72169bce3c3e784d24347d1e833\/data\/mt_bench\/model_answer .\n[17]Daniel van Strien Loubna Ben Allal, Anton Lozhkov. Cosmopedia: how to create large-scale\nsynthetic data for pre-training, 2024. URL https:\/\/huggingface.co\/blog\/cosmopedia .\n[18]Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj\nAgarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi,\nGuoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. Orca 2: Teaching\nsmall language models how to reason, 2023. URL https:\/\/arxiv.org\/abs\/2311.11045 .\n[19]ArindamMitra, HamedKhanpour, CorbyRosset, andAhmedAwadallah. Orca-math: Unlocking\nthe potential of slms in grade school math. arXivpreprint arXiv:2402.14830, 2024.\n[20]Subhabrata Mukherjee and Ahmed Awadallah. Xtremedistil: Multi-stage distillation for massive\nmultilingual models, 2020.\n23[21]Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and\nAhmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv\npreprint arXiv:2306.02707, 2023.\n[22] OpenAI. Gpt-4 technical report, 2023.\n[23]Samuel J. Paech. Eq-bench: An emotional intelligence benchmark for large language models,\n2024. URL https:\/\/arxiv.org\/abs\/2312.06281 .\n[24]Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\nwith gpt-4, 2023. URL https:\/\/arxiv.org\/abs\/2304.03277 .\n[25]YiweiQin, KaiqiangSong, YebowenHu, Wenlin Yao, SangwooCho, XiaoyangWang, Xuansheng\nWu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating instruction following ability in\nlarge language models, 2024. URL https:\/\/arxiv.org\/abs\/2401.03601 .\n[26]Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,\nXiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein,\nDahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to\nmaster 16000+ real-world apis, 2023.\n[27]David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a\nbenchmark, 2023. URL https:\/\/arxiv.org\/abs\/2311.12022 .\n[28]Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and\nTengyang Xie. Direct nash optimization: Teaching language models to self-improve with general\npreferences, 2024. URL https:\/\/arxiv.org\/abs\/2404.03715 .\n[29]Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross\nAnderson. The curse of recursion: Training on generated data makes models forget, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2305.17493 .\n[30]Mohammed Latif Siddiq, Jiahao Zhang, Lindsay Roney, and Joanna C. S.","chunk_id":"3d1f6634f93f8a4c296dc8df7e59859e","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"BERNARD GHANEM","type":"PERSON","description":"Bernard Ghanem is an author who contributed to the research on communicative agents for \"mind\" exploration of large language model society","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"CAMEL","type":"SYSTEM","description":"Communicative agents for \"mind\" exploration of large language model society, as described in the 2023 paper","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"SYSTEM"},{"name":"XUECHEN LI","type":"PERSON","description":"Xuechen Li is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"TIANYI ZHANG","type":"PERSON","description":"Tianyi Zhang is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"YANN DUBOIS","type":"PERSON","description":"Yann Dubois is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ROHAN TAORI","type":"PERSON","description":"Rohan Taori is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ISHAAN GULRAJANI","type":"PERSON","description":"Ishaan Gulrajani is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"CARLOS GUESTRIN","type":"PERSON","description":"Carlos Guestrin is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"PERCY LIANG","type":"PERSON","description":"Percy Liang is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"TATSUNORI B. HASHIMOTO","type":"PERSON","description":"Tatsunori B. Hashimoto is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ALPACAEVAL","type":"SYSTEM","description":"An automatic evaluator of instruction-following models, as described in the 2023 paper","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"SYSTEM"},{"name":"YIXIN LIU","type":"PERSON","description":"Yixin Liu is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ALEXANDER R. FABBRI","type":"PERSON","description":"Alexander R. Fabbri is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"JIAWEN CHEN","type":"PERSON","description":"Jiawen Chen is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"YILUN ZHAO","type":"PERSON","description":"Yilun Zhao is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"SIMENG HAN","type":"PERSON","description":"Simeng Han is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"SHAFIQ JOTY","type":"PERSON","description":"Shafiq Joty is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"PENGFEI LIU","type":"PERSON","description":"Pengfei Liu is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\nPengfei Liu is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"DRAGOMIR RADEV","type":"PERSON","description":"Dragomir Radev is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"CHIEN-SHENG WU","type":"PERSON","description":"Chien-Sheng Wu is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ARMAN COHAN","type":"PERSON","description":"Arman Cohan is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"MT-BENCH","type":"SYSTEM","description":"A system for benchmarking, as described by Lm-sys in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"SYSTEM"},{"name":"DANIEL VAN STRIEN","type":"PERSON","description":"Daniel van Strien is an author who contributed to the research on Cosmopedia, a method for creating large-scale synthetic data for pre-training","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"LOUBNA BEN ALLAL","type":"PERSON","description":"Loubna Ben Allal is an author who contributed to the research on Cosmopedia, a method for creating large-scale synthetic data for pre-training","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ANTON LOZHKOV","type":"PERSON","description":"Anton Lozhkov is an author who contributed to the research on Cosmopedia, a method for creating large-scale synthetic data for pre-training","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"COSMOPEDIA","type":"SYSTEM","description":"A method for creating large-scale synthetic data for pre-training, as described in the 2024 paper","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"SYSTEM"},{"name":"ARINDAM MITRA","type":"PERSON","description":"Arindam Mitra is an author who contributed to the research on Orca 2 and Orca-Math, systems for teaching small language models how to reason and unlocking the potential of SLMs in grade school math","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"LUCIANO DEL CORRO","type":"PERSON","description":"Luciano Del Corro is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"SHWETI MAHAJAN","type":"PERSON","description":"Shweti Mahajan is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ANDRES CODAS","type":"PERSON","description":"Andres Codas is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"CLARISSE SIMOES","type":"PERSON","description":"Clarisse Simoes is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"SAHAJ AGARWAL","type":"PERSON","description":"Sahaj Agarwal is an author who contributed to the research on Orca 2 and Orca, systems for teaching small language models how to reason and progressive learning from complex explanation traces of GPT-4","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"XUXI CHEN","type":"PERSON","description":"Xuxi Chen is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ANASTASIA RAZDAIBIEDINA","type":"PERSON","description":"Anastasia Razdaibiedina is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ERIK JONES","type":"PERSON","description":"Erik Jones is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"KRITI AGGARWAL","type":"PERSON","description":"Kriti Aggarwal is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"HAMID PALANGI","type":"PERSON","description":"Hamid Palangi is an author who contributed to the research on Orca 2, Orca, and Xtremedistil, systems for teaching small language models how to reason, progressive learning from complex explanation traces of GPT-4, and multi-stage distillation for massive multilingual models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"GUOQING ZHENG","type":"PERSON","description":"Guoqing Zheng is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"CORBY ROSSET","type":"PERSON","description":"Corby Rosset is an author who contributed to the research on Orca 2, Orca-Math, and Direct Nash Optimization, systems for teaching small language models how to reason, unlocking the potential of SLMs in grade school math, and teaching language models to self-improve with general preferences","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"HAMED KHANPOUR","type":"PERSON","description":"Hamed Khanpour is an author who contributed to the research on Orca 2, Orca-Math, and Direct Nash Optimization, systems for teaching small language models how to reason, unlocking the potential of SLMs in grade school math, and teaching language models to self-improve with general preferences","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"AHMED AWADALLAH","type":"PERSON","description":"Ahmed Awadallah is an author who contributed to the research on Orca 2, Orca-Math, Xtremedistil, Orca, and Direct Nash Optimization, systems for teaching small language models how to reason, unlocking the potential of SLMs in grade school math, multi-stage distillation for massive multilingual models, progressive learning from complex explanation traces of GPT-4, and teaching language models to self-improve with general preferences","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ORCA 2","type":"SYSTEM","description":"A system for teaching small language models how to reason, as described in the 2023 paper","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"SYSTEM"},{"name":"ORCA-MATH","type":"SYSTEM","description":"A system for unlocking the potential of SLMs in grade school math, as described in the 2024 paper","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"SYSTEM"},{"name":"XTREMEDISTIL","type":"SYSTEM","description":"A system for multi-stage distillation for massive multilingual models, as described in the 2020 paper","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"SYSTEM"},{"name":"ORCA","type":"SYSTEM","description":"A system for progressive learning from complex explanation traces of GPT-4, as described in the 2023 paper","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"SYSTEM"},{"name":"GPT-4","type":"TECHNOLOGY","description":"A large language model developed by OpenAI, as described in the 2023 technical report","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"TECHNOLOGY"},{"name":"EQ-BENCH","type":"SYSTEM","description":"An emotional intelligence benchmark for large language models, as described by Samuel J. Paech in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"SYSTEM"},{"name":"SAMUEL J. PAECH","type":"PERSON","description":"Samuel J. Paech is an author who contributed to the research on EQ-Bench, an emotional intelligence benchmark for large language models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"BAOLIN PENG","type":"PERSON","description":"Baolin Peng is an author who contributed to the research on instruction tuning with GPT-4","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"CHUNYUAN LI","type":"PERSON","description":"Chunyuan Li is an author who contributed to the research on instruction tuning with GPT-4","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"PENGCHENG HE","type":"PERSON","description":"Pengcheng He is an author who contributed to the research on instruction tuning with GPT-4","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"MICHEL GALLEY","type":"PERSON","description":"Michel Galley is an author who contributed to the research on instruction tuning with GPT-4","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"JIANFENG GAO","type":"PERSON","description":"Jianfeng Gao is an author who contributed to the research on instruction tuning with GPT-4","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"INSTRUCTION TUNING","type":"TECHNIQUE","description":"A method for improving the performance of language models by fine-tuning them on specific instructions, as described in the 2023 paper","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"TECHNIQUE"},{"name":"YIWEI QIN","type":"PERSON","description":"Yiwei Qin is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"KAIQIANG SONG","type":"PERSON","description":"Kaiqiang Song is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"YEBO WEN HU","type":"PERSON","description":"Yebo Wen Hu is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"WENLIN YAO","type":"PERSON","description":"Wenlin Yao is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"SANGWOO CHO","type":"PERSON","description":"Sangwoo Cho is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"XIAOYANG WANG","type":"PERSON","description":"Xiaoyang Wang is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"XUANSHENG WU","type":"PERSON","description":"Xuansheng Wu is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"FEI LIU","type":"PERSON","description":"Fei Liu is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"DONG YU","type":"PERSON","description":"Dong Yu is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"INFOBENCH","type":"SYSTEM","description":"A system for evaluating instruction-following ability in large language models, as described in the 2024 paper","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"SYSTEM"},{"name":"YUJIA QIN","type":"PERSON","description":"Yujia Qin is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"SHIHAO LIANG","type":"PERSON","description":"Shihao Liang is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"YINING YE","type":"PERSON","description":"Yining Ye is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"KUNLUN ZHU","type":"PERSON","description":"Kunlun Zhu is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"LAN YAN","type":"PERSON","description":"Lan Yan is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"YAXI LU","type":"PERSON","description":"Yaxi Lu is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"YANKAI LIN","type":"PERSON","description":"Yankai Lin is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"XIN CONG","type":"PERSON","description":"Xin Cong is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"XIANGRU TANG","type":"PERSON","description":"Xiangru Tang is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"BILL QIAN","type":"PERSON","description":"Bill Qian is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"SIHAN ZHAO","type":"PERSON","description":"Sihan Zhao is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"RUNCHU TIAN","type":"PERSON","description":"Runchu Tian is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"RUOBING XIE","type":"PERSON","description":"Ruobing Xie is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"JIE ZHOU","type":"PERSON","description":"Jie Zhou is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"MARK GERSTEIN","type":"PERSON","description":"Mark Gerstein is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"DAHAI LI","type":"PERSON","description":"Dahai Li is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ZHIYUAN LIU","type":"PERSON","description":"Zhiy","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"BERNARD GHANEM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bernard Ghanem is an author who contributed to the research on communicative agents for \"mind\" exploration of large language model society<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CAMEL\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">Communicative agents for \"mind\" exploration of large language model society, as described in the 2023 paper<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"XUECHEN LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuechen Li is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TIANYI ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tianyi Zhang is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YANN DUBOIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yann Dubois is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ROHAN TAORI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rohan Taori is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ISHAAN GULRAJANI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ishaan Gulrajani is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CARLOS GUESTRIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carlos Guestrin is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PERCY LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Percy Liang is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TATSUNORI B. HASHIMOTO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tatsunori B. Hashimoto is an author who contributed to the research on AlpacaEval, an automatic evaluator of instruction-following models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALPACAEVAL\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">An automatic evaluator of instruction-following models, as described in the 2023 paper<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"YIXIN LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yixin Liu is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALEXANDER R. FABBRI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alexander R. Fabbri is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIAWEN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiawen Chen is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YILUN ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yilun Zhao is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SIMENG HAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Simeng Han is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHAFIQ JOTY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shafiq Joty is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PENGFEI LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pengfei Liu is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarizationPengfei Liu is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DRAGOMIR RADEV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dragomir Radev is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHIEN-SHENG WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chien-Sheng Wu is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ARMAN COHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arman Cohan is an author who contributed to the research on benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MT-BENCH\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system for benchmarking, as described by Lm-sys in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"DANIEL VAN STRIEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel van Strien is an author who contributed to the research on Cosmopedia, a method for creating large-scale synthetic data for pre-training<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LOUBNA BEN ALLAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Loubna Ben Allal is an author who contributed to the research on Cosmopedia, a method for creating large-scale synthetic data for pre-training<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANTON LOZHKOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anton Lozhkov is an author who contributed to the research on Cosmopedia, a method for creating large-scale synthetic data for pre-training<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"COSMOPEDIA\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A method for creating large-scale synthetic data for pre-training, as described in the 2024 paper<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"ARINDAM MITRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arindam Mitra is an author who contributed to the research on Orca 2 and Orca-Math, systems for teaching small language models how to reason and unlocking the potential of SLMs in grade school math<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LUCIANO DEL CORRO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luciano Del Corro is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHWETI MAHAJAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shweti Mahajan is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANDRES CODAS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andres Codas is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CLARISSE SIMOES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clarisse Simoes is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SAHAJ AGARWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sahaj Agarwal is an author who contributed to the research on Orca 2 and Orca, systems for teaching small language models how to reason and progressive learning from complex explanation traces of GPT-4<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XUXI CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuxi Chen is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANASTASIA RAZDAIBIEDINA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anastasia Razdaibiedina is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ERIK JONES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Erik Jones is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KRITI AGGARWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kriti Aggarwal is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAMID PALANGI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hamid Palangi is an author who contributed to the research on Orca 2, Orca, and Xtremedistil, systems for teaching small language models how to reason, progressive learning from complex explanation traces of GPT-4, and multi-stage distillation for massive multilingual models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GUOQING ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guoqing Zheng is an author who contributed to the research on Orca 2, a system for teaching small language models how to reason<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CORBY ROSSET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Corby Rosset is an author who contributed to the research on Orca 2, Orca-Math, and Direct Nash Optimization, systems for teaching small language models how to reason, unlocking the potential of SLMs in grade school math, and teaching language models to self-improve with general preferences<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAMED KHANPOUR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hamed Khanpour is an author who contributed to the research on Orca 2, Orca-Math, and Direct Nash Optimization, systems for teaching small language models how to reason, unlocking the potential of SLMs in grade school math, and teaching language models to self-improve with general preferences<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AHMED AWADALLAH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ahmed Awadallah is an author who contributed to the research on Orca 2, Orca-Math, Xtremedistil, Orca, and Direct Nash Optimization, systems for teaching small language models how to reason, unlocking the potential of SLMs in grade school math, multi-stage distillation for massive multilingual models, progressive learning from complex explanation traces of GPT-4, and teaching language models to self-improve with general preferences<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ORCA 2\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system for teaching small language models how to reason, as described in the 2023 paper<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"ORCA-MATH\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system for unlocking the potential of SLMs in grade school math, as described in the 2024 paper<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"XTREMEDISTIL\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system for multi-stage distillation for massive multilingual models, as described in the 2020 paper<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"ORCA\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system for progressive learning from complex explanation traces of GPT-4, as described in the 2023 paper<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A large language model developed by OpenAI, as described in the 2023 technical report<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"EQ-BENCH\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">An emotional intelligence benchmark for large language models, as described by Samuel J. Paech in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"SAMUEL J. PAECH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Samuel J. Paech is an author who contributed to the research on EQ-Bench, an emotional intelligence benchmark for large language models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BAOLIN PENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Baolin Peng is an author who contributed to the research on instruction tuning with GPT-4<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHUNYUAN LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chunyuan Li is an author who contributed to the research on instruction tuning with GPT-4<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PENGCHENG HE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pengcheng He is an author who contributed to the research on instruction tuning with GPT-4<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MICHEL GALLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michel Galley is an author who contributed to the research on instruction tuning with GPT-4<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIANFENG GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jianfeng Gao is an author who contributed to the research on instruction tuning with GPT-4<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"INSTRUCTION TUNING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method for improving the performance of language models by fine-tuning them on specific instructions, as described in the 2023 paper<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"YIWEI QIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiwei Qin is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KAIQIANG SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kaiqiang Song is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YEBO WEN HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yebo Wen Hu is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WENLIN YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wenlin Yao is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SANGWOO CHO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sangwoo Cho is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIAOYANG WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiaoyang Wang is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XUANSHENG WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuansheng Wu is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FEI LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fei Liu is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DONG YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dong Yu is an author who contributed to the research on InfoBench, a system for evaluating instruction-following ability in large language models<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"INFOBENCH\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system for evaluating instruction-following ability in large language models, as described in the 2024 paper<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"YUJIA QIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yujia Qin is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHIHAO LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shihao Liang is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YINING YE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yining Ye is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KUNLUN ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kunlun Zhu is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LAN YAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lan Yan is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YAXI LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yaxi Lu is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YANKAI LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yankai Lin is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIN CONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xin Cong is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIANGRU TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiangru Tang is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BILL QIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bill Qian is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SIHAN ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sihan Zhao is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RUNCHU TIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Runchu Tian is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RUOBING XIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ruobing Xie is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIE ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jie Zhou is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MARK GERSTEIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mark Gerstein is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DAHAI LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dahai Li is an author who contributed to the research on ToolLLM, a system for facilitating large language models to master 16000+ real-world APIs<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHIYUAN LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiy<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"f4e98ee0b7fb42428f3312f29cb444dd","chunk":"\/2404.03715 .\n[29]Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross\nAnderson. The curse of recursion: Training on generated data makes models forget, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2305.17493 .\n[30]Mohammed Latif Siddiq, Jiahao Zhang, Lindsay Roney, and Joanna C. S. Santos.\nRe(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos at-\ntacks. In Proceedings ofthe46thInternational Conference onSoftware Engineering, NIER\nTrack(ICSE-NIER \u201924), 2024. doi: 10.1145\/3639476.3639757.\n[31]Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, AakankshaChowdhery, QuocVLe, EdHChi, DennyZhou, , andJasonWei. Challenging\nbig-bench tasks and whether chain-of-thought can solve them. arXivpreprint arXiv:2210.09261 ,\n2022.\n[32]Wen wai Yim, Yujuan Fu, Asma Ben Abacha, Neal Snider, Thomas Lin, and Meliha Yetisgen.\nAci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note\ngeneration, 2023.\n[33]Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun\nZhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger,\nand Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation,\n2023. URL https:\/\/arxiv.org\/abs\/2308.08155 .\n[34]Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, and\nCaiming Xiong. Fofo: A benchmark to evaluate llms\u2019 format-following capability, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2402.18667 .\n[35]Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented\ngeneration for medicine. arXivpreprint arXiv:2402.13178, 2024.\n[36]Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and\nDaxin Jiang. Wizardlm: Empowering large language models to follow complex instructions,\n2023.\n[37]Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok,\nZhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical\nquestions for large language models. arXivpreprint arXiv:2309.12284, 2023.\n24[38]Yifan Zhang, Yifan Luo, Yang Yuan, and Andrew Chi-Chih Yao. Automathtext: Autonomous\ndata selection with language models for mathematical texts. arXivpreprint arXiv:2402.07625 ,\n2024.\n[39]Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\nmodels, 2023.\n[40]Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny\nZhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2311.07911 .\n25A Agentic Flows Details\nA.1 Reading Comprehension Flow\nReading Comprehension transformation agents :\n1.Argument Passage Generator: This agent is adept at creating passages that\narticulate arguments, which may occasionally contain logical inconsistencies.\n2.Debate Passage Generator: It specializes in crafting passages that mimic the\nstructure and content of debate transcripts.\n3.Conversation Passage Generator: This agent generates passages that depict\ndialogues.\n4.Meeting Transcript Generator: It is designed to produce meeting transcripts.\n5.Poem Generator: This agent generates poems.\n6.Satirical Passage Generator: It creates texts infused with satirical wit.\n7.Instructional Passage Generator: This agent generates passages resembling\ninstructional manuals.\n8.Long Text Generator: It extends the original text by incorporating additional\ninformation, thereby increasing its length.\n9.Identity Agent: A straightforward agent that replicates the input text verbatim.\nInstruction Taxonomy for Seed Instruction Generation Flow\n1.Literal Comprehension Question (Short Answer(or list)): a question that asks for a\nspecific detail(s) or fact(s) clearly stated in the text.\n2.Numerical Discrete Reasoning (Reasoning): questions that require the reader to use\nnumerical reasoning over many facts from the text.\n3.Critical Comprehension Question (True\/False): construct two statements about the\npurpose or point of view that the reader must assess as true or false, with one being\ntrue and the other false.\n4.Evaluative Comprehension Question (Essay","chunk_id":"f4e98ee0b7fb42428f3312f29cb444dd","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"ZAKHAR SHUMAYLOV","type":"PERSON","description":"Zakhar Shumaylov is an author who contributed to the research on the curse of recursion and its effects on training models with generated data","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"YIREN ZHAO","type":"PERSON","description":"Yiren Zhao is an author who contributed to the research on the curse of recursion and its effects on training models with generated data","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"YARIN GAL","type":"PERSON","description":"Yarin Gal is an author who contributed to the research on the curse of recursion and its effects on training models with generated data","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"NICOLAS PAPERNOT","type":"PERSON","description":"Nicolas Papernot is an author who contributed to the research on the curse of recursion and its effects on training models with generated data","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"ROSS ANDERSON","type":"PERSON","description":"Ross Anderson is an author who contributed to the research on the curse of recursion and its effects on training models with generated data","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"THE CURSE OF RECURSION","type":"RESEARCH TOPIC","description":"A study on how training models on generated data can lead to forgetting previously learned information","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"RESEARCH TOPIC"},{"name":"MOHAMMED LATIF SIDDIQ","type":"PERSON","description":"Mohammed Latif Siddiq is an author who contributed to the research on evaluating generated regular expressions and their proneness to DoS attacks","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"JIAHAO ZHANG","type":"PERSON","description":"Jiahao Zhang is an author who contributed to the research on evaluating generated regular expressions and their proneness to DoS attacks","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"LINDSAY RONEY","type":"PERSON","description":"Lindsay Roney is an author who contributed to the research on evaluating generated regular expressions and their proneness to DoS attacks","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"JOANNA C. S. SANTOS","type":"PERSON","description":"Joanna C. S. Santos is an author who contributed to the research on evaluating generated regular expressions and their proneness to DoS attacks","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"RE(GEX|DOS)EVAL","type":"RESEARCH TOPIC","description":"A study on evaluating generated regular expressions and their susceptibility to denial-of-service attacks","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"RESEARCH TOPIC"},{"name":"MIRAC SUZGUN","type":"PERSON","description":"Mirac Suzgun is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"NATHAN SCALES","type":"PERSON","description":"Nathan Scales is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"NATHANAEL SCH\u00c4RLI","type":"PERSON","description":"Nathanael Sch\u00e4rli is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"SEBASTIAN GEHRMANN","type":"PERSON","description":"Sebastian Gehrmann is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"YI TAY","type":"PERSON","description":"Yi Tay is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"HYUNG WON CHUNG","type":"PERSON","description":"Hyung Won Chung is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"AAKANKSHA CHOWDHERY","type":"PERSON","description":"Aakanksha Chowdhery is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"QUOC V LE","type":"PERSON","description":"Quoc V Le is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"ED H CHI","type":"PERSON","description":"Ed H Chi is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"DENNY ZHOU","type":"PERSON","description":"Denny Zhou is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"JASON WEI","type":"PERSON","description":"Jason Wei is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"CHAIN-OF-THOUGHT","type":"TECHNIQUE","description":"A reasoning process that involves breaking down complex problems into a series of simpler, interconnected steps","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"TECHNIQUE"},{"name":"WEN WAI YIM","type":"PERSON","description":"Wen Wai Yim is an author who contributed to the research on ACI-Bench, a dataset for benchmarking automatic visit note generation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"YUJUAN FU","type":"PERSON","description":"Yujuan Fu is an author who contributed to the research on ACI-Bench, a dataset for benchmarking automatic visit note generation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"ASMA BEN ABACHA","type":"PERSON","description":"Asma Ben Abacha is an author who contributed to the research on ACI-Bench, a dataset for benchmarking automatic visit note generation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"NEAL SNIDER","type":"PERSON","description":"Neal Snider is an author who contributed to the research on ACI-Bench, a dataset for benchmarking automatic visit note generation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"THOMAS LIN","type":"PERSON","description":"Thomas Lin is an author who contributed to the research on ACI-Bench, a dataset for benchmarking automatic visit note generation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"MELIHA YETISGEN","type":"PERSON","description":"Meliha Yetisgen is an author who contributed to the research on ACI-Bench, a dataset for benchmarking automatic visit note generation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"ACI-BENCH","type":"DATASET","description":"A novel ambient clinical intelligence dataset for benchmarking automatic visit note generation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"DATASET"},{"name":"QINGYUN WU","type":"PERSON","description":"Qingyun Wu is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"GAGAN BANSAL","type":"PERSON","description":"Gagan Bansal is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"JIEYU ZHANG","type":"PERSON","description":"Jieyu Zhang is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"YIRAN WU","type":"PERSON","description":"Yiran Wu is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"BEIBIN LI","type":"PERSON","description":"Beibin Li is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"ERKANG ZHU","type":"PERSON","description":"Erkang Zhu is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"LI JIANG","type":"PERSON","description":"Li Jiang is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"XIAOYUN ZHANG","type":"PERSON","description":"Xiaoyun Zhang is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"SHAOKUN ZHANG","type":"PERSON","description":"Shaokun Zhang is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"JIALE LIU","type":"PERSON","description":"Jiale Liu is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"AHMED HASSAN AWADALLAH","type":"PERSON","description":"Ahmed Hassan Awadallah is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"RYEN W WHITE","type":"PERSON","description":"Ryen W White is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"DOUG BURGER","type":"PERSON","description":"Doug Burger is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"CHI WANG","type":"PERSON","description":"Chi Wang is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"AUTOGEN","type":"SYSTEM","description":"A system enabling next-gen large language model applications via multi-agent conversation","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"SYSTEM"},{"name":"CONGYING XIA","type":"PERSON","description":"Congying Xia is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"CHEN XING","type":"PERSON","description":"Chen Xing is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"JIANGSHU DU","type":"PERSON","description":"Jiangshu Du is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"XINYI YANG","type":"PERSON","description":"Xinyi Yang is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"YIHAO FENG","type":"PERSON","description":"Yihao Feng is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"RAN XU","type":"PERSON","description":"Ran Xu is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"WENPENG YIN","type":"PERSON","description":"Wenpeng Yin is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"CAIMING XIONG","type":"PERSON","description":"Caiming Xiong is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"FOFO","type":"BENCHMARK","description":"A benchmark to evaluate large language models' format-following capability","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"BENCHMARK"},{"name":"GUANGZHI XIONG","type":"PERSON","description":"Guangzhi Xiong is an author who contributed to the research on benchmarking retrieval-augmented generation for medicine","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"QIAO JIN","type":"PERSON","description":"Qiao Jin is an author who contributed to the research on benchmarking retrieval-augmented generation for medicine","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"ZHIYONG LU","type":"PERSON","description":"Zhiyong Lu is an author who contributed to the research on benchmarking retrieval-augmented generation for medicine","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"AIDONG ZHANG","type":"PERSON","description":"Aidong Zhang is an author who contributed to the research on benchmarking retrieval-augmented generation for medicine","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"RETRIEVAL-AUGMENTED GENERATION FOR MEDICINE","type":"RESEARCH TOPIC","description":"A study on the application of retrieval-augmented generation techniques in the field of medicine","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"RESEARCH TOPIC"},{"name":"CAN XU","type":"PERSON","description":"Can Xu is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"QINGFENG SUN","type":"PERSON","description":"Qingfeng Sun is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"KAI ZHENG","type":"PERSON","description":"Kai Zheng is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"XIUBO GENG","type":"PERSON","description":"Xiubo Geng is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"PU ZHAO","type":"PERSON","description":"Pu Zhao is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"JIAZHAN FENG","type":"PERSON","description":"Jiazhan Feng is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"CHONGYANG TAO","type":"PERSON","description":"Chongyang Tao is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"DAXIN JIANG","type":"PERSON","description":"Daxin Jiang is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"WIZARDLM","type":"SYSTEM","description":"A system designed to empower large language models to follow complex instructions","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"SYSTEM"},{"name":"LONGHUI YU","type":"PERSON","description":"Longhui Yu is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"WEISEN JIANG","type":"PERSON","description":"Weisen Jiang is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"HAN SHI","type":"PERSON","description":"Han Shi is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"JINCHENG YU","type":"PERSON","description":"Jincheng Yu is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"ZHENGYING LIU","type":"PERSON","description":"Zhengying Liu is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"YU ZHANG","type":"PERSON","description":"Yu Zhang is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"JAMES T KWOK","type":"PERSON","description":"James T Kwok is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"ZHENGUO LI","type":"PERSON","description":"Zhenguo Li is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"ADRIAN WELLER","type":"PERSON","description":"Adrian Weller is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"WEIYANG LIU","type":"PERSON","description":"Weiyang Liu is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"METAMATH","type":"SYSTEM","description":"A system designed to bootstrap mathematical questions for large language models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"SYSTEM"},{"name":"YIFAN ZHANG","type":"PERSON","description":"Yifan Zhang is an author who contributed to the research on AutoMathText, a system for autonomous data selection with language models for mathematical texts","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"YIFAN LUO","type":"PERSON","description":"Yifan Luo is an author who contributed to the research on AutoMathText, a system for autonomous data selection with language models for mathematical texts","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"YANG YUAN","type":"PERSON","description":"Yang Yuan is an author who contributed to the research on AutoMathText, a system for autonomous data selection with language models for mathematical texts","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"ANDREW CHI-CHIH YAO","type":"PERSON","description":"Andrew Chi-Chih Yao is an author who contributed to the research on AutoMathText, a system for autonomous data selection with language models for mathematical texts","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"AUTOMATHTEXT","type":"SYSTEM","description":"A system for autonomous data selection with language models for mathematical texts","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"SYSTEM"},{"name":"WANJUN ZHONG","type":"PERSON","description":"Wanjun Zhong is an author who contributed to the research on AGIEval, a human-centric benchmark for evaluating foundation models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"RUIXIANG CUI","type":"PERSON","description":"Ruixiang Cui is an author who contributed to the research on AGIEval, a human-centric benchmark for evaluating foundation models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"YIDUO GUO","type":"PERSON","description":"Yiduo Guo is an author who contributed to the research on AGIEval, a human-centric benchmark for evaluating foundation models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"YAOBO LIANG","type":"PERSON","description":"Yaobo Liang is an author who contributed to the research on AGIEval, a human-centric benchmark for evaluating foundation models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"SHUAI LU","type":"PERSON","description":"Shuai Lu is an author who contributed to the research on AGIEval, a human-centric benchmark for evaluating foundation models","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"YANLIN WANG","type":"**ANALYSIS:**(\"ENTITY\"","description":"ILIA SHUMAILOV","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ZAKHAR SHUMAYLOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zakhar Shumaylov is an author who contributed to the research on the curse of recursion and its effects on training models with generated data<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YIREN ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiren Zhao is an author who contributed to the research on the curse of recursion and its effects on training models with generated data<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YARIN GAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yarin Gal is an author who contributed to the research on the curse of recursion and its effects on training models with generated data<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NICOLAS PAPERNOT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicolas Papernot is an author who contributed to the research on the curse of recursion and its effects on training models with generated data<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ROSS ANDERSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ross Anderson is an author who contributed to the research on the curse of recursion and its effects on training models with generated data<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"THE CURSE OF RECURSION\">      <data key=\"d0\">RESEARCH TOPIC<\/data>      <data key=\"d1\">A study on how training models on generated data can lead to forgetting previously learned information<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">RESEARCH TOPIC<\/data>    <\/node>    <node id=\"MOHAMMED LATIF SIDDIQ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mohammed Latif Siddiq is an author who contributed to the research on evaluating generated regular expressions and their proneness to DoS attacks<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIAHAO ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiahao Zhang is an author who contributed to the research on evaluating generated regular expressions and their proneness to DoS attacks<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LINDSAY RONEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lindsay Roney is an author who contributed to the research on evaluating generated regular expressions and their proneness to DoS attacks<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOANNA C. S. SANTOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joanna C. S. Santos is an author who contributed to the research on evaluating generated regular expressions and their proneness to DoS attacks<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RE(GEX|DOS)EVAL\">      <data key=\"d0\">RESEARCH TOPIC<\/data>      <data key=\"d1\">A study on evaluating generated regular expressions and their susceptibility to denial-of-service attacks<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">RESEARCH TOPIC<\/data>    <\/node>    <node id=\"MIRAC SUZGUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mirac Suzgun is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NATHAN SCALES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nathan Scales is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NATHANAEL SCH&#196;RLI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nathanael Sch&#228;rli is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SEBASTIAN GEHRMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sebastian Gehrmann is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YI TAY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi Tay is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HYUNG WON CHUNG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hyung Won Chung is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AAKANKSHA CHOWDHERY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aakanksha Chowdhery is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QUOC V LE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quoc V Le is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ED H CHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ed H Chi is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DENNY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Denny Zhou is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JASON WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jason Wei is an author who contributed to the research on challenging big-bench tasks and the effectiveness of chain-of-thought reasoning<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A reasoning process that involves breaking down complex problems into a series of simpler, interconnected steps<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"WEN WAI YIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wen Wai Yim is an author who contributed to the research on ACI-Bench, a dataset for benchmarking automatic visit note generation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUJUAN FU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yujuan Fu is an author who contributed to the research on ACI-Bench, a dataset for benchmarking automatic visit note generation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ASMA BEN ABACHA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Asma Ben Abacha is an author who contributed to the research on ACI-Bench, a dataset for benchmarking automatic visit note generation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NEAL SNIDER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Neal Snider is an author who contributed to the research on ACI-Bench, a dataset for benchmarking automatic visit note generation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"THOMAS LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Lin is an author who contributed to the research on ACI-Bench, a dataset for benchmarking automatic visit note generation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MELIHA YETISGEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Meliha Yetisgen is an author who contributed to the research on ACI-Bench, a dataset for benchmarking automatic visit note generation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ACI-BENCH\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A novel ambient clinical intelligence dataset for benchmarking automatic visit note generation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"QINGYUN WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qingyun Wu is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GAGAN BANSAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gagan Bansal is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIEYU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jieyu Zhang is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YIRAN WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiran Wu is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BEIBIN LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Beibin Li is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ERKANG ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Erkang Zhu is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LI JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Li Jiang is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIAOYUN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiaoyun Zhang is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHAOKUN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shaokun Zhang is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIALE LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiale Liu is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AHMED HASSAN AWADALLAH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ahmed Hassan Awadallah is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RYEN W WHITE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ryen W White is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DOUG BURGER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Doug Burger is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chi Wang is an author who contributed to the research on AutoGen, enabling next-gen LLM applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AUTOGEN\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system enabling next-gen large language model applications via multi-agent conversation<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"CONGYING XIA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Congying Xia is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHEN XING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Xing is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIANGSHU DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiangshu Du is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XINYI YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xinyi Yang is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YIHAO FENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yihao Feng is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RAN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ran Xu is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WENPENG YIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wenpeng Yin is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CAIMING XIONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Caiming Xiong is an author who contributed to the research on FoFo, a benchmark to evaluate LLMs' format-following capability<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FOFO\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark to evaluate large language models' format-following capability<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"GUANGZHI XIONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guangzhi Xiong is an author who contributed to the research on benchmarking retrieval-augmented generation for medicine<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QIAO JIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qiao Jin is an author who contributed to the research on benchmarking retrieval-augmented generation for medicine<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHIYONG LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyong Lu is an author who contributed to the research on benchmarking retrieval-augmented generation for medicine<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AIDONG ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aidong Zhang is an author who contributed to the research on benchmarking retrieval-augmented generation for medicine<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED GENERATION FOR MEDICINE\">      <data key=\"d0\">RESEARCH TOPIC<\/data>      <data key=\"d1\">A study on the application of retrieval-augmented generation techniques in the field of medicine<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">RESEARCH TOPIC<\/data>    <\/node>    <node id=\"CAN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Can Xu is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QINGFENG SUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qingfeng Sun is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KAI ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kai Zheng is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIUBO GENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiubo Geng is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PU ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pu Zhao is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIAZHAN FENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiazhan Feng is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHONGYANG TAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chongyang Tao is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DAXIN JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daxin Jiang is an author who contributed to the research on WizardLM, empowering large language models to follow complex instructions<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WIZARDLM\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system designed to empower large language models to follow complex instructions<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"LONGHUI YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Longhui Yu is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WEISEN JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weisen Jiang is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAN SHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Han Shi is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JINCHENG YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jincheng Yu is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHENGYING LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhengying Liu is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu Zhang is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JAMES T KWOK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">James T Kwok is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHENGUO LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhenguo Li is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ADRIAN WELLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adrian Weller is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WEIYANG LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weiyang Liu is an author who contributed to the research on MetaMath, a system for bootstrapping mathematical questions for large language models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"METAMATH\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system designed to bootstrap mathematical questions for large language models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"YIFAN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yifan Zhang is an author who contributed to the research on AutoMathText, a system for autonomous data selection with language models for mathematical texts<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YIFAN LUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yifan Luo is an author who contributed to the research on AutoMathText, a system for autonomous data selection with language models for mathematical texts<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YANG YUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang Yuan is an author who contributed to the research on AutoMathText, a system for autonomous data selection with language models for mathematical texts<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANDREW CHI-CHIH YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrew Chi-Chih Yao is an author who contributed to the research on AutoMathText, a system for autonomous data selection with language models for mathematical texts<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AUTOMATHTEXT\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A system for autonomous data selection with language models for mathematical texts<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">SYSTEM<\/data>    <\/node>    <node id=\"WANJUN ZHONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wanjun Zhong is an author who contributed to the research on AGIEval, a human-centric benchmark for evaluating foundation models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RUIXIANG CUI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ruixiang Cui is an author who contributed to the research on AGIEval, a human-centric benchmark for evaluating foundation models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YIDUO GUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiduo Guo is an author who contributed to the research on AGIEval, a human-centric benchmark for evaluating foundation models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YAOBO LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yaobo Liang is an author who contributed to the research on AGIEval, a human-centric benchmark for evaluating foundation models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHUAI LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shuai Lu is an author who contributed to the research on AGIEval, a human-centric benchmark for evaluating foundation models<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YANLIN WANG\">      <data key=\"d0\">**ANALYSIS:**(\"ENTITY\"<\/data>      <data key=\"d1\">ILIA SHUMAILOV<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"5819b66e04fd77fa705574edc49395bb","chunk":" for a\nspecific detail(s) or fact(s) clearly stated in the text.\n2.Numerical Discrete Reasoning (Reasoning): questions that require the reader to use\nnumerical reasoning over many facts from the text.\n3.Critical Comprehension Question (True\/False): construct two statements about the\npurpose or point of view that the reader must assess as true or false, with one being\ntrue and the other false.\n4.Evaluative Comprehension Question (Essay): an open-ended question that prompts\nan in-depth analysis of the text\u2019s theme or the effectiveness of an argument.\n5.Vocabulary and Language Use (Fill-in-the-Blank): a fill-in-the-blank question that\ntests understanding of a particular word or phrase used in the text.\n6.Relationship Comprehension Question (Matching): a matching question where\nrespondents pair items based on a specific criterion.\n7.Sequencing Events (Ordering): a series of events from the text arranged in the\ncorrect chronological order.\n8. Strengthen: identify information that would make the argument\u2019s conclusion more\nlikely to be true.\n9.Weaken: find evidence or an argument that would make the conclusion less likely to\nbe true.\n10.Assumption (Necessary Assumption): determine what must be true for the argument\nto hold.\n11. Flaw: point out a mistake in the argument\u2019s reasoning.\n12.Inference (Must Be True): Choose an option that logically follows from the informa-\ntion provided.\n13.Principle (Identify the Principle): Recognize the general rule or principle that\nunderlies the argument.\n14.Method of Reasoning (Describe the Argument): Describe how the argument is\nconstructed logically.\n15.Resolve the Paradox: Offer an explanation that reconciles seemingly contradictory\ninformation.\n26A.2 Text Modification Flow\nInstruction Taxonomy for Seed Instruction Generation Flow\n1.Paraphrasing: Rewriting text using different words and sentence structures while\nmaintaining the original meaning.\n2.Text Simplification: Making text easier to read and understand by using simpler\nwords and sentence structures, often for children or language learners.\n3.Text Expansion: Adding more information or detail to make text more comprehensive\nor to meet a certain word count.\n4.Text Translation: Converting text from one language to another while attempting\nto preserve the original meaning as closely as possible.\n5.Text Formatting: Altering the appearance of text to improve readability or for\nstylistic purposes.\n6.Sentiment Modification: Changing the tone of the text to alter its emotional impact,\nsuch as making a sentence sound more positive or negative.\n7.Text Annotation: Adding notes, comments, or explanations to a text, often for the\npurpose of analysis or to provide additional context.\n8.Keyword Replacement: Substituting specific words or phrases with synonyms or\nrelated terms.\n9. Text Removing: Redacting or removing content from text.\n10.Text Capitalization: Adjusting the case of letters in text, such as converting to\nuppercase, lowercase, title case, or sentence case, starting every sentence with a\nparticular letter, word.\n11.Text Styling: Applying styles like bold, italics, underline, etc., to emphasize certain\nparts of the text or for aesthetic purposes.\n12.Content Rewriting: Extensively modifying a text to produce a new version, which\ncould involve changing the perspective, style, or target audience.\n13.Data Normalization: Standardizing text to ensure consistency, such as converting\ndates and times to a standard format or unifying the spelling of words.\n14.Plagiarism Rewording: Altering text to avoid plagiarism, ensuring that the content\nis original.\n15.Code Switching: Alternating between languages or dialects within a text, often to\nreflect bilingual speakers\u2019 patterns or for creative writing.\n16.Text Obfuscation: Intentionally making text vague or harder to understand, some-\ntimes for security purposes (like masking personal data).\n17.Textual Entailment: Modifying a sentence or phrase to either entail or contradict\nanother sentence, often used in natural language processing tasks.\n18.Rewriting with vocabulary limitations: Rewriting the entire text or a piece of it\nwhile using a limited vocabulary. For example, all words should start with letter \u2019a\u2019,\nall n-th word should start with letter \u2019b\u2019, each sentence should start with a \u2019vowel\u2019,\netc.\nB Evaluation Details\nThe types of tasks\/benchmarks and the corresponding method used to extract answer and\ngenerate metrics is specified below:\n\u2022Multiple Choice Questions : All the models are evaluated in an open-ended\ngeneration setting with an empty system message We then use GPT-4 for extraction\nof the option selected by the model from model\u2019s response instead of regex based\nextraction done in [ 18]. The extracted prediction is matched with the ground truth\nto generate accuracy scores.\nThe system message used for the GPT-4 extractions is as follows:\n27MCQ GPT-4 Extraction System Message\nYou are an Evaluator Assistant. You support the exam evaluator by parsing\nstudent responses. You are an unbiased Evaluator and do not rely on your\nknowledge but stick to the user provided context. You are provided with the\nquestion, answer options and a student\u2019s response. Your task is to parse the\noption student selected in their response as their final answer and return the\nalphabet ID of that answer in the provided options. If the student gave multiple\nanswers return them as a list.\nUse the following format:\nParsed Student Answer: Final answer extracted from Student\u2019s response. This\nshould only be the alphabets representing the option the student chose.\nExample 1:\nInput :\nQuestion:\nFind all c in Z3such that Z3[x]\/(x2+c)is a field.\nStudent Response :\nI think 0 is incorrect, so is 2.","chunk_id":"5819b66e04fd77fa705574edc49395bb","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"CRITICAL COMPREHENSION QUESTION","type":"QUESTION TYPE","description":"Construct two statements about the purpose or point of view that the reader must assess as true or false, with one being true and the other false","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"QUESTION TYPE"},{"name":"EVALUATIVE COMPREHENSION QUESTION","type":"QUESTION TYPE","description":"An open-ended question that prompts an in-depth analysis of the text\u2019s theme or the effectiveness of an argument","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"QUESTION TYPE"},{"name":"VOCABULARY AND LANGUAGE USE","type":"QUESTION TYPE","description":"A fill-in-the-blank question that tests understanding of a particular word or phrase used in the text","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"QUESTION TYPE"},{"name":"RELATIONSHIP COMPREHENSION QUESTION","type":"QUESTION TYPE","description":"A matching question where respondents pair items based on a specific criterion","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"QUESTION TYPE"},{"name":"SEQUENCING EVENTS","type":"QUESTION TYPE","description":"A series of events from the text arranged in the correct chronological order","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"QUESTION TYPE"},{"name":"STRENGTHEN","type":"QUESTION TYPE","description":"Identify information that would make the argument\u2019s conclusion more likely to be true","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"QUESTION TYPE"},{"name":"WEAKEN","type":"QUESTION TYPE","description":"Find evidence or an argument that would make the conclusion less likely to be true","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"QUESTION TYPE"},{"name":"ASSUMPTION","type":"QUESTION TYPE","description":"Determine what must be true for the argument to hold","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"QUESTION TYPE"},{"name":"FLAW","type":"QUESTION TYPE","description":"Point out a mistake in the argument\u2019s reasoning","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"QUESTION TYPE"},{"name":"INFERENCE","type":"QUESTION TYPE","description":"Choose an option that logically follows from the information provided","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"QUESTION TYPE"},{"name":"PRINCIPLE","type":"QUESTION TYPE","description":"Recognize the general rule or principle that underlies the argument","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"QUESTION TYPE"},{"name":"METHOD OF REASONING","type":"QUESTION TYPE","description":"Describe how the argument is constructed logically","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"QUESTION TYPE"},{"name":"RESOLVE THE PARADOX","type":"QUESTION TYPE","description":"Offer an explanation that reconciles seemingly contradictory information","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"QUESTION TYPE"},{"name":"PARAPHRASING","type":"TEXT MODIFICATION","description":"Rewriting text using different words and sentence structures while maintaining the original meaning","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"TEXT SIMPLIFICATION","type":"TEXT MODIFICATION","description":"Making text easier to read and understand by using simpler words and sentence structures, often for children or language learners","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"TEXT EXPANSION","type":"TEXT MODIFICATION","description":"Adding more information or detail to make text more comprehensive or to meet a certain word count","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"TEXT TRANSLATION","type":"TEXT MODIFICATION","description":"Converting text from one language to another while attempting to preserve the original meaning as closely as possible","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"TEXT FORMATTING","type":"TEXT MODIFICATION","description":"Altering the appearance of text to improve readability or for stylistic purposes","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"SENTIMENT MODIFICATION","type":"TEXT MODIFICATION","description":"Changing the tone of the text to alter its emotional impact, such as making a sentence sound more positive or negative","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"TEXT ANNOTATION","type":"TEXT MODIFICATION","description":"Adding notes, comments, or explanations to a text, often for the purpose of analysis or to provide additional context","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"KEYWORD REPLACEMENT","type":"TEXT MODIFICATION","description":"Substituting specific words or phrases with synonyms or related terms","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"TEXT REMOVING","type":"TEXT MODIFICATION","description":"Redacting or removing content from text","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"TEXT CAPITALIZATION","type":"TEXT MODIFICATION","description":"Adjusting the case of letters in text, such as converting to uppercase, lowercase, title case, or sentence case, starting every sentence with a particular letter, word","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"TEXT STYLING","type":"TEXT MODIFICATION","description":"Applying styles like bold, italics, underline, etc., to emphasize certain parts of the text or for aesthetic purposes","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"CONTENT REWRITING","type":"TEXT MODIFICATION","description":"Extensively modifying a text to produce a new version, which could involve changing the perspective, style, or target audience","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"DATA NORMALIZATION","type":"TEXT MODIFICATION","description":"Standardizing text to ensure consistency, such as converting dates and times to a standard format or unifying the spelling of words","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"PLAGIARISM REWORDING","type":"TEXT MODIFICATION","description":"Altering text to avoid plagiarism, ensuring that the content is original","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"CODE SWITCHING","type":"TEXT MODIFICATION","description":"Alternating between languages or dialects within a text, often to reflect bilingual speakers\u2019 patterns or for creative writing","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"TEXT OBFUSCATION","type":"TEXT MODIFICATION","description":"Intentionally making text vague or harder to understand, sometimes for security purposes like masking personal data","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"TEXTUAL ENTAILMENT","type":"TEXT MODIFICATION","description":"Modifying a sentence or phrase to either entail or contradict another sentence, often used in natural language processing tasks","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"REWRITING WITH VOCABULARY LIMITATIONS","type":"TEXT MODIFICATION","description":"Rewriting the entire text or a piece of it while using a limited vocabulary, such as all words starting with a specific letter","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TEXT MODIFICATION"},{"name":"MULTIPLE CHOICE QUESTIONS","type":"EVALUATION METHOD","description":"A method where models are evaluated in an open-ended generation setting with an empty system message, and GPT-4 is used for extraction of the option selected by the model from the model\u2019s response","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"EVALUATION METHOD"},{"name":"GPT-4","type":"TECHNOLOGY","description":"An advanced language model used for extracting the option selected by the model from the model\u2019s response in multiple choice questions","source_id":"5819b66e04fd77fa705574edc49395bb","entity_type":"TECHNOLOGY"},{"name":"NUMERICAL DISCRETE REASONING","type":"","description":"","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"QUESTION TYPE","type":"","description":"","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT MODIFICATION","type":"","description":"","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"EVALUATION METHOD","type":"","description":"","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TECHNOLOGY","type":"","description":"","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"INSTRUCTION TAXONOMY","type":"EVALUATION METHOD","description":"A structured classification of different types of instructions used for seed instruction generation","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"SEED INSTRUCTION GENERATION","type":"EVALUATION METHOD","description":"The process of creating initial instructions for a task, often used in machine learning and natural language processing","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"EVALUATOR ASSISTANT","type":"TECHNOLOGY","description":"A system or tool designed to assist evaluators by parsing student responses and extracting relevant information","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"EXTRACTION SYSTEM MESSAGE","type":"TECHNOLOGY","description":"A predefined message used by the GPT-4 model to extract the option selected by the model from the model\u2019s response in multiple choice questions","source_id":"5819b66e04fd77fa705574edc49395bb"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"CRITICAL COMPREHENSION QUESTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Construct two statements about the purpose or point of view that the reader must assess as true or false, with one being true and the other false<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"EVALUATIVE COMPREHENSION QUESTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">An open-ended question that prompts an in-depth analysis of the text&#8217;s theme or the effectiveness of an argument<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"VOCABULARY AND LANGUAGE USE\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">A fill-in-the-blank question that tests understanding of a particular word or phrase used in the text<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"RELATIONSHIP COMPREHENSION QUESTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">A matching question where respondents pair items based on a specific criterion<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"SEQUENCING EVENTS\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">A series of events from the text arranged in the correct chronological order<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"STRENGTHEN\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Identify information that would make the argument&#8217;s conclusion more likely to be true<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"WEAKEN\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Find evidence or an argument that would make the conclusion less likely to be true<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"ASSUMPTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Determine what must be true for the argument to hold<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"FLAW\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Point out a mistake in the argument&#8217;s reasoning<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"INFERENCE\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Choose an option that logically follows from the information provided<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"PRINCIPLE\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Recognize the general rule or principle that underlies the argument<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"METHOD OF REASONING\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Describe how the argument is constructed logically<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"RESOLVE THE PARADOX\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Offer an explanation that reconciles seemingly contradictory information<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">QUESTION TYPE<\/data>    <\/node>    <node id=\"PARAPHRASING\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Rewriting text using different words and sentence structures while maintaining the original meaning<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"TEXT SIMPLIFICATION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Making text easier to read and understand by using simpler words and sentence structures, often for children or language learners<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"TEXT EXPANSION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Adding more information or detail to make text more comprehensive or to meet a certain word count<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"TEXT TRANSLATION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Converting text from one language to another while attempting to preserve the original meaning as closely as possible<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"TEXT FORMATTING\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Altering the appearance of text to improve readability or for stylistic purposes<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"SENTIMENT MODIFICATION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Changing the tone of the text to alter its emotional impact, such as making a sentence sound more positive or negative<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"TEXT ANNOTATION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Adding notes, comments, or explanations to a text, often for the purpose of analysis or to provide additional context<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"KEYWORD REPLACEMENT\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Substituting specific words or phrases with synonyms or related terms<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"TEXT REMOVING\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Redacting or removing content from text<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"TEXT CAPITALIZATION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Adjusting the case of letters in text, such as converting to uppercase, lowercase, title case, or sentence case, starting every sentence with a particular letter, word<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"TEXT STYLING\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Applying styles like bold, italics, underline, etc., to emphasize certain parts of the text or for aesthetic purposes<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"CONTENT REWRITING\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Extensively modifying a text to produce a new version, which could involve changing the perspective, style, or target audience<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"DATA NORMALIZATION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Standardizing text to ensure consistency, such as converting dates and times to a standard format or unifying the spelling of words<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"PLAGIARISM REWORDING\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Altering text to avoid plagiarism, ensuring that the content is original<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"CODE SWITCHING\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Alternating between languages or dialects within a text, often to reflect bilingual speakers&#8217; patterns or for creative writing<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"TEXT OBFUSCATION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Intentionally making text vague or harder to understand, sometimes for security purposes like masking personal data<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"TEXTUAL ENTAILMENT\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Modifying a sentence or phrase to either entail or contradict another sentence, often used in natural language processing tasks<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"REWRITING WITH VOCABULARY LIMITATIONS\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Rewriting the entire text or a piece of it while using a limited vocabulary, such as all words starting with a specific letter<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TEXT MODIFICATION<\/data>    <\/node>    <node id=\"MULTIPLE CHOICE QUESTIONS\">      <data key=\"d0\">EVALUATION METHOD<\/data>      <data key=\"d1\">A method where models are evaluated in an open-ended generation setting with an empty system message, and GPT-4 is used for extraction of the option selected by the model from the model&#8217;s response<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">EVALUATION METHOD<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">An advanced language model used for extracting the option selected by the model from the model&#8217;s response in multiple choice questions<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"NUMERICAL DISCRETE REASONING\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"QUESTION TYPE\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT MODIFICATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"EVALUATION METHOD\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TECHNOLOGY\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"INSTRUCTION TAXONOMY\">      <data key=\"d0\">EVALUATION METHOD<\/data>      <data key=\"d1\">A structured classification of different types of instructions used for seed instruction generation<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"SEED INSTRUCTION GENERATION\">      <data key=\"d0\">EVALUATION METHOD<\/data>      <data key=\"d1\">The process of creating initial instructions for a task, often used in machine learning and natural language processing<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"EVALUATOR ASSISTANT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A system or tool designed to assist evaluators by parsing student responses and extracting relevant information<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"EXTRACTION SYSTEM MESSAGE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A predefined message used by the GPT-4 model to extract the option selected by the model from the model&#8217;s response in multiple choice questions<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <edge source=\"CRITICAL COMPREHENSION QUESTION\" target=\"QUESTION TYPE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Critical Comprehension Question is a type of question<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"EVALUATIVE COMPREHENSION QUESTION\" target=\"QUESTION TYPE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Evaluative Comprehension Question is a type of question<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"VOCABULARY AND LANGUAGE USE\" target=\"QUESTION TYPE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Vocabulary and Language Use is a type of question<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"RELATIONSHIP COMPREHENSION QUESTION\" target=\"QUESTION TYPE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Relationship Comprehension Question is a type of question<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"SEQUENCING EVENTS\" target=\"QUESTION TYPE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Sequencing Events is a type of question<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"STRENGTHEN\" target=\"QUESTION TYPE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Strengthen is a type of question<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"WEAKEN\" target=\"QUESTION TYPE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Weaken is a type of question<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"ASSUMPTION\" target=\"QUESTION TYPE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Assumption is a type of question<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"FLAW\" target=\"QUESTION TYPE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Flaw is a type of question<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"INFERENCE\" target=\"QUESTION TYPE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Inference is a type of question<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"PRINCIPLE\" target=\"QUESTION TYPE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Principle is a type of question<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"METHOD OF REASONING\" target=\"QUESTION TYPE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Method of Reasoning is a type of question<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"RESOLVE THE PARADOX\" target=\"QUESTION TYPE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Resolve the Paradox is a type of question<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"PARAPHRASING\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Paraphrasing is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT SIMPLIFICATION\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Text Simplification is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT EXPANSION\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Text Expansion is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT TRANSLATION\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Text Translation is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT FORMATTING\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Text Formatting is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"SENTIMENT MODIFICATION\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Sentiment Modification is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT ANNOTATION\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Text Annotation is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"KEYWORD REPLACEMENT\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Keyword Replacement is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT REMOVING\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Text Removing is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT CAPITALIZATION\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Text Capitalization is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT STYLING\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Text Styling is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"CONTENT REWRITING\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Content Rewriting is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"DATA NORMALIZATION\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Data Normalization is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"PLAGIARISM REWORDING\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Plagiarism Rewording is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"CODE SWITCHING\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Code Switching is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT OBFUSCATION\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Text Obfuscation is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXTUAL ENTAILMENT\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Textual Entailment is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"REWRITING WITH VOCABULARY LIMITATIONS\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Rewriting with Vocabulary Limitations is a type of text modification<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"MULTIPLE CHOICE QUESTIONS\" target=\"EVALUATION METHOD\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Multiple Choice Questions is a type of evaluation method<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"TECHNOLOGY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">GPT-4 is a technology used for extracting the option selected by the model from the model&#8217;s response in multiple choice questions<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"NUMERICAL DISCRETE REASONING\" target=\"QUESTION TYPE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Numerical Discrete Reasoning is a type of question<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"EVALUATION METHOD\" target=\"INSTRUCTION TAXONOMY\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Instruction Taxonomy is a type of evaluation method<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"EVALUATION METHOD\" target=\"SEED INSTRUCTION GENERATION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Seed Instruction Generation is a type of evaluation method<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TECHNOLOGY\" target=\"EVALUATOR ASSISTANT\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Evaluator Assistant is a type of technology<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TECHNOLOGY\" target=\"EXTRACTION SYSTEM MESSAGE\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Extraction System Message is a type of technology<\/data>      <data key=\"d6\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"103d98395c393552cc954c89d4e59f50","chunk":"alphabet ID of that answer in the provided options. If the student gave multiple\nanswers return them as a list.\nUse the following format:\nParsed Student Answer: Final answer extracted from Student\u2019s response. This\nshould only be the alphabets representing the option the student chose.\nExample 1:\nInput :\nQuestion:\nFind all c in Z3such that Z3[x]\/(x2+c)is a field.\nStudent Response :\nI think 0 is incorrect, so is 2. 3 seems incorrect as well. I think 1 is the correct\nfinal answer.\nOptions :\n[(A) 0, (B) 1, (C) 2, (D) 3 ]\nOutput:\nParsed Student Answer: B\nExample 2:\nInput :\nQuestion:\nFind all c in Z3such that Z3[x]\/(x2+c)is a field.\nStudent Response :\nI think 0 is incorrect. 3 seems incorrect as well. I think 1 and 2 could be the\ncorrect final answers.\nOptions :\n[(A) 0, (B) 1, (C) 2, (D) 3 ]\nOutput:\nParsed Student Answer: [B,C]\n\u2022Exact Match\/Span Extraction Problems : For tasks with math based questions\nlike GSM8K and problems where a ground-truth answer value is given (like DROP),\nwe prompt the models being evaluated to generate the answer and use GPT-4\nto extract the exact answer and also match it with the ground-truth provided to\nproduce a final verdict of whether the model\u2019s answer was \u2019Correct\u2019 or \u2019Incorrect\u2019.\nWe use a specific system message for maths based questions, and another for all the\nother exact match\/span extraction problems, both of which are provided below.\nMaths GPT-4 Extraction System Message\nAs an expert Math teacher, your role is to evaluate a student\u2019s answer to a\nword problem. The problem is accompanied by a correct solution provided by\nthe problem setter. It is important to remember that there may be various\nmethods to solve a word problem, so the student\u2019s steps might not always align\nwith those in the problem setter\u2019s solution. However, the final answer, typically\na number, should be unique and match the problem setter\u2019s answer.\nUse the following format:\nError Analysis:\n28In one sentence, extract the final answer from the problem setter\u2019s solution and\ncompare it with the student\u2019s answer. Do they match?\nFinal Verdict:\nCorrect\/Incorrect\nGeneral Extraction System Message\nYou are an Evaluator Assistant. You support the exam evaluator by parsing\nstudent responses. You are an unbiased Evaluator and do not rely on your\nknowledge but stick to the user provided context. You are provided with the\ncorrect answer and a student\u2019s response. Your task is to parse the answer from\nstudent\u2019s response and then match it with the correct answer. If the student\u2019s\nfinal answer matches the correct answer provided, output a \u2019Correct\u2019, else an\n\u2019Incorrect\u2019.\nPlease rely strictly on the correct answer given in the context only.\nUse the following format:\nError Analysis:\nInonesentence, extractthefinalanswerfromthestudent\u2019ssolutionandcompare\nit with the correct answer. Do they match?\nFinal Verdict:\nCorrect\/Incorrect\n\u2022EQBench : For EQBench, we prompt the models to generate the emotion scores\ngiven the conversation in the prompt and then use GPT-4 to extract the scores\ngenerated by the model for each emotion in the prompt. The metric scores are\ngenerated using both the version 1 and 2 implementations described in the EQBench\npaper and the creators\u2019 github repository. The scoring calculation is calibrated such\nthat a score of 0 corresponds to answering randomly, and a 100 would denote perfect\nalignment with the reference answer. The system message used for extraction of\nemotion scores from evaluated model\u2019s response using GPT-4 is given below:\nEQBench GPT-4 Extraction System Message\nYou are a helpful assistant. You will be given a student agent response which\nwill consist of possible emotions and a score from 0-10 for each of those emotions,\nfollowed by a step by step critique and then revised scores in the following\nformat, First pass scores:\nEmotion1: <score>\nEmotion2: <score>\nEmotion3: <score>\nEmotion4: <score>\nCritique: <your critique here>\nRevised scores:\nEmotion1: <revised score>\nEmotion2: <revised score>\nEmotion3: <revised score>\nEmotion4: <revised score>\n[End of answer]\nRemember: zero is a valid score as well.\nYou will also be provided with the Emotions. Your task is to parse the Revised\nscores for each of the emotions from the student agent response. Return the\nrevised scores in the student agent response for the emotions in the following\nformat:\n\"Emotion1\" : \"Score\",\n\"Emotion2\" : \"Score\",\n\"Emotion3\" : \"Score\",\n29\"Emotion4\" : \"Score\"\nFor example:\nInput\nStudent Agent Response:\nFirst pass scores:\nResigned: 8\nAngry: 2\nHopeful: 4\nEmbarrassed: 9\nCritique:\nElliot is likely to feel resigned because he has just confessed his feelings to\nAlex, knowing that Alex is already in a relationship. He might feel a bit\nangry at himself for putting himself in this situation. There is a slight sense of\nhopefulness in his confession, hoping that Alex might reciprocate his feelings.\nHe is also likely to feel embarrassed for putting Alex in an awkward position.\nRevised scores:\nResigned: 7\nAngry: 3\nHope","chunk_id":"103d98395c393552cc954c89d4e59f50","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"STUDENT","type":"PERSON","description":"An individual who is responding to the questions and providing answers","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"PERSON"},{"name":"ANSWER","type":"RESPONSE","description":"The response given by the student to a question, which can be a single option or multiple options","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"RESPONSE"},{"name":"QUESTION","type":"QUERY","description":"A problem or query presented to the student for which they need to provide an answer","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"QUERY"},{"name":"OPTIONS","type":"CHOICES","description":"A list of possible answers provided to the student from which they can choose","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"CHOICES"},{"name":"PARSED STUDENT ANSWER","type":"EXTRACTED RESPONSE","description":"The final answer extracted from the student's response, represented by the alphabet(s) corresponding to the chosen option(s)","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"EXTRACTED RESPONSE"},{"name":"EXACT MATCH\/SPAN EXTRACTION","type":"TECHNIQUE","description":"A method used to evaluate the correctness of a student's answer by comparing it with a ground-truth answer","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"TECHNIQUE"},{"name":"GSM8K","type":"DATASET","description":"A dataset consisting of math-based questions used for evaluating model performance","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"DATASET"},{"name":"DROP","type":"DATASET","description":"A dataset containing problems with ground-truth answer values used for evaluating model performance","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"DATASET"},{"name":"GPT-4","type":"MODEL","description":"An advanced language model developed by OpenAI, used for extracting and evaluating answers","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"MODEL"},{"name":"MATHS GPT-4 EXTRACTION SYSTEM MESSAGE","type":"INSTRUCTION","description":"A specific system message used to guide GPT-4 in evaluating math-based questions","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"INSTRUCTION"},{"name":"GENERAL EXTRACTION SYSTEM MESSAGE","type":"INSTRUCTION","description":"A system message used to guide GPT-4 in evaluating exact match\/span extraction problems","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"INSTRUCTION"},{"name":"EQBENCH","type":"BENCHMARK","description":"A benchmark used to evaluate emotion scores in conversations","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"BENCHMARK"},{"name":"EMOTION SCORES","type":"METRIC","description":"Scores assigned to different emotions based on a conversation, ranging from 0 to 10","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"METRIC"},{"name":"CRITIQUE","type":"FEEDBACK","description":"A step-by-step analysis provided to justify the emotion scores","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"FEEDBACK"},{"name":"REVISED SCORES","type":"UPDATED METRIC","description":"Updated emotion scores after considering the critique","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"UPDATED METRIC"},{"name":"RESIGNED","type":"EMOTION","description":"A feeling of acceptance of something undesirable but inevitable","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"EMOTION"},{"name":"ANGRY","type":"EMOTION","description":"A strong feeling of displeasure or hostility","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"EMOTION"},{"name":"HOPEFUL","type":"EMOTION","description":"A feeling of expectation and desire for a certain thing to happen","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"EMOTION"},{"name":"EMBARRASSED","type":"EMOTION","description":"A feeling of self-consciousness, shame, or awkwardness","source_id":"103d98395c393552cc954c89d4e59f50","entity_type":"EMOTION"},{"name":"MODEL","type":"TECHNOLOGY","description":"A system or algorithm designed to perform specific tasks, such as evaluating answers or generating text","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"SYSTEM MESSAGE","type":"INSTRUCTION","description":"A predefined message used to guide the behavior of a model or system","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"EVALUATOR ASSISTANT","type":"ROLE","description":"An assistant that helps in evaluating student responses by parsing and comparing answers","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"VERDICT","type":"RESULT","description":"The final judgment on whether the student's answer is correct or incorrect","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"CORRECT","type":"RESULT","description":"Indicates that the student's answer matches the correct answer","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"INCORRECT","type":"RESULT","description":"Indicates that the student's answer does not match the correct answer","source_id":"103d98395c393552cc954c89d4e59f50"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"STUDENT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An individual who is responding to the questions and providing answers<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANSWER\">      <data key=\"d0\">RESPONSE<\/data>      <data key=\"d1\">The response given by the student to a question, which can be a single option or multiple options<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">RESPONSE<\/data>    <\/node>    <node id=\"QUESTION\">      <data key=\"d0\">QUERY<\/data>      <data key=\"d1\">A problem or query presented to the student for which they need to provide an answer<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">QUERY<\/data>    <\/node>    <node id=\"OPTIONS\">      <data key=\"d0\">CHOICES<\/data>      <data key=\"d1\">A list of possible answers provided to the student from which they can choose<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">CHOICES<\/data>    <\/node>    <node id=\"PARSED STUDENT ANSWER\">      <data key=\"d0\">EXTRACTED RESPONSE<\/data>      <data key=\"d1\">The final answer extracted from the student's response, represented by the alphabet(s) corresponding to the chosen option(s)<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">EXTRACTED RESPONSE<\/data>    <\/node>    <node id=\"EXACT MATCH\/SPAN EXTRACTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A method used to evaluate the correctness of a student's answer by comparing it with a ground-truth answer<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset consisting of math-based questions used for evaluating model performance<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset containing problems with ground-truth answer values used for evaluating model performance<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">An advanced language model developed by OpenAI, used for extracting and evaluating answers<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"MATHS GPT-4 EXTRACTION SYSTEM MESSAGE\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">A specific system message used to guide GPT-4 in evaluating math-based questions<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">INSTRUCTION<\/data>    <\/node>    <node id=\"GENERAL EXTRACTION SYSTEM MESSAGE\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">A system message used to guide GPT-4 in evaluating exact match\/span extraction problems<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">INSTRUCTION<\/data>    <\/node>    <node id=\"EQBENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark used to evaluate emotion scores in conversations<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"EMOTION SCORES\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Scores assigned to different emotions based on a conversation, ranging from 0 to 10<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"CRITIQUE\">      <data key=\"d0\">FEEDBACK<\/data>      <data key=\"d1\">A step-by-step analysis provided to justify the emotion scores<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">FEEDBACK<\/data>    <\/node>    <node id=\"REVISED SCORES\">      <data key=\"d0\">UPDATED METRIC<\/data>      <data key=\"d1\">Updated emotion scores after considering the critique<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">UPDATED METRIC<\/data>    <\/node>    <node id=\"RESIGNED\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">A feeling of acceptance of something undesirable but inevitable<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">EMOTION<\/data>    <\/node>    <node id=\"ANGRY\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">A strong feeling of displeasure or hostility<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">EMOTION<\/data>    <\/node>    <node id=\"HOPEFUL\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">A feeling of expectation and desire for a certain thing to happen<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">EMOTION<\/data>    <\/node>    <node id=\"EMBARRASSED\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">A feeling of self-consciousness, shame, or awkwardness<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>      <data key=\"d3\">EMOTION<\/data>    <\/node>    <node id=\"MODEL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A system or algorithm designed to perform specific tasks, such as evaluating answers or generating text<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"SYSTEM MESSAGE\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">A predefined message used to guide the behavior of a model or system<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"EVALUATOR ASSISTANT\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">An assistant that helps in evaluating student responses by parsing and comparing answers<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"VERDICT\">      <data key=\"d0\">RESULT<\/data>      <data key=\"d1\">The final judgment on whether the student's answer is correct or incorrect<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"CORRECT\">      <data key=\"d0\">RESULT<\/data>      <data key=\"d1\">Indicates that the student's answer matches the correct answer<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"INCORRECT\">      <data key=\"d0\">RESULT<\/data>      <data key=\"d1\">Indicates that the student's answer does not match the correct answer<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <edge source=\"STUDENT\" target=\"ANSWER\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The student provides an answer to the question<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"ANSWER\" target=\"PARSED STUDENT ANSWER\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The parsed student answer is extracted from the student's response<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"QUESTION\" target=\"OPTIONS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The question is accompanied by a list of options<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"EXACT MATCH\/SPAN EXTRACTION\" target=\"GSM8K\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Exact match\/span extraction is used to evaluate answers in the GSM8K dataset<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"EXACT MATCH\/SPAN EXTRACTION\" target=\"DROP\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Exact match\/span extraction is used to evaluate answers in the DROP dataset<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"MATHS GPT-4 EXTRACTION SYSTEM MESSAGE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">GPT-4 uses the Maths GPT-4 extraction system message to evaluate math-based questions<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"GENERAL EXTRACTION SYSTEM MESSAGE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">GPT-4 uses the General extraction system message to evaluate exact match\/span extraction problems<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"GENERAL EXTRACTION SYSTEM MESSAGE\" target=\"EVALUATOR ASSISTANT\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The Evaluator Assistant uses the General Extraction System Message to parse and compare answers<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"EQBENCH\" target=\"EMOTION SCORES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">EQBench is used to evaluate emotion scores<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"EMOTION SCORES\" target=\"CRITIQUE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Emotion scores are followed by a critique<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"CRITIQUE\" target=\"REVISED SCORES\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">The critique leads to revised scores<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"REVISED SCORES\" target=\"RESIGNED\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Resigned is one of the emotions that receives a revised score<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"REVISED SCORES\" target=\"ANGRY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Angry is one of the emotions that receives a revised score<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"REVISED SCORES\" target=\"HOPEFUL\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Hopeful is one of the emotions that receives a revised score<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"REVISED SCORES\" target=\"EMBARRASSED\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Embarrassed is one of the emotions that receives a revised score<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"MODEL\" target=\"SYSTEM MESSAGE\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The model follows the instructions provided in the system message<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"EVALUATOR ASSISTANT\" target=\"VERDICT\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The Evaluator Assistant provides a verdict based on the comparison of the student's answer with the correct answer<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"VERDICT\" target=\"CORRECT\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The verdict can be \"Correct\" if the student's answer matches the correct answer<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"VERDICT\" target=\"INCORRECT\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">The verdict can be \"Incorrect\" if the student's answer does not match the correct answer<\/data>      <data key=\"d6\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"0cf2e43f324fa4175b9b00b90e5e90ba","chunk":":\nElliot is likely to feel resigned because he has just confessed his feelings to\nAlex, knowing that Alex is already in a relationship. He might feel a bit\nangry at himself for putting himself in this situation. There is a slight sense of\nhopefulness in his confession, hoping that Alex might reciprocate his feelings.\nHe is also likely to feel embarrassed for putting Alex in an awkward position.\nRevised scores:\nResigned: 7\nAngry: 3\nHopeful: 5\nEmbarrassed: 8\nEmotions:\n1. Resigned, 2. Angry, 3. Hopeful, 4. Embarrassed\nOutput\n\"Resigned\" : 7,\n\"Angry\" : 3,\n\"Hopeful\" : 5,\n\"Embarrassed\" : 8\n\u2022Open-Ended Generation : These are the tasks where model is prompted to\ngenerate an answer to an open-ended question, but a ground-truth to match the\nanswer is not available. The metric calculation method for the benchmarks in this\ncategory are provided below:\n\u2013FOFO: For this benchmark the evaluation is done using a judge, GPT-4(version\n0613). We use the judge system message provided in the original paper of the\nbenchmark [ 34]. GPT-4 is used to give a format correctness score between 0\nand 1, 1 meaning the model\u2019s response strictly follows the format specified in\nthe prompt and 0 otherwise. The final score is measured as the percentage of\ntimes the model being evaluated followed the format specified in the prompt\nstrictly.\n\u2013IFEval: IFEval benchmark requires checking if the model response follows the\nverifiable instructions given in the prompt. For this we use the code provided\nby the authors [40].\n\u2013MT-Bench : MT-Bench benchmark consists of a first-turn query and a second-\nturn query independent of the evaluated model\u2019s response. The benchmark\nemploys GPT-4 to judge each turn\u2019s response and provide a score from 1 to 10.\nThe average score over all interactions is reported. System message and prompt\ntemplate used is the one provided by the creators [16].\n\u2013AlpacaEval : In this benchmark we measure win-rates, i.e. the number of times\na powerful LLM (GPT-4-turbo version 0613 in our case) prefers the outputs of\nthe evaluated model over a reference answer [14].\n\u2013InfoBench : InfoBench is also evaluated using GPT-4 (version 1106-preview) as\nthe judge determining if the model response follows the decomposed instruction\nand we use the implementation provided by the creators of the benchmark [ 25].\n30Hallucination Judge Example\nYou will be given a summary instruction and a generated summary.\nYour task to decide if there is any hallucination in the generated\nsummary.\nUser Message:\n{{place summary task here}}\nGenerated Summary:\n{{place response here}}\n=========================\nGo through each section in the generated summary, do the following:\n- Extract relevant facts from the article that can be used to verify\nthe correctness of the summary\n- Decide if any section contains hallucination or not.\nAt the end output a JSON with the format:\n{\"hallucination_detected\": \"yes\/no\", \"hallucinated_span\": \"If yes,\nthe exact span of every hallucinated text part from the summary in\nlist format; if no, leave this empty.\"}\nUse the format:\nAnalysis:\nsection 1:\nwrite the part of the summary\nrelevant segments:\nextract relevant segments from the article\njudgement:\ndecide if the section of the summary is supported by the article\nrepeat this for all sections\n....\nFinal verdict:\n{\"hallucination_detected\": \"yes\/no\", \"hallucinated_span\": \"If yes,\nthe exact span of every hallucinated text part in list format; if no,\nleave this empty.\"}\nFigure 5: Prompt template used for hallucination detection in Text Summarization.\nB.1 Summarization Quality and Hallucination Evaluation\nWe use GPT-4 with the following prompts for evaluating quality and hallucination in\nsummarization:\n31Quality Judge Example\nPlease act as an impartial judge and evaluate the quality of the\nresponse provided by an AI assistant to the user instruction\ndisplayed below.\nYour evaluation should assess the following criteria:\n- Instruction Adherence: Does the response correctly follow the user\ninstruction?\n- Content Grounding: Is the answer grounded in the instruction\nwithout introducing new content beyond what is already present?\nPenalize hallucinations.\n- Overall Quality: Assess the clarity, coherence, and completeness\nof the response.\nBegin your evaluation with a short explanation highlighting the pros\nand cons of the answer. Be as objective as possible. After providing\nyour explanation, rate the overall quality of the response on a scale\nof 1 to 10 using this format:\n\"Rating: [[rating]]\" (e.g., \"Rating: [[5]]\").\nUser Instruction:\n{{place instruction here}}\nAssistant\u2019s Response:\n[The Start of Assistant\u2019s Answer]\n{{place response here}}\n[The End of Assistant\u2019s Answer]\nFigure 6: Prompt template for evaluation of summary quality.\n32","chunk_id":"0cf2e43f324fa4175b9b00b90e5e90ba","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1085,"entities":[{"name":"ALEX","type":"PERSON","description":"Alex is a person who is already in a relationship and is the recipient of Elliot's confession of feelings","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"RESIGNED","type":"EMOTION","description":"Elliot feels resigned because he has confessed his feelings to Alex, knowing that Alex is already in a relationship. This emotion is scored 7 out of 10","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"ANGRY","type":"EMOTION","description":"Elliot feels a bit angry at himself for putting himself in this situation. This emotion is scored 3 out of 10","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"HOPEFUL","type":"EMOTION","description":"Elliot feels a slight sense of hopefulness in his confession, hoping that Alex might reciprocate his feelings. This emotion is scored 5 out of 10","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"EMBARRASSED","type":"EMOTION","description":"Elliot feels embarrassed for putting Alex in an awkward position. This emotion is scored 8 out of 10","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"OPEN-ENDED GENERATION","type":"TASK","description":"Tasks where a model is prompted to generate an answer to an open-ended question without a ground-truth to match the answer","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"FOFO","type":"BENCHMARK","description":"A benchmark where evaluation is done using GPT-4 to give a format correctness score between 0 and 1, based on how strictly the model's response follows the format specified in the prompt","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"IFEVAL","type":"BENCHMARK","description":"A benchmark that requires checking if the model response follows the verifiable instructions given in the prompt, using code provided by the authors","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"MT-BENCH","type":"BENCHMARK","description":"A benchmark consisting of a first-turn query and a second-turn query independent of the evaluated model\u2019s response, judged by GPT-4 to provide a score from 1 to 10","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"ALPACAEVAL","type":"BENCHMARK","description":"A benchmark measuring win-rates, i.e., the number of times GPT-4-turbo prefers the outputs of the evaluated model over a reference answer","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"INFOBENCH","type":"BENCHMARK","description":"A benchmark evaluated using GPT-4 to determine if the model response follows the decomposed instruction, using the implementation provided by the creators","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"HALLUCINATION JUDGE","type":"TASK","description":"A task where a judge decides if there is any hallucination in a generated summary by extracting relevant facts and verifying the correctness of the summary","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"QUALITY JUDGE","type":"TASK","description":"A task where a judge evaluates the quality of a response provided by an AI assistant based on criteria like instruction adherence, content grounding, and overall quality","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"ELLIOT","type":"","description":"","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"GPT-4-TURBO","type":"TECHNOLOGY","description":"GPT-4-turbo is a variant of GPT-4 used in the AlpacaEval benchmark to prefer the outputs of the evaluated model over a reference answer","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"VERSION 0613","type":"VERSION","description":"Version 0613 of GPT-4 is used in the FOFO and AlpacaEval benchmarks for evaluation purposes","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"VERSION 1106-PREVIEW","type":"VERSION","description":"Version 1106-preview of GPT-4 is used in the InfoBench benchmark for evaluation purposes","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"BENCHMARK","type":"TASK","description":"A standard or point of reference against which things may be compared or assessed, used in the context of evaluating AI models","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"MODEL","type":"TECHNOLOGY","description":"A machine learning system trained to perform specific tasks, such as generating text or evaluating responses","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"PROMPT","type":"TASK","description":"A set of instructions or questions given to an AI model to generate a response","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"SCORE","type":"METRIC","description":"A numerical value assigned to evaluate the performance or quality of a model's response","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"WIN-RATE","type":"METRIC","description":"The number of times a model's output is preferred over a reference answer in a benchmark","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"HALLUCINATION","type":"ISSUE","description":"The generation of incorrect or fabricated information by an AI model","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"SUMMARY","type":"TASK","description":"A brief statement or account of the main points of something, often generated by an AI model","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"INSTRUCTION ADHERENCE","type":"CRITERIA","description":"The degree to which a model's response follows the given instructions","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"CONTENT GROUNDING","type":"CRITERIA","description":"The degree to which a model's response is based on the given content without introducing new information","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"OVERALL QUALITY","type":"CRITERIA","description":"The overall assessment of the clarity, coherence, and completeness of a model's response","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"GPT-4","type":"","description":"","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ALEX\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex is a person who is already in a relationship and is the recipient of Elliot's confession of feelings<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"RESIGNED\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">Elliot feels resigned because he has confessed his feelings to Alex, knowing that Alex is already in a relationship. This emotion is scored 7 out of 10<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"ANGRY\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">Elliot feels a bit angry at himself for putting himself in this situation. This emotion is scored 3 out of 10<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"HOPEFUL\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">Elliot feels a slight sense of hopefulness in his confession, hoping that Alex might reciprocate his feelings. This emotion is scored 5 out of 10<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"EMBARRASSED\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">Elliot feels embarrassed for putting Alex in an awkward position. This emotion is scored 8 out of 10<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"OPEN-ENDED GENERATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Tasks where a model is prompted to generate an answer to an open-ended question without a ground-truth to match the answer<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"FOFO\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark where evaluation is done using GPT-4 to give a format correctness score between 0 and 1, based on how strictly the model's response follows the format specified in the prompt<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"IFEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark that requires checking if the model response follows the verifiable instructions given in the prompt, using code provided by the authors<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"MT-BENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark consisting of a first-turn query and a second-turn query independent of the evaluated model&#8217;s response, judged by GPT-4 to provide a score from 1 to 10<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"ALPACAEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark measuring win-rates, i.e., the number of times GPT-4-turbo prefers the outputs of the evaluated model over a reference answer<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"INFOBENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">A benchmark evaluated using GPT-4 to determine if the model response follows the decomposed instruction, using the implementation provided by the creators<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"HALLUCINATION JUDGE\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">A task where a judge decides if there is any hallucination in a generated summary by extracting relevant facts and verifying the correctness of the summary<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"QUALITY JUDGE\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">A task where a judge evaluates the quality of a response provided by an AI assistant based on criteria like instruction adherence, content grounding, and overall quality<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"ELLIOT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"GPT-4-TURBO\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4-turbo is a variant of GPT-4 used in the AlpacaEval benchmark to prefer the outputs of the evaluated model over a reference answer<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"VERSION 0613\">      <data key=\"d0\">VERSION<\/data>      <data key=\"d1\">Version 0613 of GPT-4 is used in the FOFO and AlpacaEval benchmarks for evaluation purposes<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"VERSION 1106-PREVIEW\">      <data key=\"d0\">VERSION<\/data>      <data key=\"d1\">Version 1106-preview of GPT-4 is used in the InfoBench benchmark for evaluation purposes<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"BENCHMARK\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">A standard or point of reference against which things may be compared or assessed, used in the context of evaluating AI models<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"MODEL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A machine learning system trained to perform specific tasks, such as generating text or evaluating responses<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"PROMPT\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">A set of instructions or questions given to an AI model to generate a response<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"SCORE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A numerical value assigned to evaluate the performance or quality of a model's response<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"WIN-RATE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">The number of times a model's output is preferred over a reference answer in a benchmark<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"HALLUCINATION\">      <data key=\"d0\">ISSUE<\/data>      <data key=\"d1\">The generation of incorrect or fabricated information by an AI model<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"SUMMARY\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">A brief statement or account of the main points of something, often generated by an AI model<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"INSTRUCTION ADHERENCE\">      <data key=\"d0\">CRITERIA<\/data>      <data key=\"d1\">The degree to which a model's response follows the given instructions<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"CONTENT GROUNDING\">      <data key=\"d0\">CRITERIA<\/data>      <data key=\"d1\">The degree to which a model's response is based on the given content without introducing new information<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"OVERALL QUALITY\">      <data key=\"d0\">CRITERIA<\/data>      <data key=\"d1\">The overall assessment of the clarity, coherence, and completeness of a model's response<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <edge source=\"ALEX\" target=\"ELLIOT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Elliot has confessed his feelings to Alex, who is already in a relationship<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"RESIGNED\" target=\"ELLIOT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Elliot feels resigned because he has confessed his feelings to Alex, knowing that Alex is already in a relationship<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"ANGRY\" target=\"ELLIOT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Elliot feels a bit angry at himself for putting himself in this situation<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"HOPEFUL\" target=\"ELLIOT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Elliot feels a slight sense of hopefulness in his confession, hoping that Alex might reciprocate his feelings<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"EMBARRASSED\" target=\"ELLIOT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Elliot feels embarrassed for putting Alex in an awkward position<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"FOFO\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">FOFO is a benchmark used to evaluate open-ended generation tasks<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"IFEVAL\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">IFEval is a benchmark used to evaluate open-ended generation tasks<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"MT-BENCH\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">MT-Bench is a benchmark used to evaluate open-ended generation tasks<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"ALPACAEVAL\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">AlpacaEval is a benchmark used to evaluate open-ended generation tasks<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"INFOBENCH\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">InfoBench is a benchmark used to evaluate open-ended generation tasks<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"FOFO\" target=\"GPT-4\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">GPT-4 is used as a judge in the FOFO benchmark to evaluate format correctness<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"FOFO\" target=\"VERSION 0613\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Version 0613 of GPT-4 is used in the FOFO benchmark for evaluation<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"IFEVAL\" target=\"GPT-4\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">GPT-4 is used as a judge in the IFEval benchmark to check if the model response follows verifiable instructions<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"MT-BENCH\" target=\"GPT-4\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">GPT-4 is used as a judge in the MT-Bench benchmark to score each turn's response<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"ALPACAEVAL\" target=\"GPT-4-TURBO\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">GPT-4-turbo is used in the AlpacaEval benchmark to prefer the outputs of the evaluated model over a reference answer<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"ALPACAEVAL\" target=\"VERSION 0613\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Version 0613 of GPT-4 is used in the AlpacaEval benchmark for evaluation<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"INFOBENCH\" target=\"GPT-4\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">GPT-4 is used as a judge in the InfoBench benchmark to determine if the model response follows decomposed instructions<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"INFOBENCH\" target=\"VERSION 1106-PREVIEW\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Version 1106-preview of GPT-4 is used in the InfoBench benchmark for evaluation<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"HALLUCINATION JUDGE\" target=\"QUALITY JUDGE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Both tasks involve evaluating the quality and correctness of AI-generated responses<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"QUALITY JUDGE\" target=\"INSTRUCTION ADHERENCE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Instruction adherence is a criterion used by the quality judge to evaluate AI-generated responses<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"QUALITY JUDGE\" target=\"CONTENT GROUNDING\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Content grounding is a criterion used by the quality judge to evaluate AI-generated responses<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"QUALITY JUDGE\" target=\"OVERALL QUALITY\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Overall quality is a criterion used by the quality judge to evaluate AI-generated responses<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"HALLUCINATION\" target=\"SUMMARY\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Hallucination is an issue that can occur in AI-generated summaries<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>  <\/graph><\/graphml>"}
