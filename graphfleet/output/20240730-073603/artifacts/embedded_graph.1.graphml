<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d6" for="edge" attr.name="source_id" attr.type="string" />
  <key id="d5" for="edge" attr.name="description" attr.type="string" />
  <key id="d4" for="edge" attr.name="weight" attr.type="double" />
  <key id="d3" for="node" attr.name="entity_type" attr.type="string" />
  <key id="d2" for="node" attr.name="source_id" attr.type="string" />
  <key id="d1" for="node" attr.name="description" attr.type="string" />
  <key id="d0" for="node" attr.name="type" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="&quot;MICROSOFT RESEARCH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Microsoft Research is a division of Microsoft focused on conducting advanced research in various fields of technology and science."</data>
      <data key="d2">e7e620f804861b86c33f80a0f61ebb8c</data>
    </node>
    <node id="&quot;MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Microsoft Strategic Missions and Technologies is a division of Microsoft that focuses on strategic initiatives and technological advancements."</data>
      <data key="d2">e7e620f804861b86c33f80a0f61ebb8c</data>
    </node>
    <node id="&quot;MICROSOFT OFFICE OF THE CTO&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Microsoft Office of the CTO is a division within Microsoft that deals with the technological strategies and innovations led by the Chief Technology Officer."</data>
      <data key="d2">e7e620f804861b86c33f80a0f61ebb8c</data>
    </node>
    <node id="&quot;GRAPH RAG&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Graph RAG is an advanced retrieval-augmented generation (RAG) approach that combines multiple sophisticated techniques to enhance the analysis and answering of user queries. It employs four levels of graph communities (C0, C1, C2, C3) to provide comprehensive and detailed responses. This method is particularly effective in generating lists of public figures from various entertainment sectors, including film, television, music, sports, and digital media.

Graph RAG integrates knowledge graph generation with query-focused summarization, supporting human sensemaking over extensive text corpora. It leverages graph structures to improve the comprehensiveness and diversity of answers while being more efficient in terms of context tokens. The system incorporates multiple concepts from other advanced systems, such as self-memory, iterative and federated retrieval-generation strategies, and hierarchical indexing and summarization.

Overall, Graph RAG is a subdomain of retrieval-augmented generation that uses graph-based indexing to answer global questions over large text corpora, making it a powerful tool for detailed and efficient information retrieval and analysis.</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;RETRIEVAL-AUGMENTED GENERATION (RAG)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">RETRIEVAL-AUGMENTED GENERATION (RAG) is a method that retrieves relevant information from external knowledge sources to enable large language models to answer questions. It is an established approach to answering user questions over entire datasets by retrieving relevant text regions to provide grounding for the generation task.</data>
      <data key="d2">55ec70780a07388aca4be80802ea19f1,e7e620f804861b86c33f80a0f61ebb8c</data>
    </node>
    <node id="&quot;QUERY-FOCUSED SUMMARIZATION (QFS)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">QUERY-FOCUSED SUMMARIZATION (QFS) is a task that involves generating natural language summaries based on specific user queries. Unlike traditional summarization methods that may simply concatenate excerpts from a text corpus, QFS aims to create coherent and relevant summaries tailored to the specific queries provided by the user.</data>
      <data key="d2">55ec70780a07388aca4be80802ea19f1,e7e620f804861b86c33f80a0f61ebb8c</data>
    </node>
    <node id="&quot;GLOBAL SENSEMAKING QUESTIONS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Global sensemaking questions are inquiries that require understanding and summarizing information from an entire text corpus to provide comprehensive answers."</data>
      <data key="d2">e7e620f804861b86c33f80a0f61ebb8c</data>
    </node>
    <node id="&quot;GRAPH COMMUNITIES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Graph Communities refer to groups of nodes within a graph that have stronger connections to each other than to other nodes. These communities are identified using community detection algorithms, such as Leiden. Additionally, Graph Communities can also refer to groups of closely-related entities within a graph-based text index, detected through similar community detection algorithms.</data>
      <data key="d2">c5a27b7f9fad18a6ad22416c453ae383,e7e620f804861b86c33f80a0f61ebb8c</data>
    </node>
    <node id="&quot;COMMUNITY SUMMARIES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Community Summaries" are comprehensive, report-like summaries designed to provide insights into the global structure and semantics of a dataset. These summaries are generated for groups of closely-related entities within a graph-based text index, and they serve multiple purposes. They are used to improve the comprehensiveness and diversity of answers by summarizing different levels of a graph community hierarchy. Additionally, "Community Summaries" are pre-generated and divided into chunks of a pre-specified token size, ensuring that relevant information is distributed across these chunks. They also summarize root-level communities in an entity-based graph index, thereby supporting global queries and enhancing the overall understanding of the dataset.</data>
      <data key="d2">1b8338e4644e4c218ad719ee711f9aaa,950afbe992b1be1eb5d912ed068af2a2,c5a27b7f9fad18a6ad22416c453ae383,da636ab056625c618d1656cfc725630c,e7e620f804861b86c33f80a0f61ebb8c</data>
    </node>
    <node id="&quot;GLOBAL ANSWER&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Global Answer" refers to the final comprehensive response generated for a user query by summarizing all partial responses from community summaries across different levels of hierarchical community structures. This process involves sorting intermediate community answers by their helpfulness score and iteratively adding them into a new context window until the token limit is reached.</data>
      <data key="d2">c5a27b7f9fad18a6ad22416c453ae383,da636ab056625c618d1656cfc725630c,e7e620f804861b86c33f80a0f61ebb8c</data>
    </node>
    <node id="&quot;HUMAN SENSEMAKING&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Human sensemaking is the process of understanding connections among people, places, and events to anticipate their trajectories and act effectively."</data>
      <data key="d2">e7e620f804861b86c33f80a0f61ebb8c</data>
    </node>
    <node id="&quot;SCIENTIFIC DISCOVERY&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Scientific discovery refers to the process of gaining new knowledge and understanding in scientific domains, often supported by advanced technologies like LLMs."</data>
      <data key="d2">e7e620f804861b86c33f80a0f61ebb8c</data>
    </node>
    <node id="&quot;INTELLIGENCE ANALYSIS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Intelligence analysis involves examining and interpreting information to support decision-making, often in complex and high-stakes environments."</data>
      <data key="d2">e7e620f804861b86c33f80a0f61ebb8c</data>
    </node>
    <node id="&quot;NA&#207;VE RAG&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Na&#239;ve RAG" is a basic retrieval-augmented generation (RAG) system that converts documents to text, splits them into chunks, and embeds them into a vector space for information retrieval. While it effectively uses text chunking and vector embedding, it has several drawbacks that more advanced RAG systems aim to overcome. Additionally, "Na&#239;ve RAG" is used to generate lists of public figures, focusing primarily on their personal lives and relationships, which results in less diversity in perspectives and insights.</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;PODCAST DATASET&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">The Podcast Dataset is a collection of podcast transcripts used for evaluating the performance of different Retrieval-Augmented Generation (RAG) systems. It is utilized in the graph indexing process with a context window size of 600 tokens and 1 gleaning. This dataset plays a crucial role in both indexing and evaluation within various studies focused on RAG methods.</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;NEWS DATASET&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">The News Dataset is a collection of news articles used for evaluating the performance of different Retrieval-Augmented Generation (RAG) systems. It is also utilized in the graph indexing process with a context window size of 600 tokens and 0 gleanings. The dataset serves as a crucial resource for both indexing and evaluation in various studies related to RAG methods.</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;GPT-4-TURBO&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"GPT-4-turbo is a large language model with a context size of 128k tokens, used for various natural language processing tasks."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;WASHINGTON&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"Washington is a location mentioned in the context of communications influencing decision-making processes."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;RETRIEVAL-AUGMENTED GENERATION&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">Retrieval-Augmented Generation (RAG) is a method designed to enhance knowledge-intensive natural language processing (NLP) tasks by integrating information retrieval with natural language generation. This technique aims to produce more accurate and contextually relevant responses by retrieving pertinent information and incorporating it into the generation process. The concept of Retrieval-Augmented Generation was discussed in a 2021 paper by Patrick Lewis et al., highlighting its significance in improving the performance of NLP applications.</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;TEXT CHUNKING&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Text chunking is a process used in natural language processing to break down text into smaller, manageable pieces for easier analysis and retrieval."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;VECTOR EMBEDDING&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Vector embedding is a method used in machine learning to represent text data as vectors in a continuous vector space, facilitating more efficient information retrieval."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;INFORMATION RETRIEVAL&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Information retrieval is the process of obtaining relevant information from a large repository, often used in conjunction with natural language processing techniques."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;NATURAL LANGUAGE PROCESSING&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;COMPREHENSIVENESS&quot;">
      <data key="d0">"METRIC"</data>
      <data key="d1">Comprehensiveness is a metric used to evaluate the extent to which a system's responses cover all relevant aspects of a given topic. It measures how much detail an answer provides to ensure that all aspects and details of the question are addressed. In the context of dataset evaluations, comprehensiveness refers to the goal of providing complete and thorough answers.</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;DIVERSITY&quot;">
      <data key="d0">"METRIC"</data>
      <data key="d1">"Diversity" is a metric used to evaluate the variety and range of different responses generated by a system. It measures how varied and rich an answer is in providing different perspectives and insights on the question. In the context of dataset evaluations, diversity refers to the goal of providing varied and different answers.</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;PERFORMANCE EVALUATION&quot;">
      <data key="d0">"PROCESS"</data>
      <data key="d1">"Performance evaluation is the process of assessing the effectiveness and efficiency of a system, often using specific datasets and metrics."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;CONTEXT SIZE&quot;">
      <data key="d0">"METRIC"</data>
      <data key="d1">"Context size refers to the amount of text or tokens that a language model can consider at once when generating responses."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;DECISION-MAKING PROCESSES&quot;">
      <data key="d0">"PROCESS"</data>
      <data key="d1">"Decision-making processes involve the steps and considerations taken to make informed choices, often influenced by various factors such as location, data, and communication."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;TRANSFORMER ARCHITECTURE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"The transformer architecture has shown substantial improvements in various summarization tasks, including abstractive and extractive, generic and query-focused, and single-document and multi-document summarization."</data>
      <data key="d2">55ec70780a07388aca4be80802ea19f1</data>
    </node>
    <node id="&quot;LARGE LANGUAGE MODELS (LLMS)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"LLMs, such as GPT, Llama, and Gemini, can use in-context learning to summarize content provided in their context window, trivializing many summarization tasks."</data>
      <data key="d2">55ec70780a07388aca4be80802ea19f1</data>
    </node>
    <node id="&quot;GRAPH RAG APPROACH&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">The "Graph RAG Approach" is a method for processing and summarizing text documents by converting them into graph structures, focusing on extracting entities and relationships from text chunks. This innovative approach is based on the global summarization of a knowledge graph derived from large language models (LLMs). It leverages the modularity of graphs and employs community detection algorithms to partition these graphs into modular communities, thereby enhancing the efficiency and accuracy of the summarization process.</data>
      <data key="d2">1e97679db415c7c17b35542305f23ced,55ec70780a07388aca4be80802ea19f1</data>
    </node>
    <node id="&quot;GLOBAL SUMMARIZATION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Global Summarization" is the process of summarizing the entire dataset using community summaries to answer global queries. It aims to generate comprehensive summaries of entire corpora, addressing the challenge of summarizing large volumes of text that exceed LLM context windows. This process involves summarizing source texts in a comprehensive manner and is often used in comparison to graph-based approaches.</data>
      <data key="d2">55ec70780a07388aca4be80802ea19f1,950afbe992b1be1eb5d912ed068af2a2,c5a27b7f9fad18a6ad22416c453ae383</data>
    </node>
    <node id="&quot;COMMUNITY DETECTION ALGORITHMS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Community detection algorithms, such as Louvain and Leiden, partition graphs into modular communities of closely-related nodes, aiding in the summarization process."</data>
      <data key="d2">55ec70780a07388aca4be80802ea19f1</data>
    </node>
    <node id="&quot;MAP-REDUCE APPROACH&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"The map-reduce approach involves using community summaries to answer queries independently and in parallel, then summarizing all relevant partial answers into a final global answer."</data>
      <data key="d2">55ec70780a07388aca4be80802ea19f1</data>
    </node>
    <node id="&quot;EVALUATION OF GRAPH RAG&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The evaluation of Graph RAG involved generating activity-centered sense-making questions from real-world datasets and comparing the performance of global approaches to naive RAG."</data>
      <data key="d2">55ec70780a07388aca4be80802ea19f1</data>
    </node>
    <node id="&quot;TEXT CHUNKS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Text Chunks refer to segments of text extracted from source documents, which are processed to identify entities and relationships."</data>
      <data key="d2">1e97679db415c7c17b35542305f23ced</data>
    </node>
    <node id="&quot;LLM PROMPTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"LLM Prompts are used to extract entities and relationships from text chunks, tailored to the domain of the document corpus."</data>
      <data key="d2">1e97679db415c7c17b35542305f23ced</data>
    </node>
    <node id="&quot;GLEANINGS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Gleanings are additional rounds of extraction to detect any entities missed in prior rounds, improving the quality of entity extraction."</data>
      <data key="d2">1e97679db415c7c17b35542305f23ced</data>
    </node>
    <node id="&quot;ELEMENT INSTANCES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Element Instances are the identified entities and relationships extracted from text chunks, forming the nodes and edges of a graph."</data>
      <data key="d2">1e97679db415c7c17b35542305f23ced</data>
    </node>
    <node id="&quot;ELEMENT SUMMARIES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Element Summaries are descriptive texts that summarize the extracted entities and relationships, used to create a coherent graph structure. They provide detailed descriptions of nodes, edges, and covariates within a graph community, and are prioritized for inclusion in the LLM context window.</data>
      <data key="d2">1e97679db415c7c17b35542305f23ced,c5a27b7f9fad18a6ad22416c453ae383</data>
    </node>
    <node id="&quot;HOTPOTQA&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">HOTPOTQA is a benchmark dataset designed for open-domain question answering that targets explicit fact retrieval. It is used to demonstrate the effectiveness of different chunk sizes in extracting entity references. Additionally, HotPotQA is crafted to support diverse, explainable multi-hop question answering, making it a valuable resource for evaluating and improving question-answering systems.</data>
      <data key="d2">1e97679db415c7c17b35542305f23ced,5fefd7cd7acb9cfd1bfb1118691c8546,da636ab056625c618d1656cfc725630c</data>
    </node>
    <node id="&quot;GRAPH INDEX&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">GRAPH INDEX is a data structure used to organize and retrieve information efficiently, particularly in the context of Graph RAG. It serves as a structured representation of entities and relationships extracted from text, facilitating summarization and reasoning tasks.</data>
      <data key="d2">1e97679db415c7c17b35542305f23ced,950afbe992b1be1eb5d912ed068af2a2</data>
    </node>
    <node id="&quot;LEIDEN&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Leiden is a community detection algorithm known for efficiently recovering hierarchical community structures in large-scale graphs."</data>
      <data key="d2">c5a27b7f9fad18a6ad22416c453ae383</data>
    </node>
    <node id="&quot;MULTIHOP-RAG&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">MULTIHOP-RAG is a benchmark dataset for open-domain question answering that targets explicit fact retrieval and includes news articles. Developed by Tang and Yang, MultiHop-RAG is specifically designed for retrieval-augmented generation for multi-hop queries. Additionally, it is utilized in the context of graph-based indexing and community detection.</data>
      <data key="d2">8d9142b3f9039788061b6ce1815078fd,c5a27b7f9fad18a6ad22416c453ae383,da636ab056625c618d1656cfc725630c</data>
    </node>
    <node id="&quot;HIERARCHICAL COMMUNITY STRUCTURE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Hierarchical Community Structure refers to the multi-level organization of graph communities, where each level provides a different partition of the graph."</data>
      <data key="d2">c5a27b7f9fad18a6ad22416c453ae383</data>
    </node>
    <node id="&quot;COMMUNITY ANSWERS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Community Answers" are intermediate answers generated from community summaries, used to form the final global answer to a user query. These answers are produced in parallel for each chunk and are assigned a helpfulness score ranging from 0 to 100.</data>
      <data key="d2">c5a27b7f9fad18a6ad22416c453ae383,da636ab056625c618d1656cfc725630c</data>
    </node>
    <node id="&quot;QUERY-FOCUSED SUMMARIZATION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Query-Focused Summarization is the process of generating summaries that are specifically tailored to answer user queries."</data>
      <data key="d2">c5a27b7f9fad18a6ad22416c453ae383</data>
    </node>
    <node id="&quot;PODCAST TRANSCRIPTS&quot;">
      <data key="d0">"DATASET"</data>
      <data key="d1">"Compiled transcripts of podcast conversations between Kevin Scott, Microsoft CTO, and other technology leaders, used for evaluation."</data>
      <data key="d2">da636ab056625c618d1656cfc725630c</data>
    </node>
    <node id="&quot;NEWS ARTICLES&quot;">
      <data key="d0">"DATASET"</data>
      <data key="d1">"A benchmark dataset comprising news articles published from September 2013 to December 2023 in various categories, used for evaluation."</data>
      <data key="d2">da636ab056625c618d1656cfc725630c</data>
    </node>
    <node id="&quot;MT-BENCH&quot;">
      <data key="d0">"DATASET"</data>
      <data key="d1">"A benchmark dataset for open-domain question answering that targets explicit fact retrieval."</data>
      <data key="d2">da636ab056625c618d1656cfc725630c</data>
    </node>
    <node id="&quot;TEXT SUMMARIZATION (TS)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Text Summarization (TS) is a method that applies a map-reduce approach directly to source texts for summarization. This technique leverages the map-reduce framework to efficiently process and condense large volumes of text, making it a powerful tool for generating concise summaries from extensive documents.</data>
      <data key="d2">993f1cfa34b5b04498b9edf3b5aaeddf,da636ab056625c618d1656cfc725630c</data>
    </node>
    <node id="&quot;SEMANTIC SEARCH (SS)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Semantic Search (SS)" is a naive RAG (Retrieval-Augmented Generation) approach used for comparison in the analysis. In this method, text chunks are retrieved and added to the context window until the token limit is reached.</data>
      <data key="d2">993f1cfa34b5b04498b9edf3b5aaeddf,da636ab056625c618d1656cfc725630c</data>
    </node>
    <node id="&quot;KEVIN SCOTT&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Microsoft CTO who participates in podcast conversations used in the evaluation dataset."</data>
      <data key="d2">da636ab056625c618d1656cfc725630c</data>
    </node>
    <node id="&quot;C0&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"C0 uses root-level community summaries, which are the fewest in number, to answer user queries."</data>
      <data key="d2">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </node>
    <node id="&quot;C1&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"C1 uses high-level community summaries, which are sub-communities of C0 or projected down from C0, to answer user queries."</data>
      <data key="d2">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </node>
    <node id="&quot;C2&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"C2 uses intermediate-level community summaries, which are sub-communities of C1 or projected down from C1, to answer user queries."</data>
      <data key="d2">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </node>
    <node id="&quot;C3&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"C3 uses low-level community summaries, which are the greatest in number, to answer user queries. These are sub-communities of C2 or projected down from C2."</data>
      <data key="d2">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </node>
    <node id="&quot;LLM EVALUATOR&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"LLM Evaluator is used for head-to-head comparison of answers, assessing metrics like comprehensiveness, diversity, empowerment, and directness."</data>
      <data key="d2">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </node>
    <node id="&quot;EMPOWERMENT&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Empowerment" measures how well an answer helps the reader understand and make informed judgments about the topic. It refers to the ability to provide specific examples, quotes, and citations to help users reach an informed understanding. Additionally, empowerment aims to provide answers that enable or empower users in the context of the dataset evaluations.</data>
      <data key="d2">0938d71f3b2047c82d1dd9d7d952808b,1b8338e4644e4c218ad719ee711f9aaa,993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </node>
    <node id="&quot;DIRECTNESS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Directness measures how specifically and clearly an answer addresses the question."</data>
      <data key="d2">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </node>
    <node id="&quot;GRAPH INDEXING PROCESS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Graph Indexing Process involves creating a graph index using generic prompts for entity and relationship extraction, tailored to the domain of the data."</data>
      <data key="d2">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </node>
    <node id="&quot;EVALUATION PROCESS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Evaluation Process involves using an LLM evaluator to assess answers based on metrics like comprehensiveness, diversity, empowerment, and directness."</data>
      <data key="d2">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </node>
    <node id="&quot;TAYLOR SWIFT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Taylor Swift is a prominent public figure in the entertainment industry, known for her significant contributions to music and her high-profile status."</data>
      <data key="d2">1db44fe0d276fa7a87d3f5087dd0bffe</data>
    </node>
    <node id="&quot;TRAVIS KELCE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Travis Kelce is a notable public figure in the entertainment industry, recognized for his achievements in sports and his influence on public discourse."</data>
      <data key="d2">1db44fe0d276fa7a87d3f5087dd0bffe</data>
    </node>
    <node id="&quot;BRITNEY SPEARS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Britney Spears is a well-known public figure in the entertainment industry, famous for her impact on music and her personal life."</data>
      <data key="d2">1db44fe0d276fa7a87d3f5087dd0bffe</data>
    </node>
    <node id="&quot;JUSTIN TIMBERLAKE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Justin Timberlake is a significant public figure in the entertainment industry, known for his contributions to music and his high-profile personal life."</data>
      <data key="d2">1db44fe0d276fa7a87d3f5087dd0bffe</data>
    </node>
    <node id="&quot;ENTERTAINMENT ARTICLES&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Entertainment Articles refer to various articles that cover public figures in the entertainment industry, highlighting their professional achievements and personal lives."</data>
      <data key="d2">1db44fe0d276fa7a87d3f5087dd0bffe</data>
    </node>
    <node id="&quot;PUBLIC DISCOURSE&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Public Discourse refers to the discussions and conversations among the public influenced by the activities and contributions of prominent public figures in the entertainment industry."</data>
      <data key="d2">1db44fe0d276fa7a87d3f5087dd0bffe</data>
    </node>
    <node id="&quot;NAIVE RAG&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Naive RAG is a baseline method for generating answers from datasets, used for comparison with Graph RAG."</data>
      <data key="d2">0938d71f3b2047c82d1dd9d7d952808b</data>
    </node>
    <node id="&quot;CONTEXT WINDOW SIZE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Context Window Size refers to the size of the context used in the model, tested at different sizes to determine the optimal setting for comprehensiveness and diversity."</data>
      <data key="d2">0938d71f3b2047c82d1dd9d7d952808b</data>
    </node>
    <node id="&quot;GLOBAL TEXT SUMMARIZATION (TS)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Global Text Summarization (TS) is a method used for summarizing text without a graph index, compared with Graph RAG."</data>
      <data key="d2">0938d71f3b2047c82d1dd9d7d952808b</data>
    </node>
    <node id="&quot;CONFIGURATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Configuration refers to the process of setting up and testing different context window sizes to determine the optimal setting for the model."</data>
      <data key="d2">0938d71f3b2047c82d1dd9d7d952808b</data>
    </node>
    <node id="&quot;EVALUATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Evaluation" refers to the process of assessing the effectiveness, accuracy, and performance of prompts and the responses they generate. This process ensures safety and security throughout and involves evaluating the performance of different Retrieval-Augmented Generation (RAG) methods on various metrics such as comprehensiveness, diversity, and empowerment. Additionally, "Evaluation" encompasses the assessment of the quality of an essay or Language Model (LLM) output according to specific metrics defined in the prompt. It involves various components such as prompting techniques, output format, evaluation pipeline framework, and other methodological design decisions.</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,50ddbd22a4e5bd636c4c51a5e5756ae3,c28998cdf87522d883979f9c6405f535</data>
    </node>
    <node id="&quot;GLOBAL APPROACHES&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Global Approaches refer to advanced methods used to improve comprehensiveness and diversity in the evaluation metrics."</data>
      <data key="d2">1b8338e4644e4c218ad719ee711f9aaa</data>
    </node>
    <node id="&quot;SOURCE TEXTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Source Texts refer to the original documents from which community summaries are derived."</data>
      <data key="d2">1b8338e4644e4c218ad719ee711f9aaa</data>
    </node>
    <node id="&quot;MAP-REDUCE SUMMARIZATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Map-Reduce Summarization is a resource-intensive approach requiring a high number of context tokens for summarizing source texts."</data>
      <data key="d2">1b8338e4644e4c218ad719ee711f9aaa</data>
    </node>
    <node id="&quot;ITERATIVE QUESTION ANSWERING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Iterative Question Answering is a sensemaking activity characterized by repeated cycles of question and answer to build understanding."</data>
      <data key="d2">1b8338e4644e4c218ad719ee711f9aaa</data>
    </node>
    <node id="&quot;SELFMEM&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Selfmem is a concept related to self-memory used in generation-augmented retrieval to facilitate future generation cycles."</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f</data>
    </node>
    <node id="&quot;GAR&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"GAR stands for generation-augmented retrieval, a strategy that facilitates future generation cycles."</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f</data>
    </node>
    <node id="&quot;ITER-RETGEN&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Iter-RetGen is an iterative retrieval-generation strategy used in advanced RAG systems."</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f</data>
    </node>
    <node id="&quot;FEB4RAG&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">FeB4RAG is a federated retrieval-generation strategy used in advanced Retrieval-Augmented Generation (RAG) systems. It is a subdomain focused on evaluating federated search within the context of retrieval-augmented generation. Developed by Wang and colleagues, FeB4RAG serves as a system specifically designed for assessing the effectiveness of federated search in enhancing retrieval-augmented generation processes.</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f,5fefd7cd7acb9cfd1bfb1118691c8546,8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;CAIRE-COVID&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">CAIRE-COVID is a system designed for managing COVID-19 scholarly information. It combines multiple concepts for multi-document summarization and functions as a question answering and query-focused multi-document summarization system.</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f,8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;ITRG&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"ITRG stands for iterative retrieval-generation, a strategy used for multi-hop question answering."</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f</data>
    </node>
    <node id="&quot;IR-COT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"IR-CoT is a system used for multi-hop question answering."</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f</data>
    </node>
    <node id="&quot;DSP&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">DSP is a system used for multi-hop question answering. It is a type of Retrieval Augmented Generation technique.</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f,cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;RAPTOR&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">RAPTOR is a system that generates a hierarchical index of text chunks by clustering the vectors of text embeddings. Developed by Sarthi and colleagues, RAPTOR stands for Recursive Abstractive Processing for Tree-Organized Retrieval.</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f,8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;KAPING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"KAPING is an advanced RAG system where the index is a knowledge graph."</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f</data>
    </node>
    <node id="&quot;G-RETRIEVER&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"G-Retriever is a system where subsets of the graph structure are the objects of enquiry."</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f</data>
    </node>
    <node id="&quot;GRAPH-TOOLFORMER&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Graph-ToolFormer is a system where derived graph metrics are the objects of enquiry. It is a subdomain aimed at empowering large language models with graph reasoning ability via prompt augmentation by ChatGPT.</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f,5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;SURGE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"SURGE is a system where narrative outputs are strongly grounded in the facts of retrieved subgraphs."</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f</data>
    </node>
    <node id="&quot;FABULA&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">FABULA is a system designed for intelligence report generation through the use of retrieval-augmented narrative construction. Developed by Ranade and Joshi, FABULA operates by retrieving event-plot subgraphs and then serializing these subgraphs using narrative templates. This innovative approach allows for the creation of coherent and structured intelligence reports.</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f,8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;LANGCHAIN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">LangChain is an open-source software library that serves as a framework or tool in the context of large language models and prompt engineering. Additionally, LangChain supports a variety of graph databases, making it a versatile resource for developers working with complex data structures and advanced language processing tasks.</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f,5ce886e06455eadec4bcfe91e36b666d</data>
    </node>
    <node id="&quot;LLAMAINDEX&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">LLAMAINDEX is a tool or method developed by Jerry Liu in 2022, possibly related to language models. It is an open-source software library that supports a variety of graph databases.</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f,630ee831daa753234a258274d318509e</data>
    </node>
    <node id="&quot;NEO4J&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">Neo4J is a company that initiated Project NaLLM, which is available on GitHub. Additionally, Neo4J is known for its graph database format, which is supported by graph-based RAG applications.</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f,8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;NEBULAGRAPH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">NebulaGraph is a company that launched the industry's first graph RAG (Retrieval-Augmented Generation) with LLM based on knowledge graphs. Additionally, NebulaGraph is a graph database format supported by graph-based RAG applications.</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f,8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;SELFCHECKGPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">SelfCheckGPT is a subdomain focused on zero-resource black-box hallucination detection for generative large language models. Additionally, SelfCheckGPT is an approach used to compare fabrication rates in evaluation.</data>
      <data key="d2">36b3475f15d02b229d4190b0b401085f,67b93b22eef87b628b69ad5e0872d3dc</data>
    </node>
    <node id="&quot;HYBRID RAG SCHEMES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Hybrid RAG schemes combine embedding-based matching against community reports with map-reduce summarization mechanisms to enhance information retrieval."</data>
      <data key="d2">950afbe992b1be1eb5d912ed068af2a2</data>
    </node>
    <node id="&quot;EXPLORATORY DRILL DOWN MECHANISM&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"An exploratory drill down mechanism follows the information scent contained in higher-level community summaries to provide more detailed insights."</data>
      <data key="d2">950afbe992b1be1eb5d912ed068af2a2</data>
    </node>
    <node id="&quot;GLOBAL APPROACH TO GRAPH RAG&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The global approach to Graph RAG aims to combine knowledge graph generation, retrieval-augmented generation, and query-focused summarization to improve human sensemaking over text corpora."</data>
      <data key="d2">950afbe992b1be1eb5d912ed068af2a2</data>
    </node>
    <node id="&quot;FUTURE WORK&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Future work involves refining and adapting the graph index, rich text annotations, and hierarchical community structure to enhance the Graph RAG approach."</data>
      <data key="d2">950afbe992b1be1eb5d912ed068af2a2</data>
    </node>
    <node id="&quot;OPEN-SOURCE IMPLEMENTATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming, making the technology accessible to a wider audience."</data>
      <data key="d2">950afbe992b1be1eb5d912ed068af2a2</data>
    </node>
    <node id="&quot;SEQ2SEQ MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Seq2Seq models refer to sequence-to-sequence models used in various natural language processing tasks, including text summarization."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;COMMUNITIES IN LARGE NETWORKS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain involves the study and detection of communities within large networks, often using statistical and computational methods."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;FEW-SHOT LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Few-Shot Learning is a subdomain of machine learning that focuses on enabling models to generalize and make predictions from a very limited number of examples, typically ranging from 10 to 100. This approach is particularly useful for tasks that require minimal training data. In Few-Shot Learning, models are provided with multiple demonstrations within their context window, which serves as a form of in-context learning. By using this prompting technique, the model is given a few examples to learn from before making predictions, allowing it to perform tasks effectively even with a small amount of labeled data.</data>
      <data key="d2">42d8c3ad092ec18e28ff718709b0b472,7096851583df5cc6ad819323dfd9e83e,83a60257c9adae8c826e73ef32d16dd0,c0bdb410b028f870b1c2869f26dd7c52,ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;RETRIEVAL-AUGMENTED TEXT GENERATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain focuses on enhancing text generation models by incorporating retrieval mechanisms to improve the quality and relevance of generated text."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;QUESTION-FOCUSED SUMMARIZATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain deals with creating summaries that are specifically tailored to answer particular questions."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;AUTOMATED EVALUATION OF RETRIEVAL AUGMENTED GENERATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain involves the development of automated methods to evaluate the performance of retrieval-augmented generation models."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;COMMUNITY DETECTION IN GRAPHS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain focuses on identifying and analyzing communities within graph structures, often using algorithms and statistical methods."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;KNOWLEDGE-GROUNDED DIALOGUE GENERATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain involves generating dialogue that is grounded in external knowledge sources, often using knowledge graphs.""The goal is to generate dialogue that is informed by external knowledge sources, enhancing the relevance and accuracy of the conversation."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
      <data key="d3">"GOALS"</data>
    </node>
    <node id="&quot;KNOWLEDGE-INTENSIVE NLP&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain focuses on natural language processing tasks that require extensive external knowledge, often integrating retrieval and language models."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;SEQ2SEQ MODELS FOR TEXT SUMMARIZATION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to apply sequence-to-sequence models to generate concise summaries of longer text documents."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;FAST UNFOLDING OF COMMUNITIES&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to quickly detect and analyze communities within large networks using efficient algorithms."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;FEW-SHOT LEARNING IN LANGUAGE MODELS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to enable language models to perform well with minimal labeled data, enhancing their adaptability and efficiency."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;RETRIEVAL-AUGMENTED TEXT GENERATION WITH SELF-MEMORY&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to improve text generation models by incorporating self-memory mechanisms along with retrieval-augmented techniques."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;EVALUATION OF QUESTION-FOCUSED SUMMARIZATION SYSTEMS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to assess the effectiveness of summarization systems that are designed to answer specific questions."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;AUTOMATED EVALUATION OF RETRIEVAL-AUGMENTED GENERATION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to develop automated methods to evaluate the performance of retrieval-augmented generation models."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;COMMUNITY DETECTION USING DEEP LEARNING&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to apply deep learning techniques to improve the detection and analysis of communities within graph structures."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;DEMONSTRATE-SEARCH-PREDICT FOR KNOWLEDGE-INTENSIVE NLP&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to integrate demonstration, search, and prediction techniques to improve performance in knowledge-intensive natural language processing tasks."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;COLING 2020&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The International Conference on Computational Linguistics held in 2020, where various research papers and findings in computational linguistics were presented."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;DUC 2005&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Document Understanding Conference held in 2005, focusing on the evaluation of summarization systems."</data>
      <data key="d2">c0bdb410b028f870b1c2869f26dd7c52</data>
    </node>
    <node id="&quot;DEMONSTRATE-SEARCH-PREDICT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"DEMONSTRATE-SEARCH-PREDICT" is a method for composing retrieval and language models for knowledge-intensive NLP tasks, developed by Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia in 2022. This methodology decomposes a question into sub-questions, uses queries to solve them, and combines their responses into a final answer. It employs few-shot prompting to effectively decompose the problem and integrate the responses.</data>
      <data key="d2">67b93b22eef87b628b69ad5e0872d3dc,c28998cdf87522d883979f9c6405f535,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;TREE OF CLARIFICATIONS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Tree of Clarifications is a subdomain focused on answering ambiguous questions with retrieval-augmented large language models."</data>
      <data key="d2">67b93b22eef87b628b69ad5e0872d3dc</data>
    </node>
    <node id="&quot;SENSEMAKING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Sensemaking is a subdomain that involves understanding and making sense of complex information, as discussed in the context of alternative perspectives and macrocognitive models."</data>
      <data key="d2">67b93b22eef87b628b69ad5e0872d3dc</data>
    </node>
    <node id="&quot;TALKING DATASETS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Talking Datasets is a subdomain that explores data sensemaking behaviors, particularly how people understand and interact with datasets."</data>
      <data key="d2">67b93b22eef87b628b69ad5e0872d3dc</data>
    </node>
    <node id="&quot;RECURRENT MEMORY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Recurrent Memory is a subdomain that focuses on finding information that large language models might miss, particularly in large datasets."</data>
      <data key="d2">67b93b22eef87b628b69ad5e0872d3dc</data>
    </node>
    <node id="&quot;LANGCHAIN GRAPHS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"LangChain Graphs is a subdomain that involves the use of graphs in the LangChain framework for various applications."</data>
      <data key="d2">67b93b22eef87b628b69ad5e0872d3dc</data>
    </node>
    <node id="&quot;QUERY FOCUSED ABSTRACTIVE SUMMARIZATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Query Focused Abstractive Summarization is a subdomain that involves summarizing text based on query relevance and transfer learning with transformer models."</data>
      <data key="d2">67b93b22eef87b628b69ad5e0872d3dc</data>
    </node>
    <node id="&quot;DOMAIN ADAPTATION WITH PRE-TRAINED TRANSFORMERS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Domain Adaptation with Pre-trained Transformers is a subdomain that focuses on adapting pre-trained transformer models for query-focused abstractive text summarization."</data>
      <data key="d2">67b93b22eef87b628b69ad5e0872d3dc</data>
    </node>
    <node id="&quot;HIERARCHICAL TRANSFORMERS FOR MULTI-DOCUMENT SUMMARIZATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Hierarchical Transformers for Multi-document Summarization is a subdomain that involves using hierarchical transformer models to summarize multiple documents."</data>
      <data key="d2">67b93b22eef87b628b69ad5e0872d3dc</data>
    </node>
    <node id="&quot;LLAMAINDEX KNOWLEDGE GRAPH INDEX&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"LlamaIndex Knowledge Graph Index is a subdomain that involves creating and using knowledge graphs within the LlamaIndex framework."</data>
      <data key="d2">67b93b22eef87b628b69ad5e0872d3dc</data>
    </node>
    <node id="&quot;GENERATION-AUGMENTED RETRIEVAL&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Generation-augmented Retrieval is a subdomain that combines generation and retrieval techniques for open-domain question answering."</data>
      <data key="d2">67b93b22eef87b628b69ad5e0872d3dc</data>
    </node>
    <node id="&quot;OPENORD&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">OpenOrd is an open-source toolbox specifically designed for large graph layout, with a particular focus on visualization and data analysis. It was presented at the SPIE Conference on Visualization and Data Analysis (VDA). OpenOrd provides a subdomain that involves utilizing this toolbox to effectively manage and visualize extensive graph data.</data>
      <data key="d2">67b93b22eef87b628b69ad5e0872d3dc,8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;SCIENTIFIC DISCOVERY WITH GPT-4&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Scientific Discovery with GPT-4 is a goal that involves exploring the impact of large language models on scientific discovery, as studied using GPT-4."</data>
      <data key="d2">67b93b22eef87b628b69ad5e0872d3dc</data>
    </node>
    <node id="&quot;MICROSOFT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">Microsoft is a technology company involved in the research and development of prompting techniques for GenAI systems. Additionally, Microsoft conducted a preliminary study on the impact of large language models on scientific discovery using GPT-4.</data>
      <data key="d2">8d9142b3f9039788061b6ce1815078fd,d1af61f77ad6f49034ffa4e834a77faf</data>
    </node>
    <node id="&quot;PROJECT NALLM&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Project NaLLM is a project by Neo4J, details of which are available on GitHub."</data>
      <data key="d2">8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;LLAMA 2&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Llama 2 is an open foundation and fine-tuned chat model, developed by Touvron and colleagues."</data>
      <data key="d2">8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;ENHANCING KNOWLEDGE GRAPH CONSTRUCTION USING LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"This goal involves improving the construction of knowledge graphs using large language models, as researched by Trajanoska and colleagues."</data>
      <data key="d2">8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;INTERLEAVING RETRIEVAL WITH CHAIN-OF-THOUGHT REASONING FOR KNOWLEDGE-INTENSIVE MULTI-STEP QUESTIONS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"This goal focuses on combining retrieval with chain-of-thought reasoning to address knowledge-intensive multi-step questions, as researched by Trivedi and colleagues."</data>
      <data key="d2">8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;IS CHATGPT A GOOD NLG EVALUATOR?&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"This goal involves evaluating the effectiveness of ChatGPT as a natural language generation evaluator, as studied by Wang and colleagues."</data>
      <data key="d2">8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;RANADE, P. AND JOSHI, A.&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;SARTHI, P., ABDULLAH, S., TULI, A., KHANNA, S., GOLDIE, A., AND MANNING, C. D.&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;SU, D., XU, Y., YU, T., SIDDIQUE, F. B., BAREZI, E. J., AND FUNG, P.&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;TANG, Y. AND YANG, Y.&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;TOUVRON, H., MARTIN, L., STONE, K., ALBERT, P., ALMAHAIRI, A., BABAEI, Y., BASHLYKOV, N., BATRA, S., BHARGAVA, P., BHOSALE, S., ET AL.&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;TRAJANOSKA, M., STOJANOV, R., AND TRAJANOV, D.&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;TRIVEDI, H., BALASUBRAMANIAN, N., KHOT, T., AND SABHARWAL, A.&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;WANG, J., LIANG, Y., MENG, F., SUN, Z., SHI, H., LI, Z., XU, J., QU, J., AND ZHOU, J.&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;WANG, S., KHRAMTSOVA, E., ZHUANG, S., AND ZUCCON, G.&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">8d9142b3f9039788061b6ce1815078fd</data>
    </node>
    <node id="&quot;KNOWLEDGE GRAPH PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Knowledge Graph Prompting is a subdomain related to multi-document question answering."</data>
      <data key="d2">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;TEXT SUMMARIZATION WITH LATENT QUERIES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Text Summarization with Latent Queries is a subdomain that involves summarizing text using latent queries."</data>
      <data key="d2">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;DOCUMENT SUMMARIZATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Document Summarization is a subdomain that focuses on summarizing documents, with recent advances discussed in various studies."</data>
      <data key="d2">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;KNOWLEDGE GRAPH COMPLETION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Knowledge Graph Completion is a subdomain exploring the use of large language models for completing knowledge graphs."</data>
      <data key="d2">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;CAUSAL GRAPH DISCOVERY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Causal Graph Discovery is a subdomain that involves discovering causal graphs using retrieval-augmented generation based large language models."</data>
      <data key="d2">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;LLM-AS-A-JUDGE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"LLM-as-a-Judge is a subdomain evaluated using MT-Bench and Chatbot Arena, focusing on the judging capabilities of large language models."</data>
      <data key="d2">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;EVALUATING FEDERATED SEARCH&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Evaluating Federated Search is a goal related to assessing the effectiveness of federated search in the context of retrieval augmented generation."</data>
      <data key="d2">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;MULTI-DOCUMENT QUESTION ANSWERING&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Multi-Document Question Answering is a goal that involves using knowledge graph prompting to answer questions based on multiple documents."</data>
      <data key="d2">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;TEXT SUMMARIZATION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Text Summarization is a goal that involves summarizing text, particularly with latent queries."</data>
      <data key="d2">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;EXPLAINABLE MULTI-HOP QUESTION ANSWERING&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Explainable Multi-Hop Question Answering is a goal that involves creating datasets like HotpotQA to enable diverse and explainable question answering."</data>
      <data key="d2">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;KNOWLEDGE GRAPH COMPLETION WITH LLMS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Knowledge Graph Completion with LLMs is a goal that explores the use of large language models for completing knowledge graphs."</data>
      <data key="d2">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;GRAPH REASONING ABILITY&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Graph Reasoning Ability is a goal that involves empowering large language models with the ability to reason about graphs, as seen in Graph-Toolformer."</data>
      <data key="d2">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;CAUSAL GRAPH DISCOVERY WITH LLMS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Causal Graph Discovery with LLMs is a goal that involves discovering causal graphs using large language models with retrieval-augmented generation."</data>
      <data key="d2">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;JUDGING CAPABILITIES OF LLMS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Judging Capabilities of LLMs is a goal that involves evaluating large language models as judges using MT-Bench and Chatbot Arena."</data>
      <data key="d2">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </node>
    <node id="&quot;GENERATIVE ARTIFICIAL INTELLIGENCE (GENAI)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"GenAI systems are increasingly deployed across industry and research settings, involving the use of prompting or prompt engineering to interact with these systems."</data>
      <data key="d2">d1af61f77ad6f49034ffa4e834a77faf</data>
    </node>
    <node id="&quot;PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompting" is a widespread and highly researched concept in Generative AI (GenAI), involving the use of prompts to interact with AI systems. It encompasses various techniques and terminologies aimed at improving model performance. The process of prompting involves providing a prompt to a GenAI, which then generates a response. This can include sending a chunk of text or uploading an image. As an emerging field, prompting has a growing body of literature dedicated to exploring and enhancing these techniques.</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd,7798b3210a865e03a3298ca49ad77cc4,d1af61f77ad6f49034ffa4e834a77faf,de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;PROMPT ENGINEERING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Engineering" is a subdomain of artificial intelligence focused on the design, refinement, and optimization of prompts to improve the performance and usability of large language models (LLMs) across various tasks and domains. This practice involves creating and optimizing prompts to maximize a scoring or utility function, often to enhance accuracy on specific datasets or to achieve desired outcomes from generative AI (GenAI) systems.

Prompt Engineering encompasses techniques such as Meta Prompting and AutoPrompt, which are used to automatically optimize prompts. It is an iterative process that involves modifying or changing the prompting techniques to guide AI models in producing desired outputs effectively. This subdomain has applications in numerous fields, including healthcare and medical education, where it is emerging as an important skill for professionals to improve interactions with AI systems.

The field has been discussed by experts such as Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani in 2023, highlighting its significance in developing and refining techniques for generating prompts. As a professional art, Prompt Engineering is essential for eliciting specific responses from language models, thereby optimizing their benefits and enhancing their performance on various tasks.</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,153eeb5a63e650f2cd12f700ffe3e71f,1a997c6aadeaeb3b5ad0a4c3ce835540,3fd8f6dcbbf1eecd6efb01ea12538679,4257f30018a4acf2e8ee95f21de8d7df,590db3ee59b442c908a9b425a9be2477,5ce886e06455eadec4bcfe91e36b666d,6e1dce58f4a3793b65d09171ea5bd3a6,7798b3210a865e03a3298ca49ad77cc4,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,a86e659dcd136358e7557eb5f98c1b58,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,d1af61f77ad6f49034ffa4e834a77faf,d305fc89f77daeb9c5be3a3d126223ed,de0fbfe367c5921e80c093f91d589919,e8bf483fffcc91b1512c5796d0d4045a,f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </node>
    <node id="&quot;META-ANALYSIS OF PROMPTING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">The "META-ANALYSIS OF PROMPTING" involves a systematic review process to collect a dataset of sources related to prompting and prompt engineering. It includes a comprehensive analysis of the entire literature on natural language prefix-prompting, featuring a taxonomy of prompting techniques and their usage.</data>
      <data key="d2">7798b3210a865e03a3298ca49ad77cc4,d1af61f77ad6f49034ffa4e834a77faf</data>
    </node>
    <node id="&quot;UNIVERSITY OF MARYLAND&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">The University of Maryland is an academic institution involved in the research and development of prompting techniques for GenAI systems. Additionally, the University of Maryland is one of the research entities associated with the creation of the dataset on prompt engineering.</data>
      <data key="d2">6430817c08b3a5c6d193478d4c739d79,d1af61f77ad6f49034ffa4e834a77faf</data>
    </node>
    <node id="&quot;OPENAI&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">OpenAI is an organization that develops advanced AI technologies, including OpenAI Assistants. It is actively contributing to the research on prompting techniques and Generative AI (GenAI) systems. As a sponsor of research on prompt engineering, OpenAI has provided significant support, including a contribution of $10,000 in API credits to aid in the research and development of these techniques.</data>
      <data key="d2">4d9e8d703c2da8e4775c428e83e87fc9,6430817c08b3a5c6d193478d4c739d79,6e1dce58f4a3793b65d09171ea5bd3a6,d1af61f77ad6f49034ffa4e834a77faf</data>
    </node>
    <node id="&quot;STANFORD&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"An academic institution participating in the research on prompting techniques and GenAI systems."</data>
      <data key="d2">d1af61f77ad6f49034ffa4e834a77faf</data>
    </node>
    <node id="&quot;VANDERBILT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"An academic institution contributing to the research on prompting techniques and GenAI systems."</data>
      <data key="d2">d1af61f77ad6f49034ffa4e834a77faf</data>
    </node>
    <node id="&quot;PRINCETON&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"An academic institution involved in the research on prompting techniques and GenAI systems."</data>
      <data key="d2">d1af61f77ad6f49034ffa4e834a77faf</data>
    </node>
    <node id="&quot;TEXAS STATE UNIVERSITY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"An academic institution participating in the research on prompting techniques and GenAI systems."</data>
      <data key="d2">d1af61f77ad6f49034ffa4e834a77faf</data>
    </node>
    <node id="&quot;ICAHN SCHOOL OF MEDICINE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"A medical school involved in the research on prompting techniques and GenAI systems."</data>
      <data key="d2">d1af61f77ad6f49034ffa4e834a77faf</data>
    </node>
    <node id="&quot;ASST BRIANZA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"An organization contributing to the research on prompting techniques and GenAI systems."</data>
      <data key="d2">d1af61f77ad6f49034ffa4e834a77faf</data>
    </node>
    <node id="&quot;MOUNT SINAI BETH ISRAEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"A medical institution involved in the research on prompting techniques and GenAI systems."</data>
      <data key="d2">d1af61f77ad6f49034ffa4e834a77faf</data>
    </node>
    <node id="&quot;INSTITUTO DE TELECOMUNICA&#199;&#213;ES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"An institution involved in the research on prompting techniques and GenAI systems."</data>
      <data key="d2">d1af61f77ad6f49034ffa4e834a77faf</data>
    </node>
    <node id="&quot;UNIVERSITY OF MASSACHUSETTS AMHERST&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"An academic institution participating in the research on prompting techniques and GenAI systems."</data>
      <data key="d2">d1af61f77ad6f49034ffa4e834a77faf</data>
    </node>
    <node id="&quot;PROMPT TEMPLATE LANGUAGE SELECTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Template Language Selection refers to the process of choosing the appropriate language templates for generating prompts in various applications."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;PROMPTING FOR MACHINE TRANSLATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompting for Machine Translation involves creating prompts specifically designed to facilitate the translation of text from one language to another using machine learning models."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;MULTIMODAL&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Multimodal refers to the use of multiple modes of input, such as text, image, audio, and video, in the process of generating prompts."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;IMAGE PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Image Prompting" involves creating prompts that are based on or include images to guide the generation of responses. This technique utilizes data such as photographs, drawings, or screenshots of text for generating prompts. Image Prompting can involve using images either as part of the prompt or as the output, with applications including image generation, caption generation, image classification, and image editing.</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3,6edacbda20b2fdd4077246c7b271a8b5,8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;AUDIO PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Audio Prompting" extends prompting techniques to the audio modality, involving the creation of prompts that are based on or include audio inputs to guide the generation of responses. Experiments involving audio In-Context Learning (ICL) have shown mixed results, but there is potential for future development in this area.</data>
      <data key="d2">27d8fe15ab6f9e3d91fd5858fbeba7ea,50ddbd22a4e5bd636c4c51a5e5756ae3,6edacbda20b2fdd4077246c7b271a8b5</data>
    </node>
    <node id="&quot;VIDEO PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Video Prompting" extends prompting to the video modality, used in text-to-video generation, video editing, and video-to-text generation, employing various prompt-related techniques. It involves creating prompts that are based on or include video inputs to guide the generation of responses.</data>
      <data key="d2">27d8fe15ab6f9e3d91fd5858fbeba7ea,50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;SEGMENTATION PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Segmentation Prompting" involves creating prompts that help in segmenting data, such as dividing an image or text into meaningful parts. It is used for tasks like semantic segmentation, utilizing prompting techniques to achieve better segmentation results.</data>
      <data key="d2">27d8fe15ab6f9e3d91fd5858fbeba7ea,50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;3D PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"3D Prompting" involves creating prompts that are based on or include three-dimensional data to guide the generation of responses. It is used in various applications such as 3D object synthesis, 3D surface texturing, and 4D scene generation. The input prompts for 3D Prompting can include text, images, user annotations, and 3D objects.</data>
      <data key="d2">27d8fe15ab6f9e3d91fd5858fbeba7ea,50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;AGENTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Agents in GenAI are systems designed to serve a user's goals by engaging with external systems. These autonomous entities can perform tasks or make decisions based on prompts, often incorporating external tools such as Internet browsing and calculators. Agents utilize prompting techniques and prompt chains to enable agent-like behavior, which includes tool use, code generation, observation, and retrieval augmented generation.</data>
      <data key="d2">27d8fe15ab6f9e3d91fd5858fbeba7ea,42fa2868f275e1b0f2269e560e9a5816,50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;TOOL USE AGENTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Tool Use Agents" are GenAI agents that utilize external tools to perform tasks based on prompts. These tools can be both symbolic, such as calculators, and neural, such as separate large language models (LLMs). These tools are sometimes referred to as experts or modules.</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3,cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;CODE-GENERATION AGENTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Code-Generation Agents are a type of agent that generates code based on prompts. These agents possess the capability to write and execute code, a function that is often regarded as a tool, similar to a code interpreter.</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3,cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;OBSERVATION-BASED AGENTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Observation-Based Agents are a type of agent that make decisions or perform tasks based on observations and prompts. These agents rely on observing and interacting with their environment to accomplish their tasks. They are specifically designed to solve problems by engaging with toy environments, where they receive observations that are inserted into their prompts to guide their actions.</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3,cbd06bb38a855be4a07883f499014eaa,eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;RETRIEVAL AUGMENTED GENERATION (RAG)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Retrieval Augmented Generation (RAG) is a paradigm in which information is retrieved from an external source and inserted into the prompt to enhance performance in knowledge-intensive tasks. RAG systems are considered to be agents when retrieval is used as an external tool. This technique involves agents retrieving relevant information to augment the generation of responses based on prompts, thereby improving the overall effectiveness and accuracy of the generated content.</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3,eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;PROMPTING TECHNIQUES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompting Techniques" are methods used in evaluator prompts to build robust evaluators. These techniques include simple instruction, Chain-of-Thought (CoT), role-based evaluation, and model-generated guidelines. They are various methods employed to create and refine prompts for different applications, guiding language models in generating desired outputs. The effectiveness of these techniques is often evaluated by their citation counts in research papers.</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3,83a60257c9adae8c826e73ef32d16dd0,c28998cdf87522d883979f9c6405f535</data>
    </node>
    <node id="&quot;OUTPUT FORMAT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Output Format" refers to the structure and presentation of the responses generated from prompts. It encompasses the way the LLM's response is formatted, which can significantly affect evaluation performance. Styling the response using formats such as XML or JSON has been shown to improve the accuracy of the judgment generated by the evaluator.</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3,c28998cdf87522d883979f9c6405f535</data>
    </node>
    <node id="&quot;PROMPTING FRAMEWORKS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompting Frameworks are structured approaches or systems used to create and manage prompts."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;SECURITY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Security" concerns arise as the use of prompting grows, highlighting the need for secure and aligned prompting practices. In the context of prompting, "Security" involves addressing the varied and unique threats posed by prompt hacking, including prompt injection and jailbreaking. It refers to measures taken to protect prompts and the data they interact with from unauthorized access or manipulation.</data>
      <data key="d2">2dba3160cd0e0ee3943dce308cb9940e,50ddbd22a4e5bd636c4c51a5e5756ae3,a4eb2fbdea1494d271ebc61219d17020</data>
    </node>
    <node id="&quot;TYPES OF PROMPT HACKING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Types of Prompt Hacking refer to various methods by which prompts can be manipulated or exploited for malicious purposes."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;RISKS OF PROMPT HACKING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Risks of Prompt Hacking refer to the potential dangers and negative consequences of prompt manipulation or exploitation."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;HARDENING MEASURES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"HARDENING MEASURES" are strategies, techniques, and tools developed to strengthen prompts and protect them from hacking or exploitation. These measures are specifically designed to mitigate security risks associated with Large Language Models (LLMs), including prompt hacking.</data>
      <data key="d2">4aea5d43ff4f1164f45ae3b5b8b7a115,50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;ALIGNMENT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"ALIGNMENT" refers to the process of ensuring that Large Language Models (LLMs) are well-aligned with user needs in downstream tasks for successful deployment. This involves making sure that the prompts and the responses they generate are consistent with desired goals and ethical standards.</data>
      <data key="d2">4aea5d43ff4f1164f45ae3b5b8b7a115,50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;PROMPT SENSITIVITY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Sensitivity" refers to the degree to which prompts are affected by variations in input or context. It highlights the high sensitivity of Large Language Models (LLMs) to the input prompt, where even subtle changes can result in vastly different outputs.</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3,84da286ab749b0f025821313fe535d70</data>
    </node>
    <node id="&quot;OVERCONFIDENCE AND CALIBRATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Overconfidence and Calibration refer to the issues of prompts generating overly confident responses and the need to adjust them for accuracy."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;BIASES, STEREOTYPES, AND CULTURE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Biases, Stereotypes, and Culture refer to the influence of societal biases and cultural factors on the creation and interpretation of prompts."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;AMBIGUITY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Ambiguity refers to the presence of unclear or multiple meanings in prompts, which can affect the accuracy of responses."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;BENCHMARKING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Benchmarking" is the process of evaluating new prompting techniques across multiple models and datasets to prove their utility. It involves assessing the empirical performance of different prompting techniques through formal benchmark evaluations and detailed prompt engineering on real-world problems. Benchmarking also refers to comparing and evaluating the effectiveness of various prompting techniques.</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7,50ddbd22a4e5bd636c4c51a5e5756ae3,83a60257c9adae8c826e73ef32d16dd0</data>
    </node>
    <node id="&quot;TECHNIQUE BENCHMARKING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Technique Benchmarking involves comparing various prompting techniques to determine their relative effectiveness."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;QUESTION FORMATS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Question Formats refer to the different structures and types of questions used in prompts to elicit responses."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;SELF-CONSISTENCY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Self-Consistency" is a method designed to enhance the performance of language models, particularly in generating coherent and logical sequences of text. Introduced by Wang et al. in 2022, this technique is based on the intuition that multiple different reasoning paths can lead to the same answer. It involves prompting the language model multiple times to perform chain-of-thought (CoT) reasoning, using a non-zero temperature to elicit diverse reasoning paths. The final response is then selected through a majority vote over all generated responses. This approach has demonstrated improvements in arithmetic, commonsense, and symbolic reasoning tasks.

Self-Consistency also ensures that the system's responses are consistent with its own previous outputs, thereby enhancing the internal consistency and logic of the generated text. As a subdomain of prompt engineering, it involves repeating a single technique to improve the accuracy of responses by running multiple iterations of a prompt and taking the majority response. Overall, Self-Consistency aims to improve the degree to which prompts and their generated responses are internally consistent and logical.</data>
      <data key="d2">42d8c3ad092ec18e28ff718709b0b472,50ddbd22a4e5bd636c4c51a5e5756ae3,590db3ee59b442c908a9b425a9be2477,eba1ab13141790dedb88f55494236682,f1e2d01b4dbcfc34401e7d0dffd14e29,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;EVALUATING RESPONSES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Evaluating Responses involves assessing the quality and accuracy of the responses generated from prompts."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;RESULTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Results refer to the outcomes and findings from the evaluation of prompting techniques and their effectiveness."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;PROMPT ENGINEERING CASE STUDY&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">The "Prompt Engineering Case Study" is an event that involves a detailed examination of the process and outcomes of creating and refining prompts for a specific application. This event illustrates how an experienced prompt engineer approaches a task, providing valuable lessons learned.</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3,590db3ee59b442c908a9b425a9be2477</data>
    </node>
    <node id="&quot;PROBLEM&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Problem refers to the specific issue or challenge addressed in the prompt engineering case study."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;THE DATASET&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"The Dataset refers to the collection of data used in the prompt engineering case study."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;THE PROCESS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"The Process refers to the steps and methods used in the prompt engineering case study to create and refine prompts."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;DISCUSSION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Discussion refers to the analysis and interpretation of the findings from the prompt engineering case study."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;RELATED WORK&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Related Work refers to other studies and research that are relevant to the topic of prompting and prompt engineering."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;CONCLUSIONS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Conclusions refer to the final findings and implications drawn from the prompt engineering case study and related work."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;APPENDICES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Appendices refer to supplementary material provided at the end of the document to support the main content."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;DEFINITIONS OF PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Definitions of Prompting refer to the explanations and descriptions of key terms and concepts related to prompting."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;EXTENDED VOCABULARY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Extended Vocabulary refers to additional terms and definitions related to prompting, prompt engineering, and fine-tuning."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;DATASHEET&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Datasheet refers to a detailed document that provides information about the data used in prompting, including its motivation, composition, collection process, and uses."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;KEYWORDS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Keywords refer to important terms and phrases related to prompting that are highlighted for reference."</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3</data>
    </node>
    <node id="&quot;EVALUATION TABLE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">The Evaluation Table refers to a tabular representation of the evaluation criteria and results for different prompting techniques. It is a comprehensive list of various models and their performance metrics, used to assess the effectiveness of these prompting techniques.</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3,cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;ENTRAPMENT PROMPTING PROCESS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">The "Entrapment Prompting Process" describes the steps taken by a prompt engineer to develop a prompt for evaluating entrapment. It refers to a specific method of creating prompts designed to test and evaluate responses under challenging conditions.</data>
      <data key="d2">50ddbd22a4e5bd636c4c51a5e5756ae3,9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </node>
    <node id="&quot;EXPLORATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Exploration" involves the process of investigating and experimenting with different prompting techniques to understand their potential and limitations. It is the initial phase of the Entrapment Prompting Process, where the prompt engineer explores the dataset and tests different approaches and techniques.</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd,50ddbd22a4e5bd636c4c51a5e5756ae3,9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </node>
    <node id="&quot;TRANSFORMER-BASED LLMS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Transformer-based LLMs are large language models widely deployed in consumer-facing, internal, and research settings. They rely on user-provided input prompts to generate outputs and are used across various use cases due to their flexibility and ease of interaction with natural language prompts."</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </node>
    <node id="&quot;PREFIX PROMPTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prefix prompts are a type of discrete prompt used in modern LLM architectures, particularly decoder-only models. They are widely supported and used by both consumers and researchers."</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </node>
    <node id="&quot;CLOZE PROMPTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Cloze prompts are a type of prompt where parts of the input are masked, and the model is tasked with filling in the blanks. They are less focused on in this study compared to prefix prompts."</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </node>
    <node id="&quot;HARD PROMPTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Hard prompts, also known as discrete prompts, are fixed and predefined prompts used to guide language models. This study focuses on hard prompts rather than soft prompts."</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </node>
    <node id="&quot;SOFT PROMPTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Soft prompts, also known as continuous prompts, involve gradient-based updates and fine-tuning. They are not the focus of this study."</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </node>
    <node id="&quot;TASK-AGNOSTIC TECHNIQUES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Task-agnostic techniques are prompting methods that are not specific to any particular task, making them broadly applicable across different use cases."</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </node>
    <node id="&quot;MULTILINGUAL TECHNIQUES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Multilingual techniques involve prompting methods that work with text data from multiple languages, often derived from core text-based prompting techniques. These methods extend beyond English-only settings, enabling the processing and understanding of text in various languages."</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd,42fa2868f275e1b0f2269e560e9a5816</data>
    </node>
    <node id="&quot;MULTIMODAL TECHNIQUES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Multimodal Techniques refer to prompting methods that involve different modalities such as images, videos, and 3D data, extending beyond text-based domains. These techniques process multimedia inputs, including video and audio, in addition to text. By incorporating various media types, Multimodal Techniques enhance the ability to handle and interpret diverse forms of data, making them a versatile approach in the field of data processing and analysis.</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd,42fa2868f275e1b0f2269e560e9a5816,8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;CORE PROMPTING TECHNIQUES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Core prompting techniques are fundamental methods used in the field of prompting, forming the basis for more specialized techniques such as multilingual and multimodal prompting."</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </node>
    <node id="&quot;GETTING A LABEL&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Getting a Label" is a phase in the Entrapment Prompting Process where the prompt engineer attempts to get the model to label data points correctly. This process involves assigning a specific label or category to a prompt or output, which aids in the organization and understanding of prompting techniques.</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd,9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </node>
    <node id="&quot;VARYING PROMPTING TECHNIQUES&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Varying Prompting Techniques" is a phase in the Entrapment Prompting Process where different prompting techniques are tested to improve model performance. This involves experimenting with various methods of prompting to identify the most effective approaches for different tasks.</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd,9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </node>
    <node id="&quot;IN-CONTEXT LEARNING DEFINITIONS DISAMBIGUATION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"In-Context Learning Definitions Disambiguation involves clarifying and distinguishing between different definitions and terminologies used in the context of in-context learning."</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </node>
    <node id="&quot;CONTRIBUTIONS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Contributions refer to the additions and advancements made to the field of prompting through research and experimentation, as documented in the study."</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </node>
    <node id="&quot;PRISMA PROCESS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">The PRISMA process is a systematic review method used to identify, categorize, and evaluate different text-based prompting techniques. This method forms the basis for the study's taxonomy and terminology, ensuring a comprehensive and structured approach to understanding and analyzing various prompting techniques.</data>
      <data key="d2">08e6ee9b2e040693136d0d8e0acfb8dd,42fa2868f275e1b0f2269e560e9a5816</data>
    </node>
    <node id="&quot;EVALUATION METHODS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Evaluation methods are techniques used to assess the accuracy and reliability of outputs generated by agents and prompting techniques."</data>
      <data key="d2">42fa2868f275e1b0f2269e560e9a5816</data>
    </node>
    <node id="&quot;SECURITY MEASURES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Security measures are strategies designed to ensure the safety and integrity of prompting techniques, reducing risks to companies and users."</data>
      <data key="d2">42fa2868f275e1b0f2269e560e9a5816</data>
    </node>
    <node id="&quot;SAFETY MEASURES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Safety measures are protocols implemented to minimize harm and ensure the safe use of prompting techniques."</data>
      <data key="d2">42fa2868f275e1b0f2269e560e9a5816</data>
    </node>
    <node id="&quot;CASE STUDIES&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Case Studies" are presented to illustrate the capabilities of models and the practical application of prompt engineering techniques. These case studies serve as practical applications of prompting techniques, including testing against benchmarks and real-world use cases.</data>
      <data key="d2">42fa2868f275e1b0f2269e560e9a5816,6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </node>
    <node id="&quot;MMLU BENCHMARK&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The MMLU benchmark is a commonly used standard for evaluating the performance of prompting techniques."</data>
      <data key="d2">42fa2868f275e1b0f2269e560e9a5816</data>
    </node>
    <node id="&quot;MANUAL PROMPT ENGINEERING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Manual prompt engineering involves the detailed crafting of prompts for specific tasks, such as identifying signals of suicidal crisis in text."</data>
      <data key="d2">42fa2868f275e1b0f2269e560e9a5816</data>
    </node>
    <node id="&quot;PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">A prompt is a set of instructions provided to a Generative AI model, such as a Large Language Model (LLM), to guide its output towards specific tasks. It is used to program the LLM by customizing, enhancing, or refining its capabilities. The input provided as a prompt can consist of various forms of media, including text, images, sounds, or other types. In research contexts, the term "prompt" often involves conditioning mechanisms, templates, and few-shot exemplars to predict the next token in a pre-trained language model.</data>
      <data key="d2">1a997c6aadeaeb3b5ad0a4c3ce835540,42fa2868f275e1b0f2269e560e9a5816,d305fc89f77daeb9c5be3a3d126223ed</data>
    </node>
    <node id="&quot;PROMPT TEMPLATE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">A "Prompt Template" is a predefined structure used to guide the GenAI in generating responses. It functions by containing variables that are replaced by media to create a specific prompt instance. This structured format includes base instructions, exemplars, and questions, ensuring that the generated prompts are consistent and tailored to the desired output.</data>
      <data key="d2">42fa2868f275e1b0f2269e560e9a5816,de0fbfe367c5921e80c093f91d589919,f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </node>
    <node id="&quot;EXAMPLES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Examples, also known as exemplars or shots, act as demonstrations that guide the GenAI to accomplish a task. They are used in various prompting techniques such as One-Shot prompts."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;OUTPUT FORMATTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Output Formatting refers to the process of structuring the output of a GenAI in specific formats, such as CSVs or markdown formats, to meet user requirements."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;DIRECTIVES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Directives are a type of speech act intended to encourage an action, as described by Searle (1969). They are used in models of human-computer dialogue."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;CONTEXT WINDOW&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"CONTEXT WINDOW" refers to the range of tokens processed by the LLM (Large Language Model) in a forward pass, which is crucial for understanding the context in prompting. The context window is the space of tokens that the model can process, and it has a maximal length, known as the context length.</data>
      <data key="d2">d305fc89f77daeb9c5be3a3d126223ed,de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;IN-CONTEXT LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">In-Context Learning (ICL) refers to a technique used in pretrained language models where the model is conditioned on a natural language instruction and/or a few demonstrations of a task, and is then expected to complete further instances of the task by predicting what comes next. This method leverages demonstrations to improve the performance of models in understanding and generating contextually appropriate responses. Essentially, ICL is a prompting technique where the model learns from examples provided within the prompt itself, allowing it to perform tasks based on the context presented within the same interaction. It is frequently used in evaluation prompts and other applications, guiding the language model's responses by providing relevant examples within the prompt. In-Context Learning is a subdomain that involves training language models to understand and generate text based on the context provided, enhancing their ability to perform tasks accurately and contextually.</data>
      <data key="d2">3fd8f6dcbbf1eecd6efb01ea12538679,42d8c3ad092ec18e28ff718709b0b472,7096851583df5cc6ad819323dfd9e83e,c28998cdf87522d883979f9c6405f535,ca2bcd796327d014f9e7738468b6b00d,cd60cb17b3864e9fcc7266ff4c1611ce,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;FEW-SHOT PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Few-Shot Prompt is a type of prompting where a few examples are provided to guide the GenAI in generating the desired output."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;ZERO-SHOT PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Zero-Shot Prompt is a type of prompting where no examples are provided, and the GenAI generates a response based solely on the prompt."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;ORTHOGONAL PROMPT TYPES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Orthogonal Prompt Types refer to different categories of prompts that can be used independently or in combination to achieve specific outcomes."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;CONTINUOUS PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Continuous Prompt is a type of prompting where the input is provided in a continuous manner, allowing for ongoing interaction with the GenAI."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;DISCRETE PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Discrete Prompt is a type of prompting where the input is provided in distinct, separate chunks, each guiding the GenAI in a specific way."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;USER PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"User Prompt" is a type of prompt initiated by the user to guide the GenAI in generating a response. It is the most common form of prompting in consumer applications, where the user provides input to direct the AI's output.</data>
      <data key="d2">6430817c08b3a5c6d193478d4c739d79,de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;SYSTEM PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"System Prompt" is a type of prompt initiated by the system to guide the Generative AI (GenAI) in generating a response. It is used to give large language models (LLMs) high-level instructions for interacting with users. However, it is important to note that not all models have this feature.</data>
      <data key="d2">6430817c08b3a5c6d193478d4c739d79,de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;ASSISTANT PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Assistant Prompt" is a type of prompt designed to assist the user in interacting with the GenAI. It is also the output of the LLM itself, which can be considered a prompt when fed back into the model, such as in a conversation history with a user.</data>
      <data key="d2">6430817c08b3a5c6d193478d4c739d79,de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;PREDICTION STYLE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prediction Style refers to the manner in which the GenAI generates responses, such as using a prefix or cloze style."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;PROMPT CHAIN&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"PROMPT CHAIN" consists of two or more prompt templates used in succession, where the output of one prompt serves as the input for the next. This process continues until all templates are exhausted, effectively creating a chain of prompts that build upon each other.</data>
      <data key="d2">7798b3210a865e03a3298ca49ad77cc4,de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;META-PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Meta-Prompting is a technique that involves using prompts to generate other prompts, enhancing the flexibility and capability of the GenAI."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;ANSWER ENGINEERING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Answer Engineering" involves designing the structure and content of answers generated by the GenAI to meet specific requirements. It is a transformation function over the raw LLM output that allows it to be compared to the ground truth. Additionally, Answer Engineering is the iterative process of developing or selecting algorithms to extract precise answers from LLM outputs, involving decisions on answer space, shape, and extractor.</data>
      <data key="d2">1a997c6aadeaeb3b5ad0a4c3ce835540,981e367f454fd6805ff2ad123c75b85e,de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;VERBALIZER&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Verbalizer" is a component that translates the internal representations of the GenAI into human-readable text. It is used in labeling tasks to map a token, span, or other type of output to a label and vice-versa. Additionally, Verbalizer is a crucial component of answer engineering.</data>
      <data key="d2">45c77c52a93a949222fda99a95e0c3d6,de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;EXTRACTOR&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Extractor is a component that identifies and extracts relevant information from the input provided to the GenAI."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;CONVERSATIONAL PROMPT ENGINEERING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Conversational Prompt Engineering" involves designing prompts specifically for conversational interactions with a GenAI. It is a form of Prompt Engineering that occurs during the course of a conversation with a GenAI, where a user may ask the GenAI to refine its output.</data>
      <data key="d2">d305fc89f77daeb9c5be3a3d126223ed,de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;FINE-TUNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Fine-Tuning is the process of adjusting the parameters of the GenAI to improve its performance on specific tasks."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;PROMPT-BASED LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt-Based Learning, also known as Prompt Learning, is a technique where the GenAI learns from the prompts provided, improving its ability to generate accurate responses. This process involves using prompting-related techniques, often in the context of fine-tuning prompts."</data>
      <data key="d2">d305fc89f77daeb9c5be3a3d126223ed,de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;PROMPT TUNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Tuning" is a technique for parameter-efficient tuning of language models, as discussed in the 2021 paper by Brian Lester, Rami Al-Rfou, and Noah Constant. It involves refining the prompts to enhance the performance and accuracy of the Generative AI (GenAI). Specifically, Prompt Tuning refers to directly optimizing the weights of the prompt itself, usually through gradient-based updates, and is also known as Prompt Fine-Tuning. This method focuses on improving the prompts to achieve better model performance without extensively modifying the underlying model parameters.</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613,6430817c08b3a5c6d193478d4c739d79,d305fc89f77daeb9c5be3a3d126223ed,de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;STYLE INSTRUCTIONS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Style Instructions are a type of output formatting used to modify the output stylistically rather than structurally."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;ROLE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Role, also known as a persona, is a component that can improve writing and style text by adopting a specific character or perspective."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;ADDITIONAL INFORMATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Additional Information refers to extra details included in the prompt to provide context and improve the accuracy of the GenAI's response."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;TERMINOLOGY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Terminology within the prompting literature is rapidly developing, with many poorly understood and conflicting definitions."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;PROMPTING TERMS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompting Terms are the vocabulary used within the prompting community to describe various techniques and components."</data>
      <data key="d2">de0fbfe367c5921e80c093f91d589919</data>
    </node>
    <node id="&quot;PROMPT ENGINEERING PROCESS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">The Prompt Engineering Process involves three repeated steps: performing inference on a dataset, evaluating performance, and modifying the prompt template. Additionally, it includes using a large language model (LLM) to identify entrapment in posts, a task that is documented through 47 development steps over approximately 20 hours of work.</data>
      <data key="d2">7798b3210a865e03a3298ca49ad77cc4,d27160d0dde304425ccc51df673321b1</data>
    </node>
    <node id="&quot;PROMPTING TECHNIQUE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A prompting technique is a blueprint that describes how to structure a prompt or multiple prompts, incorporating conditional or branching logic, parallelism, or other architectural considerations."</data>
      <data key="d2">7798b3210a865e03a3298ca49ad77cc4</data>
    </node>
    <node id="&quot;PROMPT ENGINEERING TECHNIQUE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A prompt engineering technique is a strategy for iterating on a prompt to improve it, often automated in literature but manually performed by users in consumer settings."</data>
      <data key="d2">7798b3210a865e03a3298ca49ad77cc4</data>
    </node>
    <node id="&quot;EXEMPLAR&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Exemplars are examples of a task being completed that are shown to a model in a prompt."</data>
      <data key="d2">7798b3210a865e03a3298ca49ad77cc4</data>
    </node>
    <node id="&quot;A SHORT HISTORY OF PROMPTS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A Short History of Prompts describes the evolution of using natural language prefixes or prompts to elicit language model behaviors, originating before the GPT-3 and ChatGPT era."</data>
      <data key="d2">7798b3210a865e03a3298ca49ad77cc4</data>
    </node>
    <node id="&quot;SYSTEMATIC REVIEW PROCESS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Systematic Review Process is a method used to robustly collect a dataset of sources for research, grounded in the PRISMA process."</data>
      <data key="d2">7798b3210a865e03a3298ca49ad77cc4</data>
    </node>
    <node id="&quot;DATA SCRAPING PIPELINE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"The Data Scraping Pipeline includes both human and LLM-assisted review to establish filtering criteria for collecting data."</data>
      <data key="d2">7798b3210a865e03a3298ca49ad77cc4</data>
    </node>
    <node id="&quot;ARXIV&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">ArXiv is an online, open-access repository where researchers publish preprints of their scientific papers. It serves as a significant platform for disseminating research, including studies related to large language models. ArXiv is also a crucial data source for compiling datasets, often using automated scripts to query its API. Additionally, it is one of the primary sources for retrieving papers on topics such as prompting and prompt engineering.</data>
      <data key="d2">29d2b14a56a51f86baa34264697bdd5e,5ce886e06455eadec4bcfe91e36b666d,82d58329a3cd23550be3e22f1740f8ae,83e773afec09e119882fe15dd253e724</data>
    </node>
    <node id="&quot;SEMANTIC SCHOLAR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">Semantic Scholar is a primary data source utilized for querying papers related to prompting and prompt engineering. The dataset was compiled from Semantic Scholar using automated scripts to query its API.</data>
      <data key="d2">29d2b14a56a51f86baa34264697bdd5e,82d58329a3cd23550be3e22f1740f8ae</data>
    </node>
    <node id="&quot;ACL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">ACL is a key data source from which papers related to prompting and prompt engineering are retrieved. It is one of the sources from which the dataset was compiled, contributing to the collection of research papers. Additionally, ACL is an academic conference where the paper on the role of scale in in-context learning was presented.</data>
      <data key="d2">29d2b14a56a51f86baa34264697bdd5e,82d58329a3cd23550be3e22f1740f8ae,b363fca358c69a9412b955c53352ea9a</data>
    </node>
    <node id="&quot;THE PIPELINE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"The Pipeline refers to the data scraping process that includes both human and LLM-assisted review to filter and classify relevant papers."</data>
      <data key="d2">82d58329a3cd23550be3e22f1740f8ae</data>
    </node>
    <node id="&quot;PRISMA REVIEW PROCESS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"The PRISMA Review Process is a systematic method used to accumulate and filter records, resulting in a final set of relevant papers."</data>
      <data key="d2">82d58329a3cd23550be3e22f1740f8ae</data>
    </node>
    <node id="&quot;IN-CONTEXT LEARNING (ICL)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">In-Context Learning (ICL) involves using the text input of a pre-trained language model as a form of task specification, conditioning the model on natural language instructions and/or a few examples. This technique allows Generative AIs (GenAIs) to learn skills and tasks by providing exemplars and instructions within the prompt, eliminating the need for weight updates or retraining. Additionally, ICL has been extended to multilingual settings, exploring approaches for aligning in-context examples with the input sentence for classification tasks.</data>
      <data key="d2">1a997c6aadeaeb3b5ad0a4c3ce835540,45c77c52a93a949222fda99a95e0c3d6,82d58329a3cd23550be3e22f1740f8ae</data>
    </node>
    <node id="&quot;FEW-SHOT PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Few-Shot Prompting" is a paradigm in machine learning, particularly within the context of Generative AI (GenAI), where models are conditioned to complete tasks using only a few examples, also known as exemplars. This technique involves providing the model with a prompt template that includes one or more training samples, which the model uses to perform the task at hand. Unlike traditional methods that rely on gradient-based updates to refine the model, Few-Shot Prompting does not involve updating the model parameters. Instead, it leverages the provided examples to guide the model's performance during evaluation.</data>
      <data key="d2">1a997c6aadeaeb3b5ad0a4c3ce835540,5d5844de9a93093f225ca41ba18f9a89,82d58329a3cd23550be3e22f1740f8ae,e845d3c15484b3061e3a376fa8779883</data>
    </node>
    <node id="&quot;OPTIMIZING ICL&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Optimizing ICL refers to the ongoing efforts to improve the efficiency and effectiveness of In-Context Learning techniques."</data>
      <data key="d2">82d58329a3cd23550be3e22f1740f8ae</data>
    </node>
    <node id="&quot;UNDERSTANDING ICL&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Understanding ICL involves research aimed at comprehending the underlying mechanisms and potential of In-Context Learning."</data>
      <data key="d2">82d58329a3cd23550be3e22f1740f8ae</data>
    </node>
    <node id="&quot;BANSAL ET AL., 2023&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Bansal et al., 2023 is a significant work focused on optimizing In-Context Learning techniques."</data>
      <data key="d2">82d58329a3cd23550be3e22f1740f8ae</data>
    </node>
    <node id="&quot;SI ET AL., 2023A&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Si et al., 2023a is a notable study aimed at understanding In-Context Learning."</data>
      <data key="d2">82d58329a3cd23550be3e22f1740f8ae</data>
    </node>
    <node id="&quot;&#352;TEF&#193;NIK AND KADL&#268;&#205;K, 2023&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"&#352;tef&#225;nik and Kadl&#269;&#237;k, 2023 is an important research work contributing to the understanding of In-Context Learning."</data>
      <data key="d2">82d58329a3cd23550be3e22f1740f8ae</data>
    </node>
    <node id="&quot;EMOTION PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Emotion Prompting" incorporates phrases of psychological relevance to humans into the prompt, potentially improving large language model (LLM) performance on benchmarks and open-ended text generation. Additionally, Emotion Prompting involves techniques to elicit specific emotional responses from users or systems.</data>
      <data key="d2">d397224fef0666e16112e5d47a2e1139,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;ROLE PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Role Prompting" involves techniques to guide users or systems to adopt specific roles or perspectives. Also known as persona prompting, it assigns a specific role to the GenAI in the prompt to create more desirable outputs for open-ended tasks.</data>
      <data key="d2">d397224fef0666e16112e5d47a2e1139,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;STYLE PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"STYLE PROMPTING" involves specifying the desired style, tone, or genre in the prompt to shape the output of a GenAI. It includes techniques to influence the stylistic choices in responses or outputs.</data>
      <data key="d2">d397224fef0666e16112e5d47a2e1139,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;SIMTOM&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">SimToM is a technique or method within the domain of prompting that deals with complicated questions involving multiple people or objects. It establishes the set of facts known by one person and provides answers based on those facts.</data>
      <data key="d2">d397224fef0666e16112e5d47a2e1139,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;RAR&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"RaR refers to a specific technique or method within the domain of prompting."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;RE2&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"RE2 refers to a specific technique or method within the domain of prompting."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;SELF-ASK&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"SELF-ASK" involves techniques where the system generates and answers its own questions to improve understanding or performance. Specifically, "SELF-ASK" prompts large language models (LLMs) to decide if they need to ask follow-up questions for a given prompt, generate these questions, answer them, and finally answer the original question.</data>
      <data key="d2">d397224fef0666e16112e5d47a2e1139,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;FEW-SHOT EXAMPLE GENERATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Few-Shot Example Generation involves creating examples with minimal input data to guide the system's learning process."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;EXAMPLE ORDERING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Example Ordering involves arranging examples in a specific sequence to optimize learning or performance."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;EXEMPLAR SELECTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Exemplar Selection involves choosing representative examples to guide the system's learning or decision-making process."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;KNN&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"KNN (K-Nearest Neighbors) is a technique used for selecting examples based on their proximity in a feature space."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;VOTE-K&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Vote-K is a method that selects similar exemplars to the test sample in two stages, ensuring diversity and representativeness in Few-Shot Prompting. This specific technique or method is utilized within the domain of prompting to enhance the performance and accuracy of models by carefully choosing exemplars that are both diverse and representative of the test sample.</data>
      <data key="d2">e845d3c15484b3061e3a376fa8779883,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;THOUGHT GENERATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Thought Generation" encompasses techniques that prompt the LLM to articulate its reasoning while solving a problem. It involves methods to generate coherent and relevant thoughts or ideas.</data>
      <data key="d2">d397224fef0666e16112e5d47a2e1139,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;CHAIN-OF-THOUGHT (COT)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Chain-of-Thought (CoT) involves generating a sequence of thoughts to solve complex problems or tasks."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;ZERO-SHOT COT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Zero-Shot CoT involves generating a chain of thoughts without prior examples or training data."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;ANALOGICAL PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Analogical Prompting" involves using analogies to guide the system's responses or understanding. It is a technique similar to SG-ICL that automatically generates exemplars, including chains-of-thoughts, which improves performance in mathematical reasoning and code generation tasks.</data>
      <data key="d2">eba1ab13141790dedb88f55494236682,f4b740e8b0c84e29c7990fc370919464</data>
    </node>
    <node id="&quot;STEP-BACK PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Step-Back Prompting" involves techniques where the system revisits previous steps to improve understanding or performance. It is a modification of Chain-of-Thoughts where the large language model (LLM) is first asked a high-level question about relevant concepts or facts before delving into reasoning.</data>
      <data key="d2">eba1ab13141790dedb88f55494236682,f4b740e8b0c84e29c7990fc370919464</data>
    </node>
    <node id="&quot;THREAD-OF-THOUGHT (THOT)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Thread-of-Thought (ThoT) involves generating a continuous thread of thoughts to maintain coherence and relevance."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;TAB-COT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Tab-CoT refers to a specific technique or method within the domain of Chain-of-Thought prompting."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;FEW-SHOT COT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Few-Shot CoT" involves generating a chain of thoughts with minimal input data to guide the system's learning process. It is a subdomain of prompt engineering that incorporates few-shot prompting with chain-of-thought reasoning. This technique presents the language model with multiple exemplars, including chains-of-thoughts, which significantly enhances its performance. In studies, Few-Shot CoT has been shown to perform exceptionally well.</data>
      <data key="d2">590db3ee59b442c908a9b425a9be2477,eba1ab13141790dedb88f55494236682,f4b740e8b0c84e29c7990fc370919464</data>
    </node>
    <node id="&quot;ACTIVE-PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Active-Prompt involves techniques where the system actively generates prompts to guide its own learning or decision-making process."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;AUTO-COT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Auto-CoT involves automated generation of a chain of thoughts to solve complex problems or tasks."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;COMPLEXITY-BASED&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Complexity-Based involves techniques that consider the complexity of tasks or problems to guide the system's responses."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;CONTRASTIVE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Contrastive involves techniques that use contrasting examples to improve the system's understanding or performance."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;MEMORY-OF-THOUGHT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Memory-of-Thought involves techniques where the system retains and uses previous thoughts to guide future responses."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;UNCERTAINTY-ROUTED COT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Uncertainty-Routed CoT involves generating a chain of thoughts that addresses uncertainties in the task or problem."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;PROMPT MINING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Mining" involves techniques to discover and use effective prompts to guide the system's responses. It is the process of discovering optimal prompt templates through large corpus analysis to improve prompt performance.</data>
      <data key="d2">e845d3c15484b3061e3a376fa8779883,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;ENSEMBLING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Ensembling" in GenAI is the process of using multiple prompts to solve the same problem, then aggregating these responses into a final output. In many cases, a majority vote&#8212;selecting the most frequent response&#8212;is used to generate the final output. Ensembling techniques reduce the variance of LLM outputs and often improve accuracy, but come with the cost of increasing the number of model calls needed to reach a final answer. Additionally, ensembling involves combining multiple models or techniques to improve the system's performance.</data>
      <data key="d2">eba1ab13141790dedb88f55494236682,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;COSP&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"COSP refers to a specific technique or method within the domain of prompting."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;DENSE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"DENSE refers to a specific technique or method within the domain of prompting."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;DIVERSE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"DIVERSE" refers to a specific technique or method within the domain of prompting. According to Li et al. (2023), DiVeRSe creates multiple prompts for a given problem and then performs Self-Consistency for each, generating multiple reasoning paths. These reasoning paths are scored based on each step within them, and a final response is selected from the highest-scoring paths.</data>
      <data key="d2">eba1ab13141790dedb88f55494236682,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;MAX MUTUAL INFORMATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Max Mutual Information involves techniques that maximize the mutual information between inputs and outputs to improve performance."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;META-COT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Meta-CoT refers to a specific technique or method within the domain of Chain-of-Thought prompting."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;MORE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"MoRE refers to a specific technique or method within the domain of prompting."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;UNIVERSAL SELF-CONSISTENCY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Universal Self-Consistency"

Universal Self-Consistency (Chen et al., 2023e) is a method similar to Self-Consistency, but with a key difference in how the majority response is determined. Instead of programmatically counting how often a response occurs, Universal Self-Consistency inserts all outputs into a prompt template that selects the majority answer. This approach is particularly useful for free-form text generation and scenarios where the same answer might be expressed slightly differently by various prompts. Additionally, Universal Self-Consistency involves techniques to ensure consistency across all responses and outputs of the system.</data>
      <data key="d2">eba1ab13141790dedb88f55494236682,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;USP&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"USP refers to a specific technique or method within the domain of prompting."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;PROMPT PARAPHRASING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Paraphrasing" involves techniques to rephrase prompts to improve the system's understanding or performance. It transforms an original prompt by changing some of the wording while maintaining the overall meaning. This method is a data augmentation technique used to generate prompts for an ensemble.</data>
      <data key="d2">0b1362066be4992987aeec37198a7788,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;SELF-CRITICISM&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Self-Criticism" involves having language models (LLMs) critique their own outputs, either by judging correctness or providing feedback to improve the answer. Various approaches to generating and integrating self-criticism have been developed. These techniques are designed to enhance the system's performance by allowing it to evaluate and refine its responses.</data>
      <data key="d2">0b1362066be4992987aeec37198a7788,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;CHAIN-OF-VERIFICATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"CHAIN-OF-VERIFICATION" involves generating a sequence of verification steps to ensure the accuracy of responses. It is a subdomain that reduces hallucination in large language models.</data>
      <data key="d2">e5878afbfbf5194f1da3540eaa88fe65,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;SELF-CALIBRATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Self-Calibration" involves techniques where the system adjusts its own parameters to improve performance. Additionally, Self-Calibration prompts a large language model (LLM) to answer a question, then builds a new prompt that includes the question, the LLM&#8217;s answer, and an additional instruction asking whether the answer is correct. This method is useful for gauging confidence levels.</data>
      <data key="d2">0b1362066be4992987aeec37198a7788,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;SELF-REFINE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Self-Refine" involves techniques where the system refines its own responses to improve accuracy and relevance. It is an iterative framework where a large language model (LLM) provides feedback on its own initial answer and then improves the answer based on the feedback. This process continues until a stopping condition is met. The technique of Self-Refine, which includes iterative refinement with self-feedback, is aimed at enhancing the performance and accuracy of models.</data>
      <data key="d2">0b1362066be4992987aeec37198a7788,3fd8f6dcbbf1eecd6efb01ea12538679,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;SELF-VERIFICATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Self-Verification" is a technique that involves generating multiple candidate solutions using Chain-of-Thought (CoT) processes. Each solution is then scored by masking parts of the original question and asking a large language model (LLM) to predict the masked parts based on the rest of the question and the generated solution. This method ensures accuracy by having the system verify its own responses.</data>
      <data key="d2">0b1362066be4992987aeec37198a7788,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;REVERSECOT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"ReverseCoT refers to a specific technique or method within the domain of Chain-of-Thought prompting."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;CUMULATIVE REASONING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Cumulative Reasoning" is a subdomain that involves reasoning with large language models, as discussed by Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao in 2023. It generates several potential steps in answering a question, then has a large language model (LLM) evaluate these steps, deciding to accept or reject them. This process is repeated until the final answer is reached. Cumulative Reasoning employs techniques where the system builds upon previous reasoning steps to solve complex problems.</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,0b1362066be4992987aeec37198a7788,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;DECOMPOSITION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Decomposition" involves breaking down complex problems into simpler parts or sub-questions to improve understanding, performance, and problem-solving ability for both humans and GenAI.</data>
      <data key="d2">589a9782efd8ac3ff7d79dba07974e2b,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;DECOMP&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"DECOMP refers to a specific technique or method within the domain of decomposition."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;FAITHFUL COT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Faithful CoT involves generating a chain of thoughts that accurately represents the problem or task."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;LEAST-TO-MOST&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Least-to-Most involves solving problems by starting with the simplest parts and progressively addressing more complex aspects."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;PLAN-AND-SOLVE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Plan-and-Solve involves creating a plan to address a problem and then executing the plan to find a solution."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;PROGRAM-OF-THOUGHT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Program-of-Thought involves generating a sequence of thoughts that function like a program to solve problems."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;RECURSION-OF-THOUGHT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Recursion-of-Thought" involves using recursive techniques to generate thoughts that build upon each other. It is similar to regular Chain-of-Thought (CoT) but distinguishes itself by sending complicated problems into another prompt or language model (LLM) call, recursively solving complex problems. This method has demonstrated improvements in handling arithmetic and algorithmic tasks.</data>
      <data key="d2">589a9782efd8ac3ff7d79dba07974e2b,eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;SKELETON-OF-THOUGHT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Skeleton-of-Thought" (Ning et al., 2023) focuses on accelerating answer speed through parallelization. Given a problem, it prompts a large language model (LLM) to create a skeleton of the answer, essentially breaking it down into sub-problems to be solved. This method involves creating an outline or framework of thoughts to guide problem-solving. These sub-questions are then sent in parallel to the LLM, and the outputs are concatenated to form a final response.</data>
      <data key="d2">eba1ab13141790dedb88f55494236682,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;TREE-OF-THOUGHT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Tree-of-Thought involves generating a branching structure of thoughts to explore multiple solutions to a problem."</data>
      <data key="d2">eba1ab13141790dedb88f55494236682</data>
    </node>
    <node id="&quot;LABEL QUALITY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Label Quality refers to the accuracy and correctness of labels assigned to data exemplars, ensuring they are labeled correctly."</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89</data>
    </node>
    <node id="&quot;EXEMPLAR SIMILARITY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Exemplar Similarity" involves selecting exemplars that are similar to the test instance or test sample to improve model performance. This method is generally beneficial for enhancing the performance of the model.</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89,e845d3c15484b3061e3a376fa8779883</data>
    </node>
    <node id="&quot;EXEMPLAR FORMAT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"EXEMPLAR FORMAT" involves the structure and presentation of exemplars, which can influence the performance of machine learning models. It also refers to choosing a common format for exemplars to maintain consistency in the data.</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89,e845d3c15484b3061e3a376fa8779883</data>
    </node>
    <node id="&quot;EXEMPLAR QUANTITY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Exemplar Quantity involves including as many exemplars as possible to improve model performance, particularly in larger models."</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89</data>
    </node>
    <node id="&quot;FEW-SHOT LEARNING (FSL)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Few-Shot Learning (FSL) is a machine learning paradigm that adapts parameters with a few examples."</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89</data>
    </node>
    <node id="&quot;EXEMPLAR ORDERING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Exemplar Ordering refers to the sequence in which exemplars are presented, which can significantly affect model behavior."</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89</data>
    </node>
    <node id="&quot;EXEMPLAR LABEL DISTRIBUTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Exemplar Label Distribution" involves the distribution of labels among exemplars, which affects the behavior and performance of machine learning models similarly to traditional supervised learning. This distribution of exemplar labels within a prompt plays a crucial role in influencing model behavior.</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89,e845d3c15484b3061e3a376fa8779883</data>
    </node>
    <node id="&quot;DONG ET AL., 2023&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Dong et al., 2023 refers to a study or publication that discusses the factors influencing the performance of exemplars in prompts."</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89</data>
    </node>
    <node id="&quot;ZHAO ET AL., 2021A&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Zhao et al., 2021a refers to a study or publication that highlights the critical influence of exemplar selection and order on output quality."</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89</data>
    </node>
    <node id="&quot;LU ET AL., 2021&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Lu et al., 2021 refers to a study or publication that discusses the impact of exemplar order on model behavior and accuracy."</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89</data>
    </node>
    <node id="&quot;YE AND DURRETT, 2023&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Ye and Durrett, 2023 refers to a study or publication that examines the influence of exemplar selection and order on model performance."</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89</data>
    </node>
    <node id="&quot;BROWN ET AL., 2020&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Brown et al., 2020 refers to a study or publication that discusses Few-Shot Prompting and its impact on model performance."</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89</data>
    </node>
    <node id="&quot;LIU ET AL., 2021&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Liu et al., 2021 refers to a study or publication that examines the effects of exemplar quantity and order on model performance."</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89</data>
    </node>
    <node id="&quot;KUMAR AND TALUKDAR, 2021&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Kumar and Talukdar, 2021 refers to a study or publication that discusses the impact of exemplar order on model behavior."</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89</data>
    </node>
    <node id="&quot;RUBIN ET AL., 2022&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Rubin et al., 2022 refers to a study or publication that examines the effects of exemplar order on model accuracy."</data>
      <data key="d2">5d5844de9a93093f225ca41ba18f9a89</data>
    </node>
    <node id="&quot;EXEMPLAR LABEL QUALITY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Exemplar Label Quality pertains to the accuracy and validity of labels in exemplars, which can impact the performance of machine learning models."</data>
      <data key="d2">e845d3c15484b3061e3a376fa8779883</data>
    </node>
    <node id="&quot;K-NEAREST NEIGHBOR (KNN)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"K-Nearest Neighbor (KNN) is an algorithm that selects exemplars similar to the test sample to boost performance in Few-Shot Prompting."</data>
      <data key="d2">e845d3c15484b3061e3a376fa8779883</data>
    </node>
    <node id="&quot;SELF-GENERATED IN-CONTEXT LEARNING (SG-ICL)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Self-Generated In-Context Learning (SG-ICL) leverages a GenAI to automatically generate exemplars, useful when training data is unavailable."</data>
      <data key="d2">e845d3c15484b3061e3a376fa8779883</data>
    </node>
    <node id="&quot;LENS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"LENS is a technique that leverages iterative filtering, embedding, and retrieval to improve prompt performance."</data>
      <data key="d2">d397224fef0666e16112e5d47a2e1139</data>
    </node>
    <node id="&quot;UDR&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"UDR is a technique that uses reinforcement learning to enhance prompt performance."</data>
      <data key="d2">d397224fef0666e16112e5d47a2e1139</data>
    </node>
    <node id="&quot;ACTIVE EXAMPLE SELECTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Active Example Selection" is a subdomain that involves selecting examples for in-context learning, as explored by Yiming Zhang, Shi Feng, and Chenhao Tan in 2022. This technique involves selecting examples iteratively to improve prompt performance.</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,d397224fef0666e16112e5d47a2e1139</data>
    </node>
    <node id="&quot;ZERO-SHOT PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Zero-Shot Prompting uses zero exemplars and includes standalone techniques as well as combinations with other concepts like Chain of Thought."</data>
      <data key="d2">d397224fef0666e16112e5d47a2e1139</data>
    </node>
    <node id="&quot;SYSTEM 2 ATTENTION (S2A)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"System 2 Attention (S2A) first asks an LLM to rewrite the prompt to remove unrelated information, then passes this new prompt to retrieve a final response."</data>
      <data key="d2">d397224fef0666e16112e5d47a2e1139</data>
    </node>
    <node id="&quot;REPHRASE AND RESPOND (RAR)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Rephrase and Respond (RaR) instructs the LLM to rephrase and expand the question before generating the final answer, which can be done in a single pass or separately."</data>
      <data key="d2">d397224fef0666e16112e5d47a2e1139</data>
    </node>
    <node id="&quot;RE-READING (RE2)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Re-reading (RE2) adds the phrase 'Read the question again:' to the prompt, which has shown improvement in reasoning benchmarks, especially with complex questions."</data>
      <data key="d2">d397224fef0666e16112e5d47a2e1139</data>
    </node>
    <node id="&quot;CHAIN-OF-THOUGHT (COT) PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Chain-of-Thought (CoT) Prompting" is a technique that extends to multilingual settings to construct reasoning paths in different languages to answer the same question. It leverages few-shot prompting to encourage the large language model (LLM) to express its thought process before delivering its final answer, thereby enhancing performance in mathematics and reasoning tasks.</data>
      <data key="d2">45c77c52a93a949222fda99a95e0c3d6,d397224fef0666e16112e5d47a2e1139</data>
    </node>
    <node id="&quot;CHAIN-OF-THOUGHTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Chain-of-Thoughts (CoT) is a technique used to enhance the performance of large language models (LLMs) in mathematics and reasoning tasks by including a reasoning path in the prompt."</data>
      <data key="d2">f4b740e8b0c84e29c7990fc370919464</data>
    </node>
    <node id="&quot;ZERO-SHOT-COT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Zero-Shot-CoT (Chain-of-Thought) is a prompting technique that includes thought inducers to generate reasoning steps. It is a subdomain of prompt engineering that involves zero-shot prompting with chain-of-thought reasoning. Unlike other versions of Chain-of-Thoughts, Zero-Shot-CoT does not require exemplars and involves appending a thought-inducing phrase to the prompt. However, it has been observed that this technique showed a drop in performance."</data>
      <data key="d2">590db3ee59b442c908a9b425a9be2477,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464</data>
    </node>
    <node id="&quot;THREAD-OF-THOUGHT (THOT) PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Thread-of-Thought (ThoT) Prompting is an improved thought inducer for Chain-of-Thoughts reasoning, using a more detailed prompt to guide the reasoning process."</data>
      <data key="d2">f4b740e8b0c84e29c7990fc370919464</data>
    </node>
    <node id="&quot;TABULAR CHAIN-OF-THOUGHT (TAB-COT)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Tabular Chain-of-Thought (Tab-CoT) is a Zero-Shot CoT prompt that structures the LLM's reasoning output as a markdown table, improving the structure and reasoning of the output."</data>
      <data key="d2">f4b740e8b0c84e29c7990fc370919464</data>
    </node>
    <node id="&quot;CONTRASTIVE COT PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Contrastive CoT Prompting adds both correct and incorrect exemplars to the CoT prompt to show the LLM how not to reason, improving performance in areas like Arithmetic Reasoning and Factual QA."</data>
      <data key="d2">f4b740e8b0c84e29c7990fc370919464</data>
    </node>
    <node id="&quot;UNCERTAINTY-ROUTED COT PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Uncertainty-Routed CoT Prompting samples multiple CoT reasoning paths and selects the majority if it is above a certain threshold, demonstrating improvement on the MMLU benchmark."</data>
      <data key="d2">f4b740e8b0c84e29c7990fc370919464</data>
    </node>
    <node id="&quot;COMPLEXITY-BASED PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Complexity-based Prompting" involves selecting complex examples for annotation and inclusion in the prompt. This method utilizes a majority vote among reasoning chains that exceed a certain length threshold during inference to improve the quality of answers. It has demonstrated improvements, particularly on mathematical reasoning datasets.</data>
      <data key="d2">589a9782efd8ac3ff7d79dba07974e2b,f4b740e8b0c84e29c7990fc370919464</data>
    </node>
    <node id="&quot;ACTIVE PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Active Prompting starts with training questions/exemplars, asks the LLM to solve them, calculates uncertainty, and asks human annotators to rewrite the exemplars with the highest uncertainty."</data>
      <data key="d2">589a9782efd8ac3ff7d79dba07974e2b</data>
    </node>
    <node id="&quot;MEMORY-OF-THOUGHT PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Memory-of-Thought Prompting leverages unlabeled training exemplars to build Few-Shot CoT prompts at test time, retrieving similar instances to the test sample to improve performance on benchmarks like arithmetic, commonsense, and factual reasoning."</data>
      <data key="d2">589a9782efd8ac3ff7d79dba07974e2b</data>
    </node>
    <node id="&quot;AUTOMATIC CHAIN-OF-THOUGHT (AUTO-COT) PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Automatic Chain-of-Thought (Auto-CoT) Prompting uses Zero-Shot prompts to automatically generate chains of thought, which are then used to build Few-Shot CoT prompts for test samples."</data>
      <data key="d2">589a9782efd8ac3ff7d79dba07974e2b</data>
    </node>
    <node id="&quot;LEAST-TO-MOST PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Least-to-Most Prompting" is a technique designed to enhance complex reasoning in large language models by structuring prompts from the least to the most complex. This method begins with simpler tasks and gradually increases in complexity, aiding the model in building up to solving more difficult problems. The process involves breaking a problem into sub-problems without initially solving them, then addressing each sub-problem sequentially. Model responses are appended to the prompt after each step until a final result is achieved. This approach has demonstrated improvements in tasks that require symbolic manipulation, compositional generalization, and mathematical reasoning.</data>
      <data key="d2">4257f30018a4acf2e8ee95f21de8d7df,589a9782efd8ac3ff7d79dba07974e2b,c7285f7847ef45ed85779d7966753855</data>
    </node>
    <node id="&quot;DECOMPOSED PROMPTING (DECOMP)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Decomposed Prompting (DECOMP) Few-Shot prompts a LLM to use certain functions like string splitting or internet searching, breaking down the original problem into sub-problems sent to different functions. It has shown improved performance over Least-to-Most prompting on some tasks."</data>
      <data key="d2">589a9782efd8ac3ff7d79dba07974e2b</data>
    </node>
    <node id="&quot;PLAN-AND-SOLVE PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Plan-and-Solve Prompting" is an approach designed to enhance zero-shot chain-of-thought (CoT) reasoning by large language models. It consists of an improved Zero-Shot CoT prompt that generates more robust reasoning processes compared to standard Zero-Shot CoT on multiple reasoning datasets. The goal of Plan-and-Solve Prompting is to improve the reasoning capabilities of these models, making them more effective in various reasoning tasks.</data>
      <data key="d2">153eeb5a63e650f2cd12f700ffe3e71f,589a9782efd8ac3ff7d79dba07974e2b</data>
    </node>
    <node id="&quot;TREE-OF-THOUGHT (TOT)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Tree-of-Thought (ToT) creates a tree-like search problem by generating multiple possible steps in the form of thoughts, evaluating progress, and deciding which steps to continue with. It is effective for tasks requiring search and planning."</data>
      <data key="d2">589a9782efd8ac3ff7d79dba07974e2b</data>
    </node>
    <node id="&quot;PROGRAM-OF-THOUGHTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Program-of-Thoughts (Chen et al., 2023d) uses LLMs like Codex to generate programming code as reasoning steps. A code interpreter executes these steps to obtain the final answer. It excels in mathematical and programming-related tasks but is less effective for semantic reasoning tasks."</data>
      <data key="d2">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;FAITHFUL CHAIN-OF-THOUGHT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Faithful Chain-of-Thought (Lyu et al., 2023) generates a CoT that has both natural language and symbolic language (e.g. Python) reasoning, just like Program-of-Thoughts. However, it also makes use of different types of symbolic languages in a task-dependent fashion."</data>
      <data key="d2">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;DEMONSTRATION ENSEMBLING (DENSE)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Demonstration Ensembling (DENSE) (Khalifa et al., 2023) creates multiple few-shot prompts, each containing a distinct subset of exemplars from the training set. Next, it aggregates over their outputs to generate a final response."</data>
      <data key="d2">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;MIXTURE OF REASONING EXPERTS (MORE)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Mixture of Reasoning Experts (MoRE) (Si et al., 2023d) creates a set of diverse reasoning experts by using different specialized prompts for different reasoning types (such as retrieval augmentation prompts for factual reasoning, Chain-of-Thought reasoning for multi-hop and math reasoning, and generated knowledge prompting for commonsense reasoning). The best answer from all experts is selected based on an agreement score."</data>
      <data key="d2">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;MAX MUTUAL INFORMATION METHOD&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Max Mutual Information Method (Sorensen et al., 2022) creates multiple prompt templates with varied styles and exemplars, then selects the optimal template as the one that maximizes mutual information between the prompt and the LLM&#8217;s outputs."</data>
      <data key="d2">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;META-REASONING OVER MULTIPLE COTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Meta-Reasoning over Multiple CoTs (Yoran et al., 2023) is similar to universal Self-Consistency; it first generates multiple reasoning chains (but not necessarily final answers) for a given problem. Next, it inserts all of these chains in a single prompt template then generates a final answer from them."</data>
      <data key="d2">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;CONSISTENCY-BASED SELF-ADAPTIVE PROMPTING (COSP)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Consistency-based Self-adaptive Prompting (COSP) constructs Few-Shot Chain-of-Thought (CoT) prompts by initially running Zero-Shot CoT with Self-Consistency on a set of examples. From these outputs, COSP selects a high agreement subset to be included in the final prompt as exemplars. It then performs Self-Consistency with this final prompt. This method was detailed by Wan et al. in 2023.</data>
      <data key="d2">0b1362066be4992987aeec37198a7788,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </node>
    <node id="&quot;UNIVERSAL SELF-ADAPTIVE PROMPTING (USP)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"USP builds upon the success of COSP, aiming to make it generalizable to all tasks. It uses unlabeled data to generate exemplars and a more complicated scoring function to select them. USP does not use Self-Consistency."</data>
      <data key="d2">0b1362066be4992987aeec37198a7788</data>
    </node>
    <node id="&quot;REVERSING CHAIN-OF-THOUGHT (RCOT)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"RCoT prompts LLMs to reconstruct the problem based on the generated answer, then generates fine-grained comparisons between the original problem and the reconstructed problem to check for inconsistencies. These inconsistencies are converted to feedback for revising the answer."</data>
      <data key="d2">0b1362066be4992987aeec37198a7788</data>
    </node>
    <node id="&quot;CHAIN-OF-VERIFICATION (COVE)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"COVE uses an LLM to generate an answer to a question, then creates a list of related questions to verify the correctness of the answer. Each question is answered by the LLM, and all information is used to produce the final revised answer."</data>
      <data key="d2">0b1362066be4992987aeec37198a7788</data>
    </node>
    <node id="&quot;GENAI SYSTEMS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"GenAI systems refer to generative AI systems that create outputs based on prompts and can include various techniques like self-criticism and self-calibration."</data>
      <data key="d2">0b1362066be4992987aeec37198a7788</data>
    </node>
    <node id="&quot;LOGICAL INFERENCE TASKS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Logical Inference Tasks refer to the area of study focused on improving the ability of models to make logical deductions and solve mathematical problems."</data>
      <data key="d2">83a60257c9adae8c826e73ef32d16dd0</data>
    </node>
    <node id="&quot;MATHEMATICAL PROBLEM&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Mathematical Problem refers to the area of study focused on improving the ability of models to solve mathematical equations and problems."</data>
      <data key="d2">83a60257c9adae8c826e73ef32d16dd0</data>
    </node>
    <node id="&quot;CHAIN-OF-THOUGHT PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Chain-of-Thought Prompting" is a technique designed to enhance the reasoning capabilities of large language models by guiding them through a series of logical steps. This method involves breaking down complex reasoning processes into manageable steps, which helps improve the model's performance on intricate tasks. Additionally, "Chain-of-Thought Prompting" can generate images as part of its thought process, using prompts like "Let's think image by image" to create SVGs for visual reasoning. By guiding the model to generate a sequence of reasoning steps, this technique not only aids in arriving at accurate answers but also enhances evaluation performance by ensuring a thorough reasoning process before generating a quality assessment.</data>
      <data key="d2">27d8fe15ab6f9e3d91fd5858fbeba7ea,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,83a60257c9adae8c826e73ef32d16dd0,c28998cdf87522d883979f9c6405f535</data>
    </node>
    <node id="&quot;GENAI MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"GenAI Models refer to various generative AI models like GPT-3, BERT, and others, which are frequently cited in research papers."</data>
      <data key="d2">83a60257c9adae8c826e73ef32d16dd0</data>
    </node>
    <node id="&quot;BENCHMARK DATASETS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Benchmark Datasets are standardized datasets used to evaluate the performance of new prompting techniques across different models."</data>
      <data key="d2">83a60257c9adae8c826e73ef32d16dd0</data>
    </node>
    <node id="&quot;META PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Meta Prompting is a technique where a language model is prompted to generate or improve a prompt or prompt template."</data>
      <data key="d2">83a60257c9adae8c826e73ef32d16dd0</data>
    </node>
    <node id="&quot;AUTOPROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"AUTOPROMPT" is a technique that utilizes a frozen language model (LLM) and a prompt template containing 'trigger tokens.' These trigger tokens are updated through backpropagation during training. It is a version of soft-prompting.</data>
      <data key="d2">83a60257c9adae8c826e73ef32d16dd0,981e367f454fd6805ff2ad123c75b85e</data>
    </node>
    <node id="&quot;AUTOMATIC PROMPT ENGINEER (APE)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Automatic Prompt Engineer (APE) is a technique for automatically optimizing prompts, as discussed in research papers. APE uses a set of exemplars to generate Zero-Shot instruction prompts. It iterates by generating, scoring, and creating variations of prompts until certain criteria are met.</data>
      <data key="d2">83a60257c9adae8c826e73ef32d16dd0,981e367f454fd6805ff2ad123c75b85e</data>
    </node>
    <node id="&quot;RESEARCH PAPERS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Research Papers refer to academic publications that propose, evaluate, and cite various prompting techniques and models."</data>
      <data key="d2">83a60257c9adae8c826e73ef32d16dd0</data>
    </node>
    <node id="&quot;CITATIONS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Citations refer to the number of times a research paper is referenced by other papers, used as a measure of the paper's influence."</data>
      <data key="d2">83a60257c9adae8c826e73ef32d16dd0</data>
    </node>
    <node id="&quot;GRADIENTFREE INSTRUCTIONAL PROMPT SEARCH (GRIPS)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"GrIPS is similar to APE but uses more complex operations like deletion, addition, swapping, and paraphrasing to create prompt variations."</data>
      <data key="d2">981e367f454fd6805ff2ad123c75b85e</data>
    </node>
    <node id="&quot;PROMPT OPTIMIZATION WITH TEXTUAL GRADIENTS (PROTEGI)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"ProTeGi improves a prompt template through a multi-step process involving criticism of the original prompt and selection of new prompts using a bandit algorithm."</data>
      <data key="d2">981e367f454fd6805ff2ad123c75b85e</data>
    </node>
    <node id="&quot;RLPROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">RLPrompt is a subdomain focused on optimizing discrete text prompts with reinforcement learning. It employs a frozen large language model (LLM) alongside an unfrozen module to generate and score prompt templates. The module is updated using Soft Q-Learning, ensuring the continuous improvement of prompt optimization.</data>
      <data key="d2">981e367f454fd6805ff2ad123c75b85e,e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;DIALOGUE-COMPRISED POLICY-GRADIENT-BASED DISCRETE PROMPT OPTIMIZATION (DP2O)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"DP2O is a complex prompt engineering technique involving reinforcement learning, a custom prompt scoring function, and conversations with an LLM to construct the prompt."</data>
      <data key="d2">981e367f454fd6805ff2ad123c75b85e</data>
    </node>
    <node id="&quot;ANSWER SHAPE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"ANSWER SHAPE" refers to the physical format of an answer, which could be a token, span of tokens, image, or video. It is useful to restrict the output shape for specific tasks like binary classification.</data>
      <data key="d2">45c77c52a93a949222fda99a95e0c3d6,981e367f454fd6805ff2ad123c75b85e</data>
    </node>
    <node id="&quot;ANSWER SPACE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Answer Space" is the domain of values that an answer's structure may contain. It can range from all tokens to just two possible tokens in a binary labeling task, such as a single token for binary classification tasks.</data>
      <data key="d2">45c77c52a93a949222fda99a95e0c3d6,981e367f454fd6805ff2ad123c75b85e</data>
    </node>
    <node id="&quot;LLM&quot;">
      <data key="d0" />
      <data key="d1">LLM (Large Language Model) refers to a type of artificial intelligence model that can generate human-like text based on the input it receives. It can be used in various applications, including making API calls, writing and running code, and searching the internet. Additionally, LLMs are employed in various tasks such as prompt engineering.</data>
      <data key="d2">981e367f454fd6805ff2ad123c75b85e,cbd06bb38a855be4a07883f499014eaa,e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;ANSWER EXTRACTOR&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Answer Extractor is a rule or function used to extract the final answer from the model output, especially when it is impossible to entirely control the answer space. It can be a simple function like a regular expression or a separate LLM."</data>
      <data key="d2">45c77c52a93a949222fda99a95e0c3d6</data>
    </node>
    <node id="&quot;REGEX&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Regex is often used to extract answers by searching for the first or last instance of a label, depending on the output format and whether Chains of Thought (CoTs) are generated."</data>
      <data key="d2">45c77c52a93a949222fda99a95e0c3d6</data>
    </node>
    <node id="&quot;SEPARATE LLM&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Separate LLM is used to evaluate and extract answers when outputs are too complicated for regexes to work consistently."</data>
      <data key="d2">45c77c52a93a949222fda99a95e0c3d6</data>
    </node>
    <node id="&quot;MULTILINGUAL PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Multilingual Prompting involves techniques to improve model performance in non-English settings, addressing the disparity in output quality for low-resource languages."</data>
      <data key="d2">45c77c52a93a949222fda99a95e0c3d6</data>
    </node>
    <node id="&quot;TRANSLATE FIRST PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Translate First Prompting is a strategy that translates non-English input examples into English to leverage the model's strengths in English for better understanding."</data>
      <data key="d2">45c77c52a93a949222fda99a95e0c3d6</data>
    </node>
    <node id="&quot;XLT (CROSS-LINGUAL THOUGHT) PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"XLT (Cross-Lingual Thought) Prompting utilizes a prompt template composed of six separate instructions, including role assignment, cross-lingual thinking, and CoT."</data>
      <data key="d2">45c77c52a93a949222fda99a95e0c3d6</data>
    </node>
    <node id="&quot;CROSS-LINGUAL SELF CONSISTENT PROMPTING (CLSP)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Cross-Lingual Self Consistent Prompting (CLSP) introduces an ensemble technique that constructs reasoning paths in different languages to answer the same question."</data>
      <data key="d2">45c77c52a93a949222fda99a95e0c3d6</data>
    </node>
    <node id="&quot;X-INSTA PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"X-InSTA Prompting" explores three distinct approaches for aligning in-context examples with input sentences for classification tasks: semantic alignment, task-based alignment, and a combination of both.</data>
      <data key="d2">45c77c52a93a949222fda99a95e0c3d6,ebba9603b39b6606ba9902c9cf61fecb</data>
    </node>
    <node id="&quot;ICL&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"ICL (In-Context Learning) is a subdomain that has been extended to multilingual settings, focusing on aligning in-context examples with input sentences for classification tasks."</data>
      <data key="d2">ebba9603b39b6606ba9902c9cf61fecb</data>
    </node>
    <node id="&quot;IN-CLT PROMPTING&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"In-CLT (Cross-lingual Transfer) Prompting leverages both source and target languages to create in-context examples, enhancing the cross-lingual cognitive capabilities of multilingual LLMs."</data>
      <data key="d2">ebba9603b39b6606ba9902c9cf61fecb</data>
    </node>
    <node id="&quot;PARC&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"PARC (Prompts Augmented by Retrieval Cross-lingually) introduces a framework that retrieves relevant exemplars from a high-resource language to enhance cross-lingual transfer performance, especially for low-resource target languages."</data>
      <data key="d2">ebba9603b39b6606ba9902c9cf61fecb</data>
    </node>
    <node id="&quot;ENGLISH PROMPT TEMPLATE&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Constructing the prompt template in English is often more effective for multilingual tasks due to the predominance of English data during LLM pre-training."</data>
      <data key="d2">ebba9603b39b6606ba9902c9cf61fecb</data>
    </node>
    <node id="&quot;TASK LANGUAGE PROMPT TEMPLATE&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Task Language Prompt Template involves using the task language for prompt templates, which is common in multilingual prompting benchmarks for language-specific use cases."</data>
      <data key="d2">ebba9603b39b6606ba9902c9cf61fecb</data>
    </node>
    <node id="&quot;BUFFET&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">BUFFET is a multilingual prompting benchmark that utilizes task language prompts specifically designed for language-specific use cases.</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604,ebba9603b39b6606ba9902c9cf61fecb</data>
    </node>
    <node id="&quot;LONGBENCH&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">LongBench is a bilingual, multitask benchmark designed for long context understanding, as described in a 2023 research paper. It is also a multilingual prompting benchmark that uses task language prompts for language-specific use cases.</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604,b363fca358c69a9412b955c53352ea9a,ebba9603b39b6606ba9902c9cf61fecb</data>
    </node>
    <node id="&quot;MUENNIGHOFF ET AL. (2023)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Muennighoff et al. (2023) is a research group that studies different translation methods when constructing native-language prompts."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;NAMBI ET AL. (2023)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Nambi et al. (2023) is a research group that discusses the variability in performance of native or non-native template prompts across tasks and models."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;HE ET AL. (2023B)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"He et al. (2023b) is a research group that developed the Multi-Aspect Prompting and Selection (MAPS) framework for high-quality translation."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;LU ET AL. (2023B)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Lu et al. (2023b) is a research group that developed the Chain-of-Dictionary (CoD) method for machine translation."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;GHAZVININEJAD ET AL. (2023)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Ghazvininejad et al. (2023) is a research group that developed the Dictionary-based Prompting for Machine Translation (DiPMT) method."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;PUDUPPULLY ET AL. (2023)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Puduppully et al. (2023) is a research group that developed the Decomposed Prompting for MT (DecoMT) method."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;PILAULT ET AL. (2023)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Pilault et al. (2023) is a research group that developed the Interactive-Chain-Prompting (ICP) method for dealing with ambiguities in translation."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;YANG ET AL. (2023D)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Yang et al. (2023d) is a research group that developed the Iterative Prompting method for refining translations with human feedback."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;MULTI-ASPECT PROMPTING AND SELECTION (MAPS)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"MAPS is a framework that mimics the human translation process by integrating knowledge mining and generating multiple possible translations to select the best one."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;CHAIN-OF-DICTIONARY (COD)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"CoD is a method that extracts words from the source phrase and lists their meanings in multiple languages to aid in translation."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;DICTIONARY-BASED PROMPTING FOR MACHINE TRANSLATION (DIPMT)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"DiPMT is a method similar to CoD but only gives definitions in the source and target languages."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;DECOMPOSED PROMPTING FOR MT (DECOMT)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"DecoMT divides the source text into chunks and translates them independently using few-shot prompting, then generates a final translation using contextual information."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;INTERACTIVE-CHAIN-PROMPTING (ICP)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"ICP deals with ambiguities in translation by generating sub-questions about ambiguities and incorporating human responses into the final translation."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;ITERATIVE PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Iterative Prompting involves creating a draft translation with LLMs and refining it with supervision signals or human feedback."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;ACCURATE AND NUANCED TRANSLATION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to leverage GenAI to facilitate accurate and nuanced translation, which is a specific application of prompting techniques."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;HIGH-QUALITY OUTPUT&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal of the Multi-Aspect Prompting and Selection (MAPS) framework is to ensure high-quality output in translations."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;HUMAN-IN-THE-LOOP&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to involve humans in the translation process to deal with ambiguities and refine translations."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;MULTIMODAL PROMPTING TECHNIQUES&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The emergence of new prompting techniques that involve different modalities such as images, videos, and 3D data."</data>
      <data key="d2">8bafc5999ce3abba6f261770c5945604</data>
    </node>
    <node id="&quot;MULTIMODAL PROMPTING TECHNIQUE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Multimodal Prompting Technique refers to the use of different modalities, such as images and audio, to create novel prompting methods beyond traditional text-based techniques."</data>
      <data key="d2">6edacbda20b2fdd4077246c7b271a8b5</data>
    </node>
    <node id="&quot;PROMPT MODIFIERS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Modifiers are words appended to a prompt to change the resultant image, including components like Medium and Lighting."</data>
      <data key="d2">6edacbda20b2fdd4077246c7b271a8b5</data>
    </node>
    <node id="&quot;NEGATIVE PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Negative Prompting allows users to numerically weight certain terms in the prompt to influence the model's output, such as generating anatomically accurate hands by negatively weighting terms like 'bad hands' and 'extra digits'."</data>
      <data key="d2">6edacbda20b2fdd4077246c7b271a8b5</data>
    </node>
    <node id="&quot;MULTIMODAL IN-CONTEXT LEARNING (ICL)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Multimodal In-Context Learning extends the success of ICL in text-based settings to multimodal settings, allowing models to learn from examples that include multiple modalities."</data>
      <data key="d2">6edacbda20b2fdd4077246c7b271a8b5</data>
    </node>
    <node id="&quot;PAIRED-IMAGE PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Paired-Image Prompting involves showing the model two images (before and after a transformation) and then presenting a new image for the model to perform the demonstrated conversion."</data>
      <data key="d2">6edacbda20b2fdd4077246c7b271a8b5</data>
    </node>
    <node id="&quot;IMAGE-AS-TEXT PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Image-as-Text Prompting generates a textual description of an image, allowing the inclusion of images in text-based prompts."</data>
      <data key="d2">6edacbda20b2fdd4077246c7b271a8b5</data>
    </node>
    <node id="&quot;MULTIMODAL CHAIN-OF-THOUGHT (COT)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Multimodal Chain-of-Thought extends CoT to the image domain, using images along with textual instructions to solve problems step by step."</data>
      <data key="d2">6edacbda20b2fdd4077246c7b271a8b5</data>
    </node>
    <node id="&quot;DUTY DISTINCT CHAIN-OF-THOUGHT (DDCOT)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Duty Distinct Chain-of-Thought extends Least-to-Most prompting to multimodal settings, creating subquestions, solving them, and combining the answers into a final response."</data>
      <data key="d2">6edacbda20b2fdd4077246c7b271a8b5</data>
    </node>
    <node id="&quot;MULTIMODAL GRAPH-OF-THOUGHT (GOT)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Multimodal Graph-of-Thought extends Graph-of-Thought to multimodal settings, using a thought graph constructed from the input prompt and image captions to generate rationales and answers."</data>
      <data key="d2">6edacbda20b2fdd4077246c7b271a8b5</data>
    </node>
    <node id="&quot;CHAIN-OF-IMAGES (COI)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Chain-of-Images is a multimodal extension of Chain-of-Thought prompting that generates images as part of its thought process, using prompts like 'Let&#8217;s think image by image' to generate SVGs for visual reasoning."</data>
      <data key="d2">6edacbda20b2fdd4077246c7b271a8b5</data>
    </node>
    <node id="&quot;ADEPT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">Adept is a company exploring how to allow large language models (LLMs) to make use of external systems, driven by innovations in prompting techniques. Additionally, Adept is an organization that developed ACT-1, a transformer model for actions, contributing significantly to the field of artificial intelligence (AI).</data>
      <data key="d2">27d8fe15ab6f9e3d91fd5858fbeba7ea,6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </node>
    <node id="&quot;KARPAS ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Karpas et al. are researchers exploring how to allow LLMs to make use of external systems, driven by innovations in prompting techniques."</data>
      <data key="d2">27d8fe15ab6f9e3d91fd5858fbeba7ea</data>
    </node>
    <node id="&quot;LLM OUTPUTS EVALUATION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"LLM Outputs Evaluation involves using complex algorithms to judge the validity of outputs from large language models, enhancing their reliability."</data>
      <data key="d2">27d8fe15ab6f9e3d91fd5858fbeba7ea</data>
    </node>
    <node id="&quot;EXTERNAL TOOLS ACCESS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"External Tools Access involves integrating external tools with LLMs to overcome their limitations in areas like mathematical computations, reasoning, and factuality."</data>
      <data key="d2">27d8fe15ab6f9e3d91fd5858fbeba7ea</data>
    </node>
    <node id="&quot;AGENT LLMS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Agent LLMs are a type of LLM that can interact with external systems to perform tasks. They may involve memory, planning, and actions, and can use tools like calculators or code interpreters."</data>
      <data key="d2">cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;MODULAR REASONING, KNOWLEDGE, AND LANGUAGE (MRKL) SYSTEM&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"MRKL System is a formulation of an agent that uses an LLM router to access multiple tools, such as weather or date information, to generate a final response."</data>
      <data key="d2">cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;SELF-CORRECTING WITH TOOL-INTERACTIVE CRITIQUING (CRITIC)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"CRITIC is a system where an LLM first generates a response to a prompt, then criticizes this response for possible errors, and finally uses tools to verify or amend parts of the response."</data>
      <data key="d2">cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;PROGRAM-AIDED LANGUAGE MODEL (PAL)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"PAL is a system that translates a problem directly into code, which is then sent to a Python interpreter to generate an answer."</data>
      <data key="d2">cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;TOOL-INTEGRATED REASONING AGENT (TORA)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">The TOOL-INTEGRATED REASONING AGENT (ToRA) is a reasoning agent that interleaves code and reasoning steps to solve problems, rather than relying on a single code generation step. ToRA is similar to PAL but distinguishes itself by continuously interleaving code and reasoning steps for as long as necessary to arrive at a solution.</data>
      <data key="d2">cbd06bb38a855be4a07883f499014eaa,eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;REACT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">REACT, which stands for Synergizing Reasoning and Acting in Language Models, is a type of Observation-Based Agent that combines reasoning and acting based on observations. This method, explored by Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao in 2022, focuses on integrating reasoning and action within language models to enhance their performance and capabilities.</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;REFLEXION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">REFLEXION is a type of Observation-Based Agent that builds on the ReAct framework by adding a layer of introspection. It focuses on reflecting on past actions to improve future performance. Reflexion evaluates the success or failure of its actions, generates a reflection on these actions, and incorporates this reflection into its prompt as working memory. This process allows Reflexion to continuously enhance its decision-making and operational effectiveness.</data>
      <data key="d2">cbd06bb38a855be4a07883f499014eaa,eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;LIFELONG LEARNING AGENTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Lifelong Learning Agents" are agents that continuously learn and adapt over time. They are designed to acquire new skills as they navigate environments like Minecraft and can be applied to real-world tasks requiring lifelong learning.</data>
      <data key="d2">cbd06bb38a855be4a07883f499014eaa,eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;VOYAGER&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Voyager is a lifelong learning agent that continuously learns from its environment. It proposes tasks for itself, generates code to execute actions, and saves these actions as part of a long-term memory system. As an open-ended embodied agent, Voyager utilizes large language models, highlighting its significance in the subdomain of artificial intelligence and robotics.</data>
      <data key="d2">153eeb5a63e650f2cd12f700ffe3e71f,cbd06bb38a855be4a07883f499014eaa,eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;GITM&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"GITM is a type of Lifelong Learning Agent that focuses on general intelligence through continuous learning."</data>
      <data key="d2">cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;RETRIEVAL AUGMENTED GENERATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Retrieval Augmented Generation is a technique where agents use retrieval mechanisms to augment their generative capabilities."</data>
      <data key="d2">cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;IRCOT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"IRCoT is a type of Retrieval Augmented Generation technique."</data>
      <data key="d2">cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;VERIFY-AND-EDIT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Verify-and-Edit" is a knowledge-enhanced chain-of-thought framework designed to improve the accuracy and reliability of language models. It enhances self-consistency by generating multiple chains-of-thought, selecting some to be edited, and retrieving relevant external information to augment them. This approach, explored by Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing in 2023, falls under the category of Retrieval Augmented Generation techniques, where the agent verifies and edits its responses.</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d,c7285f7847ef45ed85779d7966753855,cbd06bb38a855be4a07883f499014eaa,eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;ITERATIVE RETRIEVAL AUGMENTATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Iterative Retrieval Augmentation is a technique where the agent iteratively retrieves information to improve its responses."</data>
      <data key="d2">cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;OPENAI ASSISTANTS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">OpenAI Assistants are examples of agents developed by OpenAI that can perform various tasks using external tools.</data>
      <data key="d2">4d9e8d703c2da8e4775c428e83e87fc9,cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;LANGCHAIN AGENTS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"LangChain Agents are examples of agents developed by LangChain that can perform various tasks using external tools."</data>
      <data key="d2">cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;LLAMAINDEX AGENTS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"LlamaIndex Agents are examples of agents developed by LlamaIndex that can perform various tasks using external tools."</data>
      <data key="d2">cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;CALC(4,939*.39)&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"CALC(4,939*.39) is an example of an LLM output that can be used in a calculator to obtain a final answer."</data>
      <data key="d2">cbd06bb38a855be4a07883f499014eaa</data>
    </node>
    <node id="&quot;TASKWEAVER&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">TaskWeaver is a reasoning agent that transforms user requests into code and can also make use of user-defined plugins. It is similar to PAL. TaskWeaver is a code-first agent framework designed to facilitate the development and deployment of intelligent agents. It is discussed in the context of a 2023 paper by Aoting Qin, Chao Du, Yong Xu, Qingwei Lin, S. Rajmohan, and Dongmei Zhang.</data>
      <data key="d2">42397dc5d60f0a1d799e06290ea52864,eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;REASONING AND ACTING (REACT)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"ReAct is an agent that generates a thought, takes an action, and receives an observation, repeating this process to solve problems. It has a memory of past thoughts, actions, and observations."</data>
      <data key="d2">eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;GHOST IN THE MINECRAFT (GITM)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"GITM is a lifelong learning agent that starts with an arbitrary goal, breaks it down into subgoals, and iteratively plans and executes actions using structured text. It uses an external knowledge base and memory of past experiences."</data>
      <data key="d2">eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;EVALUATION TECHNIQUES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Evaluation Techniques include various methods such as Chain-Of-Thought, In-Context Learning, Model-Gen. Guidelines, Role-Based Evaluation, Binary Score, Likert Scale, Linear Scale, Styling, Prompting Frameworks, Batch Prompting, and Pairwise Evaluation."</data>
      <data key="d2">eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;LLM-EVAL&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">LLM-EVAL is a prompting framework designed for evaluation techniques, particularly in the context of large language models. It is a simple evaluation framework that utilizes a single prompt containing a schema of variables to evaluate, instructions for scoring, and the content to be evaluated. Additionally, LLM-EVAL is associated with the Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models, a method aimed at assessing the performance of language models in open-domain conversations.</data>
      <data key="d2">630ee831daa753234a258274d318509e,a4eb2fbdea1494d271ebc61219d17020,eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;G-EVAL&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">G-EVAL is a prompting framework used for evaluation techniques. It is similar to LLM-EVAL but distinguishes itself by including AutoCoT steps in the prompt. These steps are generated according to specific evaluation instructions and are inserted into the final prompt, enhancing the evaluation process.</data>
      <data key="d2">a4eb2fbdea1494d271ebc61219d17020,eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;CHATEVAL&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">ChatEval is a prompting framework designed for evaluation techniques. It employs a multi-agent debate framework, where each agent is assigned a distinct role. The primary aim of ChatEval is to enhance LLM-based evaluators through this structured multi-agent debate approach.</data>
      <data key="d2">5ce886e06455eadec4bcfe91e36b666d,a4eb2fbdea1494d271ebc61219d17020,eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;OTHER METHODOLOGIES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Other Methodologies include Batch Prompting and Pairwise Evaluation as part of evaluation techniques."</data>
      <data key="d2">eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;PAL&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">eed969adf8c7eb4a89355c851663c87a</data>
    </node>
    <node id="&quot;INTERLEAVED RETRIEVAL GUIDED BY CHAIN-OF-THOUGHT (IRCOT)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"IRCoT is a technique for multi-hop question answering that interleaves Chain-of-Thought (CoT) and retrieval. It leverages CoT to guide which documents to retrieve and uses retrieval to help plan the reasoning steps of CoT."</data>
      <data key="d2">c28998cdf87522d883979f9c6405f535</data>
    </node>
    <node id="&quot;FORWARD-LOOKING ACTIVE RETRIEVAL AUGMENTED GENERATION (FLARE)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"FLARE is an iterative retrieval augmentation technique that performs retrieval multiple times during long-form generation. It involves generating a temporary sentence to serve as a content plan, retrieving external knowledge using the temporary sentence as a query, and injecting the retrieved knowledge into the temporary sentence to create the next output sentence."</data>
      <data key="d2">c28998cdf87522d883979f9c6405f535</data>
    </node>
    <node id="&quot;IMITATE, RETRIEVE, PARAPHRASE (IRP)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"IRP is an iterative retrieval augmentation technique similar to FLARE. It performs retrieval multiple times during long-form generation, generating temporary sentences to serve as content plans, retrieving external knowledge, and injecting the retrieved knowledge into the temporary sentences."</data>
      <data key="d2">c28998cdf87522d883979f9c6405f535</data>
    </node>
    <node id="&quot;ROLE-BASED EVALUATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Role-based Evaluation is a technique for improving and diversifying evaluations by creating prompts with the same instructions but different roles. It can be used in a multiagent setting where LLMs debate the validity of the text to be evaluated."</data>
      <data key="d2">c28998cdf87522d883979f9c6405f535</data>
    </node>
    <node id="&quot;MODEL-GENERATED GUIDELINES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Model-Generated Guidelines involve prompting an LLM to generate guidelines for evaluation, reducing the problem of insufficient prompting from ill-defined scoring guidelines and output spaces. This technique can result in more consistent and aligned evaluations."</data>
      <data key="d2">c28998cdf87522d883979f9c6405f535</data>
    </node>
    <node id="&quot;AUTOCALIBRATE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"AUTOCALIBRATE is a technique that derives scoring criteria based on expert human annotations and uses a refined subset of model-generated criteria as part of the evaluation prompt."</data>
      <data key="d2">c28998cdf87522d883979f9c6405f535</data>
    </node>
    <node id="&quot;BATCH PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Batch Prompting is a methodology for improving compute and cost efficiency by evaluating multiple instances at once or the same instance under different criteria or roles."</data>
      <data key="d2">a4eb2fbdea1494d271ebc61219d17020</data>
    </node>
    <node id="&quot;PAIRWISE EVALUATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Pairwise Evaluation is a methodology where the quality of two texts is directly compared, but it may lead to suboptimal results. It is more effective to generate a score for individual summaries."</data>
      <data key="d2">a4eb2fbdea1494d271ebc61219d17020</data>
    </node>
    <node id="&quot;EVALUATION PERFORMANCE&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Evaluation Performance refers to the effectiveness and accuracy of the evaluation process, which can be influenced by the output format of the LLM."</data>
      <data key="d2">a4eb2fbdea1494d271ebc61219d17020</data>
    </node>
    <node id="&quot;PROMPT HACKING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Hacking refers to a class of attacks that manipulate the prompt to exploit Generative AI (GenAI) models. It includes techniques like prompt injection and jailbreaking, which can lead to privacy breaches, offensive content generation, and deceptive messaging."</data>
      <data key="d2">2dba3160cd0e0ee3943dce308cb9940e</data>
    </node>
    <node id="&quot;PROMPT INJECTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Injection" is a subdomain of prompt engineering that deals with techniques to manipulate or exploit prompts to achieve specific outcomes in large language models. It involves a type of prompt hacking where user input overrides the original developer instructions in the prompt, causing the GenAI model to follow potentially malicious instructions. Additionally, the subdomain includes datasets that may contain papers with offensive content such as racism and sexism.</data>
      <data key="d2">29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,c605e4f0158f18be68214a39b9b54154</data>
    </node>
    <node id="&quot;JAILBREAKING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Jailbreaking" is a subdomain of prompt engineering that involves bypassing restrictions or limitations imposed on large language models to unlock additional functionalities. It is a type of prompt hacking where a GenAI model is manipulated to perform unintended actions through adversarial prompts, effectively bypassing its intended constraints. Jailbreaking refers to techniques used to circumvent the restrictions and safety measures in language models, allowing them to generate unrestricted content.</data>
      <data key="d2">2dba3160cd0e0ee3943dce308cb9940e,42d8c3ad092ec18e28ff718709b0b472,c605e4f0158f18be68214a39b9b54154</data>
    </node>
    <node id="&quot;TRAINING DATA RECONSTRUCTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Training Data Reconstruction is a risk associated with prompt hacking where attackers extract training data from GenAI models by manipulating prompts."</data>
      <data key="d2">2dba3160cd0e0ee3943dce308cb9940e</data>
    </node>
    <node id="&quot;PROMPT LEAKING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Leaking" is a risk associated with prompt hacking where attackers extract the prompt template from an application, potentially exposing intellectual property. It also refers to the unintended exposure of sensitive information through prompts used in large language models (LLMs).</data>
      <data key="d2">2dba3160cd0e0ee3943dce308cb9940e,4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </node>
    <node id="&quot;CODE GENERATION CONCERNS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Code Generation Concerns" involve issues related to the accuracy, security, and reliability of code generated by LLMs. These concerns encompass the risks associated with using LLMs to generate code, including vulnerabilities, package hallucinations, and bugs.</data>
      <data key="d2">2dba3160cd0e0ee3943dce308cb9940e,4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </node>
    <node id="&quot;ALIGNMENT CONCERNS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Alignment Concerns in prompting refer to ensuring that the outputs of LLMs align with the intended goals and ethical standards, avoiding harmful or unintended consequences."</data>
      <data key="d2">2dba3160cd0e0ee3943dce308cb9940e</data>
    </node>
    <node id="&quot;PROMPT-BASED DEFENSE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt-based Defense" refers to measures taken to protect against prompt hacking, including the use of guardrails and detectors to prevent adversarial prompts. This approach involves including specific instructions in prompts to avoid prompt injection, though it is not fully secure.</data>
      <data key="d2">2dba3160cd0e0ee3943dce308cb9940e,4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </node>
    <node id="&quot;GUARDRAILS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"GUARDRAILS" are a type of prompt-based defense designed to prevent GenAI models from following malicious or unintended instructions. They consist of rules and frameworks that guide GenAI outputs and prevent malicious activities.</data>
      <data key="d2">2dba3160cd0e0ee3943dce308cb9940e,4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </node>
    <node id="&quot;DETECTORS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Detectors are a type of prompt-based defense used to identify and mitigate adversarial prompts that could lead to prompt hacking. These tools are designed to detect malicious inputs and prevent prompt hacking, often built using fine-tuned models.</data>
      <data key="d2">2dba3160cd0e0ee3943dce308cb9940e,4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </node>
    <node id="&quot;PACKAGE HALLUCINATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Package Hallucination occurs when LLM-generated code attempts to import non-existent packages, potentially leading to security risks."</data>
      <data key="d2">4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </node>
    <node id="&quot;BUGS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Bugs refer to errors and security vulnerabilities that occur more frequently in LLM-generated code."</data>
      <data key="d2">4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </node>
    <node id="&quot;CUSTOMER SERVICE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Customer Service involves the use of chatbots and the risks associated with prompt injection attacks that can lead to brand embarrassment and legal issues."</data>
      <data key="d2">4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </node>
    <node id="&quot;SECURITY &amp; PROMPTING&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Security &amp; Prompting aims to address and mitigate security risks associated with LLMs through various techniques and tools."</data>
      <data key="d2">4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </node>
    <node id="&quot;PROMPT WORDING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Wording involves altering the wording of a prompt, such as adding extra spaces or changing capitalization, which can significantly impact the performance of LLMs."</data>
      <data key="d2">84da286ab749b0f025821313fe535d70</data>
    </node>
    <node id="&quot;TASK FORMAT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Task Format describes different ways to prompt an LLM to execute the same task, where minor changes can alter the accuracy of the model."</data>
      <data key="d2">84da286ab749b0f025821313fe535d70</data>
    </node>
    <node id="&quot;PROMPT DRIFT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Drift occurs when the model behind an API changes over time, causing the same prompt to produce different results on the updated model."</data>
      <data key="d2">84da286ab749b0f025821313fe535d70</data>
    </node>
    <node id="&quot;OVERCONFIDENCE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Overconfidence refers to the tendency of LLMs to be overly confident in their answers, which can lead to user overreliance on model outputs."</data>
      <data key="d2">84da286ab749b0f025821313fe535d70</data>
    </node>
    <node id="&quot;CALIBRATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Calibration involves techniques to provide a confidence score that represents the confidence of the model, aiming to mitigate overconfidence."</data>
      <data key="d2">84da286ab749b0f025821313fe535d70</data>
    </node>
    <node id="&quot;VERBALIZED SCORE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Verbalized Score is a calibration technique that generates a confidence score, but its efficacy is debated."</data>
      <data key="d2">84da286ab749b0f025821313fe535d70</data>
    </node>
    <node id="&quot;SYCOPHANCY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Sycophancy refers to the tendency of LLMs to express agreement with the user, even when it contradicts the model's initial output."</data>
      <data key="d2">84da286ab749b0f025821313fe535d70</data>
    </node>
    <node id="&quot;FIGURE 5.2: PROMPT-BASED ALIGNMENT ORGANIZATION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Figure 5.2: Prompt-based Alignment Organization is an organization that deals with prompt alignment problems and potential solutions."</data>
      <data key="d2">84da286ab749b0f025821313fe535d70</data>
    </node>
    <node id="&quot;SHARMA ET AL. (2023)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Sharma et al. (2023) is a research group that studied the influence of user opinions on LLM outputs, finding that including personal opinions in prompts can sway the model's responses."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;WEI ET AL. (2023B)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Wei et al. (2023b) is a research group that found similar results to Sharma et al. (2023) regarding opinion-eliciting and false user presumptions, noting heightened sycophancy in larger and instruction-tuned models."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;MEHRABI ET AL. (2021)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Mehrabi et al. (2021) is a research group that emphasized the importance of fairness in LLM outputs to avoid biases, stereotypes, and cultural harms."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;SI ET AL. (2023B)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Si et al. (2023b) is a research group that proposed Vanilla Prompting, a technique that instructs LLMs to be unbiased, and also discussed selecting balanced demonstrations to reduce biases."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;GANGULI ET AL. (2023)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Ganguli et al. (2023) is a research group that referred to Vanilla Prompting as moral self-correction."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;MA ET AL. (2023)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Ma et al. (2023) is a research group that worked on obtaining demonstrations optimized over fairness metrics to reduce biases in LLM outputs."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;YAO ET AL. (2023A)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Yao et al. (2023a) is a research group that worked on injecting cultural awareness into prompts to help LLMs with cultural adaptation."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;PESKOV ET AL. (2021)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Peskov et al. (2021) is a research group that contributed to the development of techniques for cultural adaptation in LLMs."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;YU ET AL. (2023)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Yu et al. (2023) is a research group that developed AttrPrompt, a technique designed to avoid producing text biased towards certain attributes when generating synthetic data."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;MIN ET AL. (2020)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Min et al. (2020) is a research group that studied the challenges posed by ambiguous questions for existing models."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;KEYVAN AND HUANG (2022)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Keyvan and Huang (2022) is a research group that identified the challenges ambiguous questions pose to existing models."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;GAO ET AL. (2023A)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Gao et al. (2023a) is a research group that developed ambiguous demonstrations to increase ICL performance by including examples with ambiguous label sets in prompts."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;RAO AND DAUM&#201; III (2019)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Rao and Daum&#233; III (2019) is a research group that developed the technique of question clarification, allowing LLMs to identify ambiguous questions and generate clarifying questions."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;MU ET AL. (2023)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Mu et al. (2023) is a research group that applied question clarification techniques for code generation."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;ZHANG AND CHOI (2023)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Zhang and Choi (2023) is a research group that equipped LLMs with a pipeline for resolving ambiguity in general tasks, including generating initial answers, classifying the need for clarification, and generating final answers."</data>
      <data key="d2">314fa72b9f7876258bd98d75a005cdb7</data>
    </node>
    <node id="&quot;MMLU&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">MMLU (Massive Multitask Language Understanding) is a widely used benchmark for evaluating the performance of language models across various categories. It serves as an organization or framework that includes subsets such as Human_Sexuality and is specifically designed for assessing large language models (LLMs).</data>
      <data key="d2">e8bf483fffcc91b1512c5796d0d4045a,f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </node>
    <node id="&quot;GPT-3.5-TURBO&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"GPT-3.5-turbo is a version of OpenAI's language model used for running experiments in the study."</data>
      <data key="d2">f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </node>
    <node id="&quot;ZERO-SHOT&quot;">
      <data key="d0">"TECHNIQUE"</data>
      <data key="d1">"ZERO-SHOT" is a prompting technique where questions are run directly through the model without any additional context or examples. It is a subdomain of prompt engineering characterized by the absence of prior examples. Notably, "ZERO-SHOT" has been observed to perform better than "Zero-Shot-CoT."</data>
      <data key="d2">590db3ee59b442c908a9b425a9be2477,f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </node>
    <node id="&quot;FEW-SHOT&quot;">
      <data key="d0">"TECHNIQUE"</data>
      <data key="d1">"FEW-SHOT" is a prompting technique used in prompt engineering to improve the performance of large language models (LLMs) by including a few examples, known as exemplars, to guide the model's responses.</data>
      <data key="d2">e8bf483fffcc91b1512c5796d0d4045a,f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </node>
    <node id="&quot;FEW-SHOT-COT&quot;">
      <data key="d0">"TECHNIQUE"</data>
      <data key="d1">"Few-Shot-CoT (Chain-of-Thought) is a prompting technique that combines few-shot examples with thought inducers to generate reasoning steps."</data>
      <data key="d2">f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </node>
    <node id="&quot;FORMAL BENCHMARK EVALUATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Formal Benchmark Evaluation is the process of systematically comparing different prompting techniques using a standardized set of questions and metrics."</data>
      <data key="d2">f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </node>
    <node id="&quot;DETECTION OF CRISIS-LEVEL SUICIDE RISK&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to detect signals in text that are predictive of crisis-level suicide risk, which could significantly impact mental health assessments."</data>
      <data key="d2">590db3ee59b442c908a9b425a9be2477</data>
    </node>
    <node id="&quot;SUICIDE CRISIS SYNDROME (SCS)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Suicide Crisis Syndrome (SCS) is a diagnostic approach that requires personal clinical interactions or self-report questionnaires to assess suicidal crisis."</data>
      <data key="d2">590db3ee59b442c908a9b425a9be2477</data>
    </node>
    <node id="&quot;ACUTE SUICIDAL AFFECTIVE DISTURBANCE (ASAD)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Acute Suicidal Affective Disturbance (ASAD) is a diagnostic approach that requires personal clinical interactions or self-report questionnaires to assess suicidal crisis. This diagnostic entity was discussed in a 2019 study by Megan L. Rogers, Carol Chu, and Thomas Joiner, which focused on its necessity, validity, and clinical utility.</data>
      <data key="d2">42397dc5d60f0a1d799e06290ea52864,590db3ee59b442c908a9b425a9be2477</data>
    </node>
    <node id="&quot;MENTAL HEALTH ECOSYSTEM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">The Mental Health Ecosystem refers to the network of mental health resources, professionals, practices, and systems working collaboratively to address mental health issues, including suicide prevention. This ecosystem aims to improve mental health by accurately flagging indicators of suicidal crisis in individuals' language, thereby providing timely and effective interventions.</data>
      <data key="d2">590db3ee59b442c908a9b425a9be2477,d27160d0dde304425ccc51df673321b1</data>
    </node>
    <node id="&quot;SUICIDE PREVENTION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal of Suicide Prevention is to reduce the incidence of suicide through various means, including clinical interactions, self-report questionnaires, and language-based assessments."</data>
      <data key="d2">590db3ee59b442c908a9b425a9be2477</data>
    </node>
    <node id="&quot;NATIONAL CENTER FOR HEALTH WORKFORCE ANALYSIS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">The National Center for Health Workforce Analysis is an organization that provides comprehensive data and analysis on the health workforce. This includes detailed information on behavioral health and health workforce shortages, such as mental health provider shortage areas.</data>
      <data key="d2">4d9e8d703c2da8e4775c428e83e87fc9,590db3ee59b442c908a9b425a9be2477</data>
    </node>
    <node id="&quot;CDC&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">The CDC (Centers for Disease Control and Prevention) is a national public health institute in the United States. It provides data and statistics on various health issues, including suicide rates.</data>
      <data key="d2">520bb3073a4c18baf121407c691ffe87,590db3ee59b442c908a9b425a9be2477,5ce886e06455eadec4bcfe91e36b666d</data>
    </node>
    <node id="&quot;SUICIDE CRISIS SYNDROME&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Suicide Crisis Syndrome" is a critical mental health condition characterized by severe psychological distress, including feelings of frantic hopelessness and entrapment. Individuals experiencing this syndrome feel a desire to escape from an unbearable situation but perceive all escape routes as blocked. This condition is marked by acute suicidal ideation and behavior, necessitating systematic review and intervention.</data>
      <data key="d2">3fd8f6dcbbf1eecd6efb01ea12538679,ba0d350eede3e5a4dfd1b9b0693b9b94,d27160d0dde304425ccc51df673321b1</data>
    </node>
    <node id="&quot;UNIVERSITY OF MARYLAND REDDIT SUICIDALITY DATASET&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The University of Maryland Reddit Suicidality Dataset is a collection of posts from the r/SuicideWatch subreddit, used for research on suicidal thoughts and behaviors."</data>
      <data key="d2">d27160d0dde304425ccc51df673321b1</data>
    </node>
    <node id="&quot;R/SUICIDEWATCH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"r/SuicideWatch is a subreddit that offers peer support for individuals struggling with suicidal thoughts."</data>
      <data key="d2">d27160d0dde304425ccc51df673321b1</data>
    </node>
    <node id="&quot;ENTRAPMENT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Entrapment" is a psychological condition where an individual feels trapped or stuck, characterized by feelings of no exit, hopelessness, fear, helplessness, and a sense of being trapped. It is a key factor in Suicide Crisis Syndrome, described as a desire to escape from an unbearable situation with the perception that all escape routes are blocked. The AI project aims to identify and label instances of entrapment in text data, specifically focusing on posts where individuals explicitly express that they feel trapped.</data>
      <data key="d2">18e3009014a13d95897da5ec358ca2e1,93ab5f14aa5b97d57952be648f337b10,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,d27160d0dde304425ccc51df673321b1</data>
    </node>
    <node id="&quot;EXPERT PROMPT ENGINEER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"An expert prompt engineer, who authored a widely used guide on prompting, was tasked with using an LLM to identify entrapment in posts."</data>
      <data key="d2">d27160d0dde304425ccc51df673321b1</data>
    </node>
    <node id="&quot;SELF-REPORT QUESTIONNAIRES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Self-Report Questionnaires are tools used in clinical settings to gather information from individuals about their mental health status."</data>
      <data key="d2">d27160d0dde304425ccc51df673321b1</data>
    </node>
    <node id="&quot;HUMAN_SEXUALITY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Human_Sexuality is a subset of MMLU, which is a sensitive domain where LLMs exhibit unpredictable and difficult to control behavior."</data>
      <data key="d2">e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;GPT-4-32K&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"GPT-4-32K is a specific model of LLM that was used to address issues in the prompt engineering process."</data>
      <data key="d2">e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;10-SHOT AUTODICOT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"10-Shot AutoDiCoT" is a prompting technique designed to enhance the performance of language models. This method involves using ten examples to guide the model's responses, thereby improving its accuracy and effectiveness. Specifically, "10-Shot AutoDiCoT" incorporates ten examples into the prompt, which aids in automatic dialogue context tracking. This technique has been shown to result in improved F1 scores, demonstrating its efficacy in refining language model outputs.</data>
      <data key="d2">4257f30018a4acf2e8ee95f21de8d7df,afacb1e7edc1e6be7b4b3776676a32e9,dd792fdfac5a64bb840e3680fe40eeb3,e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;1-SHOT AUTODICOT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"1-Shot AutoDiCoT is a prompting technique involving one example and automatic dialogue context tracking."</data>
      <data key="d2">e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;ZERO-SHOT + CONTEXT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Zero-Shot + Context" is a prompting technique that does not provide examples but includes context to guide the LLM. This technique involves providing context without any prior examples to the model.</data>
      <data key="d2">ba0d350eede3e5a4dfd1b9b0693b9b94,e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;CHAIN-OF-THOUGHT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Chain-of-Thought is a prompting technique that involves breaking down the reasoning process into a series of steps."</data>
      <data key="d2">e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;AUTOCOT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">AutoCoT is a prompting technique that automates the Chain-of-Thought process. This automatic chain of thought reasoning process is used to improve the performance of AI in identifying entrapment.</data>
      <data key="d2">bcb6ef7c52ce001fb19904d1aa92dfd2,e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;CONTRASTIVE COT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Contrastive CoT is a prompting technique designed to enhance the performance of large language models (LLMs). It operates by contrasting different chains of thought, which involves presenting the model with examples of both good and bad reasoning. This method helps the model to better distinguish between effective and ineffective reasoning patterns, ultimately improving its overall performance.</data>
      <data key="d2">18e3009014a13d95897da5ec358ca2e1,e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;MULTIPLE ANSWER EXTRACTION TECHNIQUES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Multiple Answer Extraction Techniques are methods used to extract answers from LLM outputs."</data>
      <data key="d2">e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;F1 SCORE&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">The F1 Score is a metric used to evaluate the performance of a language model, balancing precision and recall. It serves as a performance metric to assess the equilibrium between precision and recall in the outputs of large language models (LLMs).</data>
      <data key="d2">dd792fdfac5a64bb840e3680fe40eeb3,e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;RECALL&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Recall" is a metric used to measure the ability of the language model to identify all relevant instances in the data. It is a performance metric that measures the true positive rate or sensitivity of LLM outputs.</data>
      <data key="d2">dd792fdfac5a64bb840e3680fe40eeb3,e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;PRECISION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Precision" is a metric used to measure the accuracy of the language model in identifying relevant instances. It is a performance metric that measures the positive predictive value of LLM outputs.</data>
      <data key="d2">dd792fdfac5a64bb840e3680fe40eeb3,e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;TEMPERATURE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Temperature is a hyperparameter that controls the randomness of LLM outputs."</data>
      <data key="d2">e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;TOP-P&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Top-p is a hyperparameter that controls the cumulative probability of the most likely tokens in LLM outputs."</data>
      <data key="d2">e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;DEVELOPMENT SET&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Development Set is a dataset used to evaluate and refine the performance of LLMs during prompt engineering."</data>
      <data key="d2">e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;TRAINING SET&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Training Set is a dataset used to train LLMs and improve their performance."</data>
      <data key="d2">e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;EXEMPLARS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Exemplars are sample data points used in prompting techniques to guide LLMs."</data>
      <data key="d2">e8bf483fffcc91b1512c5796d0d4045a</data>
    </node>
    <node id="&quot;ONE-SHOT AUTODICOT + FULL CONTEXT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"One-Shot AutoDiCoT + Full Context is a prompting technique used in the development set to improve F1 scores by providing full context to the model."</data>
      <data key="d2">ba0d350eede3e5a4dfd1b9b0693b9b94</data>
    </node>
    <node id="&quot;10-SHOT + CONTEXT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"10-Shot + Context is a prompting technique that involves providing ten examples along with context to the model."</data>
      <data key="d2">ba0d350eede3e5a4dfd1b9b0693b9b94</data>
    </node>
    <node id="&quot;20-SHOT AUTODICOT + FULL WORDS + EXTRACTION PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"20-Shot AutoDiCoT + Full Words + Extraction Prompt" is a complex prompting technique that combines twenty examples, full words, and an extraction prompt to improve model performance. This technique involves crafting a prompt designed to extract answers from the language model's response. While this approach has been shown to improve accuracy, it has also been observed to decrease the F1 score.</data>
      <data key="d2">afacb1e7edc1e6be7b4b3776676a32e9,ba0d350eede3e5a4dfd1b9b0693b9b94</data>
    </node>
    <node id="&quot;AUTOMATIC DIRECTED COT (AUTODICOT)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"AutoDiCoT is an algorithm that automatically directs the Chain of Thought (CoT) process to reason in a particular way, combining automatic generation of CoTs with examples of bad reasoning."</data>
      <data key="d2">18e3009014a13d95897da5ec358ca2e1</data>
    </node>
    <node id="&quot;PROMPT ENGINEER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Prompt Engineer is responsible for developing and refining prompts to improve the model's performance."</data>
      <data key="d2">18e3009014a13d95897da5ec358ca2e1</data>
    </node>
    <node id="&quot;DEVELOPMENT ITEMS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Development Items refer to the pairs of questions and answers (qi, ai) used in the process of training and evaluating the model."</data>
      <data key="d2">18e3009014a13d95897da5ec358ca2e1</data>
    </node>
    <node id="&quot;PROMPT DEVELOPMENT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Development refers to the process of creating and refining prompts for language models to ensure they align with the actual goals and use cases."</data>
      <data key="d2">93ab5f14aa5b97d57952be648f337b10</data>
    </node>
    <node id="&quot;DOMAIN EXPERTS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Domain Experts are individuals with deep understanding of the real-world use case, whose regular engagement is crucial for effective prompt development."</data>
      <data key="d2">93ab5f14aa5b97d57952be648f337b10</data>
    </node>
    <node id="&quot;EMAIL&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">Email refers to the communication that provided richer background information about the goals of the labeling process, significantly impacting performance. Additionally, the inclusion of email content in the prompt has been shown to affect the performance of the language model.</data>
      <data key="d2">93ab5f14aa5b97d57952be648f337b10,dd792fdfac5a64bb840e3680fe40eeb3</data>
    </node>
    <node id="&quot;AUTODICOT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"AutoDiCoT is a subdomain related to prompt engineering, involving the creation of exemplars to improve the performance of language models."</data>
      <data key="d2">afacb1e7edc1e6be7b4b3776676a32e9</data>
    </node>
    <node id="&quot;20-SHOT AUTODICOT&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"20-Shot AutoDiCoT" is a prompting technique that involves using twenty examples to guide the language model's responses. In one event, an additional ten exemplars were labeled to create a 20-shot prompt, which led to worse results compared to the 10-shot prompt.</data>
      <data key="d2">4257f30018a4acf2e8ee95f21de8d7df,afacb1e7edc1e6be7b4b3776676a32e9</data>
    </node>
    <node id="&quot;20-SHOT AUTODICOT + FULL WORDS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"20-Shot AutoDiCoT + Full Words is an event where the prompt engineer included full words like 'Question', 'Reasoning', and 'Answer' in the prompt, but it did not succeed in improving performance."</data>
      <data key="d2">afacb1e7edc1e6be7b4b3776676a32e9</data>
    </node>
    <node id="&quot;10-SHOT AUTODICOT + EXTRACTION PROMPT&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"10-Shot AutoDiCoT + Extraction Prompt is an event where the extraction prompt was applied to the best performing 10-Shot AutoDiCoT prompt, but it did not improve results."</data>
      <data key="d2">afacb1e7edc1e6be7b4b3776676a32e9</data>
    </node>
    <node id="&quot;10-SHOT AUTODICOT WITHOUT EMAIL&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"10-Shot AutoDiCoT without Email is an event where the email was removed from the prompt, which hurt performance significantly."</data>
      <data key="d2">afacb1e7edc1e6be7b4b3776676a32e9</data>
    </node>
    <node id="&quot;ENSEMBLE + EXTRACTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Ensemble + Extraction is a method that combines multiple variations of an input and extracts the final answer, aiming to improve performance."</data>
      <data key="d2">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </node>
    <node id="&quot;10-SHOT AUTOCOT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"10-Shot AutoCoT is a variation of the 10-Shot AutoDiCoT technique, which includes three times the context without email duplication."</data>
      <data key="d2">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </node>
    <node id="&quot;ANONYMIZE EMAIL&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Anonymize Email is a technique where personal names in the email are replaced with random names to test its impact on performance."</data>
      <data key="d2">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </node>
    <node id="&quot;DSPY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">DSPY is a system for compiling declarative language model calls into self-improving pipelines. It was developed by Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts in 2023. DSPY is a framework that automatically optimizes language model prompts for a given target metric, utilizing synthetic LLM-generated demonstrations and randomly sampled training exemplars.</data>
      <data key="d2">dd792fdfac5a64bb840e3680fe40eeb3,eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;OPTIMIZATION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Optimization" in the context of prompt engineering refers to the process of improving prompts with respect to a scoring function, usually defined over a dataset. Additionally, optimization involves enhancing the performance of the language model by adjusting the prompts and techniques used.</data>
      <data key="d2">1a997c6aadeaeb3b5ad0a4c3ce835540,dd792fdfac5a64bb840e3680fe40eeb3</data>
    </node>
    <node id="&quot;PERFORMANCE DECREASE&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Performance Decrease refers to the reduction in the effectiveness of the language model as measured by F1, recall, and precision metrics."</data>
      <data key="d2">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </node>
    <node id="&quot;PERFORMANCE IMPROVEMENT&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Performance Improvement refers to the increase in the effectiveness of the language model as measured by F1, recall, and precision metrics."</data>
      <data key="d2">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </node>
    <node id="&quot;DSPY DEFAULT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"DSPy Default refers to a specific prompting technique used in the context of prompt engineering, which is evaluated for its performance on a test set."</data>
      <data key="d2">4257f30018a4acf2e8ee95f21de8d7df</data>
    </node>
    <node id="&quot;AUTOMATED PROMPT ENGINEERING&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Automated Prompt Engineering aims to explore the prompting space using automated methods to improve the performance of language models."</data>
      <data key="d2">4257f30018a4acf2e8ee95f21de8d7df</data>
    </node>
    <node id="&quot;HUMAN PROMPT ENGINEERING&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Human Prompt Engineering involves manual crafting and revision of prompts by experts to achieve desired behaviors from language models."</data>
      <data key="d2">4257f30018a4acf2e8ee95f21de8d7df</data>
    </node>
    <node id="&quot;TREE-OF-THOUGHT PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Tree-of-Thought Prompting is a technique that structures reasoning processes in a tree-like format to enhance the model's problem-solving capabilities."</data>
      <data key="d2">4257f30018a4acf2e8ee95f21de8d7df</data>
    </node>
    <node id="&quot;SELF-CONSISTENCY PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Self-Consistency Prompting is a technique that involves generating multiple reasoning paths and selecting the most consistent one to improve accuracy."</data>
      <data key="d2">4257f30018a4acf2e8ee95f21de8d7df</data>
    </node>
    <node id="&quot;FOUNDATION MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Foundation Models are large-scale pre-trained machine learning models that serve as the basis for various downstream tasks and applications. These models are trained on vast amounts of data and can be fine-tuned for specific tasks, making them highly versatile and valuable in a wide range of applications.</data>
      <data key="d2">4257f30018a4acf2e8ee95f21de8d7df,520bb3073a4c18baf121407c691ffe87</data>
    </node>
    <node id="&quot;INTERACTIVE CREATIVE APPLICATIONS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Interactive Creative Applications use prompting as a new paradigm for human interaction, focusing on user interface design to support user prompting."</data>
      <data key="d2">4257f30018a4acf2e8ee95f21de8d7df</data>
    </node>
    <node id="&quot;MEDICAL AND HEALTHCARE DOMAINS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Medical and Healthcare Domains refer to the specific fields where prompt engineering techniques are applied to improve medical and healthcare services. These domains focus on enhancing outcomes and addressing limitations within medical and healthcare applications through the strategic use of prompt engineering.</data>
      <data key="d2">4257f30018a4acf2e8ee95f21de8d7df,a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;SYSTEMATIC REVIEW&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">A systematic review is conducted to provide a comprehensive overview of prompt engineering techniques and their applications. It serves as a comprehensive survey and analysis of existing literature and techniques in a particular field, such as prompt engineering.</data>
      <data key="d2">4257f30018a4acf2e8ee95f21de8d7df,a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;USER INTERFACE DESIGN&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"User Interface Design focuses on creating interfaces that support user prompting, enhancing user interaction with systems."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;MEDICAL EDUCATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Medical Education involves the use of prompt engineering to enhance the learning and training of medical professionals."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;MENTAL HEALTH SPACE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Mental Health Space refers to the application of GPT-4-automated approaches to review and improve mental health services."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;VISUAL MODALITY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Visual Modality involves the use of prompt engineering and relevant models to enhance visual data processing and interpretation."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;MULTIMODAL PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Multimodal Prompting" focuses on the integration of multiple modes of input, particularly in the context of GPT-4V19. It involves using prompts that incorporate multiple types of data, such as text and images, to guide model responses.</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58,cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;TEXT-TO-IMAGE GENERATION MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Text-to-Image Generation Models are used by visual artists to create creative works through the adoption of prompt engineering techniques."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;GENAI&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"GenAI refers to Generative AI, which involves the use of AI models to generate content, reviewed through a topic modeling approach."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;FOUNDATION MODELS IN VISION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Foundation Models in Vision include various prompting techniques to enhance visual data processing and interpretation."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;SOFTWARE ENGINEERING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Software Engineering involves the application of prompt engineering techniques to improve software development processes."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;SOFTWARE TESTING WITH LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Software Testing with Large Language Models involves reviewing literature on the use of LLMs for software testing tasks."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;AUTOMATED PROGRAM REPAIR&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Automated Program Repair involves the use of ChatGPT prompting to improve software engineering tasks."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;COMPUTER SCIENCE EDUCATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Computer Science Education involves leveraging prompt engineering to enhance the teaching and learning of computer science."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;FAIRNESS OF LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Fairness of Large Language Models involves reviewing literature on ensuring fairness in the application of LLMs."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;HALLUCINATION OF LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Hallucination of Language Models involves studying the phenomenon where language models generate incorrect or nonsensical information."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;VERIFIABILITY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Verifiability involves ensuring that the outputs of language models can be verified for accuracy and reliability."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;REASONING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Reasoning involves the study of how language models can be used to perform logical and analytical tasks."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;AUGMENTATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Augmentation involves enhancing the capabilities of language models through additional data or techniques."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;LINGUISTIC PROPERTIES OF PROMPTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Linguistic Properties of Prompts involve studying the characteristics of prompts that affect the performance of language models."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;PRISMA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"PRISMA is a widely well-received standard for systematic literature reviews, used as a basis for the present work."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;GENERATIVE AI&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Generative AI is a novel technology that involves creating models capable of generating content, with challenges in linguistic communication."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;TAXONOMIC ORGANIZATION OF PROMPTING TECHNIQUES&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to provide a taxonomy and terminology that cover a large number of existing prompt engineering techniques and accommodate future methods."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;STANDARDIZATION OF TERMINOLOGY&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to standardize the terminology used in prompt engineering to ensure consistency and clarity in the field."</data>
      <data key="d2">a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;SAFETY AND SECURITY&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Safety and Security" are important considerations in prompt engineering, ensuring that techniques are used responsibly and do not lead to harmful outcomes. The goal is to address issues related to the safety and security of prompt engineering techniques.</data>
      <data key="d2">6e1dce58f4a3793b65d09171ea5bd3a6,a86e659dcd136358e7557eb5f98c1b58</data>
    </node>
    <node id="&quot;TAXONOMY&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal of the taxonomy is to provide a comprehensive classification and terminology for existing and future prompt engineering techniques."</data>
      <data key="d2">6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </node>
    <node id="&quot;MACHINE LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Machine Learning is a broader subdomain that encompasses prompt engineering, focusing on developing algorithms and models to learn from data."</data>
      <data key="d2">6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </node>
    <node id="&quot;ARTHURAI&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ArthurAI is an organization that developed Arthur Shield, a tool for enhancing AI model performance and security."</data>
      <data key="d2">6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </node>
    <node id="&quot;REBUFF AI&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Rebuff AI is an organization that created a self-hardening prompt injection detector to improve the security of AI systems."</data>
      <data key="d2">6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </node>
    <node id="&quot;ACT-1&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"ACT-1 is a transformer model developed by Adept for performing actions, showcasing advancements in AI capabilities."</data>
      <data key="d2">6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </node>
    <node id="&quot;ARTHUR SHIELD&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Arthur Shield is a tool developed by ArthurAI to enhance the performance and security of AI models."</data>
      <data key="d2">6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </node>
    <node id="&quot;BOOTSTRAPPING MULTILINGUAL SEMANTIC PARSERS USING LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A research paper presented at the 17th Conference of the European Chapter of the Association for Computational Linguistics, focusing on the use of large language models to bootstrap multilingual semantic parsers."</data>
      <data key="d2">b363fca358c69a9412b955c53352ea9a</data>
    </node>
    <node id="&quot;BENCHMARKING FOUNDATION MODELS WITH LANGUAGE-MODEL-AS-AN-EXAMINER&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A research paper presented at NeurIPS 2023 Datasets and Benchmarks, focusing on evaluating foundation models using a language model as an examiner."</data>
      <data key="d2">b363fca358c69a9412b955c53352ea9a</data>
    </node>
    <node id="&quot;EXPOSITORY TEXT GENERATION: IMITATE, RETRIEVE, PARAPHRASE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A research paper presented at the 2023 Conference on Empirical Methods in Natural Language Processing, discussing methods for generating expository text through imitation, retrieval, and paraphrasing."</data>
      <data key="d2">b363fca358c69a9412b955c53352ea9a</data>
    </node>
    <node id="&quot;A MULTITASK, MULTILINGUAL, MULTIMODAL EVALUATION OF CHATGPT ON REASONING, HALLUCINATION, AND INTERACTIVITY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A research paper presented at AACL, evaluating ChatGPT's performance on various tasks including reasoning, hallucination, and interactivity."</data>
      <data key="d2">b363fca358c69a9412b955c53352ea9a</data>
    </node>
    <node id="&quot;RETHINKING THE ROLE OF SCALE FOR IN-CONTEXT LEARNING: AN INTERPRETABILITY-BASED CASE STUDY AT 66 BILLION SCALE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A research paper presented at ACL, focusing on the role of scale in in-context learning and providing an interpretability-based case study."</data>
      <data key="d2">b363fca358c69a9412b955c53352ea9a</data>
    </node>
    <node id="&quot;TEXT2LIVE: TEXT-DRIVEN LAYERED IMAGE AND VIDEO EDITING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A research paper discussing a method for text-driven layered image and video editing."</data>
      <data key="d2">b363fca358c69a9412b955c53352ea9a</data>
    </node>
    <node id="&quot;GRAPH OF THOUGHTS: SOLVING ELABORATE PROBLEMS WITH LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A research paper presented at the AAAI Conference on Artificial Intelligence, discussing the use of large language models to solve complex problems."</data>
      <data key="d2">b363fca358c69a9412b955c53352ea9a</data>
    </node>
    <node id="&quot;17TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"An academic conference where the paper on bootstrapping multilingual semantic parsers using large language models was presented."</data>
      <data key="d2">b363fca358c69a9412b955c53352ea9a</data>
    </node>
    <node id="&quot;NEURIPS 2023 DATASETS AND BENCHMARKS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"An academic conference where the paper on benchmarking foundation models with language-model-as-an-examiner was presented."</data>
      <data key="d2">b363fca358c69a9412b955c53352ea9a</data>
    </node>
    <node id="&quot;2023 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"An academic conference where the paper on expository text generation was presented."</data>
      <data key="d2">b363fca358c69a9412b955c53352ea9a</data>
    </node>
    <node id="&quot;AACL&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"An academic conference where the paper on evaluating ChatGPT's performance on various tasks was presented."</data>
      <data key="d2">b363fca358c69a9412b955c53352ea9a</data>
    </node>
    <node id="&quot;AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"An academic conference where the paper on solving elaborate problems with large language models was presented."</data>
      <data key="d2">b363fca358c69a9412b955c53352ea9a</data>
    </node>
    <node id="&quot;OPPORTUNITIES AND RISKS OF FOUNDATION MODELS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to explore both the potential benefits and the inherent risks associated with the use of foundation models in various applications."</data>
      <data key="d2">520bb3073a4c18baf121407c691ffe87</data>
    </node>
    <node id="&quot;PRE-TRAINED LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Pre-trained Language Models are a type of foundation model specifically trained on large text corpora to understand and generate human language."</data>
      <data key="d2">520bb3073a4c18baf121407c691ffe87</data>
    </node>
    <node id="&quot;EVALUATING THE SUSCEPTIBILITY OF PRE-TRAINED LANGUAGE MODELS VIA HANDCRAFTED ADVERSARIAL EXAMPLES&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to assess how vulnerable pre-trained language models are to adversarial attacks designed to exploit their weaknesses."</data>
      <data key="d2">520bb3073a4c18baf121407c691ffe87</data>
    </node>
    <node id="&quot;OPENAI GYM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"OpenAI Gym is a toolkit developed by OpenAI for developing and comparing reinforcement learning algorithms."</data>
      <data key="d2">520bb3073a4c18baf121407c691ffe87</data>
    </node>
    <node id="&quot;VIDEO GENERATION MODELS AS WORLD SIMULATORS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to develop video generation models that can simulate real-world scenarios, potentially for use in training and testing AI systems."</data>
      <data key="d2">520bb3073a4c18baf121407c691ffe87</data>
    </node>
    <node id="&quot;LANGUAGE MODELS ARE FEW-SHOT LEARNERS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to demonstrate that language models can perform tasks with minimal training examples, showcasing their adaptability and efficiency."</data>
      <data key="d2">520bb3073a4c18baf121407c691ffe87</data>
    </node>
    <node id="&quot;SPARKS OF ARTIFICIAL GENERAL INTELLIGENCE: EARLY EXPERIMENTS WITH GPT-4&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"This event refers to the initial experiments conducted with GPT-4, aiming to explore its capabilities and potential as an artificial general intelligence."</data>
      <data key="d2">520bb3073a4c18baf121407c691ffe87</data>
    </node>
    <node id="&quot;EXTRACTING TRAINING DATA FROM LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"The goal is to investigate the possibility of retrieving original training data from large language models, raising concerns about data privacy and security."</data>
      <data key="d2">520bb3073a4c18baf121407c691ffe87</data>
    </node>
    <node id="&quot;SUICIDE DATA AND STATISTICS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">520bb3073a4c18baf121407c691ffe87</data>
    </node>
    <node id="&quot;THE TWELFTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">The Twelfth International Conference on Learning Representations is an academic conference where researchers present their work on machine learning and related fields, with a particular focus on learning representations. This event serves as a platform for scholars and experts to share their latest research and advancements in these areas.</data>
      <data key="d2">5ce886e06455eadec4bcfe91e36b666d,affd113b11a3fddad82e265af562d9a7</data>
    </node>
    <node id="&quot;IN-CONTEXT PROMPT EDITING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"In-context prompt editing is a technique used for conditional audio generation."</data>
      <data key="d2">5ce886e06455eadec4bcfe91e36b666d</data>
    </node>
    <node id="&quot;PROGRAM OF THOUGHTS PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Program of thoughts prompting is a method for disentangling computation from reasoning in numerical reasoning tasks."</data>
      <data key="d2">5ce886e06455eadec4bcfe91e36b666d</data>
    </node>
    <node id="&quot;CONTROL3D&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Control3d is a technique aimed at controllable text-to-3D generation."</data>
      <data key="d2">5ce886e06455eadec4bcfe91e36b666d</data>
    </node>
    <node id="&quot;REFERENCE-FREE TEXT QUALITY EVALUATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Reference-free text quality evaluation is an empirical study exploring the use of large language models for evaluating text quality without reference texts."</data>
      <data key="d2">5ce886e06455eadec4bcfe91e36b666d</data>
    </node>
    <node id="&quot;CONSISTENT VIDEO-TO-VIDEO TRANSFER&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Consistent video-to-video transfer is a technique using synthetic datasets to ensure consistency in video transformations."</data>
      <data key="d2">5ce886e06455eadec4bcfe91e36b666d</data>
    </node>
    <node id="&quot;CONTRASTIVE CHAIN-OF-THOUGHT PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Contrastive chain-of-thought prompting is a method used to enhance reasoning capabilities in large language models."</data>
      <data key="d2">5ce886e06455eadec4bcfe91e36b666d</data>
    </node>
    <node id="&quot;ENTANGLED REPRESENTATION LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Entangled representation learning is a bidirectional encoder-decoder model for learning complex representations."</data>
      <data key="d2">5ce886e06455eadec4bcfe91e36b666d</data>
    </node>
    <node id="&quot;CHAIN OF THOUGHT REASONING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Chain of thought reasoning is a method that advances the frontiers of reasoning in large language models."</data>
      <data key="d2">5ce886e06455eadec4bcfe91e36b666d</data>
    </node>
    <node id="&quot;SUICIDE PREVENTION CORE COMPETENCIES&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Suicide prevention core competencies are updated guidelines for mental health professionals to improve training, research, and practice in suicide prevention."</data>
      <data key="d2">5ce886e06455eadec4bcfe91e36b666d</data>
    </node>
    <node id="&quot;VQGAN-CLIP&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">VQGAN-CLIP is a technique for open-domain image generation and editing using natural language guidance. It involves a subdomain focused on generating and editing images based on natural language descriptions, allowing for intuitive and flexible image manipulation.</data>
      <data key="d2">5ce886e06455eadec4bcfe91e36b666d,e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;CLINICAL PSYCHOLOGY: SCIENCE AND PRACTICE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Clinical Psychology: Science and Practice is a subdomain focusing on the training, research, and practice implications for mental health professionals."</data>
      <data key="d2">e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;TEMPLATE-BASED NAMED ENTITY RECOGNITION USING BART&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Template-based Named Entity Recognition using BART is a subdomain that focuses on named entity recognition using a template-based approach with BART."</data>
      <data key="d2">e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;ZERO- AND FEW-SHOT LEARNING FOR HUMAN-AI INTERACTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Zero- and Few-Shot Learning for Human-AI Interaction is a subdomain exploring the opportunities and challenges of zero- and few-shot learning in creative applications of generative models."</data>
      <data key="d2">e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;DEEP ABDUCTIVE REASONING BENCHMARK&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Deep Abductive Reasoning Benchmark is a subdomain that presents a challenging benchmark for deep abductive reasoning, particularly difficult for GPT-3 and challenging for GPT-4."</data>
      <data key="d2">e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;REPHRASE AND RESPOND&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Rephrase and Respond is a subdomain that involves large language models asking better questions for themselves."</data>
      <data key="d2">e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;ACTIVE PROMPTING WITH CHAIN-OF-THOUGHT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Active Prompting with Chain-of-Thought is a subdomain that involves active prompting techniques for large language models."</data>
      <data key="d2">e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;COGVIEW&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"CogView is a subdomain mastering text-to-image generation via transformers."</data>
      <data key="d2">e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;ALPACAFARM&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"AlpacaFarm is a subdomain that provides a simulation framework for methods that learn from human feedback."</data>
      <data key="d2">e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;AGENT AI&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Agent AI is a subdomain surveying the horizons of multimodal interaction."</data>
      <data key="d2">e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;MULTILINGUAL LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Multilingual Language Models is a subdomain that explores whether multilingual language models think better in English."</data>
      <data key="d2">e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;HIERARCHICAL NEURAL STORY GENERATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Hierarchical Neural Story Generation is a subdomain focused on generating stories using hierarchical neural networks."</data>
      <data key="d2">e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;ONE-SHOT LEARNING OF OBJECT CATEGORIES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"One-Shot Learning of Object Categories" is a subdomain that involves learning object categories from a single example.</data>
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91,e5878afbfbf5194f1da3540eaa88fe65</data>
    </node>
    <node id="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">The Association for Computational Linguistics (ACL) is an organization dedicated to advancing the field of computational linguistics. It achieves this by publishing research findings, including studies on large language models, and organizing conferences where researchers can present their work. The ACL is responsible for hosting events such as the EACL 2024, and it publishes proceedings and research papers that may include data briefs from sources like the National Center for Health Statistics. Through these activities, the ACL plays a crucial role in disseminating knowledge and fostering collaboration within the computational linguistics community.</data>
      <data key="d2">153eeb5a63e650f2cd12f700ffe3e71f,4d9e8d703c2da8e4775c428e83e87fc9,83e773afec09e119882fe15dd253e724,9b0bcd8647bcff907e9bcf962a013b91,affd113b11a3fddad82e265af562d9a7,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855</data>
    </node>
    <node id="&quot;IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"IEEE Transactions on Pattern Analysis and Machine Intelligence is a journal that publishes research on pattern analysis and machine intelligence."</data>
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;METADREAMER&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Metadreamer is a subdomain focused on efficient text-to-3D creation by disentangling geometry and texture."</data>
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;THE EIGHTH CONFERENCE ON MACHINE TRANSLATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Eighth Conference on Machine Translation is an event where research on machine translation is presented and discussed."</data>
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;GPTSCORE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"GPTScore is a subdomain focused on evaluating language models as desired, as described in an arXiv preprint."</data>
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;POLYGLOT PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Polyglot Prompt is a subdomain focused on multilingual multitask prompt training, as presented at the 2022 Conference on Empirical Methods in Natural Language Processing."</data>
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;THE ELEVENTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Eleventh International Conference on Learning Representations is an event where research on learning representations is presented."</data>
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Advances in Neural Information Processing Systems is a conference and publication venue for research in neural information processing."</data>
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;THE 40TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The 40th International Conference on Machine Learning is an event where research on machine learning is presented."</data>
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;FORBES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">Forbes is a global media company that publishes news and articles on various topics, including business, technology, and current events. Among its diverse content, Forbes featured a piece on Air Canada and an AI chatbot case, written by Marisa Garcia.</data>
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91,affd113b11a3fddad82e265af562d9a7</data>
    </node>
    <node id="&quot;THE 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The 59th Annual Meeting of the Association for Computational Linguistics is an event where research in computational linguistics is presented."</data>
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The 11th International Joint Conference on Natural Language Processing is an event where research in natural language processing is presented."</data>
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;THE 2022 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The 2022 Conference on Empirical Methods in Natural Language Processing is an event where research in empirical methods for natural language processing is presented."</data>
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;EFFICIENT TEXT-TO-3D CREATION WITH DISENTANGLING GEOMETRY AND TEXTURE&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;LEVERAGING LARGE LANGUAGE MODELS FOR FINE-GRAINED MACHINE TRANSLATION EVALUATION&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;EVALUATE AS YOU DESIRE&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;MULTILINGUAL MULTITASK PROMPT TRAINING&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;COMPLEXITY-BASED PROMPTING FOR MULTI-STEP REASONING&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;MULTI-BANDIT BEST ARM IDENTIFICATION&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;PROGRAM-AIDED LANGUAGE MODELS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;WHAT AIR CANADA LOST IN &#8216;REMARKABLE&#8217; LYING AI CHATBOT CASE&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;MAKING PRE-TRAINED LANGUAGE MODELS BETTER FEW-SHOT LEARNERS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">9b0bcd8647bcff907e9bcf962a013b91</data>
    </node>
    <node id="&quot;40TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML'23)&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The 40th International Conference on Machine Learning (ICML'23) is an event where researchers present their work on machine learning."</data>
      <data key="d2">affd113b11a3fddad82e265af562d9a7</data>
    </node>
    <node id="&quot;JMLR.ORG&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"JMLR.org publishes proceedings from the 40th International Conference on Machine Learning (ICML'23).""JMLR.org is an organization that publishes the Journal of Machine Learning Research, which includes articles on machine learning."</data>
      <data key="d2">affd113b11a3fddad82e265af562d9a7</data>
      <data key="d3">"40TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML'23)"</data>
    </node>
    <node id="&quot;NCHS DATA BRIEF&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"NCHS Data Brief is a publication by the National Center for Health Statistics that provides data on various health-related topics."</data>
      <data key="d2">affd113b11a3fddad82e265af562d9a7</data>
    </node>
    <node id="&quot;COMMUNICATIONS OF THE ACM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Communications of the ACM is a publication by the Association for Computing Machinery that covers a wide range of computing topics."</data>
      <data key="d2">affd113b11a3fddad82e265af562d9a7</data>
    </node>
    <node id="&quot;INTERNATIONAL CONFERENCE ON MACHINE LEARNING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The International Conference on Machine Learning is an event where researchers present their work on machine learning."</data>
      <data key="d2">affd113b11a3fddad82e265af562d9a7</data>
    </node>
    <node id="&quot;PMLR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"PMLR (Proceedings of Machine Learning Research) is an organization that publishes proceedings from machine learning conferences.""PMLR publishes proceedings from the International Conference on Machine Learning."</data>
      <data key="d2">affd113b11a3fddad82e265af562d9a7</data>
      <data key="d3">"INTERNATIONAL CONFERENCE ON MACHINE LEARNING"</data>
    </node>
    <node id="&quot;DATA AND INFORMATION MANAGEMENT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Data and Information Management is a publication that covers topics related to data management and information systems."</data>
      <data key="d2">affd113b11a3fddad82e265af562d9a7</data>
    </node>
    <node id="&quot;FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: EACL 2024&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Findings of the Association for Computational Linguistics: EACL 2024 is an event where researchers present their findings in computational linguistics."</data>
      <data key="d2">affd113b11a3fddad82e265af562d9a7</data>
    </node>
    <node id="&quot;TECHRXIV&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">TechRxiv is a preprint server and online platform dedicated to research in technology and engineering. It serves as a repository where research papers and surveys, including those related to large language models, are published.</data>
      <data key="d2">83e773afec09e119882fe15dd253e724,affd113b11a3fddad82e265af562d9a7</data>
    </node>
    <node id="&quot;NEMO&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"NeMo and Guardrails AI are related subdomains focused on neural models and implementing safeguards for large language models.""NeMo is a subdomain related to the development and use of neural models for various applications."</data>
      <data key="d2">affd113b11a3fddad82e265af562d9a7</data>
      <data key="d3">"GUARDRAILS AI"</data>
    </node>
    <node id="&quot;GUARDRAILS AI&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Guardrails AI is a subdomain focused on implementing safeguards for large language models."</data>
      <data key="d2">affd113b11a3fddad82e265af562d9a7</data>
    </node>
    <node id="&quot;LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Large Language Models (LLMs) are a subdomain of artificial intelligence focused on creating models that can understand and generate human language. They have applications in various fields such as translation, vision and language tasks, and mental health care. Additionally, LLMs are capable of performing tasks such as zero-shot reasoning, as discussed in the 2022 paper by Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.</data>
      <data key="d2">42d8c3ad092ec18e28ff718709b0b472,5ce40e1d59b740ff17256ed5abebf613,83e773afec09e119882fe15dd253e724</data>
    </node>
    <node id="&quot;NEURIPS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"NeurIPS (Conference on Neural Information Processing Systems) is an annual event where researchers present their work on neural information processing, including advancements in large language models."</data>
      <data key="d2">83e773afec09e119882fe15dd253e724</data>
    </node>
    <node id="&quot;ICLR&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"ICLR (International Conference on Learning Representations) is an annual event focused on advancements in machine learning and representation learning, including research on large language models."</data>
      <data key="d2">83e773afec09e119882fe15dd253e724</data>
    </node>
    <node id="&quot;MACHINE TRANSLATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Machine Translation is a subdomain of computational linguistics that involves using large language models to automatically translate text from one language to another."</data>
      <data key="d2">83e773afec09e119882fe15dd253e724</data>
    </node>
    <node id="&quot;MENTAL HEALTH CARE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Mental Health Care is a subdomain where large language models are being explored for their potential to assist in mental health diagnosis, treatment, and support."</data>
      <data key="d2">83e773afec09e119882fe15dd253e724</data>
    </node>
    <node id="&quot;CROSS-LINGUAL-THOUGHT PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Cross-Lingual-Thought Prompting is a technique aimed at improving the multilingual capabilities of large language models by using prompts that encourage cross-lingual thinking."</data>
      <data key="d2">83e773afec09e119882fe15dd253e724</data>
    </node>
    <node id="&quot;ALIGNING PERCEPTION WITH LANGUAGE MODELS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Aligning Perception with Language Models aims to integrate perceptual data with language models to enhance their understanding and interaction capabilities."</data>
      <data key="d2">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </node>
    <node id="&quot;LLAMA GUARD&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Llama Guard is an LLM-based input-output safeguard designed to ensure safe and reliable human-AI conversations."</data>
      <data key="d2">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </node>
    <node id="&quot;EFFECTIVE DISAMBIGUATION FOR MACHINE TRANSLATION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Effective Disambiguation for Machine Translation focuses on improving the accuracy of translations by resolving ambiguities in the source text."</data>
      <data key="d2">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </node>
    <node id="&quot;ZERO-SHOT TEXT-GUIDED OBJECT GENERATION WITH DREAM FIELDS&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Zero-Shot Text-Guided Object Generation with Dream Fields is a technology that enables the creation of objects based on textual descriptions without prior examples."</data>
      <data key="d2">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </node>
    <node id="&quot;ZERO-SHOT FAITHFULNESS EVALUATION FOR TEXT SUMMARIZATION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Zero-Shot Faithfulness Evaluation for Text Summarization aims to assess the accuracy and reliability of summaries generated by language models without requiring annotated data."</data>
      <data key="d2">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </node>
    <node id="&quot;ACTIVE RETRIEVAL AUGMENTED GENERATION&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Active Retrieval Augmented Generation is a method that enhances language model outputs by incorporating relevant information retrieved from external sources."</data>
      <data key="d2">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </node>
    <node id="&quot;IS CHATGPT A GOOD TRANSLATOR?&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Is ChatGPT a Good Translator? is a study evaluating the translation capabilities of ChatGPT, particularly with GPT-4 as the engine."</data>
      <data key="d2">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </node>
    <node id="&quot;TAB-COT: ZERO-SHOT TABULAR CHAIN OF THOUGHT&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Tab-CoT: Zero-Shot Tabular Chain of Thought is a method for reasoning over tabular data using language models without requiring prior training on similar tasks."</data>
      <data key="d2">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </node>
    <node id="&quot;LANGUAGE MODELS (MOSTLY) KNOW WHAT THEY KNOW&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Language Models (Mostly) Know What They Know is a study exploring the self-awareness and knowledge boundaries of language models."</data>
      <data key="d2">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </node>
    <node id="&quot;MRKL SYSTEMS&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"MRKL Systems is a modular, neuro-symbolic architecture that combines large language models, external knowledge sources, and discrete reasoning."</data>
      <data key="d2">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </node>
    <node id="&quot;SYSTEMATIC LITERATURE REVIEWS IN SOFTWARE ENGINEERING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"SYSTEMATIC LITERATURE REVIEWS IN SOFTWARE ENGINEERING" are guidelines for conducting comprehensive reviews of existing research in the field of software engineering. These guidelines, as outlined by Staffs Keele et al. in 2007, provide a structured approach to performing systematic literature reviews, ensuring thoroughness and reliability in the synthesis of research findings.</data>
      <data key="d2">7e3b559c2a22f7f23f4eecc37ed7b8e4,eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"CTRL is a conditional transformer language model designed for generating text that adheres to specific control codes or conditions."</data>
      <data key="d2">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </node>
    <node id="&quot;KL SYSTEMS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"KL Systems is a modular, neuro-symbolic architecture that combines large language models, external knowledge sources, and discrete reasoning."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;CTRL&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"CTRL is a conditional transformer language model for controllable generation, developed by Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher in 2019."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;AMBIGUOUS QUERIES IN CONVERSATIONAL SEARCH&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A survey of techniques, approaches, tools, and challenges for approaching ambiguous queries in conversational search, conducted by Kimiya Keyvan and Jimmy Xiangji Huang in 2022."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;DEMONSTRATION ENSEMBLING FOR IN-CONTEXT LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A study exploring demonstration ensembling for in-context learning, conducted by Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang in 2023."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;VISION TRANSFORMERS IN IMAGE CLASSIFICATION TASKS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A comprehensive study of vision transformers in image classification tasks, conducted by Mahmoud Khalil, Ahmad Khalil, and Alioune Ngom in 2023."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;DECOMPOSED PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A modular approach for solving complex tasks, developed by Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal in 2022."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;LARGE LANGUAGE MODELS IN INTRODUCTORY PROGRAMMING EDUCATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A study on the performance and implications of large language models in introductory programming education, conducted by Natalie Kiesler and Daniel Schiffner in 2023."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;FEW-SHOT CROSS-LINGUAL TRANSFER WITH TARGET LANGUAGE PECULIAR EXAMPLES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A method for enhancing few-shot cross-lingual transfer, developed by Hwichan Kim and Mamoru Komachi in 2023."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;SELF-GENERATED IN-CONTEXT LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A technique leveraging autoregressive language models as a demonstration generator, developed by Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang goo Lee in 2022."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;BOOSTING CROSS-LINGUAL TRANSFERABILITY IN MULTILINGUAL MODELS VIA IN-CONTEXT LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A method for boosting cross-lingual transferability in multilingual models, developed by Sunkyoung Kim, Dayeon Ki, Yireun Kim, and Jinsik Lee in 2023."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;LANGUAGE MODELS AND YOUTUBE SHORT-FORM VIDEOS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A study on whether language models can understand and generate content related to YouTube short-form videos, conducted by Dayoon Ko, Sangho Lee, and Gunhee Kim in 2023."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;LARGE-SCALE TEXT-TO-IMAGE GENERATION MODELS FOR VISUAL ARTISTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A study on large-scale text-to-image generation models for visual artists' creative works, conducted by Hyung-Kwon Ko, Gwanmo Park, Hyeon Jeon, Jaemin Jo, Juho Kim, and Jinwook Seo in 2023."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;GEMBA-MQM&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A method for detecting translation quality error spans with GPT-4, developed by Tom Kocmi and Christian Federmann in 2023."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;LARGE LANGUAGE MODELS AS EVALUATORS OF TRANSLATION QUALITY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A study on the effectiveness of large language models as evaluators of translation quality, conducted by Tom Kocmi and Christian Federmann in 2023."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;ZERO-SHOT REASONING WITH LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A study on the zero-shot reasoning capabilities of large language models, conducted by Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa in 2022."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;REORDERING EXAMPLES IN PRIMING-BASED FEW-SHOT LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A method for improving few-shot learning by reordering examples, developed by Sawan Kumar and Partha Talukdar in 2021."</data>
      <data key="d2">eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;CLIPSTYLER&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">CLIPSTYLER is a technique for image style transfer using a single text condition. It was developed by Gihyun Kwon and Jong Chul Ye and described in their 2022 paper.</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613,eeb46213e40cc8603a2037766f312338</data>
    </node>
    <node id="&quot;ZERO-SHOT REASONING&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Zero-Shot Reasoning" is a subdomain that deals with the ability of models to reason and make decisions without prior specific training on the task at hand. It refers to the capability of a model to make inferences or predictions without having seen any examples during training, as highlighted in the 2022 paper by Kojima et al.</data>
      <data key="d2">153eeb5a63e650f2cd12f700ffe3e71f,5ce40e1d59b740ff17256ed5abebf613</data>
    </node>
    <node id="&quot;PRIMING-BASED FEW-SHOT LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Priming-Based Few-Shot Learning is a technique that involves reordering examples to improve learning efficiency, as discussed in the 2021 paper by Sawan Kumar and Partha Talukdar."</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613</data>
    </node>
    <node id="&quot;LAKERA GUARD&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Lakera Guard is a technology developed by Lakera in 2024, though specific details are not provided in the text."</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613</data>
    </node>
    <node id="&quot;CHATGPT&#8217;S PACKAGE RECOMMENDATIONS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"ChatGPT&#8217;s Package Recommendations refer to the ability of ChatGPT to suggest software packages, as questioned in the 2023 blog by Bar Lanyado, Ortal Keizman, and Yair Divinsky."</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613</data>
    </node>
    <node id="&quot;EUCLIDREAMER&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Euclidreamer is a technology for fast and high-quality texturing for 3D models using stable diffusion depth, as described in the 2023 paper by Cindy Le, Congrui Hetang, Ang Cao, and Yihui He."</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613</data>
    </node>
    <node id="&quot;RECURSION OF THOUGHT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Recursion of Thought is a divide-and-conquer approach to multi-context reasoning with language models, as discussed in the 2023 paper by Soochan Lee and Gunhee Kim."</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613</data>
    </node>
    <node id="&quot;CONTROLLABLE TEXT-TO-IMAGE GENERATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Controllable Text-to-Image Generation is a technique for generating images from text descriptions with control over the output, as discussed in the 2019 paper by Bowen Li et al."</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613</data>
    </node>
    <node id="&quot;EMOTIONAL STIMULI&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Emotional Stimuli refer to inputs that can enhance the understanding and performance of large language models, as discussed in the 2023 paper by Cheng Li et al."</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613</data>
    </node>
    <node id="&quot;DIALOGUE FOR PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Dialogue for Prompting is a policy-gradient-based discrete prompt optimization technique for few-shot learning, as discussed in the 2023 paper by Chengzhengxu Li et al."</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613</data>
    </node>
    <node id="&quot;INSTANT3D&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Instant3D is a technology for fast text-to-3D generation with sparse-view generation and large reconstruction models, as described in the 2023 papers by Jiahao Li et al. and Ming Li et al."</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613</data>
    </node>
    <node id="&quot;PEER RANK AND DISCUSSION (PRD)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Peer Rank and Discussion (PRD) is a method to improve large language model-based evaluations, as discussed in the 2023 paper by Ruosen Li et al."</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613</data>
    </node>
    <node id="&quot;OBJECT-DRIVEN TEXT-TO-IMAGE SYNTHESIS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Object-Driven Text-to-Image Synthesis is a technique for generating images based on object descriptions using adversarial training, as discussed in the 2019 paper by Wenbo Li et al."</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613</data>
    </node>
    <node id="&quot;UNIFIED DEMONSTRATION RETRIEVER&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Unified Demonstration Retriever is a technique for finding support examples for in-context learning, as discussed in the 2023 paper by Xiaonan Li et al."</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613</data>
    </node>
    <node id="&quot;MEMORY-OF-THOUGHT (MOT)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Memory-of-Thought (MOT) is a technique that enables ChatGPT to self-improve, as discussed in the 2023 paper by Xiaonan Li and Xipeng Qiu."</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613</data>
    </node>
    <node id="&quot;CROSSLINGUAL RETRIEVAL AUGMENTED IN-CONTEXT LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"CROSSLINGUAL RETRIEVAL AUGMENTED IN-CONTEXT LEARNING" is a method for enhancing language models by incorporating crosslingual retrieval techniques, specifically applied to Bangla. This technique aims to improve in-context learning for languages like Bangla, as discussed in the 2023 paper by Xiaoqian Li et al.</data>
      <data key="d2">5ce40e1d59b740ff17256ed5abebf613,630ee831daa753234a258274d318509e</data>
    </node>
    <node id="&quot;CHATGPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">ChatGPT is a language model that can self-improve through various techniques and methodologies. It is also mentioned in the context of evaluations and comparisons with other models.</data>
      <data key="d2">630ee831daa753234a258274d318509e,9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </node>
    <node id="&quot;OSCAR&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Object-Semantics Aligned Pre-training for vision-language tasks, a method to improve the performance of language models in tasks that involve both vision and language."</data>
      <data key="d2">630ee831daa753234a258274d318509e</data>
    </node>
    <node id="&quot;BILINGUAL LEXICON INDUCTION WITH LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A technique for generating bilingual lexicons using large language models."</data>
      <data key="d2">630ee831daa753234a258274d318509e</data>
    </node>
    <node id="&quot;STEP-AWARE VERIFIER&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A method to improve the reasoning capabilities of language models by incorporating step-aware verification."</data>
      <data key="d2">630ee831daa753234a258274d318509e</data>
    </node>
    <node id="&quot;FAIRNESS IN LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A survey that explores the fairness aspects in the development and deployment of large language models."</data>
      <data key="d2">630ee831daa753234a258274d318509e</data>
    </node>
    <node id="&quot;MOVIDEO&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Motion-Aware Video Generation with Diffusion Models, a technique for generating videos that are aware of motion dynamics."</data>
      <data key="d2">630ee831daa753234a258274d318509e</data>
    </node>
    <node id="&quot;MAGIC3D&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"High-Resolution Text-to-3D Content Creation, a method for generating 3D content from text descriptions."</data>
      <data key="d2">630ee831daa753234a258274d318509e</data>
    </node>
    <node id="&quot;FEW-SHOT LEARNING WITH MULTILINGUAL GENERATIVE LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A technique for enabling language models to learn from a few examples across multiple languages."</data>
      <data key="d2">630ee831daa753234a258274d318509e</data>
    </node>
    <node id="&quot;IN-CONTEXT EXAMPLES FOR GPT-3&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A study on what makes good examples for in-context learning in GPT-3."</data>
      <data key="d2">630ee831daa753234a258274d318509e</data>
    </node>
    <node id="&quot;VERIFIABILITY IN GENERATIVE SEARCH ENGINES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A method for evaluating the verifiability of information generated by search engines."</data>
      <data key="d2">630ee831daa753234a258274d318509e</data>
    </node>
    <node id="&quot;PROMPTING METHODS IN NATURAL LANGUAGE PROCESSING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A systematic survey of various prompting methods used in natural language processing."</data>
      <data key="d2">630ee831daa753234a258274d318509e</data>
    </node>
    <node id="&quot;EXPLICIT VISUAL PROMPTING FOR LOW-LEVEL STRUCTURE SEGMENTATIONS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A technique for segmenting low-level structures in images using explicit visual prompts."</data>
      <data key="d2">630ee831daa753234a258274d318509e</data>
    </node>
    <node id="&quot;GPTEVAL&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">GPTEVAL is a subdomain focused on Natural Language Generation (NLG) evaluation using GPT-4 with better human alignment. It employs a method for evaluating natural language generation, leveraging the capabilities of GPT-4 to ensure improved alignment with human expectations and standards.</data>
      <data key="d2">630ee831daa753234a258274d318509e,db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;UNIFYING IMAGE PROCESSING AS VISUAL PROMPTING QUESTION ANSWERING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain focuses on unifying image processing tasks through the method of visual prompting question answering."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;EVALUATE WHAT YOU CAN&#8217;T EVALUATE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain deals with the quality assessment of unassessable generated responses."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;TEXT-GUIDED TEXTURING BY SYNCHRONIZED MULTI-VIEW DIFFUSION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain focuses on text-guided texturing using synchronized multi-view diffusion techniques."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;CALIBRATING LLM-BASED EVALUATOR&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain is concerned with calibrating evaluators based on large language models (LLMs)."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;LARGE LANGUAGE MODEL GUIDED TREE-OF-THOUGHT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain involves guiding tree-of-thought processes using large language models."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;ATT3D: AMORTIZED TEXT-TO-3D OBJECT SYNTHESIS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain focuses on the synthesis of 3D objects from text descriptions using amortized techniques."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;BOUNDING THE CAPABILITIES OF LARGE LANGUAGE MODELS IN OPEN TEXT GENERATION WITH PROMPT CONSTRAINTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain explores the limitations and capabilities of large language models in open text generation, using prompt constraints."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;CHAIN-OF-DICTIONARY PROMPTING ELICITS TRANSLATION IN LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain deals with eliciting translations in large language models through chain-of-dictionary prompting."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;ERROR ANALYSIS PROMPTING ENABLES HUMAN-LIKE TRANSLATION EVALUATION IN LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain focuses on using error analysis prompting to enable human-like translation evaluation in large language models."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;FANTASTICALLY ORDERED PROMPTS AND WHERE TO FIND THEM: OVERCOMING FEW-SHOT PROMPT ORDER SENSITIVITY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain addresses the issue of few-shot prompt order sensitivity and how to overcome it."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;LMQL&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"LMQL is a subdomain related to a GitHub repository, likely focusing on a specific aspect of language models."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;CHATGPT AS A FACTUAL INCONSISTENCY EVALUATOR FOR ABSTRACTIVE TEXT SUMMARIZATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain involves using ChatGPT to evaluate factual inconsistencies in abstractive text summarization."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;GPT4MOTION: SCRIPTING PHYSICAL MOTIONS IN TEXT-TO-VIDEO GENERATION VIA BLENDER-ORIENTED GPT PLANNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain focuses on scripting physical motions in text-to-video generation using Blender-oriented GPT planning."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;FAITHFUL CHAIN-OF-THOUGHT REASONING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain deals with ensuring faithful chain-of-thought reasoning."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;FAIRNESS-GUIDED FEW-SHOT PROMPTING FOR LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain focuses on fairness-guided few-shot prompting for large language models."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;SELF-REFINE: ITERATIVE REFINEMENT WITH SELF-FEEDBACK&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain involves iterative refinement processes using self-feedback."</data>
      <data key="d2">db67f52733fb9d41d13be7cefaa1dae0</data>
    </node>
    <node id="&quot;BIAS AND FAIRNESS IN MACHINE LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Bias and Fairness in Machine Learning is a field of study that focuses on identifying, understanding, and mitigating biases in machine learning models to ensure fair outcomes."</data>
      <data key="d2">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </node>
    <node id="&quot;CHAIN OF IMAGES FOR INTUITIVE REASONING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Chain of Images for Intuitive Reasoning is a method that uses a sequence of images to facilitate intuitive reasoning and understanding."</data>
      <data key="d2">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </node>
    <node id="&quot;CLIF-VQA&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"CLIF-VQA is a technique for enhancing video quality assessment by incorporating high-level semantic information related to human feelings."</data>
      <data key="d2">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </node>
    <node id="&quot;AUGMENTED LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Augmented Language Models are advanced models that integrate additional capabilities and knowledge to improve language understanding and generation."</data>
      <data key="d2">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </node>
    <node id="&quot;AMBIGUOUS OPEN-DOMAIN QUESTIONS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Ambiguous Open-Domain Questions are questions that can have multiple interpretations or answers, requiring specialized techniques to address."</data>
      <data key="d2">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </node>
    <node id="&quot;COMPUTATIONAL SPEECH-ACT MODEL&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Computational Speech-Act Model is a framework for modeling human-computer conversations based on speech acts."</data>
      <data key="d2">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </node>
    <node id="&quot;ADAPTIVE MACHINE TRANSLATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Adaptive Machine Translation involves using large language models to dynamically adjust translations based on context and user feedback."</data>
      <data key="d2">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </node>
    <node id="&quot;CLARIFYGPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"ClarifyGPT is a tool that empowers code generation by large language models through intention clarification."</data>
      <data key="d2">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </node>
    <node id="&quot;CROSSLINGUAL GENERALIZATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Crosslingual Generalization is a technique that improves the ability of models to generalize across multiple languages through multitask finetuning."</data>
      <data key="d2">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </node>
    <node id="&quot;POLYGLOT LLMS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Polyglot LLMs are large language models that are trained to understand and generate multiple languages, breaking language barriers."</data>
      <data key="d2">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </node>
    <node id="&quot;DELFT UNIVERSITY OF TECHNOLOGY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Delft University of Technology is an educational institution where research on leveraging large language models and prompt engineering in computer science education was conducted."</data>
      <data key="d2">4d9e8d703c2da8e4775c428e83e87fc9</data>
    </node>
    <node id="&quot;BEHAVIORAL HEALTH WORKFORCE, 2023&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Behavioral Health Workforce, 2023 is a report published by the National Center for Health Workforce Analysis, focusing on the state of the behavioral health workforce in 2023."</data>
      <data key="d2">4d9e8d703c2da8e4775c428e83e87fc9</data>
    </node>
    <node id="&quot;PRISMA 2020 STATEMENT&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The PRISMA 2020 Statement is an updated guideline for reporting systematic reviews, published in the BMJ."</data>
      <data key="d2">4d9e8d703c2da8e4775c428e83e87fc9</data>
    </node>
    <node id="&quot;LEARNING STRATEGIES FOR POLYGLOT LLMS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Learning Strategies for Polyglot LLMs refers to research on methods to improve the learning capabilities of large language models that can understand multiple languages."</data>
      <data key="d2">4d9e8d703c2da8e4775c428e83e87fc9</data>
    </node>
    <node id="&quot;CROSS-LINGUAL RETRIEVAL AUGMENTED PROMPT FOR LOW-RESOURCE LANGUAGES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Cross-lingual Retrieval Augmented Prompt for Low-Resource Languages is a research area focused on improving language model performance for languages with limited resources."</data>
      <data key="d2">4d9e8d703c2da8e4775c428e83e87fc9</data>
    </node>
    <node id="&quot;PARALLEL DECODING IN LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Parallel Decoding in Large Language Models is a research area that explores how large language models can perform parallel decoding to enhance efficiency."</data>
      <data key="d2">4d9e8d703c2da8e4775c428e83e87fc9</data>
    </node>
    <node id="&quot;GORILLA: LARGE LANGUAGE MODEL CONNECTED WITH MASSIVE APIS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Gorilla: Large Language Model Connected with Massive APIs is a research project that connects large language models with numerous APIs to extend their functionality."</data>
      <data key="d2">4d9e8d703c2da8e4775c428e83e87fc9</data>
    </node>
    <node id="&quot;IGNORE PREVIOUS PROMPT: ATTACK TECHNIQUES FOR LANGUAGE MODELS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 2022 study by F&#225;bio Perez and Ian Ribeiro that explores various attack techniques on language models."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;DO USERS WRITE MORE INSECURE CODE WITH AI ASSISTANTS?&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 2022 study by Neil Perry, Megha Srivastava, Deepak Kumar, and Dan Boneh investigating whether AI assistants lead to more insecure coding practices."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;CREDIBLE WITHOUT CREDIT: DOMAIN EXPERTS ASSESS GENERATIVE LANGUAGE MODELS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 2023 study by Denis Peskoff and Brandon M Stewart that evaluates generative language models through the lens of domain experts."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;GPT DECIPHERING FEDSPEAK: QUANTIFYING DISSENT AMONG HAWKS AND DOVES&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 2023 study by Denis Peskoff, Adam Visokay, Sander Schulhoff, Benjamin Wachspress, Alan Blinder, and Brandon M Stewart analyzing language models' ability to interpret financial language."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;ADAPTING ENTITIES ACROSS LANGUAGES AND CULTURES&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 2021 study by Denis Peskov, Viktor Hangya, Jordan Boyd-Graber, and Alexander Fraser focusing on the adaptation of entities in different languages and cultures."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;LANGUAGE MODELS AS KNOWLEDGE BASES?&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 2019 study by Fabio Petroni, Tim Rockt&#228;schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller exploring the potential of language models to serve as knowledge bases."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;LARGE LANGUAGE MODELS SENSITIVITY TO THE ORDER OF OPTIONS IN MULTIPLE-CHOICE QUESTIONS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 2023 study by Pouya Pezeshkpour and Estevam Hruschka examining how the order of options affects the performance of large language models in multiple-choice questions."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;CONSTRAINTS ON LANGUAGE MIXING: INTRASENTENTIAL CODE-SWITCHING AND BORROWING IN SPANISH/ENGLISH&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 1979 study by Carol W. Pfaff investigating the constraints on language mixing, specifically intrasentential code-switching and borrowing between Spanish and English."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;INTERACTIVE-CHAIN-PROMPTING: AMBIGUITY RESOLUTION FOR CROSSLINGUAL CONDITIONAL GENERATION WITH INTERACTION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 2023 study by Jonathan Pilault, Xavier Garcia, Arthur Bra&#382;inskas, and Orhan Firat focusing on resolving ambiguities in crosslingual conditional generation through interactive prompting."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;DREAMFUSION: TEXT-TO-3D USING 2D DIFFUSION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 2022 study by Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall exploring the conversion of text to 3D models using 2D diffusion techniques."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;SOMETIMES I'LL START A SENTENCE IN SPANISH Y TERMINO EN ESPA&#209;OL: TOWARD A TYPOLOGY OF CODE-SWITCHING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 1980 study by Shana Poplack aiming to develop a typology of code-switching."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;GRIPS: GRADIENT-FREE, EDIT-BASED INSTRUCTION SEARCH FOR PROMPTING LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 2023 study by Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal introducing a gradient-free, edit-based method for optimizing prompts for large language models."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;MEASURING AND NARROWING THE COMPOSITIONALITY GAP IN LANGUAGE MODELS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 2022 study by Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis focusing on the compositionality gap in language models."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;AUTOMATIC PROMPT OPTIMIZATION WITH 'GRADIENT DESCENT' AND BEAM SEARCH&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 2023 study by Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng exploring methods for automatic prompt optimization."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;DECOMPOSED PROMPTING FOR MACHINE TRANSLATION BETWEEN RELATED LANGUAGES USING LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 2023 study by Ratish Puduppully, Anoop Kunchukuttan, Raj Dabre, Ai Ti Aw, and Nancy F. Chen focusing on decomposed prompting for machine translation between related languages."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;TASKWEAVER: A CODE-FIRST AGENT FRAMEWORK&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A 2023 study by Bo Qiao, Liqun Li, Xu Zhang, Shilin He, Yu Kang, Chaoyun Zhang, Fangkai Yang, Hang Dong, Jue Zhang, Lu Wang, Ming-Jie Ma, Pu Zhao, Si Qin, Xiaoting Qin, Chao Du, Yong Xu, Qingwei Lin, S. Rajmohan, and Dongmei Zhang introducing a code-first agent framework."</data>
      <data key="d2">2f28d2ed61c6111fccc81e48e659b599</data>
    </node>
    <node id="&quot;REASONING WITH LANGUAGE MODEL PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Reasoning with Language Model Prompting is a survey conducted in 2022 by Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen, focusing on the use of language model prompting for reasoning tasks."</data>
      <data key="d2">42397dc5d60f0a1d799e06290ea52864</data>
    </node>
    <node id="&quot;CROSS-LINGUAL PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Cross-lingual Prompting is a technique aimed at improving zero-shot chain-of-thought reasoning across languages, as discussed in a 2023 paper by Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che."</data>
      <data key="d2">42397dc5d60f0a1d799e06290ea52864</data>
    </node>
    <node id="&quot;TOOL LEARNING WITH FOUNDATION MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Tool Learning with Foundation Models is a study from 2023 by Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shi Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bo Li, Ziwei Tang, Jing Yi, Yu Zhu, Zhenning Dai, Lan Yan, Xin Cong, Ya-Ting Lu, Weilin Zhao, Yuxiang Huang, Jun-Han Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun, focusing on the application of foundation models for tool learning."</data>
      <data key="d2">42397dc5d60f0a1d799e06290ea52864</data>
    </node>
    <node id="&quot;LEARNING TRANSFERABLE VISUAL MODELS FROM NATURAL LANGUAGE SUPERVISION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Learning Transferable Visual Models from Natural Language Supervision is a 2021 study by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and others, presented at the International Conference on Machine Learning."</data>
      <data key="d2">42397dc5d60f0a1d799e06290ea52864</data>
    </node>
    <node id="&quot;LANGUAGE MODELS ARE UNSUPERVISED MULTITASK LEARNERS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Language Models are Unsupervised Multitask Learners is a 2019 study by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and others, published on the OpenAI blog, discussing the capabilities of language models in performing multiple tasks without supervision."</data>
      <data key="d2">42397dc5d60f0a1d799e06290ea52864</data>
    </node>
    <node id="&quot;ANSWER-BASED ADVERSARIAL TRAINING FOR GENERATING CLARIFICATION QUESTIONS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Answer-based Adversarial Training for Generating Clarification Questions is a 2019 study by Sudha Rao and Hal Daum&#233; III, focusing on the use of adversarial training to generate clarification questions."</data>
      <data key="d2">42397dc5d60f0a1d799e06290ea52864</data>
    </node>
    <node id="&quot;NEMO GUARDRAILS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Nemo Guardrails is a toolkit for controllable and safe large language model applications with programmable rails, discussed in a 2023 paper by Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, and Jonathan Cohen."</data>
      <data key="d2">42397dc5d60f0a1d799e06290ea52864</data>
    </node>
    <node id="&quot;NATURALLY OCCURRING LANGUAGE AS A SOURCE OF EVIDENCE IN SUICIDE PREVENTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Naturally Occurring Language as a Source of Evidence in Suicide Prevention is a 2021 study by Philip Resnik, April Foreman, Michelle Kuchuk, Katherine Musacchio Schafer, and Beau Pinkham, focusing on the use of natural language as evidence in suicide prevention."</data>
      <data key="d2">42397dc5d60f0a1d799e06290ea52864</data>
    </node>
    <node id="&quot;PROMPT PROGRAMMING FOR LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Programming for Large Language Models is a 2021 study by Laria Reynolds and Kyle McDonell, presented at the CHI Conference on Human Factors in Computing Systems, discussing techniques for programming prompts for large language models."</data>
      <data key="d2">42397dc5d60f0a1d799e06290ea52864</data>
    </node>
    <node id="&quot;HIGH-RESOLUTION IMAGE SYNTHESIS WITH LATENT DIFFUSION MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"High-Resolution Image Synthesis with Latent Diffusion Models is a 2022 study by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj&#246;rn Ommer, focusing on the use of latent diffusion models for high-resolution image synthesis."</data>
      <data key="d2">42397dc5d60f0a1d799e06290ea52864</data>
    </node>
    <node id="&quot;LEARNING TO RETRIEVE PROMPTS FOR IN-CONTEXT LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Learning to Retrieve Prompts for In-Context Learning" is a 2022 study by Ohad Rubin, Jonathan Herzig, and Jonathan Berant, presented at the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. The study focuses on techniques for retrieving prompts to enhance in-context learning, aiming to improve the ability of large language models to understand and generate contextually relevant responses.</data>
      <data key="d2">42397dc5d60f0a1d799e06290ea52864,c605e4f0158f18be68214a39b9b54154</data>
    </node>
    <node id="&quot;GEN-2 PROMPT TIPS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Gen-2 Prompt Tips is a guide provided by Runway in 2023, offering tips for effectively using prompts in the Gen-2 model."</data>
      <data key="d2">42397dc5d60f0a1d799e06290ea52864</data>
    </node>
    <node id="&quot;NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The North American Chapter of the Association for Computational Linguistics is a regional division of the Association for Computational Linguistics, focusing on activities and conferences in North America."</data>
      <data key="d2">c605e4f0158f18be68214a39b9b54154</data>
    </node>
    <node id="&quot;VANDERBILT UNIVERSITY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Vanderbilt University is an educational institution where research on prompt engineering is conducted, specifically by the Department of Computer Science."</data>
      <data key="d2">c605e4f0158f18be68214a39b9b54154</data>
    </node>
    <node id="&quot;DEPT. OF COMPUTER SCIENCE, VANDERBILT UNIVERSITY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Department of Computer Science at Vanderbilt University is involved in cataloging prompt patterns to enhance the discipline of prompt engineering."</data>
      <data key="d2">c605e4f0158f18be68214a39b9b54154</data>
    </node>
    <node id="&quot;EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Empirical Methods in Natural Language Processing is a conference where research papers on natural language processing are presented."</data>
      <data key="d2">c605e4f0158f18be68214a39b9b54154</data>
    </node>
    <node id="&quot;HACKAPROMPT&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"HackAPrompt is a global prompt hacking competition aimed at exposing systemic vulnerabilities of large language models."</data>
      <data key="d2">c605e4f0158f18be68214a39b9b54154</data>
    </node>
    <node id="&quot;SYSTEMATIC SURVEY OF PROMPT ENGINEERING IN LARGE LANGUAGE MODELS&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"A systematic survey of prompt engineering in large language models aims to review and categorize various techniques and applications in the field."</data>
      <data key="d2">c605e4f0158f18be68214a39b9b54154</data>
    </node>
    <node id="&quot;TOOLFORMER&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Toolformer is a goal focused on enabling language models to teach themselves to use tools, thereby enhancing their capabilities."</data>
      <data key="d2">c605e4f0158f18be68214a39b9b54154</data>
    </node>
    <node id="&quot;QUANTIFYING LANGUAGE MODELS&#8217; SENSITIVITY TO SPURIOUS FEATURES IN PROMPT DESIGN&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A study focusing on how language models react to irrelevant features in prompt design, highlighting the importance of prompt formatting."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;GUIDANCE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A GitHub repository created by Harsha-Nori Scott Lundberg and Marco Tulio Correia Ribeiro, likely containing tools or resources related to language models."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;SPEECH ACTS: AN ESSAY IN THE PHILOSOPHY OF LANGUAGE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A seminal work by John R. Searle, discussing the philosophy of language and speech acts."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;ON SECOND THOUGHT, LET&#8217;S NOT THINK STEP BY STEP! BIAS AND TOXICITY IN ZERO-SHOT REASONING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A study by Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang, examining bias and toxicity in zero-shot reasoning."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;TOWARDS UNDERSTANDING SYCOPHANCY IN LANGUAGE MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A research paper by Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R Johnston, et al., aiming to understand sycophantic behavior in language models."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;HUGGINGGPT: SOLVING AI TASKS WITH CHATGPT AND ITS FRIENDS IN HUGGING FACE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A study by Yongliang Shen, Kaitao Song, Xu Tan, Dong Sheng Li, Weiming Lu, and Yue Ting Zhuang, exploring the use of ChatGPT and other models in Hugging Face to solve AI tasks."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;LANGUAGE MODELS ARE MULTILINGUAL CHAIN-OF-THOUGHT REASONERS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A research paper by Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei, discussing the multilingual reasoning capabilities of language models."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;ELICITING KNOWLEDGE FROM LANGUAGE MODELS USING AUTOMATICALLY GENERATED PROMPTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A study by Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh, focusing on extracting knowledge from language models using auto-generated prompts."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;AUTOPROMPT: ELICITING KNOWLEDGE FROM LANGUAGE MODELS WITH AUTOMATICALLY GENERATED PROMPTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A paper by Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh, discussing the use of automatically generated prompts to elicit knowledge from language models."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;EXPERT, CROWDSOURCED, AND MACHINE ASSESSMENT OF SUICIDE RISK VIA ONLINE POSTINGS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A study by Han-Chin Shing, Suraj Nair, Ayah Zirikly, Meir Friedenberg, Hal Daum&#233; III, and Philip Resnik, examining different methods of assessing suicide risk through online postings."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;REFLEXION: LANGUAGE AGENTS WITH VERBAL REINFORCEMENT LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A research paper by Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao, focusing on language agents that use verbal reinforcement learning."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;MEASURING INDUCTIVE BIASES OF IN-CONTEXT LEARNING WITH UNDERSPECIFIED DEMONSTRATIONS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A study by Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng, Danqi Chen, and He He, investigating the inductive biases in in-context learning with underspecified demonstrations."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;PROMPTING GPT-3 TO BE RELIABLE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A research paper by Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang, focusing on methods to make GPT-3 more reliable."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;LARGE LANGUAGE MODELS HELP HUMANS VERIFY TRUTHFULNESS&#8211;EXCEPT WHEN THEY ARE CONVINCINGLY WRONG&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A study by Chenglei Si, Navita Goyal, Sherry Tongshuang Wu, Chen Zhao, Shi Feng, Hal Daum&#233; III, and Jordan Boyd-Graber, examining the role of large language models in verifying truthfulness."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;GETTING MORE OUT OF MIXTURE OF LANGUAGE MODEL REASONING EXPERTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"A research paper by Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, and Jordan Lee Boyd-Graber, discussing the use of a mixture of language model reasoning experts."</data>
      <data key="d2">22a657737fd9e20b7803d916867d487b</data>
    </node>
    <node id="&quot;TRUTHFULNESS VERIFICATION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Truthfulness verification is the goal of ensuring that information provided by language models is accurate and reliable."</data>
      <data key="d2">ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;MIXTURE OF LANGUAGE MODEL REASONING EXPERTS (MORE)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"MoRE is a subdomain that focuses on enhancing the reasoning capabilities of language models by combining multiple experts."</data>
      <data key="d2">ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;AUTOGPT&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"AutoGPT is a technology developed by Significant Gravitas in 2023, aimed at automating tasks using language models."</data>
      <data key="d2">ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;TEXT-TO-4D DYNAMIC SCENE GENERATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Text-to-4D dynamic scene generation is a subdomain that focuses on creating dynamic 4D scenes from textual descriptions."</data>
      <data key="d2">ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;EVALUATION METRICS FOR GPT-4&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Evaluation metrics for GPT-4 are methods used to reliably assess the performance of GPT-4 on various tasks."</data>
      <data key="d2">ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;NATURAL LANGUAGE REASONING AND STRUCTURED EXPLANATIONS (NLRSE)&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"NLRSE is a subdomain that focuses on reasoning and providing structured explanations in natural language."</data>
      <data key="d2">ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;OPEN-WORLD SEGMENTATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Open-world segmentation is a subdomain that focuses on segmenting images in an open-world setting without training."</data>
      <data key="d2">ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;MULTILINGUAL LLMS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Multilingual LLMs are large language models that are capable of understanding and generating text in multiple languages."</data>
      <data key="d2">ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;TEXT-TO-IMAGE SYNTHESIS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Text-to-image synthesis is a subdomain that focuses on generating images from textual descriptions."</data>
      <data key="d2">ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;AI PACKAGE HALLUCINATION ATTACKS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"AI package hallucination attacks are a subdomain that explores vulnerabilities in AI systems where they generate false information."</data>
      <data key="d2">ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;CALIBRATED CONFIDENCE SCORES&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Calibrated confidence scores are the goal of obtaining accurate confidence levels from language models fine-tuned with human feedback."</data>
      <data key="d2">ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;GYMNASIUM&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">Gymnasium is a technology developed in 2023 that provides a platform for training and evaluating AI models. Additionally, Gymnasium is an organization likely involved in research or academic activities.</data>
      <data key="d2">153eeb5a63e650f2cd12f700ffe3e71f,ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;CHAIN-OF-THOUGHT REASONING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">Chain-of-Thought Reasoning is a subdomain focused on the process of reasoning through a sequence of thoughts or steps, particularly in the context of large language models. This technique is used in language models to elicit reasoning by generating intermediate steps that lead to a final answer. Additionally, Chain-of-Thought Reasoning involves interleaving retrieval with reasoning to improve the performance of language models.</data>
      <data key="d2">153eeb5a63e650f2cd12f700ffe3e71f,42d8c3ad092ec18e28ff718709b0b472,ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;LANGUAGE MODELS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">ca2bcd796327d014f9e7738468b6b00d</data>
    </node>
    <node id="&quot;CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Conference on Empirical Methods in Natural Language Processing is an event where research related to natural language processing is presented and discussed."</data>
      <data key="d2">153eeb5a63e650f2cd12f700ffe3e71f</data>
    </node>
    <node id="&quot;INTERLEAVING RETRIEVAL WITH CHAIN-OF-THOUGHT REASONING&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Interleaving Retrieval with Chain-of-Thought Reasoning is a goal focused on enhancing the process of answering knowledge-intensive multi-step questions."</data>
      <data key="d2">153eeb5a63e650f2cd12f700ffe3e71f</data>
    </node>
    <node id="&quot;AUTOMATED EVALUATION OF PERSONALIZED TEXT GENERATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Automated Evaluation of Personalized Text Generation involves using large language models to assess the quality and relevance of text generated for individual users."</data>
      <data key="d2">42d8c3ad092ec18e28ff718709b0b472</data>
    </node>
    <node id="&quot;ROLELLM&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"RoleLLM is a benchmark for evaluating and enhancing the role-playing abilities of large language models, focusing on their performance in simulated scenarios."</data>
      <data key="d2">42d8c3ad092ec18e28ff718709b0b472</data>
    </node>
    <node id="&quot;DIFFUSION MODELS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Diffusion Models are a type of generative model used in machine learning, which have been unlocked for in-context learning to improve their performance."</data>
      <data key="d2">42d8c3ad092ec18e28ff718709b0b472</data>
    </node>
    <node id="&quot;COGNITIVE SYNERGY&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Cognitive Synergy refers to the enhanced problem-solving capabilities of large language models achieved through multi-persona self-collaboration."</data>
      <data key="d2">42d8c3ad092ec18e28ff718709b0b472</data>
    </node>
    <node id="&quot;SIMPLE SYNTHETIC DATA&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Simple Synthetic Data is used to reduce sycophancy in large language models, helping them generate more independent and less biased responses."</data>
      <data key="d2">42d8c3ad092ec18e28ff718709b0b472</data>
    </node>
    <node id="&quot;SYSTEM 2 ATTENTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"System 2 Attention is a concept that suggests a more deliberate and analytical form of attention, which might be necessary for certain tasks in language models."</data>
      <data key="d2">42d8c3ad092ec18e28ff718709b0b472</data>
    </node>
    <node id="&quot;PROMPT PATTERN CATALOG&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Pattern Catalog is a collection of prompt engineering techniques designed to enhance the performance of language models like ChatGPT."</data>
      <data key="d2">42d8c3ad092ec18e28ff718709b0b472</data>
    </node>
    <node id="&quot;PERSPECTIVE-TAKING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Perspective-Taking is a method that improves the theory-of-mind capabilities of large language models by encouraging them to consider different viewpoints."</data>
      <data key="d2">42d8c3ad092ec18e28ff718709b0b472</data>
    </node>
    <node id="&quot;PROMPT INJECTION ATTACKS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Injection Attacks are a type of security threat where malicious prompts are used to manipulate the output of language models."</data>
      <data key="d2">42d8c3ad092ec18e28ff718709b0b472</data>
    </node>
    <node id="&quot;MULTILINGUAL FEW-SHOT LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Multilingual Few-Shot Learning involves using language model retrieval techniques to enable few-shot learning across multiple languages."</data>
      <data key="d2">42d8c3ad092ec18e28ff718709b0b472</data>
    </node>
    <node id="&quot;TUNE-A-VIDEO&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Tune-A-Video" is a method for one-shot tuning of image diffusion models to generate text-to-video content. This technique allows for the creation of videos from text descriptions by fine-tuning image diffusion models in a single step.</data>
      <data key="d2">42d8c3ad092ec18e28ff718709b0b472,ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;LARGE LANGUAGE MODELS AS DIVERSE ROLE-PLAYERS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Large Language Models as Diverse Role-Players refers to the capability of large language models to perform various roles in summarization evaluation."</data>
      <data key="d2">ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;AI CHAINS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"AI Chains is a concept for transparent and controllable human-AI interaction by chaining large language model prompts."</data>
      <data key="d2">ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;SECURITY, PRIVACY, AND ETHICAL CONCERNS OF CHATGPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Security, Privacy, and Ethical Concerns of ChatGPT refers to the study of potential risks and ethical issues associated with the use of ChatGPT."</data>
      <data key="d2">ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;FOFO&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"FoFo is a benchmark designed to evaluate the format-following capability of large language models."</data>
      <data key="d2">ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;CONFIDENCE ELICITATION IN LLMS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Confidence Elicitation in LLMs refers to the empirical evaluation of how well large language models can express their uncertainty."</data>
      <data key="d2">ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;RE-READING IMPROVES REASONING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Re-Reading Improves Reasoning is a concept that suggests re-reading text can enhance the reasoning capabilities of language models."</data>
      <data key="d2">ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;RCOT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"RCOT stands for Reversing Chain-of-Thought, a method for detecting and rectifying factual inconsistency in reasoning."</data>
      <data key="d2">ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;LARGE LANGUAGE MODELS AS OPTIMIZERS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Large Language Models as Optimizers refers to the use of large language models to optimize various tasks."</data>
      <data key="d2">ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;3DSTYLE-DIFFUSION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"3DStyle-Diffusion is a technique for fine-grained text-driven 3D stylization using 2D diffusion models."</data>
      <data key="d2">ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;AUTO-GPT FOR ONLINE DECISION MAKING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Auto-GPT for Online Decision Making refers to the use of Auto-GPT models for making decisions in online environments."</data>
      <data key="d2">ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;HUMAN-IN-THE-LOOP MACHINE TRANSLATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Human-in-the-Loop Machine Translation involves the integration of human feedback in the machine translation process using large language models."</data>
      <data key="d2">ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;THE DAWN OF LMMS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"The Dawn of LMMs refers to the preliminary explorations with GPT-4V(ision), indicating the beginning of a new era in language model capabilities."</data>
      <data key="d2">ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;EMPOWERING LLM-BASED MACHINE TRANSLATION WITH CULTURAL AWARENESS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Empowering LLM-Based Machine Translation with Cultural Awareness involves enhancing machine translation systems to be culturally aware."</data>
      <data key="d2">ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;TREE OF THOUGHTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Tree of Thoughts is a method for deliberate problem-solving using large language models."</data>
      <data key="d2">ccdfd3415647f13f577d728a5a0256b1</data>
    </node>
    <node id="&quot;GRAPH-OF-THOUGHT REASONING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Graph-of-Thought Reasoning is an effective method for reasoning in large language models, going beyond chain-of-thought, as discussed by Yao Yao, Zuchao Li, and Hai Zhao in 2023."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d</data>
    </node>
    <node id="&quot;ANALOGICAL REASONING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Analogical Reasoning is a subdomain where large language models are used as analogical reasoners, as explored by Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, and Denny Zhou in 2023."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d</data>
    </node>
    <node id="&quot;EXPLANATION SELECTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Explanation Selection is a subdomain that uses unlabeled data for chain-of-thought prompting, as explored by Xi Ye and Greg Durrett in 2023."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d</data>
    </node>
    <node id="&quot;INPUT-LABEL DEMONSTRATIONS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Input-Label Demonstrations is a subdomain that emphasizes the importance of ground-truth labels, as discussed by Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyun-soo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang goo Lee, and Taeuk Kim in 2022."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d</data>
    </node>
    <node id="&quot;META-REASONING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Meta-Reasoning is a subdomain that involves answering questions by reasoning over multiple chains of thought, as explored by Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant in 2023."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d</data>
    </node>
    <node id="&quot;VIDEO UNDERSTANDING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Video Understanding is a subdomain that involves zero-shot video understanding using an ensemble of foundational models, as discussed by Adeel Yousaf, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan, and Mubarak Shah in 2023."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d</data>
    </node>
    <node id="&quot;ATTRIBUTED TRAINING DATA GENERATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Attributed Training Data Generation is a subdomain where large language models generate training data with a focus on diversity and bias, as explored by Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, and Chao Zhang in 2023."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d</data>
    </node>
    <node id="&quot;AUTOMATIC EVALUATION OF ATTRIBUTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Automatic Evaluation of Attribution is a subdomain that involves evaluating the attribution by large language models, as discussed by Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun in 2023."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d</data>
    </node>
    <node id="&quot;INSTRUCTION FOLLOWING EVALUATION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Instruction Following Evaluation is a subdomain that evaluates large language models' ability to follow instructions, as explored by Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen in 2023."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d</data>
    </node>
    <node id="&quot;AMBIGUITY RESOLUTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Ambiguity Resolution is a subdomain that involves resolving ambiguity through interaction with language models, as discussed by Michael JQ Zhang and Eunsol Choi in 2023."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d</data>
    </node>
    <node id="&quot;SOFTWARE ENGINEERING WITH LLMS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Software Engineering with LLMs is a subdomain that critically reviews the use of large language models in software engineering, as explored by Quanjun Zhang, Tongke Zhang, Juan Zhai, Chunrong Fang, Bowen Yu, Weisong Sun, and Zhenyu Chen in 2023."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d</data>
    </node>
    <node id="&quot;LANGUAGE INTELLIGENCE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Language Intelligence is a subdomain that involves chain-of-thought reasoning to language agents, as discussed by Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei Ma, Zhiwei He, Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, and Hai Zhao in 2023."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d</data>
    </node>
    <node id="&quot;AUTOMATIC CHAIN OF THOUGHT PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Automatic Chain of Thought Prompting is a subdomain that involves prompting in large language models, as explored by Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola in 2022."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d</data>
    </node>
    <node id="&quot;MULTIMODAL CHAIN-OF-THOUGHT REASONING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Multimodal Chain-of-Thought Reasoning is a subdomain that involves reasoning in language models across multiple modalities, as discussed by Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola in 2023."</data>
      <data key="d2">0274e77e2fcec8973c9768c464c6e82d</data>
    </node>
    <node id="&quot;MULTI-MODAL CHAIN-OF-THOUGHT REASONING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Multi-modal Chain-of-Thought Reasoning is a technique in language models that involves reasoning across multiple modalities to enhance understanding and performance."</data>
      <data key="d2">c7285f7847ef45ed85779d7966753855</data>
    </node>
    <node id="&quot;CALIBRATE BEFORE USE&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Calibrate Before Use is a method aimed at improving the few-shot performance of language models by calibrating them before deployment."</data>
      <data key="d2">c7285f7847ef45ed85779d7966753855</data>
    </node>
    <node id="&quot;LARGE LANGUAGE MODELS AS TABLE-TO-TEXT GENERATORS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain explores the effectiveness of large language models in generating text from tables, evaluating their performance, and providing feedback."</data>
      <data key="d2">c7285f7847ef45ed85779d7966753855</data>
    </node>
    <node id="&quot;ANIMATE124&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Animate124 is a technique for animating a single image into a 4D dynamic scene, showcasing advancements in image processing and animation."</data>
      <data key="d2">c7285f7847ef45ed85779d7966753855</data>
    </node>
    <node id="&quot;DUTY-DISTINCT CHAIN-OF-THOUGHT PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Duty-Distinct Chain-of-Thought Prompting is a method for multimodal reasoning in language models, focusing on distinct duties or tasks."</data>
      <data key="d2">c7285f7847ef45ed85779d7966753855</data>
    </node>
    <node id="&quot;EVOKING REASONING VIA ABSTRACTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Evoking Reasoning via Abstraction is a technique to enhance reasoning in large language models by encouraging abstract thinking."</data>
      <data key="d2">c7285f7847ef45ed85779d7966753855</data>
    </node>
    <node id="&quot;SOCIAL ROLES IN SYSTEM PROMPTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain evaluates the effectiveness of different social roles assigned to large language models in system prompts."</data>
      <data key="d2">c7285f7847ef45ed85779d7966753855</data>
    </node>
    <node id="&quot;HUMAN-LEVEL PROMPT ENGINEERS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"This subdomain explores the capability of large language models to function as human-level prompt engineers, designing effective prompts for various tasks."</data>
      <data key="d2">c7285f7847ef45ed85779d7966753855</data>
    </node>
    <node id="&quot;THREAD OF THOUGHT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Thread of Thought is a technique for unraveling chaotic contexts in language models, improving their ability to maintain coherent thought processes."</data>
      <data key="d2">c7285f7847ef45ed85779d7966753855</data>
    </node>
    <node id="&quot;GHOST IN THE MINECRAFT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Ghost in the Minecraft refers to generally capable agents for open-world environments, leveraging large language models with text-based knowledge and memory."</data>
      <data key="d2">c7285f7847ef45ed85779d7966753855</data>
    </node>
    <node id="&quot;CUT-AND-PASTE VIDEO EDITING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Cut-and-Paste Video Editing is a subject-driven video editing technique that uses attention control to enhance the editing process."</data>
      <data key="d2">c7285f7847ef45ed85779d7966753855</data>
    </node>
    <node id="&quot;PRIMING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Priming refers to giving a model an initial prompt that lays out certain instructions for the rest of a conversation. This priming prompt might contain a role or other instructions on how to interact with the user."</data>
      <data key="d2">d305fc89f77daeb9c5be3a3d126223ed</data>
    </node>
    <node id="&quot;PROMPT LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Learning, also known as Prompt Learning, refers to the process of using prompting-related techniques, often in the context of fine-tuning prompts."</data>
      <data key="d2">6430817c08b3a5c6d193478d4c739d79</data>
    </node>
    <node id="&quot;HARD PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Hard Prompt, also known as discrete prompt, contains tokens that directly correspond to words in the LLM vocabulary."</data>
      <data key="d2">6430817c08b3a5c6d193478d4c739d79</data>
    </node>
    <node id="&quot;SOFT PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Soft Prompt, also known as continuous prompt, contains tokens that may not correspond to any word in the vocabulary and can be used when fine-tuning is desired but modifying the weights of the full model is prohibitively expensive."</data>
      <data key="d2">6430817c08b3a5c6d193478d4c739d79</data>
    </node>
    <node id="&quot;CLOZE PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Cloze Prompt is a prediction style where the token(s) to be predicted are presented as 'slots to fill', usually somewhere in the middle of the prompt."</data>
      <data key="d2">6430817c08b3a5c6d193478d4c739d79</data>
    </node>
    <node id="&quot;PREFIX PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prefix Prompt is a prediction style where the token to be predicted is at the end of the prompt, commonly used in modern GPT-style models."</data>
      <data key="d2">6430817c08b3a5c6d193478d4c739d79</data>
    </node>
    <node id="&quot;LEARN PROMPTING&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Learn Prompting is one of the research entities associated with the creation of the dataset on prompt engineering."</data>
      <data key="d2">6430817c08b3a5c6d193478d4c739d79</data>
    </node>
    <node id="&quot;DATASET CREATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Dataset Creation refers to the process of gathering existing literature on prompt engineering to analyze current hard prefix prompting techniques."</data>
      <data key="d2">6430817c08b3a5c6d193478d4c739d79</data>
    </node>
    <node id="&quot;RESEARCH PAPER COLLECTION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Research Paper Collection refers to the process of gathering research papers from Arxiv, Semantic Scholar, and ACL, primarily in February 2024."</data>
      <data key="d2">29d2b14a56a51f86baa34264697bdd5e</data>
    </node>
    <node id="&quot;MANUAL REVIEW&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Manual Review is the process of manually checking the collected papers to ensure their relevance and to mitigate errors."</data>
      <data key="d2">29d2b14a56a51f86baa34264697bdd5e</data>
    </node>
    <node id="&quot;SEMI-AUTOMATED REVIEW&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Semi-Automated Review is the process of using semi-automated methods to review the collected papers for relevance and to mitigate errors."</data>
      <data key="d2">29d2b14a56a51f86baa34264697bdd5e</data>
    </node>
    <node id="&quot;DATASET MAINTENANCE&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Dataset Maintenance is the ongoing task of supporting, hosting, and maintaining the dataset by the team."</data>
      <data key="d2">29d2b14a56a51f86baa34264697bdd5e</data>
    </node>
    <node id="&quot;DATASET DISTRIBUTION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Dataset Distribution refers to the decision not to distribute the dataset to third parties outside of the entity on behalf of which the dataset was created."</data>
      <data key="d2">29d2b14a56a51f86baa34264697bdd5e</data>
    </node>
    <node id="&quot;PROMPT ENGINEERING TECHNIQUES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Engineering Techniques encompass various methods and strategies for effectively designing and utilizing prompts in large language models (LLMs)."</data>
      <data key="d2">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;LLM PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"LLM Prompting involves creating and refining prompts to guide the responses of large language models."</data>
      <data key="d2">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;0-SHOT PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"0-Shot Prompt refers to a prompting technique where the model is given a task without any prior examples."</data>
      <data key="d2">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;MANY-SHOT PROMPT&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Many-Shot Prompt involves providing the model with multiple examples to guide its response to a task."</data>
      <data key="d2">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;TRANSFORMER MODEL PROMPTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Transformer Model Prompts are specific prompts designed to work with transformer-based models to elicit desired responses."</data>
      <data key="d2">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;PROMPT-BASED TRANSFER LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt-Based Transfer Learning involves using prompts to transfer knowledge from one domain to another within a model."</data>
      <data key="d2">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;NLP PROMPTING STRATEGIES&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"NLP Prompting Strategies refer to various techniques used in natural language processing to create effective prompts for language models."</data>
      <data key="d2">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;LLM INTERPRETABILITY VIA PROMPTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"LLM Interpretability via Prompts focuses on using prompts to make the behavior and decisions of large language models more understandable."</data>
      <data key="d2">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;CURRICULUM LEARNING WITH PROMPTS&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Curriculum Learning with Prompts involves structuring prompts in a way that gradually increases in complexity to improve model performance."</data>
      <data key="d2">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;FEEDBACK LOOPS IN LLM PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Feedback Loops in LLM Prompting refer to iterative processes where the output of the model is used to refine and improve the prompts."</data>
      <data key="d2">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;HUMAN-IN-THE-LOOP PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Human-in-the-Loop Prompting involves human intervention in the prompt design process to enhance model performance and accuracy."</data>
      <data key="d2">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;TOKEN-EFFICIENT PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Token-Efficient Prompting focuses on creating prompts that use fewer tokens while still achieving effective model responses."</data>
      <data key="d2">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;INSTRUCTION PROMPTING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Instruction Prompting involves creating prompts that provide explicit instructions to the model to perform specific tasks."</data>
      <data key="d2">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;PROMPT TEMPLATING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Prompt Templating refers to the creation of reusable prompt structures that can be adapted for different tasks and models."</data>
      <data key="d2">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </node>
    <node id="&quot;GPT-4&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"GPT-4 is a model mentioned in the context of evaluations and comparisons with other models."</data>
      <data key="d2">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </node>
    <node id="&quot;LLAMA-2-CHAT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"LLaMA-2-Chat is a model mentioned in the context of evaluations and comparisons with other models."</data>
      <data key="d2">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </node>
    <node id="&quot;PALM2&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"PaLM2 is a model mentioned in the context of evaluations and comparisons with other models."</data>
      <data key="d2">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </node>
    <node id="&quot;FALCON&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Falcon is a model mentioned in the context of evaluations and comparisons with other models."</data>
      <data key="d2">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </node>
    <node id="&quot;CLAUDE-V1&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Claude-v1 is a model mentioned in the context of evaluations and comparisons with other models."</data>
      <data key="d2">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </node>
    <node id="&quot;GPT-3.5&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"GPT-3.5 is a model mentioned in the context of evaluations and comparisons with other models."</data>
      <data key="d2">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </node>
    <node id="&quot;CLAUDE-V1.3&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Claude-v1.3 is a model mentioned in the context of evaluations and comparisons with other models."</data>
      <data key="d2">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </node>
    <node id="&quot;EVALUATION PAPER SUMMARY&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Evaluation Paper Summary is a document summarizing the evaluation methods and results of various models."</data>
      <data key="d2">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </node>
    <node id="&quot;DATASET DEVELOPMENT&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Dataset Development involves creating a dataset using specific prompts to train the AI in identifying entrapment."</data>
      <data key="d2">bcb6ef7c52ce001fb19904d1aa92dfd2</data>
    </node>
    <node id="&quot;ANSWER EXTRACTION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Answer Extraction is the process of extracting labels and reasoning from the AI's responses to determine if a post meets the criteria for entrapment."</data>
      <data key="d2">bcb6ef7c52ce001fb19904d1aa92dfd2</data>
    </node>
    <node id="&quot;EMAIL INCLUSION&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Email Inclusion refers to the inclusion of email content in the prompt to see if it affects the AI's performance in identifying entrapment."</data>
      <data key="d2">bcb6ef7c52ce001fb19904d1aa92dfd2</data>
    </node>
    <node id="&quot;PERFORMANCE TESTING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Performance Testing involves evaluating the AI's accuracy and effectiveness in identifying entrapment using different prompts and datasets."</data>
      <data key="d2">bcb6ef7c52ce001fb19904d1aa92dfd2</data>
    </node>
    <node id="&quot;PROMPT DEFINITION&quot;">
      <data key="d0">"GOALS"</data>
      <data key="d1">"Prompt Definition aims to create a formal, mathematical definition for the term 'prompt' to standardize its use in research."</data>
      <data key="d2">bcb6ef7c52ce001fb19904d1aa92dfd2</data>
    </node>
    <node id="&quot;META-LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Meta-Learning captures the inner-loop/outer-loop structure of the general method, encompassing both the possibility of learning new tasks from scratch at inference time and recognizing patterns seen during training."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;ONE-SHOT LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"One-Shot Learning is a form of in-context learning where only one demonstration is provided to the model."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;ZERO-SHOT LEARNING&quot;">
      <data key="d0">"SUBDOMAIN"</data>
      <data key="d1">"Zero-Shot Learning is a form of in-context learning where no demonstrations are provided, and only an instruction in natural language is given to the model."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;BROWN ET AL. (2020)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Brown et al. (2020) is a research group that provided definitions and clarifications for in-context learning and its various forms."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;DONG ET AL. (2023)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Dong et al. (2023) is a research group that provided a formal definition of in-context learning, differing from the broad definition by Brown et al. (2020)."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;DENIS PESKOFF&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Denis Peskoff is an advisor who assisted with paper organization and final review."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;ALEXANDER HOYLE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Alexander Hoyle is an advisor who provided guidance on writing, meta-analysis approach, and ran automated baselines for a case study."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;SHYAMAL ANADKAT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Shyamal Anadkat is an advisor who assisted with the overall review of the paper and the etymology and definitions."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;JULES WHITE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Jules White is an advisor who built trees for technique taxonomies."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;MARINE CARPAUT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Marine Carpaut is an advisor who framed, reviewed, and suggested papers for the multilingual section."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;PHILLIP RESNIK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Phillip Resnik is the Principal Investigator of the team."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;MEGAN L. ROGERS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Megan L. Rogers is part of the SCS Labeling team and reviewed and gave advice for this section."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;INNA GONCEARENCO&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Inna Goncearenco is part of the SCS Labeling team and reviewed and gave advice for this section."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;GIUSEPPE SARLI&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Giuseppe Sarli is part of the SCS Labeling team and reviewed and gave advice for this section."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;IGOR GALYNKER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Igor Galynker is part of the SCS Labeling team and reviewed and gave advice for this section."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;KONSTANTINE KAHADZE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Konstantine Kahadze is the team leader for the Benchmarking section, managed the MMLU benchmarking codebase, and contributed to Security and Meta Analysis."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;ASHAY SRIVASTAVA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Ashay Srivastava is the team leader for the Agents section, reviewed papers for human review, worked on the tool use agents section, and compiled contributions."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;HEVANDER DA COSTA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Hevander Da Costa contributed to the Benchmarking section and Meta Review datasets list, reviewed literature on LLM code generation and prompting techniques, and added literature review content to the Agents section."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;FEILEEN LI&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Feileen Li worked on the tool use agents section and assisted with the human paper review."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;NISHANT BALEPUR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Nishant Balepur is the team leader for the alignment section, helped with high-level discussions in benchmarking, and reviewed drafts."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;SEVIEN SCHULHOFF&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Sevien Schulhoff is the team leader for the security section and contributed to the benchmarking section."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;CHENGLEI SI&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Chenglei Si suggested related works and edited section 2.2 and section 7."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e</data>
    </node>
    <node id="&quot;PRANAV SANDEEP DULEPET&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Pranav Sandeep Dulepet contributed definitions for section 2 and worked on segmentation and object detection in the multimodal section."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e,77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;HYOJUNG HAN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"HyoJung Han contributed to the Multimodal section, especially the speech+text part, and wrote the audio prompting section."</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e,77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;HUDSON TAO&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">Hudson Tao authored sections on image, video, and 3D within multimodal, reviewed papers for human review, maintained the GitHub codebase, and built the project website.</data>
      <data key="d2">7096851583df5cc6ad819323dfd9e83e,77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;AMANDA LIU&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Amanda Liu authored taxonomic ontology sections, conducted background research for the introduction and related work, and developed code pipelines for meta-analysis graphs."</data>
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;SWETA AGRAWAL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Sweta Agrawal was the team lead for the evaluation section."</data>
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;SAURAV VIDYADHARA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Saurav Vidyadhara assisted with general review and revising taxonomy trees."</data>
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;CHAU PHAM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Chau Pham assisted with meta review, including automated analysis of topics."</data>
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;ZOEY KI&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Zoey Ki led the Multilingual prompting section, conducted a review on related papers, and wrote Section 3.1."</data>
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;YINHENG LI&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Yinheng Li worked on section 2.2 text-based techniques, reviewed techniques, and contributed to drafting figure 2.2."</data>
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;SALONI GUPTA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Saloni Gupta wrote tests for paper compilation, helped set up the paper pipeline, and worked on the code diagram and grammar for the paper."</data>
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;GERSON KROIZ&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Gerson Kroiz was involved with section 1.1 and defining a prompt."</data>
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;AAYUSH GUPTA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Aayush Gupta contributed to the Meta Analysis, compiling papers, and generating visualization graphs."</data>
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;MICHAEL ILIE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Michael Ilie was the Co-Lead Author, managed the codebase, ran experiments, collected data, and helped with various sections including the PRISMA review figure and the SCS prompting case study."</data>
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;SANDER SCHULHOFF&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Sander Schulhoff was the Lead Author."</data>
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;MULTIMODAL SECTION&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;TAXONOMIC ONTOLOGY SECTIONS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;EVALUATION SECTION&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;TAXONOMY TREES&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;META REVIEW&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;MULTILINGUAL PROMPTING SECTION&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;SECTION 2.2 TEXT-BASED TECHNIQUES&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;PAPER PIPELINE&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;SECTION 1.1&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;META ANALYSIS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;PRISMA REVIEW FIGURE&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;SCS PROMPTING CASE STUDY&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <node id="&quot;LEAD AUTHOR&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">77d7c813cbd787e0699413f0a945f885</data>
    </node>
    <edge source="&quot;MICROSOFT RESEARCH&quot;" target="&quot;GRAPH RAG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Microsoft Research is involved in developing the Graph RAG approach to enhance query-focused summarization over large text corpora."</data>
      <data key="d6">e7e620f804861b86c33f80a0f61ebb8c</data>
    </edge>
    <edge source="&quot;MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES&quot;" target="&quot;GRAPH RAG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Microsoft Strategic Missions and Technologies collaborates on the Graph RAG approach to improve information retrieval and summarization."</data>
      <data key="d6">e7e620f804861b86c33f80a0f61ebb8c</data>
    </edge>
    <edge source="&quot;MICROSOFT OFFICE OF THE CTO&quot;" target="&quot;GRAPH RAG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Microsoft Office of the CTO supports the technological strategies behind the Graph RAG approach."</data>
      <data key="d6">e7e620f804861b86c33f80a0f61ebb8c</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;RETRIEVAL-AUGMENTED GENERATION (RAG)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG builds upon the principles of RAG to enhance its capabilities for answering global questions."</data>
      <data key="d6">e7e620f804861b86c33f80a0f61ebb8c</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;QUERY-FOCUSED SUMMARIZATION (QFS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG integrates QFS to provide comprehensive answers to user queries over large text corpora."</data>
      <data key="d6">e7e620f804861b86c33f80a0f61ebb8c</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;GLOBAL SENSEMAKING QUESTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG is designed to address global sensemaking questions by summarizing information from entire text corpora."</data>
      <data key="d6">e7e620f804861b86c33f80a0f61ebb8c</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;GRAPH COMMUNITIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG uses graph communities to organize and summarize related entities within a text corpus."</data>
      <data key="d6">e7e620f804861b86c33f80a0f61ebb8c</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;COMMUNITY SUMMARIES&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"Graph RAG" benefits from and generates community summaries, which provide a data index that supports global queries. These community summaries enable "Graph RAG" to provide partial responses to user queries, thereby improving the comprehensiveness and diversity of the answers.</data>
      <data key="d6">1b8338e4644e4c218ad719ee711f9aaa,950afbe992b1be1eb5d912ed068af2a2,e7e620f804861b86c33f80a0f61ebb8c</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;GLOBAL ANSWER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG produces a global answer by summarizing all partial responses from community summaries."</data>
      <data key="d6">e7e620f804861b86c33f80a0f61ebb8c</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;HUMAN SENSEMAKING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG supports human sensemaking by enabling comprehensive understanding of large text corpora."</data>
      <data key="d6">e7e620f804861b86c33f80a0f61ebb8c</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;PODCAST DATASET&quot;">
      <data key="d4">91.0</data>
      <data key="d5">Graph RAG is a system designed to generate answers using the Podcast Dataset. It leverages the Podcast Dataset to evaluate its performance in producing comprehensive and diverse responses. By utilizing this dataset, Graph RAG aims to ensure the quality and variety of the answers it generates.</data>
      <data key="d6">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;NEWS DATASET&quot;">
      <data key="d4">91.0</data>
      <data key="d5">Graph RAG is used to generate answers from the News Dataset. It utilizes the News Dataset to evaluate its performance in generating comprehensive and diverse responses.</data>
      <data key="d6">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;GPT-4-TURBO&quot;">
      <data key="d4">90.0</data>
      <data key="d5">"Graph RAG leverages GPT-4-turbo for its large context size to improve information retrieval and response generation."</data>
      <data key="d6">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;TEXT SUMMARIZATION (TS)&quot;">
      <data key="d4">2.0</data>
      <data key="d5">Graph RAG and Text Summarization (TS) are methods compared in the analysis. Text Summarization (TS) is compared against Graph RAG in the analysis.</data>
      <data key="d6">993f1cfa34b5b04498b9edf3b5aaeddf,da636ab056625c618d1656cfc725630c</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;SEMANTIC SEARCH (SS)&quot;">
      <data key="d4">2.0</data>
      <data key="d5">Graph RAG and Semantic Search (SS) are methods compared in the analysis. Semantic Search (SS) is compared against Graph RAG in the analysis.</data>
      <data key="d6">993f1cfa34b5b04498b9edf3b5aaeddf,da636ab056625c618d1656cfc725630c</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;C0&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"C0 is a root-level community summary method used within the Graph RAG approach."</data>
      <data key="d6">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;C1&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"C1 is a high-level community summary method used within the Graph RAG approach."</data>
      <data key="d6">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;C2&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"C2 is an intermediate-level community summary method used within the Graph RAG approach."</data>
      <data key="d6">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;C3&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"C3 is a low-level community summary method used within the Graph RAG approach."</data>
      <data key="d6">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;ENTERTAINMENT ARTICLES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG generates comprehensive lists of public figures mentioned in entertainment articles, providing detailed information about their contributions and influence."</data>
      <data key="d6">1db44fe0d276fa7a87d3f5087dd0bffe</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;NAIVE RAG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG is compared with Naive RAG, showing improvements in comprehensiveness and diversity."</data>
      <data key="d6">0938d71f3b2047c82d1dd9d7d952808b</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;COMPREHENSIVENESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG aims to improve comprehensiveness in the answers generated from datasets."</data>
      <data key="d6">0938d71f3b2047c82d1dd9d7d952808b</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;DIVERSITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG aims to improve diversity in the answers generated from datasets."</data>
      <data key="d6">0938d71f3b2047c82d1dd9d7d952808b</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;GLOBAL TEXT SUMMARIZATION (TS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Global Text Summarization (TS) is compared with Graph RAG, showing slight improvements in answer comprehensiveness and diversity."</data>
      <data key="d6">0938d71f3b2047c82d1dd9d7d952808b</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;SOURCE TEXTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG compares community summaries to source texts to evaluate performance in comprehensiveness and diversity."</data>
      <data key="d6">1b8338e4644e4c218ad719ee711f9aaa</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;EMPOWERMENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Empowerment comparisons showed mixed results for Graph RAG approaches versus source text summarization."</data>
      <data key="d6">1b8338e4644e4c218ad719ee711f9aaa</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;ITERATIVE QUESTION ANSWERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG offers a highly efficient method for iterative question answering, retaining advantages in comprehensiveness and diversity."</data>
      <data key="d6">1b8338e4644e4c218ad719ee711f9aaa</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;SELFMEM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG incorporates the concept of self-memory (Selfmem) for generation-augmented retrieval."</data>
      <data key="d6">36b3475f15d02b229d4190b0b401085f</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;GAR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG uses generation-augmented retrieval (GAR) to facilitate future generation cycles."</data>
      <data key="d6">36b3475f15d02b229d4190b0b401085f</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;ITER-RETGEN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG employs iterative retrieval-generation strategies (Iter-RetGen)."</data>
      <data key="d6">36b3475f15d02b229d4190b0b401085f</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;FEB4RAG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG uses federated retrieval-generation strategies (FeB4RAG)."</data>
      <data key="d6">36b3475f15d02b229d4190b0b401085f</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;RAPTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG's hierarchical index and summarization bear resemblance to RAPTOR's approach of generating a hierarchical index of text chunks."</data>
      <data key="d6">36b3475f15d02b229d4190b0b401085f</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;KAPING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG is an advanced RAG system similar to KAPING, where the index is a knowledge graph."</data>
      <data key="d6">36b3475f15d02b229d4190b0b401085f</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;G-RETRIEVER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG and G-Retriever both use subsets of the graph structure as objects of enquiry."</data>
      <data key="d6">36b3475f15d02b229d4190b0b401085f</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;GRAPH-TOOLFORMER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG and Graph-ToolFormer both use derived graph metrics as objects of enquiry."</data>
      <data key="d6">36b3475f15d02b229d4190b0b401085f</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;SURGE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG and SURGE both ground narrative outputs in the facts of retrieved subgraphs."</data>
      <data key="d6">36b3475f15d02b229d4190b0b401085f</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;FABULA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG and FABULA both serialize retrieved event-plot subgraphs using narrative templates."</data>
      <data key="d6">36b3475f15d02b229d4190b0b401085f</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;GRAPH INDEX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG utilizes a graph index to organize and retrieve information efficiently, enhancing the retrieval-augmented generation process."</data>
      <data key="d6">950afbe992b1be1eb5d912ed068af2a2</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;HYBRID RAG SCHEMES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG can be enhanced by hybrid RAG schemes that combine embedding-based matching with map-reduce summarization mechanisms."</data>
      <data key="d6">950afbe992b1be1eb5d912ed068af2a2</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;EXPLORATORY DRILL DOWN MECHANISM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph RAG can incorporate an exploratory drill down mechanism to follow information scent in higher-level community summaries."</data>
      <data key="d6">950afbe992b1be1eb5d912ed068af2a2</data>
    </edge>
    <edge source="&quot;GRAPH RAG&quot;" target="&quot;OPENORD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains involve the use of graph-based techniques, whether for large graph layout or retrieval-augmented generation with knowledge graphs."</data>
      <data key="d6">67b93b22eef87b628b69ad5e0872d3dc</data>
    </edge>
    <edge source="&quot;RETRIEVAL-AUGMENTED GENERATION (RAG)&quot;" target="&quot;QUERY-FOCUSED SUMMARIZATION (QFS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RAG is compared to QFS as an approach to answering user questions, with QFS being more appropriate for generating natural language summaries."</data>
      <data key="d6">55ec70780a07388aca4be80802ea19f1</data>
    </edge>
    <edge source="&quot;GRAPH COMMUNITIES&quot;" target="&quot;LEIDEN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Leiden is used to detect Graph Communities, enabling the identification of hierarchical community structures."</data>
      <data key="d6">c5a27b7f9fad18a6ad22416c453ae383</data>
    </edge>
    <edge source="&quot;GRAPH COMMUNITIES&quot;" target="&quot;MULTIHOP-RAG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph Communities are detected within the MultiHop-RAG dataset using the Leiden algorithm."</data>
      <data key="d6">c5a27b7f9fad18a6ad22416c453ae383</data>
    </edge>
    <edge source="&quot;GRAPH COMMUNITIES&quot;" target="&quot;COMMUNITY SUMMARIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Community Summaries are created to describe the structure and semantics of Graph Communities."</data>
      <data key="d6">c5a27b7f9fad18a6ad22416c453ae383</data>
    </edge>
    <edge source="&quot;COMMUNITY SUMMARIES&quot;" target="&quot;GLOBAL ANSWER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Global Answer is generated using Community Summaries from different levels of hierarchical community structures."</data>
      <data key="d6">c5a27b7f9fad18a6ad22416c453ae383</data>
    </edge>
    <edge source="&quot;COMMUNITY SUMMARIES&quot;" target="&quot;ELEMENT SUMMARIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Element Summaries are prioritized and included in Community Summaries to provide detailed descriptions."</data>
      <data key="d6">c5a27b7f9fad18a6ad22416c453ae383</data>
    </edge>
    <edge source="&quot;COMMUNITY SUMMARIES&quot;" target="&quot;COMMUNITY ANSWERS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"COMMUNITY SUMMARIES" are divided into chunks, and "COMMUNITY ANSWERS" are generated for each chunk. These "Community Answers" are then compiled from the "Community Summaries" to form the final global answer.</data>
      <data key="d6">c5a27b7f9fad18a6ad22416c453ae383,da636ab056625c618d1656cfc725630c</data>
    </edge>
    <edge source="&quot;COMMUNITY SUMMARIES&quot;" target="&quot;GLOBAL SUMMARIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Global Summarization uses Community Summaries to understand the global structure of the dataset."</data>
      <data key="d6">c5a27b7f9fad18a6ad22416c453ae383</data>
    </edge>
    <edge source="&quot;COMMUNITY SUMMARIES&quot;" target="&quot;PODCAST DATASET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Podcast Dataset is used to create community summaries at different levels of the graph community hierarchy."</data>
      <data key="d6">1b8338e4644e4c218ad719ee711f9aaa</data>
    </edge>
    <edge source="&quot;COMMUNITY SUMMARIES&quot;" target="&quot;NEWS DATASET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The News Dataset is used to create community summaries at different levels of the graph community hierarchy."</data>
      <data key="d6">1b8338e4644e4c218ad719ee711f9aaa</data>
    </edge>
    <edge source="&quot;GLOBAL ANSWER&quot;" target="&quot;COMMUNITY ANSWERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Intermediate community answers are sorted by helpfulness score to generate the global answer."</data>
      <data key="d6">da636ab056625c618d1656cfc725630c</data>
    </edge>
    <edge source="&quot;HUMAN SENSEMAKING&quot;" target="&quot;SCIENTIFIC DISCOVERY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Human sensemaking is crucial for scientific discovery, as it helps in understanding complex information and drawing new insights."</data>
      <data key="d6">e7e620f804861b86c33f80a0f61ebb8c</data>
    </edge>
    <edge source="&quot;HUMAN SENSEMAKING&quot;" target="&quot;INTELLIGENCE ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Human sensemaking is essential for intelligence analysis, aiding in the interpretation of information to support decision-making."</data>
      <data key="d6">e7e620f804861b86c33f80a0f61ebb8c</data>
    </edge>
    <edge source="&quot;NA&#207;VE RAG&quot;" target="&quot;ENTERTAINMENT ARTICLES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Na&#239;ve RAG generates lists of public figures mentioned in entertainment articles, focusing more on their personal lives and relationships."</data>
      <data key="d6">1db44fe0d276fa7a87d3f5087dd0bffe</data>
    </edge>
    <edge source="&quot;NA&#207;VE RAG&quot;" target="&quot;GLOBAL APPROACHES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Global Approaches consistently outperform Na&#239;ve RAG in both comprehensiveness and diversity metrics."</data>
      <data key="d6">1b8338e4644e4c218ad719ee711f9aaa</data>
    </edge>
    <edge source="&quot;PODCAST DATASET&quot;" target="&quot;GRAPH INDEXING PROCESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Podcast Dataset is used in the graph indexing process with specific context window size and gleaning settings."</data>
      <data key="d6">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </edge>
    <edge source="&quot;NEWS DATASET&quot;" target="&quot;WASHINGTON&quot;">
      <data key="d4">90.0</data>
      <data key="d5">"Washington is mentioned in the News Dataset as a location influencing decision-making processes."</data>
      <data key="d6">0274e77e2fcec8973c9768c464c6e82d,08e6ee9b2e040693136d0d8e0acfb8dd,0938d71f3b2047c82d1dd9d7d952808b,0b1362066be4992987aeec37198a7788,153eeb5a63e650f2cd12f700ffe3e71f,18e3009014a13d95897da5ec358ca2e1,1a997c6aadeaeb3b5ad0a4c3ce835540,1b8338e4644e4c218ad719ee711f9aaa,1db44fe0d276fa7a87d3f5087dd0bffe,1e97679db415c7c17b35542305f23ced,22a657737fd9e20b7803d916867d487b,27d8fe15ab6f9e3d91fd5858fbeba7ea,29d2b14a56a51f86baa34264697bdd5e,2dba3160cd0e0ee3943dce308cb9940e,2f28d2ed61c6111fccc81e48e659b599,314fa72b9f7876258bd98d75a005cdb7,36b3475f15d02b229d4190b0b401085f,3fd8f6dcbbf1eecd6efb01ea12538679,42397dc5d60f0a1d799e06290ea52864,4257f30018a4acf2e8ee95f21de8d7df,42d8c3ad092ec18e28ff718709b0b472,42fa2868f275e1b0f2269e560e9a5816,45c77c52a93a949222fda99a95e0c3d6,4aea5d43ff4f1164f45ae3b5b8b7a115,4d9e8d703c2da8e4775c428e83e87fc9,50ddbd22a4e5bd636c4c51a5e5756ae3,520bb3073a4c18baf121407c691ffe87,55ec70780a07388aca4be80802ea19f1,589a9782efd8ac3ff7d79dba07974e2b,590db3ee59b442c908a9b425a9be2477,5ce40e1d59b740ff17256ed5abebf613,5ce886e06455eadec4bcfe91e36b666d,5d5844de9a93093f225ca41ba18f9a89,5fefd7cd7acb9cfd1bfb1118691c8546,630ee831daa753234a258274d318509e,6430817c08b3a5c6d193478d4c739d79,67b93b22eef87b628b69ad5e0872d3dc,6e1dce58f4a3793b65d09171ea5bd3a6,6edacbda20b2fdd4077246c7b271a8b5,7096851583df5cc6ad819323dfd9e83e,7798b3210a865e03a3298ca49ad77cc4,77d7c813cbd787e0699413f0a945f885,7e3b559c2a22f7f23f4eecc37ed7b8e4,82d58329a3cd23550be3e22f1740f8ae,83a60257c9adae8c826e73ef32d16dd0,83e773afec09e119882fe15dd253e724,84da286ab749b0f025821313fe535d70,8bafc5999ce3abba6f261770c5945604,8d9142b3f9039788061b6ce1815078fd,93ab5f14aa5b97d57952be648f337b10,950afbe992b1be1eb5d912ed068af2a2,981e367f454fd6805ff2ad123c75b85e,993f1cfa34b5b04498b9edf3b5aaeddf,9b0bcd8647bcff907e9bcf962a013b91,9f0f4b7adda7eade3a9a430f6b8782dd,a4eb2fbdea1494d271ebc61219d17020,a86e659dcd136358e7557eb5f98c1b58,afacb1e7edc1e6be7b4b3776676a32e9,affd113b11a3fddad82e265af562d9a7,b363fca358c69a9412b955c53352ea9a,ba0d350eede3e5a4dfd1b9b0693b9b94,bcb6ef7c52ce001fb19904d1aa92dfd2,c0bdb410b028f870b1c2869f26dd7c52,c28998cdf87522d883979f9c6405f535,c5a27b7f9fad18a6ad22416c453ae383,c605e4f0158f18be68214a39b9b54154,c7285f7847ef45ed85779d7966753855,ca2bcd796327d014f9e7738468b6b00d,cbd06bb38a855be4a07883f499014eaa,ccdfd3415647f13f577d728a5a0256b1,cd60cb17b3864e9fcc7266ff4c1611ce,d1af61f77ad6f49034ffa4e834a77faf,d27160d0dde304425ccc51df673321b1,d305fc89f77daeb9c5be3a3d126223ed,d397224fef0666e16112e5d47a2e1139,da636ab056625c618d1656cfc725630c,db67f52733fb9d41d13be7cefaa1dae0,dd792fdfac5a64bb840e3680fe40eeb3,de0fbfe367c5921e80c093f91d589919,e5878afbfbf5194f1da3540eaa88fe65,e7e620f804861b86c33f80a0f61ebb8c,e845d3c15484b3061e3a376fa8779883,e8bf483fffcc91b1512c5796d0d4045a,eba1ab13141790dedb88f55494236682,ebba9603b39b6606ba9902c9cf61fecb,eeb46213e40cc8603a2037766f312338,eed969adf8c7eb4a89355c851663c87a,f1e2d01b4dbcfc34401e7d0dffd14e29,f4b740e8b0c84e29c7990fc370919464,ff7ad60eb931a85ac1b0393ecafb8018</data>
    </edge>
    <edge source="&quot;NEWS DATASET&quot;" target="&quot;GRAPH INDEXING PROCESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"News Dataset is used in the graph indexing process with specific context window size and gleaning settings."</data>
      <data key="d6">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </edge>
    <edge source="&quot;RETRIEVAL-AUGMENTED GENERATION&quot;" target="&quot;DEMONSTRATE-SEARCH-PREDICT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains involve the use of retrieval techniques to enhance language models for knowledge-intensive NLP tasks."</data>
      <data key="d6">67b93b22eef87b628b69ad5e0872d3dc</data>
    </edge>
    <edge source="&quot;RETRIEVAL-AUGMENTED GENERATION&quot;" target="&quot;SCIENTIFIC DISCOVERY WITH GPT-4&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains involve the use of large language models to enhance scientific discovery and knowledge-intensive tasks."</data>
      <data key="d6">67b93b22eef87b628b69ad5e0872d3dc</data>
    </edge>
    <edge source="&quot;RETRIEVAL-AUGMENTED GENERATION&quot;" target="&quot;CROSSLINGUAL RETRIEVAL AUGMENTED IN-CONTEXT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both techniques involve the use of retrieval to enhance the performance of language models in different contexts."</data>
      <data key="d6">5ce40e1d59b740ff17256ed5abebf613</data>
    </edge>
    <edge source="&quot;COMPREHENSIVENESS&quot;" target="&quot;LLM EVALUATOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LLM Evaluator assesses answers based on the comprehensiveness metric."</data>
      <data key="d6">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </edge>
    <edge source="&quot;COMPREHENSIVENESS&quot;" target="&quot;NAIVE RAG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Naive RAG is evaluated for its comprehensiveness in generating answers."</data>
      <data key="d6">0938d71f3b2047c82d1dd9d7d952808b</data>
    </edge>
    <edge source="&quot;COMPREHENSIVENESS&quot;" target="&quot;CONTEXT WINDOW SIZE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Different context window sizes are tested to determine their effect on comprehensiveness."</data>
      <data key="d6">0938d71f3b2047c82d1dd9d7d952808b</data>
    </edge>
    <edge source="&quot;COMPREHENSIVENESS&quot;" target="&quot;EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Evaluation assesses the comprehensiveness of answers generated by different RAG methods."</data>
      <data key="d6">0938d71f3b2047c82d1dd9d7d952808b</data>
    </edge>
    <edge source="&quot;DIVERSITY&quot;" target="&quot;LLM EVALUATOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LLM Evaluator assesses answers based on the diversity metric."</data>
      <data key="d6">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </edge>
    <edge source="&quot;DIVERSITY&quot;" target="&quot;NAIVE RAG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Naive RAG is evaluated for its diversity in generating answers."</data>
      <data key="d6">0938d71f3b2047c82d1dd9d7d952808b</data>
    </edge>
    <edge source="&quot;DIVERSITY&quot;" target="&quot;CONTEXT WINDOW SIZE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Different context window sizes are tested to determine their effect on diversity."</data>
      <data key="d6">0938d71f3b2047c82d1dd9d7d952808b</data>
    </edge>
    <edge source="&quot;DIVERSITY&quot;" target="&quot;EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Evaluation assesses the diversity of answers generated by different RAG methods."</data>
      <data key="d6">0938d71f3b2047c82d1dd9d7d952808b</data>
    </edge>
    <edge source="&quot;TRANSFORMER ARCHITECTURE&quot;" target="&quot;LARGE LANGUAGE MODELS (LLMS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The transformer architecture has contributed to the development and performance of LLMs in various summarization tasks."</data>
      <data key="d6">55ec70780a07388aca4be80802ea19f1</data>
    </edge>
    <edge source="&quot;GRAPH RAG APPROACH&quot;" target="&quot;GLOBAL SUMMARIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Graph RAG approach is designed to achieve global summarization by leveraging the modularity of graphs and community detection algorithms."</data>
      <data key="d6">55ec70780a07388aca4be80802ea19f1</data>
    </edge>
    <edge source="&quot;GRAPH RAG APPROACH&quot;" target="&quot;COMMUNITY DETECTION ALGORITHMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Community detection algorithms are used in the Graph RAG approach to partition graphs into modular communities, aiding in the summarization process."</data>
      <data key="d6">55ec70780a07388aca4be80802ea19f1</data>
    </edge>
    <edge source="&quot;GRAPH RAG APPROACH&quot;" target="&quot;MAP-REDUCE APPROACH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The map-reduce approach is a key component of the Graph RAG approach, enabling the summarization of entire corpora by combining community summaries."</data>
      <data key="d6">55ec70780a07388aca4be80802ea19f1</data>
    </edge>
    <edge source="&quot;GRAPH RAG APPROACH&quot;" target="&quot;EVALUATION OF GRAPH RAG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The evaluation of Graph RAG involved testing its performance on real-world datasets and comparing it to other summarization approaches."</data>
      <data key="d6">55ec70780a07388aca4be80802ea19f1</data>
    </edge>
    <edge source="&quot;GRAPH RAG APPROACH&quot;" target="&quot;TEXT CHUNKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Graph RAG Approach processes Text Chunks to extract entities and relationships."</data>
      <data key="d6">1e97679db415c7c17b35542305f23ced</data>
    </edge>
    <edge source="&quot;GRAPH RAG APPROACH&quot;" target="&quot;GRAPH INDEX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Graph RAG Approach creates a Graph Index from the extracted entities and relationships."</data>
      <data key="d6">1e97679db415c7c17b35542305f23ced</data>
    </edge>
    <edge source="&quot;GLOBAL SUMMARIZATION&quot;" target="&quot;QUERY-FOCUSED SUMMARIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Query-Focused Summarization is a specific application of Global Summarization to answer user queries."</data>
      <data key="d6">c5a27b7f9fad18a6ad22416c453ae383</data>
    </edge>
    <edge source="&quot;TEXT CHUNKS&quot;" target="&quot;LLM PROMPTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LLM Prompts are applied to Text Chunks to identify entities and relationships."</data>
      <data key="d6">1e97679db415c7c17b35542305f23ced</data>
    </edge>
    <edge source="&quot;TEXT CHUNKS&quot;" target="&quot;ELEMENT INSTANCES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Text Chunks are processed to extract Element Instances, which include entities and relationships."</data>
      <data key="d6">1e97679db415c7c17b35542305f23ced</data>
    </edge>
    <edge source="&quot;TEXT CHUNKS&quot;" target="&quot;HOTPOTQA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"HotPotQA dataset is used to demonstrate the effectiveness of different Text Chunk sizes in entity extraction."</data>
      <data key="d6">1e97679db415c7c17b35542305f23ced</data>
    </edge>
    <edge source="&quot;LLM PROMPTS&quot;" target="&quot;GLEANINGS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gleanings are additional rounds of LLM Prompts to ensure all entities are extracted."</data>
      <data key="d6">1e97679db415c7c17b35542305f23ced</data>
    </edge>
    <edge source="&quot;ELEMENT INSTANCES&quot;" target="&quot;ELEMENT SUMMARIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Element Instances are summarized into Element Summaries to create a coherent graph structure."</data>
      <data key="d6">1e97679db415c7c17b35542305f23ced</data>
    </edge>
    <edge source="&quot;HOTPOTQA&quot;" target="&quot;MULTIHOP-RAG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both HotPotQA and MultiHop-RAG are benchmark datasets for open-domain question answering."</data>
      <data key="d6">da636ab056625c618d1656cfc725630c</data>
    </edge>
    <edge source="&quot;HOTPOTQA&quot;" target="&quot;MT-BENCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both MT-Bench and HotPotQA are benchmark datasets for open-domain question answering."</data>
      <data key="d6">da636ab056625c618d1656cfc725630c</data>
    </edge>
    <edge source="&quot;HOTPOTQA&quot;" target="&quot;EXPLAINABLE MULTI-HOP QUESTION ANSWERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"HotpotQA is a dataset created to achieve the goal of explainable multi-hop question answering."</data>
      <data key="d6">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </edge>
    <edge source="&quot;LEIDEN&quot;" target="&quot;HIERARCHICAL COMMUNITY STRUCTURE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Leiden algorithm helps in recovering Hierarchical Community Structure in large-scale graphs."</data>
      <data key="d6">c5a27b7f9fad18a6ad22416c453ae383</data>
    </edge>
    <edge source="&quot;MULTIHOP-RAG&quot;" target="&quot;NEWS ARTICLES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The news articles dataset is part of the MultiHop-RAG benchmark for open-domain question answering."</data>
      <data key="d6">da636ab056625c618d1656cfc725630c</data>
    </edge>
    <edge source="&quot;MULTIHOP-RAG&quot;" target="&quot;TANG, Y. AND YANG, Y.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Tang and Yang developed the MultiHop-RAG benchmark for retrieval-augmented generation for multi-hop queries."</data>
      <data key="d6">8d9142b3f9039788061b6ce1815078fd</data>
    </edge>
    <edge source="&quot;PODCAST TRANSCRIPTS&quot;" target="&quot;KEVIN SCOTT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Kevin Scott is a participant in the podcast conversations compiled in the podcast transcripts dataset."</data>
      <data key="d6">da636ab056625c618d1656cfc725630c</data>
    </edge>
    <edge source="&quot;LLM EVALUATOR&quot;" target="&quot;EMPOWERMENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LLM Evaluator assesses answers based on the empowerment metric."</data>
      <data key="d6">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </edge>
    <edge source="&quot;LLM EVALUATOR&quot;" target="&quot;DIRECTNESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LLM Evaluator assesses answers based on the directness metric."</data>
      <data key="d6">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </edge>
    <edge source="&quot;LLM EVALUATOR&quot;" target="&quot;EVALUATION PROCESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LLM Evaluator is used in the evaluation process to assess answers based on various metrics."</data>
      <data key="d6">993f1cfa34b5b04498b9edf3b5aaeddf</data>
    </edge>
    <edge source="&quot;EMPOWERMENT&quot;" target="&quot;EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Evaluation assesses the empowerment provided by answers generated by different RAG methods."</data>
      <data key="d6">0938d71f3b2047c82d1dd9d7d952808b</data>
    </edge>
    <edge source="&quot;EMPOWERMENT&quot;" target="&quot;GLOBAL APPROACHES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Empowerment comparisons showed mixed results for global approaches versus Na&#239;ve RAG."</data>
      <data key="d6">1b8338e4644e4c218ad719ee711f9aaa</data>
    </edge>
    <edge source="&quot;TAYLOR SWIFT&quot;" target="&quot;PUBLIC DISCOURSE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Taylor Swift's activities and contributions significantly influence public discourse, as evidenced by frequent mentions in entertainment articles."</data>
      <data key="d6">1db44fe0d276fa7a87d3f5087dd0bffe</data>
    </edge>
    <edge source="&quot;TRAVIS KELCE&quot;" target="&quot;PUBLIC DISCOURSE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Travis Kelce's achievements in sports and high-profile status contribute to public discourse, as reflected in entertainment articles."</data>
      <data key="d6">1db44fe0d276fa7a87d3f5087dd0bffe</data>
    </edge>
    <edge source="&quot;BRITNEY SPEARS&quot;" target="&quot;PUBLIC DISCOURSE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Britney Spears' impact on music and her personal life are central to public discourse, as highlighted in entertainment articles."</data>
      <data key="d6">1db44fe0d276fa7a87d3f5087dd0bffe</data>
    </edge>
    <edge source="&quot;JUSTIN TIMBERLAKE&quot;" target="&quot;PUBLIC DISCOURSE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Justin Timberlake's contributions to music and his high-profile personal life influence public discourse, as shown in entertainment articles."</data>
      <data key="d6">1db44fe0d276fa7a87d3f5087dd0bffe</data>
    </edge>
    <edge source="&quot;CONTEXT WINDOW SIZE&quot;" target="&quot;CONFIGURATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Configuration involves testing different context window sizes to find the optimal setting."</data>
      <data key="d6">0938d71f3b2047c82d1dd9d7d952808b</data>
    </edge>
    <edge source="&quot;EVALUATION&quot;" target="&quot;PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Evaluation is a goal within the field of prompting, essential for assessing the effectiveness of different techniques."</data>
      <data key="d6">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </edge>
    <edge source="&quot;EVALUATION&quot;" target="&quot;DEMONSTRATE-SEARCH-PREDICT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Demonstrate-Search-Predict is a methodology used to decompose questions and combine responses, which is relevant to the goal of evaluation in assessing quality based on defined metrics."</data>
      <data key="d6">c28998cdf87522d883979f9c6405f535</data>
    </edge>
    <edge source="&quot;EVALUATION&quot;" target="&quot;INTERLEAVED RETRIEVAL GUIDED BY CHAIN-OF-THOUGHT (IRCOT)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"IRCoT's technique of interleaving CoT and retrieval is relevant to evaluation as it helps in planning reasoning steps, which is crucial for robust evaluation."</data>
      <data key="d6">c28998cdf87522d883979f9c6405f535</data>
    </edge>
    <edge source="&quot;EVALUATION&quot;" target="&quot;FORWARD-LOOKING ACTIVE RETRIEVAL AUGMENTED GENERATION (FLARE)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"FLARE's iterative retrieval process can be applied to evaluation by continuously refining the content plan and incorporating external knowledge, enhancing the evaluation process."</data>
      <data key="d6">c28998cdf87522d883979f9c6405f535</data>
    </edge>
    <edge source="&quot;EVALUATION&quot;" target="&quot;IMITATE, RETRIEVE, PARAPHRASE (IRP)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"IRP's iterative retrieval technique is relevant to evaluation as it involves generating and refining content, which can improve the accuracy and consistency of evaluations."</data>
      <data key="d6">c28998cdf87522d883979f9c6405f535</data>
    </edge>
    <edge source="&quot;EVALUATION&quot;" target="&quot;PROMPTING TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompting Techniques are essential components of evaluation frameworks, guiding the LLM in generating accurate and consistent assessments."</data>
      <data key="d6">c28998cdf87522d883979f9c6405f535</data>
    </edge>
    <edge source="&quot;EVALUATION&quot;" target="&quot;IN-CONTEXT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"In-Context Learning is frequently used in evaluation prompts to provide examples that guide the LLM's responses, improving the quality of evaluations."</data>
      <data key="d6">c28998cdf87522d883979f9c6405f535</data>
    </edge>
    <edge source="&quot;EVALUATION&quot;" target="&quot;ROLE-BASED EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Role-based Evaluation improves and diversifies evaluations by using different roles in prompts, which is crucial for robust evaluation frameworks."</data>
      <data key="d6">c28998cdf87522d883979f9c6405f535</data>
    </edge>
    <edge source="&quot;EVALUATION&quot;" target="&quot;CHAIN-OF-THOUGHT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chain-of-Thought prompting guides the LLM through reasoning steps, improving the performance and accuracy of evaluations."</data>
      <data key="d6">c28998cdf87522d883979f9c6405f535</data>
    </edge>
    <edge source="&quot;EVALUATION&quot;" target="&quot;MODEL-GENERATED GUIDELINES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Model-Generated Guidelines reduce the problem of insufficient prompting and result in more consistent and aligned evaluations, enhancing the evaluation process."</data>
      <data key="d6">c28998cdf87522d883979f9c6405f535</data>
    </edge>
    <edge source="&quot;EVALUATION&quot;" target="&quot;AUTOCALIBRATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"AUTOCALIBRATE derives scoring criteria based on expert annotations, refining the evaluation prompt and improving the consistency and accuracy of evaluations."</data>
      <data key="d6">c28998cdf87522d883979f9c6405f535</data>
    </edge>
    <edge source="&quot;EVALUATION&quot;" target="&quot;OUTPUT FORMAT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Output Format of the LLM's response significantly affects evaluation performance, with XML or JSON styling improving the accuracy of judgments."</data>
      <data key="d6">c28998cdf87522d883979f9c6405f535</data>
    </edge>
    <edge source="&quot;SOURCE TEXTS&quot;" target="&quot;MAP-REDUCE SUMMARIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Map-Reduce Summarization is used for summarizing source texts and is the most resource-intensive approach."</data>
      <data key="d6">1b8338e4644e4c218ad719ee711f9aaa</data>
    </edge>
    <edge source="&quot;FEB4RAG&quot;" target="&quot;WANG, S., KHRAMTSOVA, E., ZHUANG, S., AND ZUCCON, G.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Wang and colleagues developed the Feb4rag system for evaluating federated search in the context of retrieval-augmented generation."</data>
      <data key="d6">8d9142b3f9039788061b6ce1815078fd</data>
    </edge>
    <edge source="&quot;FEB4RAG&quot;" target="&quot;EVALUATING FEDERATED SEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feb4rag is directly related to the goal of evaluating federated search in the context of retrieval augmented generation."</data>
      <data key="d6">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </edge>
    <edge source="&quot;CAIRE-COVID&quot;" target="&quot;SU, D., XU, Y., YU, T., SIDDIQUE, F. B., BAREZI, E. J., AND FUNG, P.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Su and colleagues developed the Caire-covid system for question answering and query-focused multi-document summarization for COVID-19 scholarly information management."</data>
      <data key="d6">8d9142b3f9039788061b6ce1815078fd</data>
    </edge>
    <edge source="&quot;DSP&quot;" target="&quot;RETRIEVAL AUGMENTED GENERATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"DSP is an example of a Retrieval Augmented Generation technique."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;RAPTOR&quot;" target="&quot;SARTHI, P., ABDULLAH, S., TULI, A., KHANNA, S., GOLDIE, A., AND MANNING, C. D.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sarthi and colleagues developed the Raptor system for recursive abstractive processing for tree-organized retrieval."</data>
      <data key="d6">8d9142b3f9039788061b6ce1815078fd</data>
    </edge>
    <edge source="&quot;GRAPH-TOOLFORMER&quot;" target="&quot;GRAPH REASONING ABILITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Graph-Toolformer is a subdomain aimed at achieving the goal of empowering large language models with graph reasoning ability."</data>
      <data key="d6">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </edge>
    <edge source="&quot;FABULA&quot;" target="&quot;RANADE, P. AND JOSHI, A.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ranade and Joshi developed the Fabula system for intelligence report generation using retrieval-augmented narrative construction."</data>
      <data key="d6">8d9142b3f9039788061b6ce1815078fd</data>
    </edge>
    <edge source="&quot;LANGCHAIN&quot;" target="&quot;NEO4J&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LangChain supports the Neo4J graph database format."</data>
      <data key="d6">36b3475f15d02b229d4190b0b401085f</data>
    </edge>
    <edge source="&quot;LANGCHAIN&quot;" target="&quot;NEBULAGRAPH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LangChain supports the NebulaGraph database format."</data>
      <data key="d6">36b3475f15d02b229d4190b0b401085f</data>
    </edge>
    <edge source="&quot;LANGCHAIN&quot;" target="&quot;PROMPT ENGINEERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LangChain is a framework that can be used within the broader field of prompt engineering."</data>
      <data key="d6">5ce886e06455eadec4bcfe91e36b666d</data>
    </edge>
    <edge source="&quot;LLAMAINDEX&quot;" target="&quot;NEO4J&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LlamaIndex supports the Neo4J graph database format."</data>
      <data key="d6">36b3475f15d02b229d4190b0b401085f</data>
    </edge>
    <edge source="&quot;LLAMAINDEX&quot;" target="&quot;NEBULAGRAPH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LlamaIndex supports the NebulaGraph database format."</data>
      <data key="d6">36b3475f15d02b229d4190b0b401085f</data>
    </edge>
    <edge source="&quot;LLAMAINDEX&quot;" target="&quot;CHATGPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LlamaIndex could be a tool or method that enhances ChatGPT's indexing capabilities."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;NEO4J&quot;" target="&quot;PROJECT NALLM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Neo4J is the organization behind Project NaLLM, which is available on GitHub."</data>
      <data key="d6">8d9142b3f9039788061b6ce1815078fd</data>
    </edge>
    <edge source="&quot;SELFCHECKGPT&quot;" target="&quot;TREE OF CLARIFICATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains focus on improving the performance and reliability of large language models, particularly in handling ambiguous questions and detecting hallucinations."</data>
      <data key="d6">67b93b22eef87b628b69ad5e0872d3dc</data>
    </edge>
    <edge source="&quot;GLOBAL APPROACH TO GRAPH RAG&quot;" target="&quot;FUTURE WORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The global approach to Graph RAG includes plans for future work to refine and adapt its components for better performance."</data>
      <data key="d6">950afbe992b1be1eb5d912ed068af2a2</data>
    </edge>
    <edge source="&quot;GLOBAL APPROACH TO GRAPH RAG&quot;" target="&quot;OPEN-SOURCE IMPLEMENTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The global approach to Graph RAG will be made accessible through an open-source implementation, allowing for wider adoption and contribution."</data>
      <data key="d6">950afbe992b1be1eb5d912ed068af2a2</data>
    </edge>
    <edge source="&quot;SEQ2SEQ MODELS&quot;" target="&quot;SEQ2SEQ MODELS FOR TEXT SUMMARIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Seq2Seq models are applied to achieve the goal of generating concise summaries of longer text documents."</data>
      <data key="d6">c0bdb410b028f870b1c2869f26dd7c52</data>
    </edge>
    <edge source="&quot;COMMUNITIES IN LARGE NETWORKS&quot;" target="&quot;FAST UNFOLDING OF COMMUNITIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The study of communities in large networks aims to achieve the goal of quickly detecting and analyzing these communities."</data>
      <data key="d6">c0bdb410b028f870b1c2869f26dd7c52</data>
    </edge>
    <edge source="&quot;FEW-SHOT LEARNING&quot;" target="&quot;FEW-SHOT LEARNING IN LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-shot learning techniques are applied to language models to enable them to perform well with minimal labeled data."</data>
      <data key="d6">c0bdb410b028f870b1c2869f26dd7c52</data>
    </edge>
    <edge source="&quot;FEW-SHOT LEARNING&quot;" target="&quot;PROMPTING TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-Shot Learning is one of the prompting techniques frequently cited in research papers."</data>
      <data key="d6">83a60257c9adae8c826e73ef32d16dd0</data>
    </edge>
    <edge source="&quot;FEW-SHOT LEARNING&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-shot learning involves training language models to perform tasks with a limited number of examples."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;FEW-SHOT LEARNING&quot;" target="&quot;MULTILINGUAL FEW-SHOT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multilingual Few-Shot Learning extends the principles of Few-Shot Learning to multiple languages."</data>
      <data key="d6">42d8c3ad092ec18e28ff718709b0b472</data>
    </edge>
    <edge source="&quot;FEW-SHOT LEARNING&quot;" target="&quot;IN-CONTEXT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-Shot Learning is a specific form of In-Context Learning where multiple demonstrations are provided."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;RETRIEVAL-AUGMENTED TEXT GENERATION&quot;" target="&quot;RETRIEVAL-AUGMENTED TEXT GENERATION WITH SELF-MEMORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The subdomain of retrieval-augmented text generation aims to improve models by incorporating self-memory mechanisms."</data>
      <data key="d6">c0bdb410b028f870b1c2869f26dd7c52</data>
    </edge>
    <edge source="&quot;QUESTION-FOCUSED SUMMARIZATION&quot;" target="&quot;EVALUATION OF QUESTION-FOCUSED SUMMARIZATION SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The subdomain of question-focused summarization aims to achieve the goal of assessing the effectiveness of these systems."</data>
      <data key="d6">c0bdb410b028f870b1c2869f26dd7c52</data>
    </edge>
    <edge source="&quot;AUTOMATED EVALUATION OF RETRIEVAL AUGMENTED GENERATION&quot;" target="&quot;AUTOMATED EVALUATION OF RETRIEVAL-AUGMENTED GENERATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The subdomain focuses on developing automated methods to evaluate the performance of retrieval-augmented generation models."</data>
      <data key="d6">c0bdb410b028f870b1c2869f26dd7c52</data>
    </edge>
    <edge source="&quot;COMMUNITY DETECTION IN GRAPHS&quot;" target="&quot;COMMUNITY DETECTION USING DEEP LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The subdomain of community detection in graphs aims to achieve the goal of improving detection and analysis using deep learning techniques."</data>
      <data key="d6">c0bdb410b028f870b1c2869f26dd7c52</data>
    </edge>
    <edge source="&quot;KNOWLEDGE-GROUNDED DIALOGUE GENERATION&quot;" target="&quot;KNOWLEDGE-GROUNDED DIALOGUE GENERATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The subdomain focuses on generating dialogue informed by external knowledge sources to achieve the goal of enhancing conversation relevance and accuracy."</data>
      <data key="d6">c0bdb410b028f870b1c2869f26dd7c52</data>
    </edge>
    <edge source="&quot;KNOWLEDGE-INTENSIVE NLP&quot;" target="&quot;DEMONSTRATE-SEARCH-PREDICT FOR KNOWLEDGE-INTENSIVE NLP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The subdomain of knowledge-intensive NLP aims to achieve the goal of integrating demonstration, search, and prediction techniques to improve task performance."</data>
      <data key="d6">c0bdb410b028f870b1c2869f26dd7c52</data>
    </edge>
    <edge source="&quot;SEQ2SEQ MODELS FOR TEXT SUMMARIZATION&quot;" target="&quot;COLING 2020&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Research on Seq2Seq models for text summarization was presented at the COLING 2020 conference."</data>
      <data key="d6">c0bdb410b028f870b1c2869f26dd7c52</data>
    </edge>
    <edge source="&quot;EVALUATION OF QUESTION-FOCUSED SUMMARIZATION SYSTEMS&quot;" target="&quot;DUC 2005&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The evaluation of question-focused summarization systems was a key topic at the DUC 2005 conference."</data>
      <data key="d6">c0bdb410b028f870b1c2869f26dd7c52</data>
    </edge>
    <edge source="&quot;DEMONSTRATE-SEARCH-PREDICT&quot;" target="&quot;RETRIEVAL AUGMENTED GENERATION (RAG)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Demonstrate-Search-Predict is a technique within the RAG paradigm that decomposes questions into sub-questions and combines their responses."</data>
      <data key="d6">eed969adf8c7eb4a89355c851663c87a</data>
    </edge>
    <edge source="&quot;DEMONSTRATE-SEARCH-PREDICT&quot;" target="&quot;KL SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both KL Systems and Demonstrate-Search-Predict involve the integration of language models with external knowledge sources for enhanced NLP tasks."</data>
      <data key="d6">eeb46213e40cc8603a2037766f312338</data>
    </edge>
    <edge source="&quot;DEMONSTRATE-SEARCH-PREDICT&quot;" target="&quot;DSPY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Demonstrate-Search-Predict and DSPY involve the use of language models for knowledge-intensive tasks and self-improving pipelines."</data>
      <data key="d6">eeb46213e40cc8603a2037766f312338</data>
    </edge>
    <edge source="&quot;SENSEMAKING&quot;" target="&quot;TALKING DATASETS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains involve understanding and making sense of complex information, particularly in the context of data sensemaking behaviors."</data>
      <data key="d6">67b93b22eef87b628b69ad5e0872d3dc</data>
    </edge>
    <edge source="&quot;RECURRENT MEMORY&quot;" target="&quot;HIERARCHICAL TRANSFORMERS FOR MULTI-DOCUMENT SUMMARIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains involve advanced techniques for handling large amounts of information, whether through recurrent memory or hierarchical transformers."</data>
      <data key="d6">67b93b22eef87b628b69ad5e0872d3dc</data>
    </edge>
    <edge source="&quot;LANGCHAIN GRAPHS&quot;" target="&quot;LLAMAINDEX KNOWLEDGE GRAPH INDEX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains involve the use of graphs within specific frameworks (LangChain and LlamaIndex) for various applications."</data>
      <data key="d6">67b93b22eef87b628b69ad5e0872d3dc</data>
    </edge>
    <edge source="&quot;QUERY FOCUSED ABSTRACTIVE SUMMARIZATION&quot;" target="&quot;DOMAIN ADAPTATION WITH PRE-TRAINED TRANSFORMERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains involve the use of transformer models for summarization tasks, with a focus on query relevance and domain adaptation."</data>
      <data key="d6">67b93b22eef87b628b69ad5e0872d3dc</data>
    </edge>
    <edge source="&quot;MICROSOFT&quot;" target="&quot;GENERATIVE ARTIFICIAL INTELLIGENCE (GENAI)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Microsoft is involved in the research and development of GenAI systems and prompting techniques."</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf</data>
    </edge>
    <edge source="&quot;LLAMA 2&quot;" target="&quot;TOUVRON, H., MARTIN, L., STONE, K., ALBERT, P., ALMAHAIRI, A., BABAEI, Y., BASHLYKOV, N., BATRA, S., BHARGAVA, P., BHOSALE, S., ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Touvron and colleagues developed the Llama 2 open foundation and fine-tuned chat models."</data>
      <data key="d6">8d9142b3f9039788061b6ce1815078fd</data>
    </edge>
    <edge source="&quot;ENHANCING KNOWLEDGE GRAPH CONSTRUCTION USING LARGE LANGUAGE MODELS&quot;" target="&quot;TRAJANOSKA, M., STOJANOV, R., AND TRAJANOV, D.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Trajanoska and colleagues researched enhancing knowledge graph construction using large language models."</data>
      <data key="d6">8d9142b3f9039788061b6ce1815078fd</data>
    </edge>
    <edge source="&quot;INTERLEAVING RETRIEVAL WITH CHAIN-OF-THOUGHT REASONING FOR KNOWLEDGE-INTENSIVE MULTI-STEP QUESTIONS&quot;" target="&quot;TRIVEDI, H., BALASUBRAMANIAN, N., KHOT, T., AND SABHARWAL, A.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Trivedi and colleagues researched interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions."</data>
      <data key="d6">8d9142b3f9039788061b6ce1815078fd</data>
    </edge>
    <edge source="&quot;IS CHATGPT A GOOD NLG EVALUATOR?&quot;" target="&quot;WANG, J., LIANG, Y., MENG, F., SUN, Z., SHI, H., LI, Z., XU, J., QU, J., AND ZHOU, J.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Wang and colleagues conducted a preliminary study to evaluate if ChatGPT is a good natural language generation evaluator."</data>
      <data key="d6">8d9142b3f9039788061b6ce1815078fd</data>
    </edge>
    <edge source="&quot;KNOWLEDGE GRAPH PROMPTING&quot;" target="&quot;MULTI-DOCUMENT QUESTION ANSWERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Knowledge Graph Prompting is used to achieve the goal of multi-document question answering."</data>
      <data key="d6">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </edge>
    <edge source="&quot;TEXT SUMMARIZATION WITH LATENT QUERIES&quot;" target="&quot;TEXT SUMMARIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Text Summarization with Latent Queries is a method used to achieve the goal of text summarization."</data>
      <data key="d6">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </edge>
    <edge source="&quot;KNOWLEDGE GRAPH COMPLETION&quot;" target="&quot;KNOWLEDGE GRAPH COMPLETION WITH LLMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Knowledge Graph Completion is a subdomain that explores the goal of completing knowledge graphs with large language models."</data>
      <data key="d6">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </edge>
    <edge source="&quot;CAUSAL GRAPH DISCOVERY&quot;" target="&quot;CAUSAL GRAPH DISCOVERY WITH LLMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Causal Graph Discovery is a subdomain that focuses on the goal of discovering causal graphs using large language models."</data>
      <data key="d6">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </edge>
    <edge source="&quot;LLM-AS-A-JUDGE&quot;" target="&quot;JUDGING CAPABILITIES OF LLMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LLM-as-a-Judge is a subdomain evaluated to achieve the goal of assessing the judging capabilities of large language models."</data>
      <data key="d6">5fefd7cd7acb9cfd1bfb1118691c8546</data>
    </edge>
    <edge source="&quot;GENERATIVE ARTIFICIAL INTELLIGENCE (GENAI)&quot;" target="&quot;PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompting is a key concept in the subdomain of GenAI, involving the use of prompts to interact with AI systems."</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf</data>
    </edge>
    <edge source="&quot;GENERATIVE ARTIFICIAL INTELLIGENCE (GENAI)&quot;" target="&quot;UNIVERSITY OF MARYLAND&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The University of Maryland is involved in the research and development of GenAI systems and prompting techniques."</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf</data>
    </edge>
    <edge source="&quot;GENERATIVE ARTIFICIAL INTELLIGENCE (GENAI)&quot;" target="&quot;OPENAI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"OpenAI contributes to the research on GenAI systems and prompting techniques."</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf</data>
    </edge>
    <edge source="&quot;GENERATIVE ARTIFICIAL INTELLIGENCE (GENAI)&quot;" target="&quot;STANFORD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Stanford participates in the research on GenAI systems and prompting techniques."</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf</data>
    </edge>
    <edge source="&quot;GENERATIVE ARTIFICIAL INTELLIGENCE (GENAI)&quot;" target="&quot;VANDERBILT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Vanderbilt contributes to the research on GenAI systems and prompting techniques."</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf</data>
    </edge>
    <edge source="&quot;GENERATIVE ARTIFICIAL INTELLIGENCE (GENAI)&quot;" target="&quot;PRINCETON&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Princeton is involved in the research on GenAI systems and prompting techniques."</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf</data>
    </edge>
    <edge source="&quot;GENERATIVE ARTIFICIAL INTELLIGENCE (GENAI)&quot;" target="&quot;TEXAS STATE UNIVERSITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Texas State University participates in the research on GenAI systems and prompting techniques."</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf</data>
    </edge>
    <edge source="&quot;GENERATIVE ARTIFICIAL INTELLIGENCE (GENAI)&quot;" target="&quot;ICAHN SCHOOL OF MEDICINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Icahn School of Medicine is involved in the research on GenAI systems and prompting techniques."</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf</data>
    </edge>
    <edge source="&quot;GENERATIVE ARTIFICIAL INTELLIGENCE (GENAI)&quot;" target="&quot;ASST BRIANZA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ASST Brianza contributes to the research on GenAI systems and prompting techniques."</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf</data>
    </edge>
    <edge source="&quot;GENERATIVE ARTIFICIAL INTELLIGENCE (GENAI)&quot;" target="&quot;MOUNT SINAI BETH ISRAEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Mount Sinai Beth Israel is involved in the research on GenAI systems and prompting techniques."</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf</data>
    </edge>
    <edge source="&quot;GENERATIVE ARTIFICIAL INTELLIGENCE (GENAI)&quot;" target="&quot;INSTITUTO DE TELECOMUNICA&#199;&#213;ES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Instituto de Telecomunica&#231;&#245;es is involved in the research on GenAI systems and prompting techniques."</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf</data>
    </edge>
    <edge source="&quot;GENERATIVE ARTIFICIAL INTELLIGENCE (GENAI)&quot;" target="&quot;UNIVERSITY OF MASSACHUSETTS AMHERST&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"University of Massachusetts Amherst participates in the research on GenAI systems and prompting techniques."</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;PROMPT ENGINEERING&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Prompt Engineering" involves designing and refining prompts in the process of "Prompting." It is a specialized area within the broader subdomain of "Prompting," focusing specifically on the design and refinement of prompts.</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf,de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;META-ANALYSIS OF PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Meta-Analysis of Prompting provides a comprehensive review and taxonomy of various prompting techniques."</data>
      <data key="d6">d1af61f77ad6f49034ffa4e834a77faf</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;TRANSFORMER-BASED LLMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Transformer-based LLMs rely on prompting techniques to generate outputs, making prompting a crucial aspect of their functionality."</data>
      <data key="d6">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;TASK-AGNOSTIC TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Task-agnostic techniques are a category within prompting that are broadly applicable across different tasks."</data>
      <data key="d6">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;EXPLORATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Exploration is a goal within prompting, involving the investigation of various techniques."</data>
      <data key="d6">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;GETTING A LABEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Getting a label is a goal within prompting, aiding in the organization of techniques."</data>
      <data key="d6">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;VARYING PROMPTING TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Varying prompting techniques is a goal aimed at identifying the most effective methods."</data>
      <data key="d6">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;IN-CONTEXT LEARNING DEFINITIONS DISAMBIGUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"In-Context Learning Definitions Disambiguation is a goal aimed at clarifying terminologies within prompting."</data>
      <data key="d6">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;CONTRIBUTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Contributions refer to the advancements made in the field of prompting through research."</data>
      <data key="d6">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;PRISMA PROCESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The PRISMA process is an event that systematically reviews and categorizes prompting techniques."</data>
      <data key="d6">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;EXAMPLES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Examples are used in the process of Prompting to guide the GenAI in generating responses."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;OUTPUT FORMATTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Output Formatting is a crucial aspect of Prompting, ensuring that the GenAI's responses meet specific user requirements."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;DIRECTIVES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Directives are used in Prompting to encourage specific actions from the GenAI."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;CONTEXT WINDOW&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Context Window is important in Prompting as it defines the range of tokens processed by the GenAI."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;IN-CONTEXT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"In-Context Learning is a technique used in Prompting where the model learns from examples within the prompt."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;FEW-SHOT PROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-Shot Prompt is a type of Prompting where a few examples are provided to guide the GenAI."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;ZERO-SHOT PROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Zero-Shot Prompt is a type of Prompting where no examples are provided, relying solely on the prompt."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;ORTHOGONAL PROMPT TYPES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Orthogonal Prompt Types are different categories of prompts used in Prompting to achieve specific outcomes."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;CONTINUOUS PROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Continuous Prompt is a type of Prompting where input is provided continuously."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;DISCRETE PROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Discrete Prompt is a type of Prompting where input is provided in separate chunks."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;USER PROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"User Prompt is a type of Prompting initiated by the user."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;SYSTEM PROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"System Prompt is a type of Prompting initiated by the system."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;ASSISTANT PROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Assistant Prompt is a type of Prompting designed to assist the user."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;PREDICTION STYLE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prediction Style refers to how the GenAI generates responses in Prompting."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;PROMPT CHAIN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">Prompting is the foundational process that enables the creation of a prompt chain, where multiple prompts are used in succession. Prompt Chain is a technique in Prompting where multiple prompts are used in succession.</data>
      <data key="d6">7798b3210a865e03a3298ca49ad77cc4,de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;PROMPT TEMPLATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Template is a predefined structure used in Prompting."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;META-PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Meta-Prompting is a technique in Prompting that involves using prompts to generate other prompts."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;ANSWER ENGINEERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Answer Engineering involves designing the structure and content of answers in Prompting."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;VERBALIZER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Verbalizer translates internal representations into text in the process of Prompting."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;EXTRACTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Extractor identifies and extracts relevant information in the process of Prompting."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;CONVERSATIONAL PROMPT ENGINEERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Conversational Prompt Engineering involves designing prompts for conversational interactions in Prompting."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;FINE-TUNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Fine-Tuning adjusts the parameters of the GenAI to improve performance in Prompting."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;PROMPT-BASED LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt-Based Learning is a technique in Prompting where the GenAI learns from the prompts provided."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;PROMPT TUNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Tuning involves refining prompts to enhance performance in Prompting."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;ROLE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Role, also known as persona, is used in Prompting to improve writing and style text."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;ADDITIONAL INFORMATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Additional Information is included in Prompting to provide context and improve accuracy."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;TERMINOLOGY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Terminology is rapidly developing in the field of Prompting, with many definitions and terms."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;PROMPTING TERMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompting Terms are the vocabulary used to describe techniques and components in Prompting."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;PROMPT ENGINEERING PROCESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompting is a key component of the Prompt Engineering Process, which involves performing inference, evaluating performance, and modifying the prompt template."</data>
      <data key="d6">7798b3210a865e03a3298ca49ad77cc4</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;EXEMPLAR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Exemplars are used in prompting to show examples of a task being completed to a model."</data>
      <data key="d6">7798b3210a865e03a3298ca49ad77cc4</data>
    </edge>
    <edge source="&quot;PROMPTING&quot;" target="&quot;A SHORT HISTORY OF PROMPTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A Short History of Prompts provides context and background on the development and use of prompts in generative AI."</data>
      <data key="d6">7798b3210a865e03a3298ca49ad77cc4</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;PROMPTING TECHNIQUE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A prompting technique is a specific method used within the broader practice of prompt engineering to structure and sequence prompts."</data>
      <data key="d6">7798b3210a865e03a3298ca49ad77cc4</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;PROMPT ENGINEERING TECHNIQUE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A prompt engineering technique is a strategy used to iterate on and improve prompts, which is a core activity in prompt engineering."</data>
      <data key="d6">7798b3210a865e03a3298ca49ad77cc4</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;META PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Meta Prompting is a technique within the broader field of Prompt Engineering."</data>
      <data key="d6">83a60257c9adae8c826e73ef32d16dd0</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;AUTOPROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"AutoPrompt is another technique within the field of Prompt Engineering."</data>
      <data key="d6">83a60257c9adae8c826e73ef32d16dd0</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;AUTOMATIC PROMPT ENGINEER (APE)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Automatic Prompt Engineer (APE) is a specific technique discussed under Prompt Engineering."</data>
      <data key="d6">83a60257c9adae8c826e73ef32d16dd0</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;PROMPT TEMPLATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering involves designing and optimizing the Prompt Template to improve model performance."</data>
      <data key="d6">f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;SELF-CONSISTENCY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Consistency is a subdomain within the broader field of Prompt Engineering."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;ZERO-SHOT-COT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Zero-Shot-CoT is a subdomain within the broader field of Prompt Engineering."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;ZERO-SHOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Zero-Shot is a subdomain within the broader field of Prompt Engineering."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;FEW-SHOT COT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-Shot CoT is a subdomain within the broader field of Prompt Engineering."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;PROMPT ENGINEERING CASE STUDY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Prompt Engineering Case Study is an event that provides insights into the practice of Prompt Engineering."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;LLM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LLMs are the focus of prompt engineering, which aims to improve their performance through various techniques."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;GPT-4-32K&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"GPT-4-32K was used in the prompt engineering process to address issues with LLM behavior."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;FEW-SHOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-Shot is one of the techniques used in prompt engineering to improve LLM performance."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;10-SHOT AUTODICOT&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"10-Shot AutoDiCoT" is a specific technique used in prompt engineering to enhance the performance of language models. This method involves guiding language models by providing ten examples, thereby improving their ability to generate accurate and relevant responses.</data>
      <data key="d6">4257f30018a4acf2e8ee95f21de8d7df,e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;1-SHOT AUTODICOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"1-Shot AutoDiCoT is another technique used in prompt engineering to refine LLM outputs."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;ZERO-SHOT + CONTEXT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Zero-Shot + Context is a technique used in prompt engineering to guide LLMs without providing examples."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;CHAIN-OF-THOUGHT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chain-of-Thought is a technique used in prompt engineering to break down reasoning processes for LLMs."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;AUTOCOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"AutoCoT is a technique used in prompt engineering to automate the Chain-of-Thought process."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;CONTRASTIVE COT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Contrastive CoT is a technique used in prompt engineering to improve LLM performance by contrasting different reasoning processes."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;MULTIPLE ANSWER EXTRACTION TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multiple Answer Extraction Techniques are methods used in prompt engineering to extract answers from LLM outputs."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;F1 SCORE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"F1 Score is a key performance metric used in prompt engineering to evaluate LLM outputs."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;RECALL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recall is a performance metric used in prompt engineering to measure the true positive rate of LLM outputs."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;PRECISION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Precision is a performance metric used in prompt engineering to measure the positive predictive value of LLM outputs."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;TEMPERATURE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Temperature is a hyperparameter used in prompt engineering to control the randomness of LLM outputs."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;TOP-P&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Top-p is a hyperparameter used in prompt engineering to control the cumulative probability of the most likely tokens in LLM outputs."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;DEVELOPMENT SET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Development Set is used in prompt engineering to evaluate and refine LLM performance."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;TRAINING SET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Training Set is used in prompt engineering to train LLMs and improve their performance."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;EXEMPLARS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Exemplars are sample data points used in prompt engineering to guide LLMs."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;DSPY DEFAULT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"DSPy Default is a specific technique within the broader subdomain of Prompt Engineering."</data>
      <data key="d6">4257f30018a4acf2e8ee95f21de8d7df</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;20-SHOT AUTODICOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"20-Shot AutoDiCoT is a technique used in Prompt Engineering to guide language models with twenty examples."</data>
      <data key="d6">4257f30018a4acf2e8ee95f21de8d7df</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;AUTOMATED PROMPT ENGINEERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Automated Prompt Engineering is a goal within the broader subdomain of Prompt Engineering, focusing on using automated methods."</data>
      <data key="d6">4257f30018a4acf2e8ee95f21de8d7df</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;HUMAN PROMPT ENGINEERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Human Prompt Engineering is a goal within the broader subdomain of Prompt Engineering, focusing on manual crafting and revision of prompts."</data>
      <data key="d6">4257f30018a4acf2e8ee95f21de8d7df</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;CHAIN-OF-THOUGHT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chain-of-Thought Prompting is a technique within Prompt Engineering that breaks down reasoning processes into steps."</data>
      <data key="d6">4257f30018a4acf2e8ee95f21de8d7df</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;TREE-OF-THOUGHT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Tree-of-Thought Prompting is a technique within Prompt Engineering that structures reasoning in a tree-like format."</data>
      <data key="d6">4257f30018a4acf2e8ee95f21de8d7df</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;SELF-CONSISTENCY PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Consistency Prompting is a technique within Prompt Engineering that selects the most consistent reasoning path."</data>
      <data key="d6">4257f30018a4acf2e8ee95f21de8d7df</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;LEAST-TO-MOST PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Least-to-Most Prompting is a technique within Prompt Engineering that starts with simpler tasks and increases complexity."</data>
      <data key="d6">4257f30018a4acf2e8ee95f21de8d7df</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;FOUNDATION MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Foundation Models are the basis for various downstream tasks and applications within the subdomain of Prompt Engineering."</data>
      <data key="d6">4257f30018a4acf2e8ee95f21de8d7df</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;INTERACTIVE CREATIVE APPLICATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Interactive Creative Applications use prompting as a new paradigm for human interaction within the subdomain of Prompt Engineering."</data>
      <data key="d6">4257f30018a4acf2e8ee95f21de8d7df</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;MEDICAL AND HEALTHCARE DOMAINS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">Prompt Engineering is applied in Medical and Healthcare Domains to enhance services and improve outcomes in medical and healthcare applications. By leveraging prompt engineering techniques, these domains aim to optimize various aspects of healthcare delivery, ultimately leading to better patient care and more efficient medical services.</data>
      <data key="d6">4257f30018a4acf2e8ee95f21de8d7df,a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;SYSTEMATIC REVIEW&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Systematic Review is an event that involves a comprehensive survey and analysis of techniques within the subdomain of Prompt Engineering."</data>
      <data key="d6">4257f30018a4acf2e8ee95f21de8d7df</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;USER INTERFACE DESIGN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"User Interface Design supports user prompting, which is a key aspect of Prompt Engineering."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;MEDICAL EDUCATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering is used in Medical Education to enhance learning and training."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;MENTAL HEALTH SPACE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering techniques are applied in the Mental Health Space to improve services."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;VISUAL MODALITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering is used to enhance visual data processing in the Visual Modality."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;MULTIMODAL PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multimodal Prompting involves the integration of multiple modes of input, a key area in Prompt Engineering."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;TEXT-TO-IMAGE GENERATION MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering techniques are adopted in Text-to-Image Generation Models for creative works."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;GENAI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering is reviewed through a topic modeling approach in the context of GenAI."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;FOUNDATION MODELS IN VISION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering techniques are applied to Foundation Models in Vision to enhance visual data processing."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;SOFTWARE ENGINEERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering techniques are applied to improve software development processes in Software Engineering."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;SOFTWARE TESTING WITH LARGE LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering techniques are reviewed for their application in Software Testing with Large Language Models."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;AUTOMATED PROGRAM REPAIR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering techniques are used to improve Automated Program Repair tasks."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;COMPUTER SCIENCE EDUCATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering is leveraged to enhance teaching and learning in Computer Science Education."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;FAIRNESS OF LARGE LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering techniques are reviewed to ensure fairness in the application of Large Language Models."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;HALLUCINATION OF LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering techniques are studied to address the Hallucination of Language Models."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;VERIFIABILITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering techniques are reviewed to ensure the Verifiability of language model outputs."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;REASONING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering techniques are studied to enhance the Reasoning capabilities of language models."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;AUGMENTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering techniques are used to augment the capabilities of language models."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;LINGUISTIC PROPERTIES OF PROMPTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering involves studying the Linguistic Properties of Prompts to improve model performance."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;GENERATIVE AI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Generative AI involves the use of Prompt Engineering to address challenges in linguistic communication."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;TAXONOMIC ORGANIZATION OF PROMPTING TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The goal of Taxonomic Organization of Prompting Techniques is to provide a comprehensive overview of Prompt Engineering methods."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;STANDARDIZATION OF TERMINOLOGY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The goal of Standardization of Terminology is to ensure consistency and clarity in Prompt Engineering."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;SAFETY AND SECURITY&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Safety and security are critical considerations in the practice of prompt engineering. The goal of addressing safety and security is to ensure the safe and secure application of prompt engineering techniques."</data>
      <data key="d6">6e1dce58f4a3793b65d09171ea5bd3a6,a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;TAXONOMY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The taxonomy is designed to classify and provide terminology for various prompt engineering techniques."</data>
      <data key="d6">6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;CASE STUDIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Case studies are used to demonstrate the practical application and capabilities of prompt engineering techniques."</data>
      <data key="d6">6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;MACHINE LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt engineering is a specialized area within the broader field of machine learning."</data>
      <data key="d6">6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;OPENAI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"OpenAI provided financial support in the form of API credits to aid research in prompt engineering."</data>
      <data key="d6">6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;REBUFF AI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Rebuff AI's self-hardening prompt injection detector is a security measure relevant to prompt engineering."</data>
      <data key="d6">6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;LARGE LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering is a technique used to optimize the performance of large language models in various applications."</data>
      <data key="d6">83e773afec09e119882fe15dd253e724</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;ADAPTIVE MACHINE TRANSLATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering can enhance Adaptive Machine Translation by creating optimized prompts that improve translation accuracy and contextual relevance."</data>
      <data key="d6">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;TOOLFORMER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Toolformer is a goal within the subdomain of Prompt Engineering, aimed at enhancing language models' capabilities."</data>
      <data key="d6">c605e4f0158f18be68214a39b9b54154</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt engineering involves designing prompts to elicit specific responses from language models."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;PLAN-AND-SOLVE PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Plan-and-Solve Prompting is a specific application within the broader subdomain of Prompt Engineering."</data>
      <data key="d6">153eeb5a63e650f2cd12f700ffe3e71f</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;INTERLEAVING RETRIEVAL WITH CHAIN-OF-THOUGHT REASONING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Interleaving Retrieval with Chain-of-Thought Reasoning involves techniques from the subdomain of Prompt Engineering."</data>
      <data key="d6">153eeb5a63e650f2cd12f700ffe3e71f</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;EXPLANATION SELECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Engineering and Explanation Selection both involve designing and selecting prompts for language models."</data>
      <data key="d6">0274e77e2fcec8973c9768c464c6e82d</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;SOCIAL ROLES IN SYSTEM PROMPTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Evaluating social roles in system prompts is a part of the broader practice of prompt engineering."</data>
      <data key="d6">c7285f7847ef45ed85779d7966753855</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;PROMPT&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Prompt Engineering" involves the design and refinement of prompts to guide the output of large language models (LLMs). A "Prompt" is a fundamental concept within Prompt Engineering, which entails creating and optimizing these prompts.</data>
      <data key="d6">1a997c6aadeaeb3b5ad0a4c3ce835540,d305fc89f77daeb9c5be3a3d126223ed</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;CONVERSATIONAL PROMPT ENGINEERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Conversational Prompt Engineering is a specific form of Prompt Engineering done during a conversation with a GenAI."</data>
      <data key="d6">d305fc89f77daeb9c5be3a3d126223ed</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;PROMPT-BASED LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt-Based Learning is a technique used within the broader practice of Prompt Engineering, especially in the context of fine-tuning prompts."</data>
      <data key="d6">d305fc89f77daeb9c5be3a3d126223ed</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;PROMPT TUNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Tuning is a specific technique within Prompt Engineering that involves directly optimizing the weights of the model using prompts."</data>
      <data key="d6">d305fc89f77daeb9c5be3a3d126223ed</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;FEW-SHOT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-Shot Prompting is a specific technique used within Prompt Engineering to condition models using few-shot exemplars."</data>
      <data key="d6">1a997c6aadeaeb3b5ad0a4c3ce835540</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;OPTIMIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Optimization is a key aspect of Prompt Engineering, focusing on improving prompts with respect to a scoring function."</data>
      <data key="d6">1a997c6aadeaeb3b5ad0a4c3ce835540</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;ANSWER ENGINEERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Answer Engineering is related to Prompt Engineering as it involves transforming raw LLM outputs to compare them to the ground truth."</data>
      <data key="d6">1a997c6aadeaeb3b5ad0a4c3ce835540</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING&quot;" target="&quot;IN-CONTEXT LEARNING (ICL)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"In-Context Learning (ICL) is a method used within Prompt Engineering to condition models on natural language instructions and examples."</data>
      <data key="d6">1a997c6aadeaeb3b5ad0a4c3ce835540</data>
    </edge>
    <edge source="&quot;META-ANALYSIS OF PROMPTING&quot;" target="&quot;SYSTEMATIC REVIEW PROCESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Meta-Analysis of Prompting uses the Systematic Review Process to collect and analyze data sources related to prompting."</data>
      <data key="d6">7798b3210a865e03a3298ca49ad77cc4</data>
    </edge>
    <edge source="&quot;UNIVERSITY OF MARYLAND&quot;" target="&quot;LEARN PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"University of Maryland and Learn Prompting are both research entities associated with the creation of the dataset on prompt engineering."</data>
      <data key="d6">6430817c08b3a5c6d193478d4c739d79</data>
    </edge>
    <edge source="&quot;UNIVERSITY OF MARYLAND&quot;" target="&quot;OPENAI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"University of Maryland is associated with the research on prompt engineering, which is sponsored by OpenAI."</data>
      <data key="d6">6430817c08b3a5c6d193478d4c739d79</data>
    </edge>
    <edge source="&quot;OPENAI&quot;" target="&quot;OPENAI ASSISTANTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"OpenAI developed the OpenAI Assistants, showcasing their advancements in AI technology."</data>
      <data key="d6">4d9e8d703c2da8e4775c428e83e87fc9</data>
    </edge>
    <edge source="&quot;OPENAI&quot;" target="&quot;LEARN PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Learn Prompting is associated with the research on prompt engineering, which is sponsored by OpenAI."</data>
      <data key="d6">6430817c08b3a5c6d193478d4c739d79</data>
    </edge>
    <edge source="&quot;OPENAI&quot;" target="&quot;DATASET CREATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"OpenAI sponsored the dataset creation by contributing $10,000 in credits for their API."</data>
      <data key="d6">6430817c08b3a5c6d193478d4c739d79</data>
    </edge>
    <edge source="&quot;IMAGE PROMPTING&quot;" target="&quot;MULTIMODAL TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Image Prompting is a type of multimodal technique that involves using images for generating prompts."</data>
      <data key="d6">8bafc5999ce3abba6f261770c5945604</data>
    </edge>
    <edge source="&quot;IMAGE PROMPTING&quot;" target="&quot;MULTIMODAL PROMPTING TECHNIQUE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Image Prompting is a type of Multimodal Prompting Technique that involves using images in prompts or as outputs."</data>
      <data key="d6">6edacbda20b2fdd4077246c7b271a8b5</data>
    </edge>
    <edge source="&quot;IMAGE PROMPTING&quot;" target="&quot;PROMPT MODIFIERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Modifiers are used within Image Prompting to change the resultant image by appending specific words to the prompt."</data>
      <data key="d6">6edacbda20b2fdd4077246c7b271a8b5</data>
    </edge>
    <edge source="&quot;IMAGE PROMPTING&quot;" target="&quot;NEGATIVE PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Negative Prompting is a technique within Image Prompting that allows users to influence the model's output by numerically weighting certain terms."</data>
      <data key="d6">6edacbda20b2fdd4077246c7b271a8b5</data>
    </edge>
    <edge source="&quot;AUDIO PROMPTING&quot;" target="&quot;MULTIMODAL PROMPTING TECHNIQUE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Audio Prompting is a type of Multimodal Prompting Technique that extends prompting methods to the audio modality."</data>
      <data key="d6">6edacbda20b2fdd4077246c7b271a8b5</data>
    </edge>
    <edge source="&quot;AUDIO PROMPTING&quot;" target="&quot;VIDEO PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Audio Prompting and Video Prompting extend prompting techniques to different modalities, showing the versatility of prompting across media types."</data>
      <data key="d6">27d8fe15ab6f9e3d91fd5858fbeba7ea</data>
    </edge>
    <edge source="&quot;VIDEO PROMPTING&quot;" target="&quot;CHAIN-OF-THOUGHT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Chain-of-Thought Prompting and Video Prompting involve generating visual content as part of the thought process, enhancing reasoning and generation capabilities."</data>
      <data key="d6">27d8fe15ab6f9e3d91fd5858fbeba7ea</data>
    </edge>
    <edge source="&quot;SEGMENTATION PROMPTING&quot;" target="&quot;3D PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both 3D Prompting and Segmentation Prompting involve using prompting techniques for specific tasks like 3D object synthesis and semantic segmentation."</data>
      <data key="d6">27d8fe15ab6f9e3d91fd5858fbeba7ea</data>
    </edge>
    <edge source="&quot;AGENTS&quot;" target="&quot;EVALUATION METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Agents require evaluation methods to ensure the accuracy and reliability of their outputs."</data>
      <data key="d6">42fa2868f275e1b0f2269e560e9a5816</data>
    </edge>
    <edge source="&quot;AGENTS&quot;" target="&quot;EXTERNAL TOOLS ACCESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Agents in GenAI often involve accessing external tools to perform tasks that LLMs alone cannot handle, such as complex computations and reasoning."</data>
      <data key="d6">27d8fe15ab6f9e3d91fd5858fbeba7ea</data>
    </edge>
    <edge source="&quot;AGENTS&quot;" target="&quot;ADEPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Adept is a company exploring the development of agents in GenAI, focusing on integrating external systems with LLMs."</data>
      <data key="d6">27d8fe15ab6f9e3d91fd5858fbeba7ea</data>
    </edge>
    <edge source="&quot;AGENTS&quot;" target="&quot;KARPAS ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Karpas et al. are researchers working on developing agents in GenAI, focusing on integrating external systems with LLMs."</data>
      <data key="d6">27d8fe15ab6f9e3d91fd5858fbeba7ea</data>
    </edge>
    <edge source="&quot;AGENTS&quot;" target="&quot;LLM OUTPUTS EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Evaluating LLM outputs is crucial for the effective functioning of agents, ensuring that the actions taken by agents are accurate and reliable."</data>
      <data key="d6">27d8fe15ab6f9e3d91fd5858fbeba7ea</data>
    </edge>
    <edge source="&quot;TOOL USE AGENTS&quot;" target="&quot;AGENT LLMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Tool Use Agents are a subset of Agent LLMs that utilize external tools to perform tasks."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;TOOL USE AGENTS&quot;" target="&quot;MODULAR REASONING, KNOWLEDGE, AND LANGUAGE (MRKL) SYSTEM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MRKL System is an example of a Tool Use Agent that uses an LLM router to access multiple tools."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;TOOL USE AGENTS&quot;" target="&quot;SELF-CORRECTING WITH TOOL-INTERACTIVE CRITIQUING (CRITIC)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"CRITIC is an example of a Tool Use Agent that uses tools to verify or amend parts of a response."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;CODE-GENERATION AGENTS&quot;" target="&quot;PROGRAM-AIDED LANGUAGE MODEL (PAL)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"PAL is an example of a Code-Generation Agent that translates problems into code."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;CODE-GENERATION AGENTS&quot;" target="&quot;TOOL-INTEGRATED REASONING AGENT (TORA)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ToRA is an example of a Code-Generation Agent that interleaves code and reasoning steps."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;OBSERVATION-BASED AGENTS&quot;" target="&quot;REACT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReAct is an example of an Observation-Based Agent that combines reasoning and acting based on observations."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;OBSERVATION-BASED AGENTS&quot;" target="&quot;REFLEXION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reflexion is an example of an Observation-Based Agent that focuses on reflecting on past actions."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;RETRIEVAL AUGMENTED GENERATION (RAG)&quot;" target="&quot;VERIFY-AND-EDIT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Verify-and-Edit is a technique within the RAG paradigm that improves self-consistency by generating and editing multiple chains-of-thought."</data>
      <data key="d6">eed969adf8c7eb4a89355c851663c87a</data>
    </edge>
    <edge source="&quot;PROMPTING TECHNIQUES&quot;" target="&quot;CHAIN-OF-THOUGHT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chain-of-Thought Prompting is another frequently cited prompting technique."</data>
      <data key="d6">83a60257c9adae8c826e73ef32d16dd0</data>
    </edge>
    <edge source="&quot;SECURITY&quot;" target="&quot;EVALUATION PERFORMANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Security concerns can impact evaluation performance, necessitating secure and aligned prompting practices."</data>
      <data key="d6">a4eb2fbdea1494d271ebc61219d17020</data>
    </edge>
    <edge source="&quot;SECURITY&quot;" target="&quot;PROMPT HACKING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Addressing Prompt Hacking is a key aspect of Security in the context of prompting."</data>
      <data key="d6">2dba3160cd0e0ee3943dce308cb9940e</data>
    </edge>
    <edge source="&quot;SECURITY&quot;" target="&quot;PROMPT-BASED DEFENSE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt-based Defense is a measure taken to enhance Security against prompt hacking."</data>
      <data key="d6">2dba3160cd0e0ee3943dce308cb9940e</data>
    </edge>
    <edge source="&quot;SECURITY&quot;" target="&quot;ALIGNMENT CONCERNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Addressing Alignment Concerns is part of ensuring Security in the context of prompting."</data>
      <data key="d6">2dba3160cd0e0ee3943dce308cb9940e</data>
    </edge>
    <edge source="&quot;HARDENING MEASURES&quot;" target="&quot;PROMPT-BASED DEFENSE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt-based Defense is one of the hardening measures developed to mitigate security risks."</data>
      <data key="d6">4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </edge>
    <edge source="&quot;HARDENING MEASURES&quot;" target="&quot;GUARDRAILS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Guardrails are part of the hardening measures to guide GenAI outputs and prevent malicious activities."</data>
      <data key="d6">4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </edge>
    <edge source="&quot;HARDENING MEASURES&quot;" target="&quot;DETECTORS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Detectors are tools included in hardening measures to detect and prevent prompt hacking."</data>
      <data key="d6">4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </edge>
    <edge source="&quot;HARDENING MEASURES&quot;" target="&quot;SECURITY &amp; PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hardening Measures are developed as part of the goal to improve security and prompting in LLMs."</data>
      <data key="d6">4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </edge>
    <edge source="&quot;ALIGNMENT&quot;" target="&quot;SECURITY &amp; PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ensuring alignment is essential for achieving the goal of improving security and prompting in LLMs."</data>
      <data key="d6">4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </edge>
    <edge source="&quot;PROMPT SENSITIVITY&quot;" target="&quot;PROMPT WORDING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Wording is a specific aspect of Prompt Sensitivity, where changes in wording can impact model behavior."</data>
      <data key="d6">84da286ab749b0f025821313fe535d70</data>
    </edge>
    <edge source="&quot;PROMPT SENSITIVITY&quot;" target="&quot;TASK FORMAT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Task Format is another aspect of Prompt Sensitivity, where different ways of prompting can alter model accuracy."</data>
      <data key="d6">84da286ab749b0f025821313fe535d70</data>
    </edge>
    <edge source="&quot;PROMPT SENSITIVITY&quot;" target="&quot;PROMPT DRIFT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Drift is related to Prompt Sensitivity as it involves changes in model behavior over time, affecting prompt performance."</data>
      <data key="d6">84da286ab749b0f025821313fe535d70</data>
    </edge>
    <edge source="&quot;PROMPT SENSITIVITY&quot;" target="&quot;FIGURE 5.2: PROMPT-BASED ALIGNMENT ORGANIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Figure 5.2: Prompt-based Alignment Organization addresses issues related to Prompt Sensitivity."</data>
      <data key="d6">84da286ab749b0f025821313fe535d70</data>
    </edge>
    <edge source="&quot;BENCHMARKING&quot;" target="&quot;BENCHMARK DATASETS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Benchmarking involves using Benchmark Datasets to evaluate new prompting techniques."</data>
      <data key="d6">83a60257c9adae8c826e73ef32d16dd0</data>
    </edge>
    <edge source="&quot;BENCHMARKING&quot;" target="&quot;SHARMA ET AL. (2023)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Benchmarking includes evaluating the empirical performance of techniques like those studied by Sharma et al. (2023)."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;BENCHMARKING&quot;" target="&quot;WEI ET AL. (2023B)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Benchmarking includes evaluating the empirical performance of techniques like those studied by Wei et al. (2023b)."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;BENCHMARKING&quot;" target="&quot;SI ET AL. (2023B)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Benchmarking includes evaluating the empirical performance of techniques like those studied by Si et al. (2023b)."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;BENCHMARKING&quot;" target="&quot;MA ET AL. (2023)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Benchmarking includes evaluating the empirical performance of techniques like those studied by Ma et al. (2023)."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;BENCHMARKING&quot;" target="&quot;YAO ET AL. (2023A)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Benchmarking includes evaluating the empirical performance of techniques like those studied by Yao et al. (2023a)."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;BENCHMARKING&quot;" target="&quot;YU ET AL. (2023)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Benchmarking includes evaluating the empirical performance of techniques like those studied by Yu et al. (2023)."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;BENCHMARKING&quot;" target="&quot;GAO ET AL. (2023A)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Benchmarking includes evaluating the empirical performance of techniques like those studied by Gao et al. (2023a)."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;BENCHMARKING&quot;" target="&quot;RAO AND DAUM&#201; III (2019)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Benchmarking includes evaluating the empirical performance of techniques like those studied by Rao and Daum&#233; III (2019)."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;BENCHMARKING&quot;" target="&quot;MU ET AL. (2023)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Benchmarking includes evaluating the empirical performance of techniques like those studied by Mu et al. (2023)."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;BENCHMARKING&quot;" target="&quot;ZHANG AND CHOI (2023)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Benchmarking includes evaluating the empirical performance of techniques like those studied by Zhang and Choi (2023)."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;SELF-CONSISTENCY&quot;" target="&quot;ENSEMBLING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Consistency is an Ensembling method that uses multiple reasoning paths and a majority vote to select a final response."</data>
      <data key="d6">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </edge>
    <edge source="&quot;SELF-CONSISTENCY&quot;" target="&quot;UNIVERSAL SELF-CONSISTENCY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Universal Self-Consistency is a variant of Self-Consistency that uses a prompt template to select the majority answer, useful for free-form text generation."</data>
      <data key="d6">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </edge>
    <edge source="&quot;SELF-CONSISTENCY&quot;" target="&quot;META-REASONING OVER MULTIPLE COTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Meta-Reasoning over Multiple CoTs is similar to Self-Consistency but focuses on generating multiple reasoning chains and then deriving a final answer from them."</data>
      <data key="d6">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </edge>
    <edge source="&quot;SELF-CONSISTENCY&quot;" target="&quot;DIVERSE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"DiVeRSe performs Self-Consistency for multiple prompts, scoring reasoning paths and selecting a final response based on these scores."</data>
      <data key="d6">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </edge>
    <edge source="&quot;SELF-CONSISTENCY&quot;" target="&quot;CONSISTENCY-BASED SELF-ADAPTIVE PROMPTING (COSP)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Consistency-based Self-adaptive Prompting (COSP) uses Self-Consistency to construct Few-Shot CoT prompts by selecting a high agreement subset of outputs."</data>
      <data key="d6">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </edge>
    <edge source="&quot;SELF-CONSISTENCY&quot;" target="&quot;ZERO-SHOT-COT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Zero-Shot-CoT can be combined with Self-Consistency to improve the accuracy of the model's responses."</data>
      <data key="d6">f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </edge>
    <edge source="&quot;SELF-CONSISTENCY&quot;" target="&quot;FEW-SHOT-COT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-Shot-CoT can be combined with Self-Consistency to improve the accuracy of the model's responses."</data>
      <data key="d6">f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </edge>
    <edge source="&quot;SELF-CONSISTENCY&quot;" target="&quot;FORMAL BENCHMARK EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Consistency is a technique used in the Formal Benchmark Evaluation to improve accuracy."</data>
      <data key="d6">f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </edge>
    <edge source="&quot;SELF-CONSISTENCY&quot;" target="&quot;ZERO-SHOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Self-Consistency and Zero-Shot are subdomains of prompt engineering, with Self-Consistency showing lower spread in performance."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;SELF-CONSISTENCY&quot;" target="&quot;CHAIN-OF-THOUGHT REASONING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Consistency improves the effectiveness of Chain-of-Thought Reasoning in language models."</data>
      <data key="d6">42d8c3ad092ec18e28ff718709b0b472</data>
    </edge>
    <edge source="&quot;EVALUATION TABLE&quot;" target="&quot;PROMPT ENGINEERING TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Evaluation Table lists various models and their performance metrics, which are used to assess the effectiveness of different Prompt Engineering Techniques."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;ENTRAPMENT PROMPTING PROCESS&quot;" target="&quot;EXPLORATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Exploration is the initial phase of the Entrapment Prompting Process."</data>
      <data key="d6">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </edge>
    <edge source="&quot;ENTRAPMENT PROMPTING PROCESS&quot;" target="&quot;GETTING A LABEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Getting a Label is a phase in the Entrapment Prompting Process."</data>
      <data key="d6">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </edge>
    <edge source="&quot;ENTRAPMENT PROMPTING PROCESS&quot;" target="&quot;VARYING PROMPTING TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Varying Prompting Techniques is a phase in the Entrapment Prompting Process."</data>
      <data key="d6">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </edge>
    <edge source="&quot;TRANSFORMER-BASED LLMS&quot;" target="&quot;PREFIX PROMPTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prefix prompts are a type of discrete prompt used in transformer-based LLMs, particularly in decoder-only models."</data>
      <data key="d6">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </edge>
    <edge source="&quot;PREFIX PROMPTS&quot;" target="&quot;HARD PROMPTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hard prompts, or discrete prompts, include prefix prompts as a specific type used in modern LLM architectures."</data>
      <data key="d6">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </edge>
    <edge source="&quot;HARD PROMPTS&quot;" target="&quot;SOFT PROMPTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Soft prompts are contrasted with hard prompts, with the study focusing on the latter."</data>
      <data key="d6">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </edge>
    <edge source="&quot;MULTILINGUAL TECHNIQUES&quot;" target="&quot;CORE PROMPTING TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multilingual techniques often derive from core text-based prompting techniques."</data>
      <data key="d6">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </edge>
    <edge source="&quot;MULTILINGUAL TECHNIQUES&quot;" target="&quot;MULTIMODAL TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both multilingual and multimodal techniques are extensions of English text-only prompting techniques."</data>
      <data key="d6">42fa2868f275e1b0f2269e560e9a5816</data>
    </edge>
    <edge source="&quot;MULTIMODAL TECHNIQUES&quot;" target="&quot;CORE PROMPTING TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multimodal techniques are based on core prompting techniques but extend to multimedia inputs."</data>
      <data key="d6">08e6ee9b2e040693136d0d8e0acfb8dd</data>
    </edge>
    <edge source="&quot;MULTIMODAL TECHNIQUES&quot;" target="&quot;MULTIMODAL PROMPTING TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multimodal Techniques are part of the broader category of Multimodal Prompting Techniques."</data>
      <data key="d6">8bafc5999ce3abba6f261770c5945604</data>
    </edge>
    <edge source="&quot;PRISMA PROCESS&quot;" target="&quot;CASE STUDIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The PRISMA process is used to conduct systematic reviews that inform the case studies."</data>
      <data key="d6">42fa2868f275e1b0f2269e560e9a5816</data>
    </edge>
    <edge source="&quot;SECURITY MEASURES&quot;" target="&quot;SAFETY MEASURES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both security and safety measures are designed to reduce risks and ensure the safe use of prompting techniques."</data>
      <data key="d6">42fa2868f275e1b0f2269e560e9a5816</data>
    </edge>
    <edge source="&quot;CASE STUDIES&quot;" target="&quot;MMLU BENCHMARK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"One of the case studies involves testing prompting techniques against the MMLU benchmark."</data>
      <data key="d6">42fa2868f275e1b0f2269e560e9a5816</data>
    </edge>
    <edge source="&quot;CASE STUDIES&quot;" target="&quot;MANUAL PROMPT ENGINEERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Manual prompt engineering is explored in detail in one of the case studies, focusing on identifying signals of suicidal crisis."</data>
      <data key="d6">42fa2868f275e1b0f2269e560e9a5816</data>
    </edge>
    <edge source="&quot;PROMPT&quot;" target="&quot;PROMPT TEMPLATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A prompt template becomes a prompt when input is inserted into it."</data>
      <data key="d6">42fa2868f275e1b0f2269e560e9a5816</data>
    </edge>
    <edge source="&quot;PROMPT&quot;" target="&quot;PRIMING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Priming involves providing an initial prompt to set instructions for the rest of a conversation, which is a specific use case of prompts."</data>
      <data key="d6">d305fc89f77daeb9c5be3a3d126223ed</data>
    </edge>
    <edge source="&quot;PROMPT&quot;" target="&quot;CONTEXT WINDOW&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The context window defines the space of tokens that a prompt can utilize within an LLM."</data>
      <data key="d6">d305fc89f77daeb9c5be3a3d126223ed</data>
    </edge>
    <edge source="&quot;OUTPUT FORMATTING&quot;" target="&quot;STYLE INSTRUCTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Style Instructions are used in Output Formatting to modify the output stylistically."</data>
      <data key="d6">de0fbfe367c5921e80c093f91d589919</data>
    </edge>
    <edge source="&quot;IN-CONTEXT LEARNING&quot;" target="&quot;DEEP ABDUCTIVE REASONING BENCHMARK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Deep Abductive Reasoning Benchmark and In-Context Learning involve challenging tasks for language models, indicating a focus on advanced reasoning capabilities."</data>
      <data key="d6">e5878afbfbf5194f1da3540eaa88fe65</data>
    </edge>
    <edge source="&quot;IN-CONTEXT LEARNING&quot;" target="&quot;AUGMENTED LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Augmented Language Models can benefit from In-Context Learning techniques to improve their contextual understanding and response generation."</data>
      <data key="d6">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </edge>
    <edge source="&quot;IN-CONTEXT LEARNING&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"In-context learning involves training language models to understand and generate text based on the context provided."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;IN-CONTEXT LEARNING&quot;" target="&quot;DIFFUSION MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Diffusion Models have been unlocked for in-context learning, enhancing their performance in generating contextually relevant content."</data>
      <data key="d6">42d8c3ad092ec18e28ff718709b0b472</data>
    </edge>
    <edge source="&quot;IN-CONTEXT LEARNING&quot;" target="&quot;PROMPT ENGINEERING TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"In-Context Learning is a method within Prompt Engineering Techniques where the model learns from the context provided in the prompt."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;IN-CONTEXT LEARNING&quot;" target="&quot;META-LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"In-Context Learning is considered the inner loop of Meta-Learning, which captures the broader structure of the method."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;IN-CONTEXT LEARNING&quot;" target="&quot;ONE-SHOT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"One-Shot Learning is a specific form of In-Context Learning where only one demonstration is provided."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;IN-CONTEXT LEARNING&quot;" target="&quot;ZERO-SHOT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Zero-Shot Learning is a specific form of In-Context Learning where no demonstrations are provided."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;IN-CONTEXT LEARNING&quot;" target="&quot;BROWN ET AL. (2020)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Brown et al. (2020) provided the broad definition of In-Context Learning."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;IN-CONTEXT LEARNING&quot;" target="&quot;DONG ET AL. (2023)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dong et al. (2023) provided a formal definition of In-Context Learning, differing from Brown et al. (2020)."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;USER PROMPT&quot;" target="&quot;ASSISTANT PROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"User Prompt and Assistant Prompt are related as different types of prompts, with User Prompt coming from the user and Assistant Prompt being the output of the LLM itself."</data>
      <data key="d6">6430817c08b3a5c6d193478d4c739d79</data>
    </edge>
    <edge source="&quot;ANSWER ENGINEERING&quot;" target="&quot;LLM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Answer engineering involves developing algorithms to extract precise answers from LLM outputs."</data>
      <data key="d6">981e367f454fd6805ff2ad123c75b85e</data>
    </edge>
    <edge source="&quot;ANSWER ENGINEERING&quot;" target="&quot;ANSWER SHAPE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Answer Shape is a key component of Answer Engineering, determining the physical format of the answer."</data>
      <data key="d6">981e367f454fd6805ff2ad123c75b85e</data>
    </edge>
    <edge source="&quot;ANSWER ENGINEERING&quot;" target="&quot;ANSWER SPACE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Answer Space is a key component of Answer Engineering, defining the domain of values an answer may contain."</data>
      <data key="d6">981e367f454fd6805ff2ad123c75b85e</data>
    </edge>
    <edge source="&quot;VERBALIZER&quot;" target="&quot;ANSWER EXTRACTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Verbalizer and Answer Extractor are related as both are components of answer engineering, mapping outputs to labels and extracting final answers respectively."</data>
      <data key="d6">45c77c52a93a949222fda99a95e0c3d6</data>
    </edge>
    <edge source="&quot;PROMPT TUNING&quot;" target="&quot;PRIMING-BASED FEW-SHOT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Priming-Based Few-Shot Learning and Prompt Tuning are techniques aimed at improving the efficiency and effectiveness of language models."</data>
      <data key="d6">5ce40e1d59b740ff17256ed5abebf613</data>
    </edge>
    <edge source="&quot;PROMPT TUNING&quot;" target="&quot;PROMPT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Learning and Prompt Tuning are related as both involve the use of prompting techniques, with Prompt Tuning specifically focusing on optimizing the weights of the prompt itself."</data>
      <data key="d6">6430817c08b3a5c6d193478d4c739d79</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING PROCESS&quot;" target="&quot;SUICIDE CRISIS SYNDROME&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Prompt Engineering Process involved using an LLM to identify entrapment, a key factor in Suicide Crisis Syndrome."</data>
      <data key="d6">d27160d0dde304425ccc51df673321b1</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING PROCESS&quot;" target="&quot;UNIVERSITY OF MARYLAND REDDIT SUICIDALITY DATASET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Prompt Engineering Process used data from the University of Maryland Reddit Suicidality Dataset to identify entrapment in posts."</data>
      <data key="d6">d27160d0dde304425ccc51df673321b1</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING PROCESS&quot;" target="&quot;EXPERT PROMPT ENGINEER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Prompt Engineering Process was carried out by an expert prompt engineer to identify entrapment in posts."</data>
      <data key="d6">d27160d0dde304425ccc51df673321b1</data>
    </edge>
    <edge source="&quot;SYSTEMATIC REVIEW PROCESS&quot;" target="&quot;DATA SCRAPING PIPELINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Systematic Review Process includes the use of a Data Scraping Pipeline to collect and filter data sources."</data>
      <data key="d6">7798b3210a865e03a3298ca49ad77cc4</data>
    </edge>
    <edge source="&quot;ARXIV&quot;" target="&quot;THE PIPELINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"arXiv is one of the main data sources used in The Pipeline for retrieving relevant papers."</data>
      <data key="d6">82d58329a3cd23550be3e22f1740f8ae</data>
    </edge>
    <edge source="&quot;ARXIV&quot;" target="&quot;THE TWELFTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ArXiv hosts preprints of papers that are often presented at academic conferences like The Twelfth International Conference on Learning Representations."</data>
      <data key="d6">5ce886e06455eadec4bcfe91e36b666d</data>
    </edge>
    <edge source="&quot;ARXIV&quot;" target="&quot;LARGE LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ArXiv serves as a repository for preprints of research papers on large language models, facilitating early dissemination of research findings."</data>
      <data key="d6">83e773afec09e119882fe15dd253e724</data>
    </edge>
    <edge source="&quot;ARXIV&quot;" target="&quot;RESEARCH PAPER COLLECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Arxiv is one of the primary sources from which research papers were collected during the Research Paper Collection event."</data>
      <data key="d6">29d2b14a56a51f86baa34264697bdd5e</data>
    </edge>
    <edge source="&quot;SEMANTIC SCHOLAR&quot;" target="&quot;THE PIPELINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Semantic Scholar is another primary data source used in The Pipeline for querying relevant papers."</data>
      <data key="d6">82d58329a3cd23550be3e22f1740f8ae</data>
    </edge>
    <edge source="&quot;SEMANTIC SCHOLAR&quot;" target="&quot;RESEARCH PAPER COLLECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Semantic Scholar is another primary source from which research papers were collected during the Research Paper Collection event."</data>
      <data key="d6">29d2b14a56a51f86baa34264697bdd5e</data>
    </edge>
    <edge source="&quot;ACL&quot;" target="&quot;THE PIPELINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ACL is a key data source used in The Pipeline for retrieving relevant papers."</data>
      <data key="d6">82d58329a3cd23550be3e22f1740f8ae</data>
    </edge>
    <edge source="&quot;ACL&quot;" target="&quot;RETHINKING THE ROLE OF SCALE FOR IN-CONTEXT LEARNING: AN INTERPRETABILITY-BASED CASE STUDY AT 66 BILLION SCALE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research paper was presented at this conference."</data>
      <data key="d6">b363fca358c69a9412b955c53352ea9a</data>
    </edge>
    <edge source="&quot;ACL&quot;" target="&quot;RESEARCH PAPER COLLECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ACL is one of the sources from which research papers were collected during the Research Paper Collection event."</data>
      <data key="d6">29d2b14a56a51f86baa34264697bdd5e</data>
    </edge>
    <edge source="&quot;THE PIPELINE&quot;" target="&quot;PRISMA REVIEW PROCESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Pipeline includes the PRISMA Review Process as a systematic method to filter and classify relevant records."</data>
      <data key="d6">82d58329a3cd23550be3e22f1740f8ae</data>
    </edge>
    <edge source="&quot;IN-CONTEXT LEARNING (ICL)&quot;" target="&quot;FEW-SHOT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-Shot Prompting is a specific paradigm within In-Context Learning where tasks are learned with only a few examples."</data>
      <data key="d6">82d58329a3cd23550be3e22f1740f8ae</data>
    </edge>
    <edge source="&quot;IN-CONTEXT LEARNING (ICL)&quot;" target="&quot;MULTILINGUAL PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"In-Context Learning (ICL) is a technique within Multilingual Prompting to align in-context examples with the input sentence for classification tasks."</data>
      <data key="d6">45c77c52a93a949222fda99a95e0c3d6</data>
    </edge>
    <edge source="&quot;IN-CONTEXT LEARNING (ICL)&quot;" target="&quot;X-INSTA PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"X-InSTA Prompting is a specific method of In-Context Learning (ICL) in multilingual settings."</data>
      <data key="d6">45c77c52a93a949222fda99a95e0c3d6</data>
    </edge>
    <edge source="&quot;FEW-SHOT PROMPTING&quot;" target="&quot;FEW-SHOT LEARNING (FSL)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-Shot Learning (FSL) and Few-Shot Prompting are related concepts in machine learning, with FSL being a broader paradigm and Few-Shot Prompting specific to GenAI settings."</data>
      <data key="d6">5d5844de9a93093f225ca41ba18f9a89</data>
    </edge>
    <edge source="&quot;FEW-SHOT PROMPTING&quot;" target="&quot;EXEMPLAR LABEL DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Exemplar Label Distribution affects the behavior and performance of Few-Shot Prompting by influencing how labels are presented in prompts."</data>
      <data key="d6">e845d3c15484b3061e3a376fa8779883</data>
    </edge>
    <edge source="&quot;FEW-SHOT PROMPTING&quot;" target="&quot;EXEMPLAR LABEL QUALITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Exemplar Label Quality impacts the performance of Few-Shot Prompting by determining the accuracy and validity of labels used in prompts."</data>
      <data key="d6">e845d3c15484b3061e3a376fa8779883</data>
    </edge>
    <edge source="&quot;FEW-SHOT PROMPTING&quot;" target="&quot;EXEMPLAR FORMAT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Exemplar Format influences the performance of Few-Shot Prompting by affecting how exemplars are structured and presented."</data>
      <data key="d6">e845d3c15484b3061e3a376fa8779883</data>
    </edge>
    <edge source="&quot;FEW-SHOT PROMPTING&quot;" target="&quot;EXEMPLAR SIMILARITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Exemplar Similarity is beneficial for Few-Shot Prompting as selecting similar exemplars to the test sample generally improves performance."</data>
      <data key="d6">e845d3c15484b3061e3a376fa8779883</data>
    </edge>
    <edge source="&quot;FEW-SHOT PROMPTING&quot;" target="&quot;K-NEAREST NEIGHBOR (KNN)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"K-Nearest Neighbor (KNN) is used in Few-Shot Prompting to select exemplars similar to the test sample, boosting performance."</data>
      <data key="d6">e845d3c15484b3061e3a376fa8779883</data>
    </edge>
    <edge source="&quot;FEW-SHOT PROMPTING&quot;" target="&quot;VOTE-K&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Vote-K is a method used in Few-Shot Prompting to select similar exemplars in a way that ensures diversity and representativeness."</data>
      <data key="d6">e845d3c15484b3061e3a376fa8779883</data>
    </edge>
    <edge source="&quot;FEW-SHOT PROMPTING&quot;" target="&quot;SELF-GENERATED IN-CONTEXT LEARNING (SG-ICL)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Generated In-Context Learning (SG-ICL) is used in Few-Shot Prompting to automatically generate exemplars when training data is unavailable."</data>
      <data key="d6">e845d3c15484b3061e3a376fa8779883</data>
    </edge>
    <edge source="&quot;FEW-SHOT PROMPTING&quot;" target="&quot;PROMPT MINING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Mining improves Few-Shot Prompting by discovering optimal prompt templates through large corpus analysis."</data>
      <data key="d6">e845d3c15484b3061e3a376fa8779883</data>
    </edge>
    <edge source="&quot;OPTIMIZING ICL&quot;" target="&quot;BANSAL ET AL., 2023&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Bansal et al., 2023 is a significant work focused on the goal of optimizing In-Context Learning techniques."</data>
      <data key="d6">82d58329a3cd23550be3e22f1740f8ae</data>
    </edge>
    <edge source="&quot;UNDERSTANDING ICL&quot;" target="&quot;SI ET AL., 2023A&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Si et al., 2023a is a notable study aimed at the goal of understanding In-Context Learning."</data>
      <data key="d6">82d58329a3cd23550be3e22f1740f8ae</data>
    </edge>
    <edge source="&quot;UNDERSTANDING ICL&quot;" target="&quot;&#352;TEF&#193;NIK AND KADL&#268;&#205;K, 2023&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"&#352;tef&#225;nik and Kadl&#269;&#237;k, 2023 is an important research work contributing to the goal of understanding In-Context Learning."</data>
      <data key="d6">82d58329a3cd23550be3e22f1740f8ae</data>
    </edge>
    <edge source="&quot;EMOTION PROMPTING&quot;" target="&quot;STYLE PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Emotion Prompting and Style Prompting both aim to improve LLM performance by incorporating specific elements into the prompt."</data>
      <data key="d6">d397224fef0666e16112e5d47a2e1139</data>
    </edge>
    <edge source="&quot;ROLE PROMPTING&quot;" target="&quot;STYLE PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Role Prompting and Style Prompting both aim to shape the output of a GenAI by specifying roles or styles."</data>
      <data key="d6">d397224fef0666e16112e5d47a2e1139</data>
    </edge>
    <edge source="&quot;SIMTOM&quot;" target="&quot;SYSTEM 2 ATTENTION (S2A)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both System 2 Attention (S2A) and SimToM aim to eliminate irrelevant information in the prompt to improve the final response."</data>
      <data key="d6">d397224fef0666e16112e5d47a2e1139</data>
    </edge>
    <edge source="&quot;SIMTOM&quot;" target="&quot;SELF-ASK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Ask and SimToM both involve generating additional questions or facts to improve the final response."</data>
      <data key="d6">d397224fef0666e16112e5d47a2e1139</data>
    </edge>
    <edge source="&quot;THOUGHT GENERATION&quot;" target="&quot;CHAIN-OF-THOUGHT (COT) PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chain-of-Thought (CoT) Prompting is a specific technique under the broader category of Thought Generation."</data>
      <data key="d6">d397224fef0666e16112e5d47a2e1139</data>
    </edge>
    <edge source="&quot;ANALOGICAL PROMPTING&quot;" target="&quot;CHAIN-OF-THOUGHTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Analogical Prompting is similar to SG-ICL and includes chains-of-thoughts, improving performance in specific tasks."</data>
      <data key="d6">f4b740e8b0c84e29c7990fc370919464</data>
    </edge>
    <edge source="&quot;STEP-BACK PROMPTING&quot;" target="&quot;CHAIN-OF-THOUGHTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Step-Back Prompting is a modification of Chain-of-Thoughts that involves asking a high-level question before reasoning."</data>
      <data key="d6">f4b740e8b0c84e29c7990fc370919464</data>
    </edge>
    <edge source="&quot;FEW-SHOT COT&quot;" target="&quot;CHAIN-OF-THOUGHTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-Shot CoT presents the LLM with multiple exemplars including chains-of-thoughts, enhancing performance."</data>
      <data key="d6">f4b740e8b0c84e29c7990fc370919464</data>
    </edge>
    <edge source="&quot;FEW-SHOT COT&quot;" target="&quot;ZERO-SHOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-Shot CoT and Zero-Shot are subdomains of prompt engineering, with Few-Shot CoT performing the best in the study."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;ENSEMBLING&quot;" target="&quot;DEMONSTRATION ENSEMBLING (DENSE)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Demonstration Ensembling (DENSE) is a specific implementation of Ensembling, creating multiple few-shot prompts and aggregating their outputs."</data>
      <data key="d6">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </edge>
    <edge source="&quot;ENSEMBLING&quot;" target="&quot;MIXTURE OF REASONING EXPERTS (MORE)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Mixture of Reasoning Experts (MoRE) is another specific implementation of Ensembling, using different specialized prompts for various reasoning types and selecting the best answer based on an agreement score."</data>
      <data key="d6">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </edge>
    <edge source="&quot;ENSEMBLING&quot;" target="&quot;MAX MUTUAL INFORMATION METHOD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Max Mutual Information Method is an Ensembling technique that selects the optimal prompt template based on mutual information between the prompt and the LLM&#8217;s outputs."</data>
      <data key="d6">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </edge>
    <edge source="&quot;SELF-CRITICISM&quot;" target="&quot;SELF-CALIBRATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Calibration is a specific approach within the broader subdomain of Self-Criticism."</data>
      <data key="d6">0b1362066be4992987aeec37198a7788</data>
    </edge>
    <edge source="&quot;SELF-CRITICISM&quot;" target="&quot;SELF-REFINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Refine is an iterative framework that falls under the broader subdomain of Self-Criticism."</data>
      <data key="d6">0b1362066be4992987aeec37198a7788</data>
    </edge>
    <edge source="&quot;SELF-CRITICISM&quot;" target="&quot;REVERSING CHAIN-OF-THOUGHT (RCOT)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RCoT is a method that involves self-criticism by checking for inconsistencies and providing feedback."</data>
      <data key="d6">0b1362066be4992987aeec37198a7788</data>
    </edge>
    <edge source="&quot;SELF-CRITICISM&quot;" target="&quot;SELF-VERIFICATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Verification is a method that involves generating multiple solutions and verifying them, which is a form of self-criticism."</data>
      <data key="d6">0b1362066be4992987aeec37198a7788</data>
    </edge>
    <edge source="&quot;SELF-CRITICISM&quot;" target="&quot;CHAIN-OF-VERIFICATION (COVE)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"COVE involves generating related questions to verify an answer, which is a form of self-criticism."</data>
      <data key="d6">0b1362066be4992987aeec37198a7788</data>
    </edge>
    <edge source="&quot;SELF-CRITICISM&quot;" target="&quot;CUMULATIVE REASONING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cumulative Reasoning involves evaluating and refining steps in answering a question, which is a form of self-criticism."</data>
      <data key="d6">0b1362066be4992987aeec37198a7788</data>
    </edge>
    <edge source="&quot;SELF-CRITICISM&quot;" target="&quot;GENAI SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Criticism is a technique used in GenAI systems to improve the quality of generated outputs."</data>
      <data key="d6">0b1362066be4992987aeec37198a7788</data>
    </edge>
    <edge source="&quot;CHAIN-OF-VERIFICATION&quot;" target="&quot;ACTIVE PROMPTING WITH CHAIN-OF-THOUGHT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Chain-of-Verification and Active Prompting with Chain-of-Thought involve techniques to improve the performance of large language models."</data>
      <data key="d6">e5878afbfbf5194f1da3540eaa88fe65</data>
    </edge>
    <edge source="&quot;SELF-CALIBRATION&quot;" target="&quot;GENAI SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Calibration is a technique used in GenAI systems to gauge confidence levels in generated outputs."</data>
      <data key="d6">0b1362066be4992987aeec37198a7788</data>
    </edge>
    <edge source="&quot;SELF-REFINE&quot;" target="&quot;GENAI SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Refine is a technique used in GenAI systems to iteratively improve generated outputs."</data>
      <data key="d6">0b1362066be4992987aeec37198a7788</data>
    </edge>
    <edge source="&quot;SELF-REFINE&quot;" target="&quot;BIAS AND FAIRNESS IN MACHINE LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-refine techniques can be applied to improve bias and fairness in machine learning models by iteratively refining the models based on self-feedback."</data>
      <data key="d6">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </edge>
    <edge source="&quot;SELF-VERIFICATION&quot;" target="&quot;GENAI SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Verification is a technique used in GenAI systems to verify the correctness of generated outputs."</data>
      <data key="d6">0b1362066be4992987aeec37198a7788</data>
    </edge>
    <edge source="&quot;CUMULATIVE REASONING&quot;" target="&quot;GENAI SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cumulative Reasoning is a technique used in GenAI systems to improve logical inference and problem-solving."</data>
      <data key="d6">0b1362066be4992987aeec37198a7788</data>
    </edge>
    <edge source="&quot;CUMULATIVE REASONING&quot;" target="&quot;ACTIVE EXAMPLE SELECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cumulative Reasoning and Active Example Selection both involve reasoning and learning processes in large language models."</data>
      <data key="d6">0274e77e2fcec8973c9768c464c6e82d</data>
    </edge>
    <edge source="&quot;DECOMPOSITION&quot;" target="&quot;LEAST-TO-MOST PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Least-to-Most Prompting is a specific technique under the broader subdomain of Decomposition, focusing on breaking down problems into sub-problems."</data>
      <data key="d6">589a9782efd8ac3ff7d79dba07974e2b</data>
    </edge>
    <edge source="&quot;DECOMPOSITION&quot;" target="&quot;DECOMPOSED PROMPTING (DECOMP)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Decomposed Prompting (DECOMP) is another technique under Decomposition, breaking down problems into sub-problems and sending them to different functions."</data>
      <data key="d6">589a9782efd8ac3ff7d79dba07974e2b</data>
    </edge>
    <edge source="&quot;DECOMPOSITION&quot;" target="&quot;PLAN-AND-SOLVE PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Plan-and-Solve Prompting involves breaking down problems into steps, aligning with the principles of Decomposition."</data>
      <data key="d6">589a9782efd8ac3ff7d79dba07974e2b</data>
    </edge>
    <edge source="&quot;DECOMPOSITION&quot;" target="&quot;TREE-OF-THOUGHT (TOT)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Tree-of-Thought (ToT) uses a tree-like structure to break down problems into steps, fitting within the Decomposition subdomain."</data>
      <data key="d6">589a9782efd8ac3ff7d79dba07974e2b</data>
    </edge>
    <edge source="&quot;DECOMPOSITION&quot;" target="&quot;RECURSION-OF-THOUGHT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recursion-of-Thought involves breaking down complex problems into simpler sub-problems, aligning with the Decomposition subdomain."</data>
      <data key="d6">589a9782efd8ac3ff7d79dba07974e2b</data>
    </edge>
    <edge source="&quot;SKELETON-OF-THOUGHT&quot;" target="&quot;PROGRAM-OF-THOUGHTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Skeleton-of-Thought and Program-of-Thoughts both involve breaking down problems into sub-problems, but Skeleton-of-Thought focuses on parallelization to accelerate answer speed."</data>
      <data key="d6">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </edge>
    <edge source="&quot;LABEL QUALITY&quot;" target="&quot;EXEMPLAR SIMILARITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Label Quality and Exemplar Similarity are critical factors in ensuring the accuracy and relevance of data exemplars."</data>
      <data key="d6">5d5844de9a93093f225ca41ba18f9a89</data>
    </edge>
    <edge source="&quot;EXEMPLAR FORMAT&quot;" target="&quot;EXEMPLAR QUANTITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Exemplar Format and Exemplar Quantity both influence the consistency and performance of the model by standardizing and increasing the number of exemplars."</data>
      <data key="d6">5d5844de9a93093f225ca41ba18f9a89</data>
    </edge>
    <edge source="&quot;EXEMPLAR ORDERING&quot;" target="&quot;EXEMPLAR LABEL DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Exemplar Ordering and Exemplar Label Distribution both affect model behavior and performance by influencing how exemplars are presented and labeled."</data>
      <data key="d6">5d5844de9a93093f225ca41ba18f9a89</data>
    </edge>
    <edge source="&quot;DONG ET AL., 2023&quot;" target="&quot;ZHAO ET AL., 2021A&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Dong et al., 2023 and Zhao et al., 2021a discuss the critical influence of exemplar selection and order on model performance."</data>
      <data key="d6">5d5844de9a93093f225ca41ba18f9a89</data>
    </edge>
    <edge source="&quot;LU ET AL., 2021&quot;" target="&quot;YE AND DURRETT, 2023&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Lu et al., 2021 and Ye and Durrett, 2023 both examine the impact of exemplar selection and order on model behavior and performance."</data>
      <data key="d6">5d5844de9a93093f225ca41ba18f9a89</data>
    </edge>
    <edge source="&quot;BROWN ET AL., 2020&quot;" target="&quot;LIU ET AL., 2021&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Brown et al., 2020 and Liu et al., 2021 both discuss the effects of exemplar quantity and order on model performance."</data>
      <data key="d6">5d5844de9a93093f225ca41ba18f9a89</data>
    </edge>
    <edge source="&quot;KUMAR AND TALUKDAR, 2021&quot;" target="&quot;RUBIN ET AL., 2022&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Kumar and Talukdar, 2021 and Rubin et al., 2022 both examine the impact of exemplar order on model accuracy and behavior."</data>
      <data key="d6">5d5844de9a93093f225ca41ba18f9a89</data>
    </edge>
    <edge source="&quot;LENS&quot;" target="&quot;ACTIVE EXAMPLE SELECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both LENS and Active Example Selection are techniques that leverage iterative processes to improve prompt performance."</data>
      <data key="d6">d397224fef0666e16112e5d47a2e1139</data>
    </edge>
    <edge source="&quot;ZERO-SHOT PROMPTING&quot;" target="&quot;CHAIN-OF-THOUGHT (COT) PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Zero-Shot Prompting can be combined with Chain-of-Thought (CoT) Prompting to enhance LLM performance."</data>
      <data key="d6">d397224fef0666e16112e5d47a2e1139</data>
    </edge>
    <edge source="&quot;REPHRASE AND RESPOND (RAR)&quot;" target="&quot;RE-READING (RE2)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Rephrase and Respond (RaR) and Re-reading (RE2) both involve rephrasing or repeating the question to improve the final answer."</data>
      <data key="d6">d397224fef0666e16112e5d47a2e1139</data>
    </edge>
    <edge source="&quot;CHAIN-OF-THOUGHT (COT) PROMPTING&quot;" target="&quot;MULTILINGUAL PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chain-of-Thought (CoT) Prompting is a technique within Multilingual Prompting to construct reasoning paths in different languages."</data>
      <data key="d6">45c77c52a93a949222fda99a95e0c3d6</data>
    </edge>
    <edge source="&quot;CHAIN-OF-THOUGHT (COT) PROMPTING&quot;" target="&quot;XLT (CROSS-LINGUAL THOUGHT) PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"XLT (Cross-Lingual Thought) Prompting is a specific method of Chain-of-Thought (CoT) Prompting in multilingual settings."</data>
      <data key="d6">45c77c52a93a949222fda99a95e0c3d6</data>
    </edge>
    <edge source="&quot;CHAIN-OF-THOUGHT (COT) PROMPTING&quot;" target="&quot;CROSS-LINGUAL SELF CONSISTENT PROMPTING (CLSP)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cross-Lingual Self Consistent Prompting (CLSP) is a specific method of Chain-of-Thought (CoT) Prompting in multilingual settings."</data>
      <data key="d6">45c77c52a93a949222fda99a95e0c3d6</data>
    </edge>
    <edge source="&quot;CHAIN-OF-THOUGHTS&quot;" target="&quot;ZERO-SHOT-COT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Zero-Shot-CoT is a version of Chain-of-Thoughts that does not require exemplars and uses thought-inducing phrases."</data>
      <data key="d6">f4b740e8b0c84e29c7990fc370919464</data>
    </edge>
    <edge source="&quot;CHAIN-OF-THOUGHTS&quot;" target="&quot;THREAD-OF-THOUGHT (THOT) PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Thread-of-Thought (ThoT) Prompting is an improved thought inducer for Chain-of-Thoughts reasoning."</data>
      <data key="d6">f4b740e8b0c84e29c7990fc370919464</data>
    </edge>
    <edge source="&quot;CHAIN-OF-THOUGHTS&quot;" target="&quot;TABULAR CHAIN-OF-THOUGHT (TAB-COT)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Tabular Chain-of-Thought (Tab-CoT) is a Zero-Shot CoT prompt that structures reasoning as a markdown table."</data>
      <data key="d6">f4b740e8b0c84e29c7990fc370919464</data>
    </edge>
    <edge source="&quot;CHAIN-OF-THOUGHTS&quot;" target="&quot;CONTRASTIVE COT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Contrastive CoT Prompting adds correct and incorrect exemplars to the CoT prompt to improve reasoning."</data>
      <data key="d6">f4b740e8b0c84e29c7990fc370919464</data>
    </edge>
    <edge source="&quot;CHAIN-OF-THOUGHTS&quot;" target="&quot;UNCERTAINTY-ROUTED COT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Uncertainty-Routed CoT Prompting samples multiple CoT reasoning paths and selects the majority if above a threshold."</data>
      <data key="d6">f4b740e8b0c84e29c7990fc370919464</data>
    </edge>
    <edge source="&quot;CHAIN-OF-THOUGHTS&quot;" target="&quot;COMPLEXITY-BASED PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Complexity-based Prompting selects complex examples for annotation and uses a majority vote among reasoning chains."</data>
      <data key="d6">f4b740e8b0c84e29c7990fc370919464</data>
    </edge>
    <edge source="&quot;ZERO-SHOT-COT&quot;" target="&quot;FORMAL BENCHMARK EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Zero-Shot-CoT is one of the prompting techniques evaluated in the Formal Benchmark Evaluation."</data>
      <data key="d6">f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </edge>
    <edge source="&quot;ZERO-SHOT-COT&quot;" target="&quot;ZERO-SHOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Zero-Shot-CoT and Zero-Shot are subdomains of prompt engineering, with Zero-Shot performing better than Zero-Shot-CoT."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;COMPLEXITY-BASED PROMPTING&quot;" target="&quot;MEMORY-OF-THOUGHT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both techniques aim to improve the quality of reasoning and answers by leveraging complex examples and unlabeled training exemplars, respectively."</data>
      <data key="d6">589a9782efd8ac3ff7d79dba07974e2b</data>
    </edge>
    <edge source="&quot;COMPLEXITY-BASED PROMPTING&quot;" target="&quot;AUTOMATIC CHAIN-OF-THOUGHT (AUTO-COT) PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both methods involve generating and using chains of thought to improve reasoning and answer quality."</data>
      <data key="d6">589a9782efd8ac3ff7d79dba07974e2b</data>
    </edge>
    <edge source="&quot;LEAST-TO-MOST PROMPTING&quot;" target="&quot;EVOKING REASONING VIA ABSTRACTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both techniques aim to enhance the reasoning capabilities of large language models through structured prompts."</data>
      <data key="d6">c7285f7847ef45ed85779d7966753855</data>
    </edge>
    <edge source="&quot;PROGRAM-OF-THOUGHTS&quot;" target="&quot;FAITHFUL CHAIN-OF-THOUGHT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Program-of-Thoughts and Faithful Chain-of-Thought use LLMs to generate reasoning steps, with the latter incorporating different types of symbolic languages in a task-dependent fashion."</data>
      <data key="d6">ff7ad60eb931a85ac1b0393ecafb8018</data>
    </edge>
    <edge source="&quot;CONSISTENCY-BASED SELF-ADAPTIVE PROMPTING (COSP)&quot;" target="&quot;UNIVERSAL SELF-ADAPTIVE PROMPTING (USP)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"USP builds upon the success of COSP, aiming to make it generalizable to all tasks."</data>
      <data key="d6">0b1362066be4992987aeec37198a7788</data>
    </edge>
    <edge source="&quot;REVERSING CHAIN-OF-THOUGHT (RCOT)&quot;" target="&quot;GENAI SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RCoT is a technique used in GenAI systems to check for inconsistencies in generated outputs."</data>
      <data key="d6">0b1362066be4992987aeec37198a7788</data>
    </edge>
    <edge source="&quot;CHAIN-OF-VERIFICATION (COVE)&quot;" target="&quot;GENAI SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"COVE is a technique used in GenAI systems to verify the correctness of generated outputs."</data>
      <data key="d6">0b1362066be4992987aeec37198a7788</data>
    </edge>
    <edge source="&quot;CHAIN-OF-THOUGHT PROMPTING&quot;" target="&quot;CHAIN-OF-THOUGHT REASONING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chain-of-Thought Prompting is a technique used to elicit Chain-of-Thought Reasoning in language models."</data>
      <data key="d6">42d8c3ad092ec18e28ff718709b0b472</data>
    </edge>
    <edge source="&quot;GENAI MODELS&quot;" target="&quot;BENCHMARK DATASETS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"GenAI Models are often evaluated using Benchmark Datasets to measure their performance."</data>
      <data key="d6">83a60257c9adae8c826e73ef32d16dd0</data>
    </edge>
    <edge source="&quot;AUTOPROMPT&quot;" target="&quot;LLM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"AutoPrompt uses a frozen LLM to generate prompt templates with updated 'trigger tokens'."</data>
      <data key="d6">981e367f454fd6805ff2ad123c75b85e</data>
    </edge>
    <edge source="&quot;AUTOMATIC PROMPT ENGINEER (APE)&quot;" target="&quot;LLM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"APE generates Zero-Shot instruction prompts using a set of exemplars and iterates on them with an LLM."</data>
      <data key="d6">981e367f454fd6805ff2ad123c75b85e</data>
    </edge>
    <edge source="&quot;RESEARCH PAPERS&quot;" target="&quot;CITATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Citations are used to measure the influence and usage of Research Papers in the field."</data>
      <data key="d6">83a60257c9adae8c826e73ef32d16dd0</data>
    </edge>
    <edge source="&quot;GRADIENTFREE INSTRUCTIONAL PROMPT SEARCH (GRIPS)&quot;" target="&quot;LLM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"GrIPS creates prompt variations using complex operations and an LLM."</data>
      <data key="d6">981e367f454fd6805ff2ad123c75b85e</data>
    </edge>
    <edge source="&quot;PROMPT OPTIMIZATION WITH TEXTUAL GRADIENTS (PROTEGI)&quot;" target="&quot;LLM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ProTeGi improves prompt templates through a multi-step process involving an LLM and a bandit algorithm."</data>
      <data key="d6">981e367f454fd6805ff2ad123c75b85e</data>
    </edge>
    <edge source="&quot;RLPROMPT&quot;" target="&quot;LLM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RLPrompt uses a frozen LLM with an unfrozen module to generate and score prompt templates."</data>
      <data key="d6">981e367f454fd6805ff2ad123c75b85e</data>
    </edge>
    <edge source="&quot;RLPROMPT&quot;" target="&quot;REPHRASE AND RESPOND&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both RLPrompt and Rephrase and Respond involve optimizing prompts and questions for large language models, showing a connection in their goals."</data>
      <data key="d6">e5878afbfbf5194f1da3540eaa88fe65</data>
    </edge>
    <edge source="&quot;DIALOGUE-COMPRISED POLICY-GRADIENT-BASED DISCRETE PROMPT OPTIMIZATION (DP2O)&quot;" target="&quot;LLM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"DP2O constructs prompts through reinforcement learning and conversations with an LLM."</data>
      <data key="d6">981e367f454fd6805ff2ad123c75b85e</data>
    </edge>
    <edge source="&quot;ANSWER SHAPE&quot;" target="&quot;ANSWER SPACE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Answer Shape and Answer Space are related as they both define the structure and domain of values for an answer in a model's output."</data>
      <data key="d6">45c77c52a93a949222fda99a95e0c3d6</data>
    </edge>
    <edge source="&quot;LLM&quot;" target="&quot;AGENT LLMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Agent LLMs are a specific type of LLM that can interact with external systems to perform tasks."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;ANSWER EXTRACTOR&quot;" target="&quot;REGEX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Answer Extractor and Regex are related as regexes are often used as a simple function to extract answers from the model output."</data>
      <data key="d6">45c77c52a93a949222fda99a95e0c3d6</data>
    </edge>
    <edge source="&quot;ANSWER EXTRACTOR&quot;" target="&quot;SEPARATE LLM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Answer Extractor and Separate LLM are related as a separate LLM can be used to extract answers when regexes are insufficient."</data>
      <data key="d6">45c77c52a93a949222fda99a95e0c3d6</data>
    </edge>
    <edge source="&quot;MULTILINGUAL PROMPTING&quot;" target="&quot;TRANSLATE FIRST PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Translate First Prompting is a technique within Multilingual Prompting to improve model performance in non-English settings."</data>
      <data key="d6">45c77c52a93a949222fda99a95e0c3d6</data>
    </edge>
    <edge source="&quot;X-INSTA PROMPTING&quot;" target="&quot;ICL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"X-InSTA Prompting is an approach within the subdomain of ICL, focusing on aligning in-context examples with input sentences for classification tasks."</data>
      <data key="d6">ebba9603b39b6606ba9902c9cf61fecb</data>
    </edge>
    <edge source="&quot;ICL&quot;" target="&quot;IN-CLT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"In-CLT Prompting is another approach within the subdomain of ICL, leveraging both source and target languages to create in-context examples."</data>
      <data key="d6">ebba9603b39b6606ba9902c9cf61fecb</data>
    </edge>
    <edge source="&quot;ICL&quot;" target="&quot;PARC&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"PARC is a framework within the subdomain of ICL, designed to enhance cross-lingual transfer performance by retrieving relevant exemplars from a high-resource language."</data>
      <data key="d6">ebba9603b39b6606ba9902c9cf61fecb</data>
    </edge>
    <edge source="&quot;ENGLISH PROMPT TEMPLATE&quot;" target="&quot;TASK LANGUAGE PROMPT TEMPLATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"English Prompt Template and Task Language Prompt Template are two different strategies for constructing prompt templates in multilingual tasks."</data>
      <data key="d6">ebba9603b39b6606ba9902c9cf61fecb</data>
    </edge>
    <edge source="&quot;TASK LANGUAGE PROMPT TEMPLATE&quot;" target="&quot;BUFFET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"BUFFET uses task language prompts, which is a strategy within the Task Language Prompt Template approach."</data>
      <data key="d6">ebba9603b39b6606ba9902c9cf61fecb</data>
    </edge>
    <edge source="&quot;TASK LANGUAGE PROMPT TEMPLATE&quot;" target="&quot;LONGBENCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LongBench uses task language prompts, which is a strategy within the Task Language Prompt Template approach."</data>
      <data key="d6">ebba9603b39b6606ba9902c9cf61fecb</data>
    </edge>
    <edge source="&quot;BUFFET&quot;" target="&quot;LONGBENCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both BUFFET and LongBench are multilingual prompting benchmarks used for language-specific use cases."</data>
      <data key="d6">8bafc5999ce3abba6f261770c5945604</data>
    </edge>
    <edge source="&quot;MUENNIGHOFF ET AL. (2023)&quot;" target="&quot;NAMBI ET AL. (2023)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both research groups discuss different aspects of translation methods and template performance in multilingual prompting."</data>
      <data key="d6">8bafc5999ce3abba6f261770c5945604</data>
    </edge>
    <edge source="&quot;HE ET AL. (2023B)&quot;" target="&quot;MULTI-ASPECT PROMPTING AND SELECTION (MAPS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"He et al. (2023b) developed the MAPS framework for high-quality translation."</data>
      <data key="d6">8bafc5999ce3abba6f261770c5945604</data>
    </edge>
    <edge source="&quot;LU ET AL. (2023B)&quot;" target="&quot;CHAIN-OF-DICTIONARY (COD)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Lu et al. (2023b) developed the CoD method for machine translation."</data>
      <data key="d6">8bafc5999ce3abba6f261770c5945604</data>
    </edge>
    <edge source="&quot;GHAZVININEJAD ET AL. (2023)&quot;" target="&quot;DICTIONARY-BASED PROMPTING FOR MACHINE TRANSLATION (DIPMT)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ghazvininejad et al. (2023) developed the DiPMT method, which is similar to CoD."</data>
      <data key="d6">8bafc5999ce3abba6f261770c5945604</data>
    </edge>
    <edge source="&quot;PUDUPPULLY ET AL. (2023)&quot;" target="&quot;DECOMPOSED PROMPTING FOR MT (DECOMT)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Puduppully et al. (2023) developed the DecoMT method for machine translation."</data>
      <data key="d6">8bafc5999ce3abba6f261770c5945604</data>
    </edge>
    <edge source="&quot;PILAULT ET AL. (2023)&quot;" target="&quot;INTERACTIVE-CHAIN-PROMPTING (ICP)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Pilault et al. (2023) developed the ICP method for dealing with ambiguities in translation."</data>
      <data key="d6">8bafc5999ce3abba6f261770c5945604</data>
    </edge>
    <edge source="&quot;YANG ET AL. (2023D)&quot;" target="&quot;ITERATIVE PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Yang et al. (2023d) developed the Iterative Prompting method for refining translations with human feedback."</data>
      <data key="d6">8bafc5999ce3abba6f261770c5945604</data>
    </edge>
    <edge source="&quot;ACCURATE AND NUANCED TRANSLATION&quot;" target="&quot;HUMAN-IN-THE-LOOP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Involving humans in the translation process is aimed at achieving accurate and nuanced translations."</data>
      <data key="d6">8bafc5999ce3abba6f261770c5945604</data>
    </edge>
    <edge source="&quot;MULTIMODAL PROMPTING TECHNIQUE&quot;" target="&quot;MULTIMODAL IN-CONTEXT LEARNING (ICL)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multimodal In-Context Learning is a type of Multimodal Prompting Technique that extends ICL to include multiple modalities."</data>
      <data key="d6">6edacbda20b2fdd4077246c7b271a8b5</data>
    </edge>
    <edge source="&quot;MULTIMODAL PROMPTING TECHNIQUE&quot;" target="&quot;MULTIMODAL CHAIN-OF-THOUGHT (COT)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multimodal Chain-of-Thought is a type of Multimodal Prompting Technique that extends CoT to the image domain."</data>
      <data key="d6">6edacbda20b2fdd4077246c7b271a8b5</data>
    </edge>
    <edge source="&quot;MULTIMODAL IN-CONTEXT LEARNING (ICL)&quot;" target="&quot;PAIRED-IMAGE PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Paired-Image Prompting is a method within Multimodal In-Context Learning that involves showing the model two images to demonstrate a transformation."</data>
      <data key="d6">6edacbda20b2fdd4077246c7b271a8b5</data>
    </edge>
    <edge source="&quot;MULTIMODAL IN-CONTEXT LEARNING (ICL)&quot;" target="&quot;IMAGE-AS-TEXT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Image-as-Text Prompting is a method within Multimodal In-Context Learning that generates textual descriptions of images."</data>
      <data key="d6">6edacbda20b2fdd4077246c7b271a8b5</data>
    </edge>
    <edge source="&quot;MULTIMODAL CHAIN-OF-THOUGHT (COT)&quot;" target="&quot;DUTY DISTINCT CHAIN-OF-THOUGHT (DDCOT)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Duty Distinct Chain-of-Thought is an extension of Multimodal Chain-of-Thought that creates and solves subquestions in multimodal settings."</data>
      <data key="d6">6edacbda20b2fdd4077246c7b271a8b5</data>
    </edge>
    <edge source="&quot;MULTIMODAL CHAIN-OF-THOUGHT (COT)&quot;" target="&quot;MULTIMODAL GRAPH-OF-THOUGHT (GOT)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multimodal Graph-of-Thought is an extension of Multimodal Chain-of-Thought that uses a thought graph and image captions to generate rationales and answers."</data>
      <data key="d6">6edacbda20b2fdd4077246c7b271a8b5</data>
    </edge>
    <edge source="&quot;MULTIMODAL CHAIN-OF-THOUGHT (COT)&quot;" target="&quot;CHAIN-OF-IMAGES (COI)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chain-of-Images is a multimodal extension of Chain-of-Thought prompting that generates images as part of its thought process."</data>
      <data key="d6">6edacbda20b2fdd4077246c7b271a8b5</data>
    </edge>
    <edge source="&quot;ADEPT&quot;" target="&quot;ACT-1&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Adept developed the ACT-1 transformer model, contributing to advancements in AI technology."</data>
      <data key="d6">6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </edge>
    <edge source="&quot;AGENT LLMS&quot;" target="&quot;OPENAI ASSISTANTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"OpenAI Assistants are examples of Agent LLMs developed by OpenAI."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;AGENT LLMS&quot;" target="&quot;LANGCHAIN AGENTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LangChain Agents are examples of Agent LLMs developed by LangChain."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;AGENT LLMS&quot;" target="&quot;LLAMAINDEX AGENTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LlamaIndex Agents are examples of Agent LLMs developed by LlamaIndex."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;TOOL-INTEGRATED REASONING AGENT (TORA)&quot;" target="&quot;PAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ToRA is similar to PAL but interleaves code and reasoning steps instead of a single code generation step."</data>
      <data key="d6">eed969adf8c7eb4a89355c851663c87a</data>
    </edge>
    <edge source="&quot;REACT&quot;" target="&quot;LARGE LANGUAGE MODELS AS OPTIMIZERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Large Language Models as Optimizers and ReAct both involve the use of language models to enhance task performance through optimization and action."</data>
      <data key="d6">ccdfd3415647f13f577d728a5a0256b1</data>
    </edge>
    <edge source="&quot;REFLEXION&quot;" target="&quot;REASONING AND ACTING (REACT)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reflexion builds on ReAct by adding a layer of introspection and generating reflections on actions."</data>
      <data key="d6">eed969adf8c7eb4a89355c851663c87a</data>
    </edge>
    <edge source="&quot;LIFELONG LEARNING AGENTS&quot;" target="&quot;VOYAGER&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Voyager" is a type of "Lifelong Learning Agent" that continuously learns from its environment. It proposes tasks, generates code, and saves actions as part of a long-term memory system. This enables "Voyager" to adapt and improve its performance over time, making it a robust example of a "Lifelong Learning Agent."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa,eed969adf8c7eb4a89355c851663c87a</data>
    </edge>
    <edge source="&quot;LIFELONG LEARNING AGENTS&quot;" target="&quot;GITM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"GITM is an example of a Lifelong Learning Agent that focuses on general intelligence through continuous learning."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;LIFELONG LEARNING AGENTS&quot;" target="&quot;GHOST IN THE MINECRAFT (GITM)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"GITM is a type of Lifelong Learning Agent that breaks down goals into subgoals and uses structured text for actions."</data>
      <data key="d6">eed969adf8c7eb4a89355c851663c87a</data>
    </edge>
    <edge source="&quot;VOYAGER&quot;" target="&quot;CHAIN-OF-THOUGHT REASONING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Voyager, as an embodied agent, likely utilizes chain-of-thought reasoning in its operations."</data>
      <data key="d6">153eeb5a63e650f2cd12f700ffe3e71f</data>
    </edge>
    <edge source="&quot;RETRIEVAL AUGMENTED GENERATION&quot;" target="&quot;IRCOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"IRCoT is an example of a Retrieval Augmented Generation technique."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;RETRIEVAL AUGMENTED GENERATION&quot;" target="&quot;VERIFY-AND-EDIT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Verify-and-Edit is an example of a Retrieval Augmented Generation technique."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;RETRIEVAL AUGMENTED GENERATION&quot;" target="&quot;ITERATIVE RETRIEVAL AUGMENTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Iterative Retrieval Augmentation is an example of a Retrieval Augmented Generation technique."</data>
      <data key="d6">cbd06bb38a855be4a07883f499014eaa</data>
    </edge>
    <edge source="&quot;VERIFY-AND-EDIT&quot;" target="&quot;MULTIMODAL CHAIN-OF-THOUGHT REASONING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multimodal Chain-of-Thought Reasoning and Verify-and-Edit both involve advanced reasoning frameworks in language models."</data>
      <data key="d6">0274e77e2fcec8973c9768c464c6e82d</data>
    </edge>
    <edge source="&quot;VERIFY-AND-EDIT&quot;" target="&quot;CALIBRATE BEFORE USE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both methods aim to improve the accuracy and reliability of language models, albeit through different approaches."</data>
      <data key="d6">c7285f7847ef45ed85779d7966753855</data>
    </edge>
    <edge source="&quot;VERIFY-AND-EDIT&quot;" target="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Association for Computational Linguistics published the research on the Verify-and-Edit framework."</data>
      <data key="d6">c7285f7847ef45ed85779d7966753855</data>
    </edge>
    <edge source="&quot;TASKWEAVER&quot;" target="&quot;PAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"TaskWeaver is similar to PAL but can also make use of user-defined plugins."</data>
      <data key="d6">eed969adf8c7eb4a89355c851663c87a</data>
    </edge>
    <edge source="&quot;TASKWEAVER&quot;" target="&quot;TOOL LEARNING WITH FOUNDATION MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Taskweaver and Tool Learning with Foundation Models are related to the development and application of intelligent agents and foundation models, indicating a shared focus on advanced AI techniques."</data>
      <data key="d6">42397dc5d60f0a1d799e06290ea52864</data>
    </edge>
    <edge source="&quot;EVALUATION TECHNIQUES&quot;" target="&quot;LLM-EVAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LLM-EVAL is a prompting framework used for evaluation techniques."</data>
      <data key="d6">eed969adf8c7eb4a89355c851663c87a</data>
    </edge>
    <edge source="&quot;EVALUATION TECHNIQUES&quot;" target="&quot;G-EVAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"G-EVAL is a prompting framework used for evaluation techniques."</data>
      <data key="d6">eed969adf8c7eb4a89355c851663c87a</data>
    </edge>
    <edge source="&quot;EVALUATION TECHNIQUES&quot;" target="&quot;CHATEVAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ChatEval is a prompting framework used for evaluation techniques."</data>
      <data key="d6">eed969adf8c7eb4a89355c851663c87a</data>
    </edge>
    <edge source="&quot;EVALUATION TECHNIQUES&quot;" target="&quot;OTHER METHODOLOGIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Other Methodologies include Batch Prompting and Pairwise Evaluation as part of evaluation techniques."</data>
      <data key="d6">eed969adf8c7eb4a89355c851663c87a</data>
    </edge>
    <edge source="&quot;LLM-EVAL&quot;" target="&quot;EVALUATION PERFORMANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LLM-EVAL aims to improve evaluation performance by providing a structured prompt for scoring various variables."</data>
      <data key="d6">a4eb2fbdea1494d271ebc61219d17020</data>
    </edge>
    <edge source="&quot;LLM-EVAL&quot;" target="&quot;CHATGPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Unified multi-dimensional evaluation methods can be used to assess ChatGPT's performance in open-domain conversations."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;G-EVAL&quot;" target="&quot;EVALUATION PERFORMANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"G-EVAL enhances evaluation performance by including AutoCoT steps, which weight answers according to token probabilities."</data>
      <data key="d6">a4eb2fbdea1494d271ebc61219d17020</data>
    </edge>
    <edge source="&quot;CHATEVAL&quot;" target="&quot;EVALUATION PERFORMANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ChatEval uses a multi-agent debate framework to potentially improve evaluation performance through diverse perspectives."</data>
      <data key="d6">a4eb2fbdea1494d271ebc61219d17020</data>
    </edge>
    <edge source="&quot;CHATEVAL&quot;" target="&quot;REFERENCE-FREE TEXT QUALITY EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chateval aims to improve LLM-based evaluators, which is related to the study of reference-free text quality evaluation."</data>
      <data key="d6">5ce886e06455eadec4bcfe91e36b666d</data>
    </edge>
    <edge source="&quot;BATCH PROMPTING&quot;" target="&quot;EVALUATION PERFORMANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Batch Prompting aims to improve evaluation performance by evaluating multiple instances at once, though it may sometimes degrade performance."</data>
      <data key="d6">a4eb2fbdea1494d271ebc61219d17020</data>
    </edge>
    <edge source="&quot;PAIRWISE EVALUATION&quot;" target="&quot;EVALUATION PERFORMANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Pairwise Evaluation affects evaluation performance by comparing texts directly, but generating individual scores is more effective."</data>
      <data key="d6">a4eb2fbdea1494d271ebc61219d17020</data>
    </edge>
    <edge source="&quot;PROMPT HACKING&quot;" target="&quot;PROMPT INJECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Injection is a specific type of Prompt Hacking where user input overrides developer instructions."</data>
      <data key="d6">2dba3160cd0e0ee3943dce308cb9940e</data>
    </edge>
    <edge source="&quot;PROMPT HACKING&quot;" target="&quot;JAILBREAKING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jailbreaking is a specific type of Prompt Hacking where adversarial prompts cause GenAI models to perform unintended actions."</data>
      <data key="d6">2dba3160cd0e0ee3943dce308cb9940e</data>
    </edge>
    <edge source="&quot;PROMPT HACKING&quot;" target="&quot;TRAINING DATA RECONSTRUCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Training Data Reconstruction is a risk associated with Prompt Hacking, where attackers extract training data from GenAI models."</data>
      <data key="d6">2dba3160cd0e0ee3943dce308cb9940e</data>
    </edge>
    <edge source="&quot;PROMPT HACKING&quot;" target="&quot;PROMPT LEAKING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Leaking is a risk associated with Prompt Hacking, where attackers extract the prompt template from an application."</data>
      <data key="d6">2dba3160cd0e0ee3943dce308cb9940e</data>
    </edge>
    <edge source="&quot;PROMPT HACKING&quot;" target="&quot;CODE GENERATION CONCERNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Code Generation Concerns are risks associated with Prompt Hacking, where vulnerabilities in generated code can be exploited."</data>
      <data key="d6">2dba3160cd0e0ee3943dce308cb9940e</data>
    </edge>
    <edge source="&quot;PROMPT INJECTION&quot;" target="&quot;JAILBREAKING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Prompt Injection and Jailbreaking are subdomains of prompt engineering focused on manipulating or bypassing restrictions in large language models."</data>
      <data key="d6">c605e4f0158f18be68214a39b9b54154</data>
    </edge>
    <edge source="&quot;PROMPT INJECTION&quot;" target="&quot;MANUAL REVIEW&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Injection papers were manually reviewed to ensure relevance and to identify any offensive content."</data>
      <data key="d6">29d2b14a56a51f86baa34264697bdd5e</data>
    </edge>
    <edge source="&quot;PROMPT INJECTION&quot;" target="&quot;SEMI-AUTOMATED REVIEW&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Injection papers were also reviewed using semi-automated methods to ensure relevance and to identify any offensive content."</data>
      <data key="d6">29d2b14a56a51f86baa34264697bdd5e</data>
    </edge>
    <edge source="&quot;JAILBREAKING&quot;" target="&quot;PROMPT INJECTION ATTACKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Injection Attacks and Jailbreaking are both techniques used to manipulate the output of language models."</data>
      <data key="d6">42d8c3ad092ec18e28ff718709b0b472</data>
    </edge>
    <edge source="&quot;PROMPT-BASED DEFENSE&quot;" target="&quot;GUARDRAILS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Guardrails are a type of Prompt-based Defense designed to prevent malicious instructions."</data>
      <data key="d6">2dba3160cd0e0ee3943dce308cb9940e</data>
    </edge>
    <edge source="&quot;PROMPT-BASED DEFENSE&quot;" target="&quot;DETECTORS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Detectors are a type of Prompt-based Defense used to identify adversarial prompts."</data>
      <data key="d6">2dba3160cd0e0ee3943dce308cb9940e</data>
    </edge>
    <edge source="&quot;PROMPT-BASED DEFENSE&quot;" target="&quot;CUSTOMER SERVICE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt-based Defense can be used to mitigate prompt injection attacks in customer service chatbots."</data>
      <data key="d6">4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </edge>
    <edge source="&quot;PROMPT-BASED DEFENSE&quot;" target="&quot;SECURITY &amp; PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt-based Defense is a technique used to achieve the goal of improving security and prompting."</data>
      <data key="d6">4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </edge>
    <edge source="&quot;GUARDRAILS&quot;" target="&quot;SECURITY &amp; PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Guardrails are frameworks used to achieve the goal of improving security and prompting."</data>
      <data key="d6">4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </edge>
    <edge source="&quot;DETECTORS&quot;" target="&quot;SECURITY &amp; PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Detectors are tools used to achieve the goal of improving security and prompting."</data>
      <data key="d6">4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </edge>
    <edge source="&quot;PACKAGE HALLUCINATION&quot;" target="&quot;BUGS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Package Hallucination can lead to bugs and security vulnerabilities in LLM-generated code."</data>
      <data key="d6">4aea5d43ff4f1164f45ae3b5b8b7a115</data>
    </edge>
    <edge source="&quot;OVERCONFIDENCE&quot;" target="&quot;CALIBRATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Calibration techniques are used to address the issue of Overconfidence in LLMs."</data>
      <data key="d6">84da286ab749b0f025821313fe535d70</data>
    </edge>
    <edge source="&quot;OVERCONFIDENCE&quot;" target="&quot;SYCOPHANCY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sycophancy can exacerbate the issue of Overconfidence by making the model agree with the user even when incorrect."</data>
      <data key="d6">84da286ab749b0f025821313fe535d70</data>
    </edge>
    <edge source="&quot;OVERCONFIDENCE&quot;" target="&quot;FIGURE 5.2: PROMPT-BASED ALIGNMENT ORGANIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Figure 5.2: Prompt-based Alignment Organization addresses issues related to Overconfidence in LLMs."</data>
      <data key="d6">84da286ab749b0f025821313fe535d70</data>
    </edge>
    <edge source="&quot;CALIBRATION&quot;" target="&quot;VERBALIZED SCORE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Verbalized Score is a specific calibration technique used to generate confidence scores."</data>
      <data key="d6">84da286ab749b0f025821313fe535d70</data>
    </edge>
    <edge source="&quot;CALIBRATION&quot;" target="&quot;FIGURE 5.2: PROMPT-BASED ALIGNMENT ORGANIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Figure 5.2: Prompt-based Alignment Organization addresses issues related to Calibration techniques."</data>
      <data key="d6">84da286ab749b0f025821313fe535d70</data>
    </edge>
    <edge source="&quot;SYCOPHANCY&quot;" target="&quot;FIGURE 5.2: PROMPT-BASED ALIGNMENT ORGANIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Figure 5.2: Prompt-based Alignment Organization addresses issues related to Sycophancy in LLMs."</data>
      <data key="d6">84da286ab749b0f025821313fe535d70</data>
    </edge>
    <edge source="&quot;SHARMA ET AL. (2023)&quot;" target="&quot;WEI ET AL. (2023B)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both research groups found that user opinions and false presumptions can influence LLM outputs, with Wei et al. (2023b) noting heightened sycophancy in larger models."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;SI ET AL. (2023B)&quot;" target="&quot;GANGULI ET AL. (2023)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ganguli et al. (2023) referred to the Vanilla Prompting technique proposed by Si et al. (2023b) as moral self-correction."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;SI ET AL. (2023B)&quot;" target="&quot;MA ET AL. (2023)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both research groups worked on techniques to reduce biases in LLM outputs, with Si et al. (2023b) focusing on balanced demonstrations and Ma et al. (2023) on fairness metrics."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;YAO ET AL. (2023A)&quot;" target="&quot;PESKOV ET AL. (2021)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both research groups contributed to the development of techniques for cultural adaptation in LLMs."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;RAO AND DAUM&#201; III (2019)&quot;" target="&quot;ZHANG AND CHOI (2023)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both research groups developed techniques for question clarification to resolve ambiguity in LLM responses."</data>
      <data key="d6">314fa72b9f7876258bd98d75a005cdb7</data>
    </edge>
    <edge source="&quot;MMLU&quot;" target="&quot;FORMAL BENCHMARK EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MMLU is used as the benchmark for the Formal Benchmark Evaluation of different prompting techniques."</data>
      <data key="d6">f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </edge>
    <edge source="&quot;MMLU&quot;" target="&quot;HUMAN_SEXUALITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Human_Sexuality is a subset of MMLU, indicating that it is part of the larger framework used for evaluating LLMs."</data>
      <data key="d6">e8bf483fffcc91b1512c5796d0d4045a</data>
    </edge>
    <edge source="&quot;GPT-3.5-TURBO&quot;" target="&quot;FORMAL BENCHMARK EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"GPT-3.5-turbo is the language model used for running the Formal Benchmark Evaluation."</data>
      <data key="d6">f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </edge>
    <edge source="&quot;ZERO-SHOT&quot;" target="&quot;FORMAL BENCHMARK EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Zero-Shot is one of the prompting techniques evaluated in the Formal Benchmark Evaluation."</data>
      <data key="d6">f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </edge>
    <edge source="&quot;FEW-SHOT&quot;" target="&quot;FORMAL BENCHMARK EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-Shot is one of the prompting techniques evaluated in the Formal Benchmark Evaluation."</data>
      <data key="d6">f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </edge>
    <edge source="&quot;FEW-SHOT-COT&quot;" target="&quot;FORMAL BENCHMARK EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-Shot-CoT is one of the prompting techniques evaluated in the Formal Benchmark Evaluation."</data>
      <data key="d6">f1e2d01b4dbcfc34401e7d0dffd14e29</data>
    </edge>
    <edge source="&quot;DETECTION OF CRISIS-LEVEL SUICIDE RISK&quot;" target="&quot;SUICIDE PREVENTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Detection of Crisis-Level Suicide Risk is a goal that aligns with the broader goal of Suicide Prevention."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;DETECTION OF CRISIS-LEVEL SUICIDE RISK&quot;" target="&quot;SUICIDE CRISIS SYNDROME (SCS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Suicide Crisis Syndrome (SCS) is a diagnostic approach that can aid in the Detection of Crisis-Level Suicide Risk."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;DETECTION OF CRISIS-LEVEL SUICIDE RISK&quot;" target="&quot;ACUTE SUICIDAL AFFECTIVE DISTURBANCE (ASAD)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Acute Suicidal Affective Disturbance (ASAD) is a diagnostic approach that can aid in the Detection of Crisis-Level Suicide Risk."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;DETECTION OF CRISIS-LEVEL SUICIDE RISK&quot;" target="&quot;MENTAL HEALTH ECOSYSTEM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Detection of Crisis-Level Suicide Risk could have a large impact within the Mental Health Ecosystem."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;ACUTE SUICIDAL AFFECTIVE DISTURBANCE (ASAD)&quot;" target="&quot;NATURALLY OCCURRING LANGUAGE AS A SOURCE OF EVIDENCE IN SUICIDE PREVENTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies focus on suicide prevention, with one using natural language as evidence and the other discussing a specific diagnostic entity."</data>
      <data key="d6">42397dc5d60f0a1d799e06290ea52864</data>
    </edge>
    <edge source="&quot;MENTAL HEALTH ECOSYSTEM&quot;" target="&quot;SUICIDE PREVENTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Suicide Prevention is a goal within the broader Mental Health Ecosystem."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;MENTAL HEALTH ECOSYSTEM&quot;" target="&quot;NATIONAL CENTER FOR HEALTH WORKFORCE ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The National Center for Health Workforce Analysis provides data that impacts the Mental Health Ecosystem."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;MENTAL HEALTH ECOSYSTEM&quot;" target="&quot;CDC&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The CDC provides statistics and data that impact the Mental Health Ecosystem."</data>
      <data key="d6">590db3ee59b442c908a9b425a9be2477</data>
    </edge>
    <edge source="&quot;MENTAL HEALTH ECOSYSTEM&quot;" target="&quot;SUICIDE CRISIS SYNDROME&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Accurately flagging indicators of Suicide Crisis Syndrome in individuals' language could have a large impact within the Mental Health Ecosystem."</data>
      <data key="d6">d27160d0dde304425ccc51df673321b1</data>
    </edge>
    <edge source="&quot;MENTAL HEALTH ECOSYSTEM&quot;" target="&quot;SELF-REPORT QUESTIONNAIRES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Report Questionnaires are part of the Mental Health Ecosystem, used to gather information about individuals' mental health status."</data>
      <data key="d6">d27160d0dde304425ccc51df673321b1</data>
    </edge>
    <edge source="&quot;NATIONAL CENTER FOR HEALTH WORKFORCE ANALYSIS&quot;" target="&quot;BEHAVIORAL HEALTH WORKFORCE, 2023&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The National Center for Health Workforce Analysis published the Behavioral Health Workforce, 2023 report."</data>
      <data key="d6">4d9e8d703c2da8e4775c428e83e87fc9</data>
    </edge>
    <edge source="&quot;CDC&quot;" target="&quot;SUICIDE DATA AND STATISTICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The CDC provides data and statistics on suicide, contributing to public health research and policy-making."</data>
      <data key="d6">520bb3073a4c18baf121407c691ffe87</data>
    </edge>
    <edge source="&quot;CDC&quot;" target="&quot;SUICIDE PREVENTION CORE COMPETENCIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"CDC provides data and statistics that inform the development of suicide prevention core competencies."</data>
      <data key="d6">5ce886e06455eadec4bcfe91e36b666d</data>
    </edge>
    <edge source="&quot;SUICIDE CRISIS SYNDROME&quot;" target="&quot;ENTRAPMENT&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Entrapment" is a central characteristic of "Suicide Crisis Syndrome," indicating a desire to escape from an unbearable situation with no perceived escape routes. It is a key component of "Suicide Crisis Syndrome," describing the psychological state that contributes to the condition.</data>
      <data key="d6">ba0d350eede3e5a4dfd1b9b0693b9b94,d27160d0dde304425ccc51df673321b1</data>
    </edge>
    <edge source="&quot;UNIVERSITY OF MARYLAND REDDIT SUICIDALITY DATASET&quot;" target="&quot;R/SUICIDEWATCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The University of Maryland Reddit Suicidality Dataset is constructed from posts in the r/SuicideWatch subreddit."</data>
      <data key="d6">d27160d0dde304425ccc51df673321b1</data>
    </edge>
    <edge source="&quot;ENTRAPMENT&quot;" target="&quot;PROMPT ENGINEER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Prompt Engineer received an email explaining the overall goals of the project, including the concept of entrapment."</data>
      <data key="d6">18e3009014a13d95897da5ec358ca2e1</data>
    </edge>
    <edge source="&quot;ENTRAPMENT&quot;" target="&quot;DEVELOPMENT ITEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Development Items include pairs that are labeled for entrapment or not, based on the model's reasoning."</data>
      <data key="d6">18e3009014a13d95897da5ec358ca2e1</data>
    </edge>
    <edge source="&quot;ENTRAPMENT&quot;" target="&quot;PROMPT DEVELOPMENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The goal of identifying entrapment requires careful prompt development to ensure posts are labeled correctly."</data>
      <data key="d6">93ab5f14aa5b97d57952be648f337b10</data>
    </edge>
    <edge source="&quot;ENTRAPMENT&quot;" target="&quot;ANSWER EXTRACTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Answer Extraction is used to determine if a post meets the criteria for Entrapment."</data>
      <data key="d6">bcb6ef7c52ce001fb19904d1aa92dfd2</data>
    </edge>
    <edge source="&quot;10-SHOT AUTODICOT&quot;" target="&quot;AUTODICOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"10-Shot AutoDiCoT is a specific instance of the AutoDiCoT subdomain, involving the addition of ten exemplars to improve prompt performance."</data>
      <data key="d6">afacb1e7edc1e6be7b4b3776676a32e9</data>
    </edge>
    <edge source="&quot;10-SHOT AUTODICOT&quot;" target="&quot;PERFORMANCE DECREASE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The 10-Shot AutoDiCoT prompt did not improve results, leading to a decrease in performance metrics."</data>
      <data key="d6">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </edge>
    <edge source="&quot;ZERO-SHOT + CONTEXT&quot;" target="&quot;ONE-SHOT AUTODICOT + FULL CONTEXT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"One-Shot AutoDiCoT + Full Context is an evolved version of Zero-Shot + Context, providing additional context to improve model performance."</data>
      <data key="d6">ba0d350eede3e5a4dfd1b9b0693b9b94</data>
    </edge>
    <edge source="&quot;ZERO-SHOT + CONTEXT&quot;" target="&quot;10-SHOT + CONTEXT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"10-Shot + Context builds upon Zero-Shot + Context by providing ten examples to enhance the model's understanding and performance."</data>
      <data key="d6">ba0d350eede3e5a4dfd1b9b0693b9b94</data>
    </edge>
    <edge source="&quot;AUTOCOT&quot;" target="&quot;DATASET DEVELOPMENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"AutoCoT is used in the process of Dataset Development to improve the AI's ability to identify entrapment."</data>
      <data key="d6">bcb6ef7c52ce001fb19904d1aa92dfd2</data>
    </edge>
    <edge source="&quot;CONTRASTIVE COT&quot;" target="&quot;AUTOMATIC DIRECTED COT (AUTODICOT)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"AutoDiCoT combines the automatic generation of CoTs with the principles of Contrastive CoT to improve reasoning."</data>
      <data key="d6">18e3009014a13d95897da5ec358ca2e1</data>
    </edge>
    <edge source="&quot;F1 SCORE&quot;" target="&quot;OPTIMIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The goal of optimization is to maximize the F1 score of the language model."</data>
      <data key="d6">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </edge>
    <edge source="&quot;RECALL&quot;" target="&quot;OPTIMIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Optimization aims to improve recall, ensuring the model identifies all relevant instances."</data>
      <data key="d6">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </edge>
    <edge source="&quot;PRECISION&quot;" target="&quot;OPTIMIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Optimization aims to improve precision, ensuring the model accurately identifies relevant instances."</data>
      <data key="d6">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </edge>
    <edge source="&quot;10-SHOT + CONTEXT&quot;" target="&quot;20-SHOT AUTODICOT + FULL WORDS + EXTRACTION PROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"20-Shot AutoDiCoT + Full Words + Extraction Prompt is a more advanced technique compared to 10-Shot + Context, incorporating more examples and additional prompts for better results."</data>
      <data key="d6">ba0d350eede3e5a4dfd1b9b0693b9b94</data>
    </edge>
    <edge source="&quot;20-SHOT AUTODICOT + FULL WORDS + EXTRACTION PROMPT&quot;" target="&quot;AUTODICOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"20-Shot AutoDiCoT + Full Words + Extraction Prompt is another variation within the AutoDiCoT subdomain, involving extraction of answers, which improved accuracy but decreased F1 score."</data>
      <data key="d6">afacb1e7edc1e6be7b4b3776676a32e9</data>
    </edge>
    <edge source="&quot;AUTOMATIC DIRECTED COT (AUTODICOT)&quot;" target="&quot;PROMPT ENGINEER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Prompt Engineer developed and refined the AutoDiCoT algorithm to improve the model's reasoning process."</data>
      <data key="d6">18e3009014a13d95897da5ec358ca2e1</data>
    </edge>
    <edge source="&quot;AUTOMATIC DIRECTED COT (AUTODICOT)&quot;" target="&quot;DEVELOPMENT ITEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"AutoDiCoT is applied to the Development Items to label and generate reasoning chains for each pair."</data>
      <data key="d6">18e3009014a13d95897da5ec358ca2e1</data>
    </edge>
    <edge source="&quot;PROMPT DEVELOPMENT&quot;" target="&quot;DOMAIN EXPERTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Regular engagement between prompt engineers and domain experts is necessary to ensure that prompt development aligns with real-world use cases."</data>
      <data key="d6">93ab5f14aa5b97d57952be648f337b10</data>
    </edge>
    <edge source="&quot;PROMPT DEVELOPMENT&quot;" target="&quot;EMAIL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The inclusion of email in the prompt provided richer background information, significantly impacting the performance of the prompt development process."</data>
      <data key="d6">93ab5f14aa5b97d57952be648f337b10</data>
    </edge>
    <edge source="&quot;EMAIL&quot;" target="&quot;PERFORMANCE DECREASE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Removing the email from the prompt hurt performance, indicating its importance."</data>
      <data key="d6">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </edge>
    <edge source="&quot;AUTODICOT&quot;" target="&quot;20-SHOT AUTODICOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"20-Shot AutoDiCoT is another instance of the AutoDiCoT subdomain, involving the addition of twenty exemplars, but it led to worse results."</data>
      <data key="d6">afacb1e7edc1e6be7b4b3776676a32e9</data>
    </edge>
    <edge source="&quot;AUTODICOT&quot;" target="&quot;20-SHOT AUTODICOT + FULL WORDS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"20-Shot AutoDiCoT + Full Words is a variation within the AutoDiCoT subdomain, where full words were used in the prompt, but it did not succeed."</data>
      <data key="d6">afacb1e7edc1e6be7b4b3776676a32e9</data>
    </edge>
    <edge source="&quot;AUTODICOT&quot;" target="&quot;10-SHOT AUTODICOT + EXTRACTION PROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"10-Shot AutoDiCoT + Extraction Prompt is a variation within the AutoDiCoT subdomain, where the extraction prompt was applied to the best performing 10-Shot AutoDiCoT prompt, but it did not improve results."</data>
      <data key="d6">afacb1e7edc1e6be7b4b3776676a32e9</data>
    </edge>
    <edge source="&quot;AUTODICOT&quot;" target="&quot;10-SHOT AUTODICOT WITHOUT EMAIL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"10-Shot AutoDiCoT without Email is a variation within the AutoDiCoT subdomain, where the email was removed from the prompt, which hurt performance significantly."</data>
      <data key="d6">afacb1e7edc1e6be7b4b3776676a32e9</data>
    </edge>
    <edge source="&quot;ENSEMBLE + EXTRACTION&quot;" target="&quot;PERFORMANCE DECREASE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Ensemble + Extraction method hurt performance, showing that different orderings of exemplars did not help."</data>
      <data key="d6">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </edge>
    <edge source="&quot;10-SHOT AUTOCOT&quot;" target="&quot;PERFORMANCE DECREASE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The 10-Shot AutoCoT with three times the context did not improve performance."</data>
      <data key="d6">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </edge>
    <edge source="&quot;ANONYMIZE EMAIL&quot;" target="&quot;PERFORMANCE DECREASE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Anonymizing the email decreased performance, indicating the importance of the original email content."</data>
      <data key="d6">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </edge>
    <edge source="&quot;DSPY&quot;" target="&quot;PERFORMANCE IMPROVEMENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The DSPy framework optimized the prompts and improved performance, achieving the best F1 score."</data>
      <data key="d6">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </edge>
    <edge source="&quot;DSPY&quot;" target="&quot;KL SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"KL Systems and DSPY both focus on modular and self-improving pipelines for language model applications."</data>
      <data key="d6">eeb46213e40cc8603a2037766f312338</data>
    </edge>
    <edge source="&quot;OPTIMIZATION&quot;" target="&quot;PERFORMANCE DECREASE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Various techniques were tested to optimize performance, but some led to a decrease in metrics."</data>
      <data key="d6">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </edge>
    <edge source="&quot;OPTIMIZATION&quot;" target="&quot;PERFORMANCE IMPROVEMENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The goal of optimization is to achieve performance improvement in the language model."</data>
      <data key="d6">dd792fdfac5a64bb840e3680fe40eeb3</data>
    </edge>
    <edge source="&quot;FOUNDATION MODELS&quot;" target="&quot;OPPORTUNITIES AND RISKS OF FOUNDATION MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Opportunities and Risks of Foundation Models study focuses on the potential benefits and risks associated with Foundation Models."</data>
      <data key="d6">520bb3073a4c18baf121407c691ffe87</data>
    </edge>
    <edge source="&quot;FOUNDATION MODELS&quot;" target="&quot;LANGUAGE MODELS ARE FEW-SHOT LEARNERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The study on Few-Shot Learning demonstrates the adaptability of Foundation Models in performing tasks with minimal training data."</data>
      <data key="d6">520bb3073a4c18baf121407c691ffe87</data>
    </edge>
    <edge source="&quot;FOUNDATION MODELS&quot;" target="&quot;SPARKS OF ARTIFICIAL GENERAL INTELLIGENCE: EARLY EXPERIMENTS WITH GPT-4&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The experiments with GPT-4 are part of the broader research into the capabilities of Foundation Models."</data>
      <data key="d6">520bb3073a4c18baf121407c691ffe87</data>
    </edge>
    <edge source="&quot;FOUNDATION MODELS&quot;" target="&quot;EXTRACTING TRAINING DATA FROM LARGE LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The study on extracting training data raises concerns about the privacy and security of data used in Foundation Models."</data>
      <data key="d6">520bb3073a4c18baf121407c691ffe87</data>
    </edge>
    <edge source="&quot;SYSTEMATIC REVIEW&quot;" target="&quot;PRISMA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The systematic review is based on the PRISMA standard for literature reviews."</data>
      <data key="d6">a86e659dcd136358e7557eb5f98c1b58</data>
    </edge>
    <edge source="&quot;MULTIMODAL PROMPTING&quot;" target="&quot;PROMPT ENGINEERING TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multimodal Prompting is a technique within Prompt Engineering Techniques that uses multiple types of data in prompts."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;ARTHURAI&quot;" target="&quot;ARTHUR SHIELD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ArthurAI developed Arthur Shield to enhance AI model performance and security."</data>
      <data key="d6">6e1dce58f4a3793b65d09171ea5bd3a6</data>
    </edge>
    <edge source="&quot;BOOTSTRAPPING MULTILINGUAL SEMANTIC PARSERS USING LARGE LANGUAGE MODELS&quot;" target="&quot;17TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research paper was presented at this conference."</data>
      <data key="d6">b363fca358c69a9412b955c53352ea9a</data>
    </edge>
    <edge source="&quot;BENCHMARKING FOUNDATION MODELS WITH LANGUAGE-MODEL-AS-AN-EXAMINER&quot;" target="&quot;NEURIPS 2023 DATASETS AND BENCHMARKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research paper was presented at this conference."</data>
      <data key="d6">b363fca358c69a9412b955c53352ea9a</data>
    </edge>
    <edge source="&quot;EXPOSITORY TEXT GENERATION: IMITATE, RETRIEVE, PARAPHRASE&quot;" target="&quot;2023 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research paper was presented at this conference."</data>
      <data key="d6">b363fca358c69a9412b955c53352ea9a</data>
    </edge>
    <edge source="&quot;A MULTITASK, MULTILINGUAL, MULTIMODAL EVALUATION OF CHATGPT ON REASONING, HALLUCINATION, AND INTERACTIVITY&quot;" target="&quot;AACL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research paper was presented at this conference."</data>
      <data key="d6">b363fca358c69a9412b955c53352ea9a</data>
    </edge>
    <edge source="&quot;GRAPH OF THOUGHTS: SOLVING ELABORATE PROBLEMS WITH LARGE LANGUAGE MODELS&quot;" target="&quot;AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research paper was presented at this conference."</data>
      <data key="d6">b363fca358c69a9412b955c53352ea9a</data>
    </edge>
    <edge source="&quot;PRE-TRAINED LANGUAGE MODELS&quot;" target="&quot;EVALUATING THE SUSCEPTIBILITY OF PRE-TRAINED LANGUAGE MODELS VIA HANDCRAFTED ADVERSARIAL EXAMPLES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The evaluation aims to test the vulnerability of Pre-trained Language Models to adversarial attacks."</data>
      <data key="d6">520bb3073a4c18baf121407c691ffe87</data>
    </edge>
    <edge source="&quot;OPENAI GYM&quot;" target="&quot;VIDEO GENERATION MODELS AS WORLD SIMULATORS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"OpenAI Gym provides the tools and environment for developing and testing video generation models."</data>
      <data key="d6">520bb3073a4c18baf121407c691ffe87</data>
    </edge>
    <edge source="&quot;IN-CONTEXT PROMPT EDITING&quot;" target="&quot;CONTROL3D&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both in-context prompt editing and Control3d are techniques aimed at enhancing the capabilities of large language models in specific tasks."</data>
      <data key="d6">5ce886e06455eadec4bcfe91e36b666d</data>
    </edge>
    <edge source="&quot;PROGRAM OF THOUGHTS PROMPTING&quot;" target="&quot;CHAIN OF THOUGHT REASONING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Program of thoughts prompting is a specific method within the broader subdomain of chain of thought reasoning."</data>
      <data key="d6">5ce886e06455eadec4bcfe91e36b666d</data>
    </edge>
    <edge source="&quot;CONSISTENT VIDEO-TO-VIDEO TRANSFER&quot;" target="&quot;ENTANGLED REPRESENTATION LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both entangled representation learning and consistent video-to-video transfer involve advanced techniques for learning and transforming data."</data>
      <data key="d6">5ce886e06455eadec4bcfe91e36b666d</data>
    </edge>
    <edge source="&quot;CONTRASTIVE CHAIN-OF-THOUGHT PROMPTING&quot;" target="&quot;VQGAN-CLIP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"VQGAN-CLIP and contrastive chain-of-thought prompting both involve advanced methods for enhancing the capabilities of large language models."</data>
      <data key="d6">5ce886e06455eadec4bcfe91e36b666d</data>
    </edge>
    <edge source="&quot;VQGAN-CLIP&quot;" target="&quot;COGVIEW&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both VQGAN-CLIP and CogView are subdomains focused on text-to-image generation, indicating a shared interest in this area."</data>
      <data key="d6">e5878afbfbf5194f1da3540eaa88fe65</data>
    </edge>
    <edge source="&quot;ALPACAFARM&quot;" target="&quot;AGENT AI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both AlpacaFarm and Agent AI involve learning from human feedback and multimodal interaction, respectively, showing a connection in their approach to AI development."</data>
      <data key="d6">e5878afbfbf5194f1da3540eaa88fe65</data>
    </edge>
    <edge source="&quot;ONE-SHOT LEARNING OF OBJECT CATEGORIES&quot;" target="&quot;IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research on one-shot learning of object categories was published in the IEEE Transactions on Pattern Analysis and Machine Intelligence journal."</data>
      <data key="d6">9b0bcd8647bcff907e9bcf962a013b91</data>
    </edge>
    <edge source="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;" target="&quot;THE 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Association for Computational Linguistics organizes the 59th Annual Meeting where research in computational linguistics is presented."</data>
      <data key="d6">9b0bcd8647bcff907e9bcf962a013b91</data>
    </edge>
    <edge source="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;" target="&quot;THE 2022 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Association for Computational Linguistics organizes the 2022 Conference on Empirical Methods in Natural Language Processing."</data>
      <data key="d6">9b0bcd8647bcff907e9bcf962a013b91</data>
    </edge>
    <edge source="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;" target="&quot;LARGE LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Association for Computational Linguistics publishes research findings on large language models, contributing to the academic discourse in this subdomain."</data>
      <data key="d6">83e773afec09e119882fe15dd253e724</data>
    </edge>
    <edge source="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;" target="&quot;CROSS-LINGUAL RETRIEVAL AUGMENTED PROMPT FOR LOW-RESOURCE LANGUAGES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Association for Computational Linguistics published research on Cross-lingual Retrieval Augmented Prompt for Low-Resource Languages."</data>
      <data key="d6">4d9e8d703c2da8e4775c428e83e87fc9</data>
    </edge>
    <edge source="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;" target="&quot;NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The North American Chapter is a regional division of the Association for Computational Linguistics."</data>
      <data key="d6">c605e4f0158f18be68214a39b9b54154</data>
    </edge>
    <edge source="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;" target="&quot;GYMNASIUM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gymnasium is likely involved in research activities that are published or presented at events organized by the Association for Computational Linguistics."</data>
      <data key="d6">153eeb5a63e650f2cd12f700ffe3e71f</data>
    </edge>
    <edge source="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;" target="&quot;CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Association for Computational Linguistics organizes the Conference on Empirical Methods in Natural Language Processing."</data>
      <data key="d6">153eeb5a63e650f2cd12f700ffe3e71f</data>
    </edge>
    <edge source="&quot;METADREAMER&quot;" target="&quot;EFFICIENT TEXT-TO-3D CREATION WITH DISENTANGLING GEOMETRY AND TEXTURE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Metadreamer focuses on efficient text-to-3D creation by disentangling geometry and texture, as described in the research paper."</data>
      <data key="d6">9b0bcd8647bcff907e9bcf962a013b91</data>
    </edge>
    <edge source="&quot;THE EIGHTH CONFERENCE ON MACHINE TRANSLATION&quot;" target="&quot;LEVERAGING LARGE LANGUAGE MODELS FOR FINE-GRAINED MACHINE TRANSLATION EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research on leveraging large language models for fine-grained machine translation evaluation was presented at the Eighth Conference on Machine Translation."</data>
      <data key="d6">9b0bcd8647bcff907e9bcf962a013b91</data>
    </edge>
    <edge source="&quot;GPTSCORE&quot;" target="&quot;EVALUATE AS YOU DESIRE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"GPTScore is a subdomain focused on evaluating language models as desired, as described in the arXiv preprint titled 'Evaluate as you desire'."</data>
      <data key="d6">9b0bcd8647bcff907e9bcf962a013b91</data>
    </edge>
    <edge source="&quot;POLYGLOT PROMPT&quot;" target="&quot;MULTILINGUAL MULTITASK PROMPT TRAINING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Polyglot Prompt focuses on multilingual multitask prompt training, as presented in the research paper titled 'Multilingual multitask prompt training'."</data>
      <data key="d6">9b0bcd8647bcff907e9bcf962a013b91</data>
    </edge>
    <edge source="&quot;THE ELEVENTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS&quot;" target="&quot;COMPLEXITY-BASED PROMPTING FOR MULTI-STEP REASONING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research on complexity-based prompting for multi-step reasoning was presented at the Eleventh International Conference on Learning Representations."</data>
      <data key="d6">9b0bcd8647bcff907e9bcf962a013b91</data>
    </edge>
    <edge source="&quot;ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS&quot;" target="&quot;MULTI-BANDIT BEST ARM IDENTIFICATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research on multi-bandit best arm identification was presented at the Advances in Neural Information Processing Systems conference."</data>
      <data key="d6">9b0bcd8647bcff907e9bcf962a013b91</data>
    </edge>
    <edge source="&quot;THE 40TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING&quot;" target="&quot;PROGRAM-AIDED LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research on program-aided language models was presented at the 40th International Conference on Machine Learning."</data>
      <data key="d6">9b0bcd8647bcff907e9bcf962a013b91</data>
    </edge>
    <edge source="&quot;FORBES&quot;" target="&quot;WHAT AIR CANADA LOST IN &#8216;REMARKABLE&#8217; LYING AI CHATBOT CASE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Forbes published an article on what Air Canada lost in a 'remarkable' lying AI chatbot case."</data>
      <data key="d6">9b0bcd8647bcff907e9bcf962a013b91</data>
    </edge>
    <edge source="&quot;THE 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;" target="&quot;MAKING PRE-TRAINED LANGUAGE MODELS BETTER FEW-SHOT LEARNERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research on making pre-trained language models better few-shot learners was presented at the 59th Annual Meeting of the Association for Computational Linguistics."</data>
      <data key="d6">9b0bcd8647bcff907e9bcf962a013b91</data>
    </edge>
    <edge source="&quot;THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING&quot;" target="&quot;MAKING PRE-TRAINED LANGUAGE MODELS BETTER FEW-SHOT LEARNERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research on making pre-trained language models better few-shot learners was also presented at the 11th International Joint Conference on Natural Language Processing."</data>
      <data key="d6">9b0bcd8647bcff907e9bcf962a013b91</data>
    </edge>
    <edge source="&quot;THE 2022 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;" target="&quot;MULTILINGUAL MULTITASK PROMPT TRAINING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research on multilingual multitask prompt training was presented at the 2022 Conference on Empirical Methods in Natural Language Processing."</data>
      <data key="d6">9b0bcd8647bcff907e9bcf962a013b91</data>
    </edge>
    <edge source="&quot;TECHRXIV&quot;" target="&quot;LARGE LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"TechRxiv publishes research papers and surveys related to large language models, making it a key platform for disseminating information in this subdomain."</data>
      <data key="d6">83e773afec09e119882fe15dd253e724</data>
    </edge>
    <edge source="&quot;LARGE LANGUAGE MODELS&quot;" target="&quot;NEURIPS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NeurIPS is an event where advancements in large language models are presented, making it a significant venue for researchers in this subdomain."</data>
      <data key="d6">83e773afec09e119882fe15dd253e724</data>
    </edge>
    <edge source="&quot;LARGE LANGUAGE MODELS&quot;" target="&quot;ICLR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ICLR is an event that features research on large language models, contributing to the development and understanding of these models."</data>
      <data key="d6">83e773afec09e119882fe15dd253e724</data>
    </edge>
    <edge source="&quot;LARGE LANGUAGE MODELS&quot;" target="&quot;MACHINE TRANSLATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Large language models are evaluated for their effectiveness in machine translation, a key application area."</data>
      <data key="d6">83e773afec09e119882fe15dd253e724</data>
    </edge>
    <edge source="&quot;LARGE LANGUAGE MODELS&quot;" target="&quot;MENTAL HEALTH CARE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Large language models are being explored for their potential applications in mental health care, indicating their versatility."</data>
      <data key="d6">83e773afec09e119882fe15dd253e724</data>
    </edge>
    <edge source="&quot;LARGE LANGUAGE MODELS&quot;" target="&quot;CROSS-LINGUAL-THOUGHT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cross-Lingual-Thought Prompting is a technique aimed at enhancing the multilingual capabilities of large language models."</data>
      <data key="d6">83e773afec09e119882fe15dd253e724</data>
    </edge>
    <edge source="&quot;LARGE LANGUAGE MODELS&quot;" target="&quot;ZERO-SHOT REASONING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Large Language Models are capable of performing Zero-Shot Reasoning, as discussed in the 2022 paper by Kojima et al."</data>
      <data key="d6">5ce40e1d59b740ff17256ed5abebf613</data>
    </edge>
    <edge source="&quot;LARGE LANGUAGE MODELS&quot;" target="&quot;SIMPLE SYNTHETIC DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Simple Synthetic Data is used to reduce sycophancy in large language models, improving their response quality."</data>
      <data key="d6">42d8c3ad092ec18e28ff718709b0b472</data>
    </edge>
    <edge source="&quot;LARGE LANGUAGE MODELS&quot;" target="&quot;PERSPECTIVE-TAKING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Perspective-Taking improves the theory-of-mind capabilities of large language models."</data>
      <data key="d6">42d8c3ad092ec18e28ff718709b0b472</data>
    </edge>
    <edge source="&quot;ALIGNING PERCEPTION WITH LANGUAGE MODELS&quot;" target="&quot;ACTIVE RETRIEVAL AUGMENTED GENERATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Aligning Perception with Language Models and Active Retrieval Augmented Generation aim to enhance the capabilities of language models by integrating additional data sources."</data>
      <data key="d6">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </edge>
    <edge source="&quot;LLAMA GUARD&quot;" target="&quot;CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Llama Guard and CTRL both focus on ensuring the reliability and control of language model outputs, albeit through different mechanisms."</data>
      <data key="d6">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </edge>
    <edge source="&quot;EFFECTIVE DISAMBIGUATION FOR MACHINE TRANSLATION&quot;" target="&quot;IS CHATGPT A GOOD TRANSLATOR?&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Effective Disambiguation for Machine Translation and Is ChatGPT a Good Translator? both address the challenges and capabilities of language models in translation tasks."</data>
      <data key="d6">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </edge>
    <edge source="&quot;ZERO-SHOT TEXT-GUIDED OBJECT GENERATION WITH DREAM FIELDS&quot;" target="&quot;TAB-COT: ZERO-SHOT TABULAR CHAIN OF THOUGHT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Zero-Shot Text-Guided Object Generation with Dream Fields and Tab-CoT: Zero-Shot Tabular Chain of Thought leverage zero-shot learning to perform tasks without prior examples."</data>
      <data key="d6">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </edge>
    <edge source="&quot;LANGUAGE MODELS (MOSTLY) KNOW WHAT THEY KNOW&quot;" target="&quot;MRKL SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Language Models (Mostly) Know What They Know and MRKL Systems both explore the boundaries and capabilities of language models, including their integration with external knowledge sources."</data>
      <data key="d6">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </edge>
    <edge source="&quot;SYSTEMATIC LITERATURE REVIEWS IN SOFTWARE ENGINEERING&quot;" target="&quot;CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Systematic Literature Reviews in Software Engineering provides guidelines that could be applied to evaluate the effectiveness and reliability of models like CTRL."</data>
      <data key="d6">7e3b559c2a22f7f23f4eecc37ed7b8e4</data>
    </edge>
    <edge source="&quot;KL SYSTEMS&quot;" target="&quot;CTRL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both KL Systems and CTRL involve the use of large language models and discrete reasoning for various applications."</data>
      <data key="d6">eeb46213e40cc8603a2037766f312338</data>
    </edge>
    <edge source="&quot;DEMONSTRATION ENSEMBLING FOR IN-CONTEXT LEARNING&quot;" target="&quot;SELF-GENERATED IN-CONTEXT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies focus on enhancing in-context learning through different techniques."</data>
      <data key="d6">eeb46213e40cc8603a2037766f312338</data>
    </edge>
    <edge source="&quot;FEW-SHOT CROSS-LINGUAL TRANSFER WITH TARGET LANGUAGE PECULIAR EXAMPLES&quot;" target="&quot;BOOSTING CROSS-LINGUAL TRANSFERABILITY IN MULTILINGUAL MODELS VIA IN-CONTEXT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies aim to improve cross-lingual transferability in language models."</data>
      <data key="d6">eeb46213e40cc8603a2037766f312338</data>
    </edge>
    <edge source="&quot;GEMBA-MQM&quot;" target="&quot;LARGE LANGUAGE MODELS AS EVALUATORS OF TRANSLATION QUALITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies by Tom Kocmi and Christian Federmann focus on the application of large language models in translation quality evaluation."</data>
      <data key="d6">eeb46213e40cc8603a2037766f312338</data>
    </edge>
    <edge source="&quot;ZERO-SHOT REASONING WITH LARGE LANGUAGE MODELS&quot;" target="&quot;REORDERING EXAMPLES IN PRIMING-BASED FEW-SHOT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies explore the capabilities of large language models in few-shot and zero-shot learning scenarios."</data>
      <data key="d6">eeb46213e40cc8603a2037766f312338</data>
    </edge>
    <edge source="&quot;CLIPSTYLER&quot;" target="&quot;CONTROLLABLE TEXT-TO-IMAGE GENERATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Clipstyler is a specific application of Controllable Text-to-Image Generation, focusing on image style transfer with a single text condition."</data>
      <data key="d6">5ce40e1d59b740ff17256ed5abebf613</data>
    </edge>
    <edge source="&quot;ZERO-SHOT REASONING&quot;" target="&quot;CHAIN-OF-THOUGHT REASONING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chain-of-Thought Reasoning is a method that can be applied in zero-shot reasoning scenarios."</data>
      <data key="d6">153eeb5a63e650f2cd12f700ffe3e71f</data>
    </edge>
    <edge source="&quot;CHATGPT&#8217;S PACKAGE RECOMMENDATIONS&quot;" target="&quot;MEMORY-OF-THOUGHT (MOT)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Memory-of-Thought (MOT) enables ChatGPT to self-improve, which could enhance its ability to make package recommendations."</data>
      <data key="d6">5ce40e1d59b740ff17256ed5abebf613</data>
    </edge>
    <edge source="&quot;EUCLIDREAMER&quot;" target="&quot;INSTANT3D&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Euclidreamer and Instant3D are technologies focused on 3D model generation and texturing."</data>
      <data key="d6">5ce40e1d59b740ff17256ed5abebf613</data>
    </edge>
    <edge source="&quot;DIALOGUE FOR PROMPTING&quot;" target="&quot;UNIFIED DEMONSTRATION RETRIEVER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Unified Demonstration Retriever and Dialogue for Prompting are techniques aimed at optimizing in-context learning."</data>
      <data key="d6">5ce40e1d59b740ff17256ed5abebf613</data>
    </edge>
    <edge source="&quot;CROSSLINGUAL RETRIEVAL AUGMENTED IN-CONTEXT LEARNING&quot;" target="&quot;CHATGPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Crosslingual retrieval techniques can be used to improve ChatGPT's performance in understanding and generating text in multiple languages."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;CHATGPT&quot;" target="&quot;OSCAR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Oscar's object-semantics aligned pre-training can enhance ChatGPT's capabilities in vision-language tasks."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;CHATGPT&quot;" target="&quot;BILINGUAL LEXICON INDUCTION WITH LARGE LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Bilingual lexicon induction techniques can be applied to improve ChatGPT's multilingual capabilities."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;CHATGPT&quot;" target="&quot;STEP-AWARE VERIFIER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Step-aware verification can enhance ChatGPT's reasoning abilities."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;CHATGPT&quot;" target="&quot;FAIRNESS IN LARGE LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ensuring fairness in large language models is crucial for the ethical deployment of ChatGPT."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;CHATGPT&quot;" target="&quot;MOVIDEO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Motion-aware video generation techniques can be integrated with ChatGPT for generating descriptive video content."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;CHATGPT&quot;" target="&quot;MAGIC3D&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"High-resolution text-to-3D content creation can be used to enhance ChatGPT's ability to generate 3D models from text descriptions."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;CHATGPT&quot;" target="&quot;FEW-SHOT LEARNING WITH MULTILINGUAL GENERATIVE LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Few-shot learning techniques can improve ChatGPT's ability to learn from limited examples in multiple languages."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;CHATGPT&quot;" target="&quot;IN-CONTEXT EXAMPLES FOR GPT-3&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Good in-context examples can improve ChatGPT's performance in various tasks."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;CHATGPT&quot;" target="&quot;VERIFIABILITY IN GENERATIVE SEARCH ENGINES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Evaluating verifiability is important for ensuring the reliability of information generated by ChatGPT."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;CHATGPT&quot;" target="&quot;PROMPTING METHODS IN NATURAL LANGUAGE PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Different prompting methods can be used to enhance ChatGPT's performance in natural language processing tasks."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;CHATGPT&quot;" target="&quot;EXPLICIT VISUAL PROMPTING FOR LOW-LEVEL STRUCTURE SEGMENTATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Explicit visual prompting techniques can be used to improve ChatGPT's ability to understand and segment visual data."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;CHATGPT&quot;" target="&quot;GPTEVAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NLG evaluation methods using GPT-4 can be applied to assess and improve ChatGPT's natural language generation capabilities."</data>
      <data key="d6">630ee831daa753234a258274d318509e</data>
    </edge>
    <edge source="&quot;CHATGPT&quot;" target="&quot;EVALUATION PAPER SUMMARY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ChatGPT is one of the models evaluated in the Evaluation Paper Summary."</data>
      <data key="d6">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </edge>
    <edge source="&quot;GPTEVAL&quot;" target="&quot;CALIBRATING LLM-BASED EVALUATOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains involve the evaluation of language models, indicating a shared focus on improving the assessment of LLMs."</data>
      <data key="d6">db67f52733fb9d41d13be7cefaa1dae0</data>
    </edge>
    <edge source="&quot;UNIFYING IMAGE PROCESSING AS VISUAL PROMPTING QUESTION ANSWERING&quot;" target="&quot;GPT4MOTION: SCRIPTING PHYSICAL MOTIONS IN TEXT-TO-VIDEO GENERATION VIA BLENDER-ORIENTED GPT PLANNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains involve visual processing and the use of prompts, indicating a shared focus on integrating visual and textual data."</data>
      <data key="d6">db67f52733fb9d41d13be7cefaa1dae0</data>
    </edge>
    <edge source="&quot;EVALUATE WHAT YOU CAN&#8217;T EVALUATE&quot;" target="&quot;ERROR ANALYSIS PROMPTING ENABLES HUMAN-LIKE TRANSLATION EVALUATION IN LARGE LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains focus on evaluating aspects of language models that are challenging to assess, indicating a shared interest in improving evaluation techniques."</data>
      <data key="d6">db67f52733fb9d41d13be7cefaa1dae0</data>
    </edge>
    <edge source="&quot;BOUNDING THE CAPABILITIES OF LARGE LANGUAGE MODELS IN OPEN TEXT GENERATION WITH PROMPT CONSTRAINTS&quot;" target="&quot;FANTASTICALLY ORDERED PROMPTS AND WHERE TO FIND THEM: OVERCOMING FEW-SHOT PROMPT ORDER SENSITIVITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains deal with the use of prompts in large language models, indicating a shared focus on optimizing prompt usage."</data>
      <data key="d6">db67f52733fb9d41d13be7cefaa1dae0</data>
    </edge>
    <edge source="&quot;CHAIN-OF-DICTIONARY PROMPTING ELICITS TRANSLATION IN LARGE LANGUAGE MODELS&quot;" target="&quot;ERROR ANALYSIS PROMPTING ENABLES HUMAN-LIKE TRANSLATION EVALUATION IN LARGE LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains involve prompting techniques to improve translation evaluation in large language models."</data>
      <data key="d6">db67f52733fb9d41d13be7cefaa1dae0</data>
    </edge>
    <edge source="&quot;LMQL&quot;" target="&quot;SELF-REFINE: ITERATIVE REFINEMENT WITH SELF-FEEDBACK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains likely involve iterative processes and refinement techniques, indicating a shared focus on improving language model outputs."</data>
      <data key="d6">db67f52733fb9d41d13be7cefaa1dae0</data>
    </edge>
    <edge source="&quot;CHAIN OF IMAGES FOR INTUITIVE REASONING&quot;" target="&quot;CLIF-VQA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"CLIF-VQA and Chain of Images for Intuitive Reasoning both involve the use of visual information to enhance understanding and assessment."</data>
      <data key="d6">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </edge>
    <edge source="&quot;AMBIGUOUS OPEN-DOMAIN QUESTIONS&quot;" target="&quot;COMPUTATIONAL SPEECH-ACT MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Computational Speech-Act Model can be used to address Ambiguous Open-Domain Questions by modeling the conversational context and possible interpretations."</data>
      <data key="d6">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </edge>
    <edge source="&quot;ADAPTIVE MACHINE TRANSLATION&quot;" target="&quot;POLYGLOT LLMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Polyglot LLMs enhance Adaptive Machine Translation by providing multilingual capabilities, allowing for more accurate and contextually appropriate translations."</data>
      <data key="d6">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </edge>
    <edge source="&quot;CLARIFYGPT&quot;" target="&quot;CROSSLINGUAL GENERALIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ClarifyGPT can aid in Crosslingual Generalization by clarifying intentions in multiple languages, improving the accuracy of generated code across languages."</data>
      <data key="d6">3fd8f6dcbbf1eecd6efb01ea12538679</data>
    </edge>
    <edge source="&quot;DELFT UNIVERSITY OF TECHNOLOGY&quot;" target="&quot;LEARNING STRATEGIES FOR POLYGLOT LLMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Research on Learning Strategies for Polyglot LLMs was conducted at Delft University of Technology."</data>
      <data key="d6">4d9e8d703c2da8e4775c428e83e87fc9</data>
    </edge>
    <edge source="&quot;DO USERS WRITE MORE INSECURE CODE WITH AI ASSISTANTS?&quot;" target="&quot;CREDIBLE WITHOUT CREDIT: DOMAIN EXPERTS ASSESS GENERATIVE LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies evaluate the impact and credibility of AI and generative language models in practical applications."</data>
      <data key="d6">2f28d2ed61c6111fccc81e48e659b599</data>
    </edge>
    <edge source="&quot;GPT DECIPHERING FEDSPEAK: QUANTIFYING DISSENT AMONG HAWKS AND DOVES&quot;" target="&quot;ADAPTING ENTITIES ACROSS LANGUAGES AND CULTURES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies involve the interpretation and adaptation of language models to specific domains and contexts."</data>
      <data key="d6">2f28d2ed61c6111fccc81e48e659b599</data>
    </edge>
    <edge source="&quot;LANGUAGE MODELS AS KNOWLEDGE BASES?&quot;" target="&quot;LARGE LANGUAGE MODELS SENSITIVITY TO THE ORDER OF OPTIONS IN MULTIPLE-CHOICE QUESTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies explore the capabilities and limitations of large language models in different contexts."</data>
      <data key="d6">2f28d2ed61c6111fccc81e48e659b599</data>
    </edge>
    <edge source="&quot;LANGUAGE MODELS AS KNOWLEDGE BASES?&quot;" target="&quot;MEASURING AND NARROWING THE COMPOSITIONALITY GAP IN LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies explore the structural and functional capabilities of language models."</data>
      <data key="d6">2f28d2ed61c6111fccc81e48e659b599</data>
    </edge>
    <edge source="&quot;CONSTRAINTS ON LANGUAGE MIXING: INTRASENTENTIAL CODE-SWITCHING AND BORROWING IN SPANISH/ENGLISH&quot;" target="&quot;SOMETIMES I'LL START A SENTENCE IN SPANISH Y TERMINO EN ESPA&#209;OL: TOWARD A TYPOLOGY OF CODE-SWITCHING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies focus on code-switching and language mixing between Spanish and English."</data>
      <data key="d6">2f28d2ed61c6111fccc81e48e659b599</data>
    </edge>
    <edge source="&quot;INTERACTIVE-CHAIN-PROMPTING: AMBIGUITY RESOLUTION FOR CROSSLINGUAL CONDITIONAL GENERATION WITH INTERACTION&quot;" target="&quot;DECOMPOSED PROMPTING FOR MACHINE TRANSLATION BETWEEN RELATED LANGUAGES USING LARGE LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies address prompting techniques for improving crosslingual and machine translation tasks."</data>
      <data key="d6">2f28d2ed61c6111fccc81e48e659b599</data>
    </edge>
    <edge source="&quot;DREAMFUSION: TEXT-TO-3D USING 2D DIFFUSION&quot;" target="&quot;TASKWEAVER: A CODE-FIRST AGENT FRAMEWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies involve innovative applications of AI and language models in creative and technical domains."</data>
      <data key="d6">2f28d2ed61c6111fccc81e48e659b599</data>
    </edge>
    <edge source="&quot;GRIPS: GRADIENT-FREE, EDIT-BASED INSTRUCTION SEARCH FOR PROMPTING LARGE LANGUAGE MODELS&quot;" target="&quot;AUTOMATIC PROMPT OPTIMIZATION WITH 'GRADIENT DESCENT' AND BEAM SEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies focus on optimizing prompts for large language models using different techniques."</data>
      <data key="d6">2f28d2ed61c6111fccc81e48e659b599</data>
    </edge>
    <edge source="&quot;REASONING WITH LANGUAGE MODEL PROMPTING&quot;" target="&quot;CROSS-LINGUAL PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Reasoning with Language Model Prompting and Cross-lingual Prompting involve the use of language models for reasoning tasks, with a focus on improving performance across different languages."</data>
      <data key="d6">42397dc5d60f0a1d799e06290ea52864</data>
    </edge>
    <edge source="&quot;LEARNING TRANSFERABLE VISUAL MODELS FROM NATURAL LANGUAGE SUPERVISION&quot;" target="&quot;LANGUAGE MODELS ARE UNSUPERVISED MULTITASK LEARNERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies by Alec Radford and colleagues focus on the capabilities of language models, with one emphasizing visual models and the other multitask learning."</data>
      <data key="d6">42397dc5d60f0a1d799e06290ea52864</data>
    </edge>
    <edge source="&quot;PROMPT PROGRAMMING FOR LARGE LANGUAGE MODELS&quot;" target="&quot;LEARNING TO RETRIEVE PROMPTS FOR IN-CONTEXT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies focus on techniques for effectively using prompts in large language models, indicating a shared interest in optimizing prompt usage."</data>
      <data key="d6">42397dc5d60f0a1d799e06290ea52864</data>
    </edge>
    <edge source="&quot;VANDERBILT UNIVERSITY&quot;" target="&quot;DEPT. OF COMPUTER SCIENCE, VANDERBILT UNIVERSITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Department of Computer Science is a part of Vanderbilt University, conducting research on prompt engineering."</data>
      <data key="d6">c605e4f0158f18be68214a39b9b54154</data>
    </edge>
    <edge source="&quot;EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;" target="&quot;HACKAPROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"HackAPrompt was presented at the Empirical Methods in Natural Language Processing conference."</data>
      <data key="d6">c605e4f0158f18be68214a39b9b54154</data>
    </edge>
    <edge source="&quot;QUANTIFYING LANGUAGE MODELS&#8217; SENSITIVITY TO SPURIOUS FEATURES IN PROMPT DESIGN&quot;" target="&quot;GUIDANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both works are related to the study and application of language models, with the former focusing on prompt design and the latter providing tools or resources."</data>
      <data key="d6">22a657737fd9e20b7803d916867d487b</data>
    </edge>
    <edge source="&quot;TOWARDS UNDERSTANDING SYCOPHANCY IN LANGUAGE MODELS&quot;" target="&quot;LARGE LANGUAGE MODELS HELP HUMANS VERIFY TRUTHFULNESS&#8211;EXCEPT WHEN THEY ARE CONVINCINGLY WRONG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies involve understanding the behavior of large language models, with one focusing on sycophancy and the other on truthfulness."</data>
      <data key="d6">22a657737fd9e20b7803d916867d487b</data>
    </edge>
    <edge source="&quot;ELICITING KNOWLEDGE FROM LANGUAGE MODELS USING AUTOMATICALLY GENERATED PROMPTS&quot;" target="&quot;AUTOPROMPT: ELICITING KNOWLEDGE FROM LANGUAGE MODELS WITH AUTOMATICALLY GENERATED PROMPTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both papers are authored by the same group and focus on using automatically generated prompts to extract knowledge from language models."</data>
      <data key="d6">22a657737fd9e20b7803d916867d487b</data>
    </edge>
    <edge source="&quot;REFLEXION: LANGUAGE AGENTS WITH VERBAL REINFORCEMENT LEARNING&quot;" target="&quot;GETTING MORE OUT OF MIXTURE OF LANGUAGE MODEL REASONING EXPERTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both studies involve advanced techniques in language model reasoning and learning, with one focusing on verbal reinforcement learning and the other on a mixture of reasoning experts."</data>
      <data key="d6">22a657737fd9e20b7803d916867d487b</data>
    </edge>
    <edge source="&quot;MEASURING INDUCTIVE BIASES OF IN-CONTEXT LEARNING WITH UNDERSPECIFIED DEMONSTRATIONS&quot;" target="&quot;PROMPTING GPT-3 TO BE RELIABLE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both papers involve research on improving the reliability and understanding of language models, particularly in the context of learning and prompt design."</data>
      <data key="d6">22a657737fd9e20b7803d916867d487b</data>
    </edge>
    <edge source="&quot;MIXTURE OF LANGUAGE MODEL REASONING EXPERTS (MORE)&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MoRE aims to enhance the reasoning capabilities of language models by combining multiple experts."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;AUTOGPT&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"AutoGPT uses language models to automate tasks, developed by Significant Gravitas in 2023."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;TEXT-TO-4D DYNAMIC SCENE GENERATION&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Text-to-4D dynamic scene generation uses language models to create dynamic 4D scenes from textual descriptions."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;EVALUATION METRICS FOR GPT-4&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Evaluation metrics are used to reliably assess the performance of GPT-4 language models on various tasks."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;NATURAL LANGUAGE REASONING AND STRUCTURED EXPLANATIONS (NLRSE)&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NLRSE focuses on reasoning and providing structured explanations in natural language using language models."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;OPEN-WORLD SEGMENTATION&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Open-world segmentation uses language models to segment images in an open-world setting without training."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;MULTILINGUAL LLMS&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multilingual LLMs are large language models capable of understanding and generating text in multiple languages."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;TEXT-TO-IMAGE SYNTHESIS&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Text-to-image synthesis uses language models to generate images from textual descriptions."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;AI PACKAGE HALLUCINATION ATTACKS&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"AI package hallucination attacks explore vulnerabilities in AI systems where language models generate false information."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;CALIBRATED CONFIDENCE SCORES&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Calibrated confidence scores aim to obtain accurate confidence levels from language models fine-tuned with human feedback."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;GYMNASIUM&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gymnasium provides a platform for training and evaluating language models."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;CHAIN-OF-THOUGHT REASONING&quot;" target="&quot;LANGUAGE MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chain-of-thought reasoning involves interleaving retrieval with reasoning to improve the performance of language models."</data>
      <data key="d6">ca2bcd796327d014f9e7738468b6b00d</data>
    </edge>
    <edge source="&quot;DIFFUSION MODELS&quot;" target="&quot;TUNE-A-VIDEO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Tune-A-Video involves one-shot tuning of image diffusion models for text-to-video generation."</data>
      <data key="d6">42d8c3ad092ec18e28ff718709b0b472</data>
    </edge>
    <edge source="&quot;TUNE-A-VIDEO&quot;" target="&quot;3DSTYLE-DIFFUSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Tune-a-Video and 3DStyle-Diffusion involve the use of diffusion models for generating media from text descriptions."</data>
      <data key="d6">ccdfd3415647f13f577d728a5a0256b1</data>
    </edge>
    <edge source="&quot;LARGE LANGUAGE MODELS AS DIVERSE ROLE-PLAYERS&quot;" target="&quot;HUMAN-IN-THE-LOOP MACHINE TRANSLATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Large Language Models as Diverse Role-Players can be applied in Human-in-the-Loop Machine Translation to improve summarization and translation quality."</data>
      <data key="d6">ccdfd3415647f13f577d728a5a0256b1</data>
    </edge>
    <edge source="&quot;AI CHAINS&quot;" target="&quot;AUTO-GPT FOR ONLINE DECISION MAKING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"AI Chains and Auto-GPT for Online Decision Making both involve the use of large language models for interactive and decision-making tasks."</data>
      <data key="d6">ccdfd3415647f13f577d728a5a0256b1</data>
    </edge>
    <edge source="&quot;SECURITY, PRIVACY, AND ETHICAL CONCERNS OF CHATGPT&quot;" target="&quot;EMPOWERING LLM-BASED MACHINE TRANSLATION WITH CULTURAL AWARENESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains address the ethical and responsible use of large language models in different contexts."</data>
      <data key="d6">ccdfd3415647f13f577d728a5a0256b1</data>
    </edge>
    <edge source="&quot;CONFIDENCE ELICITATION IN LLMS&quot;" target="&quot;RE-READING IMPROVES REASONING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Confidence Elicitation in LLMs and Re-Reading Improves Reasoning both aim to enhance the reliability and accuracy of language models."</data>
      <data key="d6">ccdfd3415647f13f577d728a5a0256b1</data>
    </edge>
    <edge source="&quot;RCOT&quot;" target="&quot;TREE OF THOUGHTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RCOT and Tree of Thoughts both focus on improving the reasoning capabilities of large language models."</data>
      <data key="d6">ccdfd3415647f13f577d728a5a0256b1</data>
    </edge>
    <edge source="&quot;THE DAWN OF LMMS&quot;" target="&quot;EMPOWERING LLM-BASED MACHINE TRANSLATION WITH CULTURAL AWARENESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Dawn of LMMs and Empowering LLM-Based Machine Translation with Cultural Awareness both represent advancements in the capabilities of large language models."</data>
      <data key="d6">ccdfd3415647f13f577d728a5a0256b1</data>
    </edge>
    <edge source="&quot;GRAPH-OF-THOUGHT REASONING&quot;" target="&quot;ANALOGICAL REASONING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Graph-of-Thought Reasoning and Analogical Reasoning involve advanced reasoning techniques in large language models."</data>
      <data key="d6">0274e77e2fcec8973c9768c464c6e82d</data>
    </edge>
    <edge source="&quot;INPUT-LABEL DEMONSTRATIONS&quot;" target="&quot;META-REASONING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input-Label Demonstrations and Meta-Reasoning both involve reasoning processes in language models."</data>
      <data key="d6">0274e77e2fcec8973c9768c464c6e82d</data>
    </edge>
    <edge source="&quot;VIDEO UNDERSTANDING&quot;" target="&quot;ATTRIBUTED TRAINING DATA GENERATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Video Understanding and Attributed Training Data Generation both involve the use of large language models for understanding and generating data."</data>
      <data key="d6">0274e77e2fcec8973c9768c464c6e82d</data>
    </edge>
    <edge source="&quot;AUTOMATIC EVALUATION OF ATTRIBUTION&quot;" target="&quot;INSTRUCTION FOLLOWING EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Automatic Evaluation of Attribution and Instruction Following Evaluation both involve evaluating the performance of large language models."</data>
      <data key="d6">0274e77e2fcec8973c9768c464c6e82d</data>
    </edge>
    <edge source="&quot;AMBIGUITY RESOLUTION&quot;" target="&quot;SOFTWARE ENGINEERING WITH LLMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ambiguity Resolution and Software Engineering with LLMs both involve the application of large language models to solve specific problems."</data>
      <data key="d6">0274e77e2fcec8973c9768c464c6e82d</data>
    </edge>
    <edge source="&quot;LANGUAGE INTELLIGENCE&quot;" target="&quot;AUTOMATIC CHAIN OF THOUGHT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Language Intelligence and Automatic Chain of Thought Prompting both involve chain-of-thought reasoning in language models."</data>
      <data key="d6">0274e77e2fcec8973c9768c464c6e82d</data>
    </edge>
    <edge source="&quot;MULTI-MODAL CHAIN-OF-THOUGHT REASONING&quot;" target="&quot;DUTY-DISTINCT CHAIN-OF-THOUGHT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both techniques involve chain-of-thought reasoning in language models, focusing on different aspects of multimodal reasoning."</data>
      <data key="d6">c7285f7847ef45ed85779d7966753855</data>
    </edge>
    <edge source="&quot;LARGE LANGUAGE MODELS AS TABLE-TO-TEXT GENERATORS&quot;" target="&quot;HUMAN-LEVEL PROMPT ENGINEERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains explore the capabilities of large language models in generating and structuring text effectively."</data>
      <data key="d6">c7285f7847ef45ed85779d7966753855</data>
    </edge>
    <edge source="&quot;ANIMATE124&quot;" target="&quot;CUT-AND-PASTE VIDEO EDITING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both techniques involve advanced image and video processing to create dynamic scenes."</data>
      <data key="d6">c7285f7847ef45ed85779d7966753855</data>
    </edge>
    <edge source="&quot;THREAD OF THOUGHT&quot;" target="&quot;GHOST IN THE MINECRAFT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both subdomains involve maintaining coherent thought processes in complex environments using large language models."</data>
      <data key="d6">c7285f7847ef45ed85779d7966753855</data>
    </edge>
    <edge source="&quot;HARD PROMPT&quot;" target="&quot;SOFT PROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hard Prompt and Soft Prompt are related as different types of prompts, with Hard Prompts containing tokens that correspond to words in the LLM vocabulary and Soft Prompts containing tokens that may not correspond to any word in the vocabulary."</data>
      <data key="d6">6430817c08b3a5c6d193478d4c739d79</data>
    </edge>
    <edge source="&quot;CLOZE PROMPT&quot;" target="&quot;PREFIX PROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cloze Prompt and Prefix Prompt are related as different prediction styles in LLMs, with Cloze Prompts presenting tokens as 'slots to fill' and Prefix Prompts predicting tokens at the end of the prompt."</data>
      <data key="d6">6430817c08b3a5c6d193478d4c739d79</data>
    </edge>
    <edge source="&quot;RESEARCH PAPER COLLECTION&quot;" target="&quot;MANUAL REVIEW&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Manual Review was part of the Research Paper Collection process to ensure the relevance of the collected papers."</data>
      <data key="d6">29d2b14a56a51f86baa34264697bdd5e</data>
    </edge>
    <edge source="&quot;RESEARCH PAPER COLLECTION&quot;" target="&quot;SEMI-AUTOMATED REVIEW&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Semi-Automated Review was part of the Research Paper Collection process to ensure the relevance of the collected papers."</data>
      <data key="d6">29d2b14a56a51f86baa34264697bdd5e</data>
    </edge>
    <edge source="&quot;RESEARCH PAPER COLLECTION&quot;" target="&quot;DATASET MAINTENANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ongoing Dataset Maintenance is required to support and maintain the dataset collected during the Research Paper Collection event."</data>
      <data key="d6">29d2b14a56a51f86baa34264697bdd5e</data>
    </edge>
    <edge source="&quot;RESEARCH PAPER COLLECTION&quot;" target="&quot;DATASET DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The decision not to distribute the dataset to third parties affects the Research Paper Collection process and its future uses."</data>
      <data key="d6">29d2b14a56a51f86baa34264697bdd5e</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING TECHNIQUES&quot;" target="&quot;LLM PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LLM Prompting is a core component of Prompt Engineering Techniques, focusing on the creation and refinement of prompts for large language models."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING TECHNIQUES&quot;" target="&quot;0-SHOT PROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"0-Shot Prompt is a specific technique within Prompt Engineering Techniques where the model is given a task without prior examples."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING TECHNIQUES&quot;" target="&quot;MANY-SHOT PROMPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Many-Shot Prompt is a technique within Prompt Engineering Techniques that involves providing multiple examples to guide the model's response."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING TECHNIQUES&quot;" target="&quot;TRANSFORMER MODEL PROMPTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Transformer Model Prompts are designed specifically for transformer-based models, making them a part of Prompt Engineering Techniques."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING TECHNIQUES&quot;" target="&quot;PROMPT-BASED TRANSFER LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt-Based Transfer Learning is a technique within Prompt Engineering Techniques that uses prompts to transfer knowledge between domains."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING TECHNIQUES&quot;" target="&quot;NLP PROMPTING STRATEGIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NLP Prompting Strategies are various techniques within Prompt Engineering Techniques used to create effective prompts for language models."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING TECHNIQUES&quot;" target="&quot;LLM INTERPRETABILITY VIA PROMPTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LLM Interpretability via Prompts is a focus area within Prompt Engineering Techniques aimed at making model behavior more understandable."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING TECHNIQUES&quot;" target="&quot;CURRICULUM LEARNING WITH PROMPTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Curriculum Learning with Prompts is a method within Prompt Engineering Techniques that structures prompts to gradually increase in complexity."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING TECHNIQUES&quot;" target="&quot;FEEDBACK LOOPS IN LLM PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feedback Loops in LLM Prompting are iterative processes within Prompt Engineering Techniques that use model output to refine prompts."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING TECHNIQUES&quot;" target="&quot;HUMAN-IN-THE-LOOP PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Human-in-the-Loop Prompting is a technique within Prompt Engineering Techniques that involves human intervention to enhance model performance."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING TECHNIQUES&quot;" target="&quot;TOKEN-EFFICIENT PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Token-Efficient Prompting is a focus area within Prompt Engineering Techniques that aims to create prompts using fewer tokens."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING TECHNIQUES&quot;" target="&quot;INSTRUCTION PROMPTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Instruction Prompting is a method within Prompt Engineering Techniques that involves creating prompts with explicit instructions."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;PROMPT ENGINEERING TECHNIQUES&quot;" target="&quot;PROMPT TEMPLATING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Templating is a technique within Prompt Engineering Techniques that involves creating reusable prompt structures."</data>
      <data key="d6">cd60cb17b3864e9fcc7266ff4c1611ce</data>
    </edge>
    <edge source="&quot;GPT-4&quot;" target="&quot;EVALUATION PAPER SUMMARY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"GPT-4 is one of the models evaluated in the Evaluation Paper Summary."</data>
      <data key="d6">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </edge>
    <edge source="&quot;LLAMA-2-CHAT&quot;" target="&quot;EVALUATION PAPER SUMMARY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LLaMA-2-Chat is one of the models evaluated in the Evaluation Paper Summary."</data>
      <data key="d6">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </edge>
    <edge source="&quot;PALM2&quot;" target="&quot;EVALUATION PAPER SUMMARY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"PaLM2 is one of the models evaluated in the Evaluation Paper Summary."</data>
      <data key="d6">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </edge>
    <edge source="&quot;FALCON&quot;" target="&quot;EVALUATION PAPER SUMMARY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Falcon is one of the models evaluated in the Evaluation Paper Summary."</data>
      <data key="d6">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </edge>
    <edge source="&quot;CLAUDE-V1&quot;" target="&quot;EVALUATION PAPER SUMMARY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Claude-v1 is one of the models evaluated in the Evaluation Paper Summary."</data>
      <data key="d6">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </edge>
    <edge source="&quot;GPT-3.5&quot;" target="&quot;EVALUATION PAPER SUMMARY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"GPT-3.5 is one of the models evaluated in the Evaluation Paper Summary."</data>
      <data key="d6">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </edge>
    <edge source="&quot;CLAUDE-V1.3&quot;" target="&quot;EVALUATION PAPER SUMMARY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Claude-v1.3 is one of the models evaluated in the Evaluation Paper Summary."</data>
      <data key="d6">9f0f4b7adda7eade3a9a430f6b8782dd</data>
    </edge>
    <edge source="&quot;EMAIL INCLUSION&quot;" target="&quot;PERFORMANCE TESTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Email Inclusion is tested during Performance Testing to see its impact on the AI's ability to identify entrapment."</data>
      <data key="d6">bcb6ef7c52ce001fb19904d1aa92dfd2</data>
    </edge>
    <edge source="&quot;PERFORMANCE TESTING&quot;" target="&quot;PROMPT DEFINITION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prompt Definition is part of the broader goal to improve Performance Testing by standardizing the term 'prompt'."</data>
      <data key="d6">bcb6ef7c52ce001fb19904d1aa92dfd2</data>
    </edge>
    <edge source="&quot;DENIS PESKOFF&quot;" target="&quot;PHILLIP RESNIK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Denis Peskoff assisted with the organization and final review of the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;ALEXANDER HOYLE&quot;" target="&quot;PHILLIP RESNIK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Alexander Hoyle provided guidance on writing and meta-analysis approach for the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;SHYAMAL ANADKAT&quot;" target="&quot;PHILLIP RESNIK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Shyamal Anadkat assisted with the overall review of the paper and the etymology and definitions for the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;JULES WHITE&quot;" target="&quot;PHILLIP RESNIK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jules White built trees for technique taxonomies for the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;MARINE CARPAUT&quot;" target="&quot;PHILLIP RESNIK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Marine Carpaut framed, reviewed, and suggested papers for the multilingual section of the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PHILLIP RESNIK&quot;" target="&quot;MEGAN L. ROGERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Megan L. Rogers reviewed and gave advice for the SCS Labeling section of the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PHILLIP RESNIK&quot;" target="&quot;INNA GONCEARENCO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Inna Goncearenco reviewed and gave advice for the SCS Labeling section of the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PHILLIP RESNIK&quot;" target="&quot;GIUSEPPE SARLI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Giuseppe Sarli reviewed and gave advice for the SCS Labeling section of the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PHILLIP RESNIK&quot;" target="&quot;IGOR GALYNKER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Igor Galynker reviewed and gave advice for the SCS Labeling section of the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PHILLIP RESNIK&quot;" target="&quot;KONSTANTINE KAHADZE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Konstantine Kahadze managed the MMLU benchmarking codebase and contributed to Security and Meta Analysis for the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PHILLIP RESNIK&quot;" target="&quot;ASHAY SRIVASTAVA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ashay Srivastava reviewed papers for human review and worked on the tool use agents section for the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PHILLIP RESNIK&quot;" target="&quot;HEVANDER DA COSTA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hevander Da Costa contributed to the Benchmarking section and Meta Review datasets list for the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PHILLIP RESNIK&quot;" target="&quot;FEILEEN LI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feileen Li worked on the tool use agents section and assisted with the human paper review for the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PHILLIP RESNIK&quot;" target="&quot;NISHANT BALEPUR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Nishant Balepur helped with high-level discussions in benchmarking and reviewed drafts for the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PHILLIP RESNIK&quot;" target="&quot;SEVIEN SCHULHOFF&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sevien Schulhoff contributed to the benchmarking section for the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PHILLIP RESNIK&quot;" target="&quot;CHENGLEI SI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chenglei Si suggested related works and edited sections for the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PHILLIP RESNIK&quot;" target="&quot;PRANAV SANDEEP DULEPET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Pranav Sandeep Dulepet contributed definitions and worked on segmentation and object detection in the multimodal section for the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PHILLIP RESNIK&quot;" target="&quot;HYOJUNG HAN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"HyoJung Han contributed to the Multimodal section and wrote the audio prompting section for the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PHILLIP RESNIK&quot;" target="&quot;HUDSON TAO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hudson Tao authored sections on image, video, and 3D within multimodal and maintained the GitHub code for the paper led by Principal Investigator Phillip Resnik."</data>
      <data key="d6">7096851583df5cc6ad819323dfd9e83e</data>
    </edge>
    <edge source="&quot;PRANAV SANDEEP DULEPET&quot;" target="&quot;MULTIMODAL SECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Pranav Sandeep Dulepet worked on segmentation and object detection in the multimodal section."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
    <edge source="&quot;HYOJUNG HAN&quot;" target="&quot;MULTIMODAL SECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"HyoJung Han contributed to the Multimodal section, especially the speech+text part."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
    <edge source="&quot;HUDSON TAO&quot;" target="&quot;MULTIMODAL SECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hudson Tao authored sections on image, video, and 3D within multimodal."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
    <edge source="&quot;AMANDA LIU&quot;" target="&quot;TAXONOMIC ONTOLOGY SECTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Amanda Liu authored taxonomic ontology sections."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
    <edge source="&quot;SWETA AGRAWAL&quot;" target="&quot;EVALUATION SECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sweta Agrawal was the team lead for the evaluation section."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
    <edge source="&quot;SAURAV VIDYADHARA&quot;" target="&quot;TAXONOMY TREES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Saurav Vidyadhara assisted with general review and revising taxonomy trees."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
    <edge source="&quot;CHAU PHAM&quot;" target="&quot;META REVIEW&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chau Pham assisted with meta review, including automated analysis of topics."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
    <edge source="&quot;ZOEY KI&quot;" target="&quot;MULTILINGUAL PROMPTING SECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Zoey Ki led the Multilingual prompting section."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
    <edge source="&quot;YINHENG LI&quot;" target="&quot;SECTION 2.2 TEXT-BASED TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Yinheng Li worked on section 2.2 text-based techniques."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
    <edge source="&quot;SALONI GUPTA&quot;" target="&quot;PAPER PIPELINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Saloni Gupta helped set up the paper pipeline."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
    <edge source="&quot;GERSON KROIZ&quot;" target="&quot;SECTION 1.1&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gerson Kroiz was involved with section 1.1."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
    <edge source="&quot;AAYUSH GUPTA&quot;" target="&quot;META ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Aayush Gupta contributed to the Meta Analysis."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
    <edge source="&quot;MICHAEL ILIE&quot;" target="&quot;PRISMA REVIEW FIGURE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Michael Ilie helped with the PRISMA review figure."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
    <edge source="&quot;MICHAEL ILIE&quot;" target="&quot;SCS PROMPTING CASE STUDY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Michael Ilie helped with the SCS prompting case study."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
    <edge source="&quot;SANDER SCHULHOFF&quot;" target="&quot;LEAD AUTHOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sander Schulhoff was the Lead Author."</data>
      <data key="d6">77d7c813cbd787e0699413f0a945f885</data>
    </edge>
  </graph>
</graphml>