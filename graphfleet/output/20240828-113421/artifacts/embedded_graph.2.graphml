<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d6" for="edge" attr.name="source_id" attr.type="string" />
  <key id="d5" for="edge" attr.name="description" attr.type="string" />
  <key id="d4" for="edge" attr.name="weight" attr.type="double" />
  <key id="d3" for="node" attr.name="entity_type" attr.type="string" />
  <key id="d2" for="node" attr.name="source_id" attr.type="string" />
  <key id="d1" for="node" attr.name="description" attr.type="string" />
  <key id="d0" for="node" attr.name="type" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="DARREN EDGE">
      <data key="d0">PERSON</data>
      <data key="d1">Darren Edge is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="HA TRINH">
      <data key="d0">PERSON</data>
      <data key="d1">Ha Trinh is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="NEWMAN CHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Newman Cheng is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="JOSHUA BRADLEY">
      <data key="d0">PERSON</data>
      <data key="d1">Joshua Bradley is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="ALEX CHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Alex Chao is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="APURVA MODY">
      <data key="d0">PERSON</data>
      <data key="d1">Apurva Mody is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="STEVEN TRUITT">
      <data key="d0">PERSON</data>
      <data key="d1">Steven Truitt is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="JONATHAN LARSON">
      <data key="d0">PERSON</data>
      <data key="d1">Jonathan Larson is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="MICROSOFT RESEARCH">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Microsoft Research is the organization where Darren Edge, Ha Trinh, and Jonathan Larson are affiliated. It is also the institution where the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows" are based.</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Microsoft Strategic Missions and Technologies is the organization where Newman Cheng, Joshua Bradley, and Steven Truitt are affiliated</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="MICROSOFT OFFICE OF THE CTO">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Microsoft Office of the CTO is the organization where Alex Chao and Apurva Mody are affiliated</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="GRAPH RAG">
      <data key="d0">METHOD/APPROACH</data>
      <data key="d1">Graph RAG is a method that leverages graph communities to answer user queries by using a self-generated graph index. This approach enables efficient and comprehensive summarization and iterative question answering. It is particularly designed to handle question answering over private text corpora, scaling effectively with both the generality of user questions and the quantity of source text to be indexed. Graph RAG utilizes the natural modularity of graphs to partition data for global summarization, and it is based on the global summarization of an LLM-derived knowledge graph.</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,26b2dad01a219bc034ac7d6a32d07582,64476a39d7d8b87b399e3bd3cead79c7,ac21ebe9a9d70d691c717f961d3f10c8,c8e8019de153e439d6a79dcf209b943b,edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="RETRIEVAL-AUGMENTED GENERATION (RAG)">
      <data key="d0">METHOD/APPROACH</data>
      <data key="d1">Retrieval-augmented generation (RAG) is a method to retrieve relevant information from an external knowledge source to enable large language models to answer questions over private and/or previously unseen document collections</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">METHOD/APPROACH</data>
    </node>
    <node id="QUERY-FOCUSED SUMMARIZATION (QFS)">
      <data key="d0">METHOD/APPROACH</data>
      <data key="d1">Query-focused summarization (QFS) is a task that generates natural language summaries based on specific user queries</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">METHOD/APPROACH</data>
    </node>
    <node id="LARGE LANGUAGE MODELS (LLMS)">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Large language models (LLMs) are advanced models used to automate human-like sensemaking in complex domains</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="COMMUNITY DETECTION">
      <data key="d0">PROCESS</data>
      <data key="d1">Community detection is the process of partitioning a graph into communities of nodes with stronger connections to one another than to other nodes. This process is used to partition a graph index into groups of elements that can be summarized in parallel.</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7,e66ed885a08f92cc69f4895302c33047</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="LEIDEN">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Leiden is a community detection algorithm used to partition graphs into modular communities. It is particularly known for its efficiency in recovering hierarchical community structures, making it highly suitable for large-scale graphs. Additionally, Leiden is utilized in the Graph RAG (Region Adjacency Graph) approach for community detection, further showcasing its versatility and effectiveness in various graph-based applications.</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7,e66ed885a08f92cc69f4895302c33047</data>
      <data key="d3">ALGORITHM</data>
    </node>
    <node id="GLOBAL SENSEMAKING QUESTIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Global sensemaking questions are questions that require understanding connections among people, places, and events to anticipate their trajectories and act effectively</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="OPEN-SOURCE IMPLEMENTATION">
      <data key="d0">RESOURCE</data>
      <data key="d1">An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">RESOURCE</data>
    </node>
    <node id="MICROSOFT">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Microsoft is the company associated with the research and development of the Graph RAG approach. Kevin Scott serves as the Chief Technology Officer (CTO) at Microsoft, and the company is mentioned in the context of the podcast transcripts dataset. Microsoft is also the organization behind the study titled "The impact of large language models on scientific discovery: a preliminary study using GPT-4." Additionally, Microsoft produces the podcast "Behind the Tech" and has published the technical report "ChatGPT for robotics: Design principles and model abilities."</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,34d0bb2211fc795fe1096442e086a2b3,4930fce6da868f894757a9da465807ba,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">ORGANIZATION</data>
    </node>
    <node id="RANADE AND JOSHI">
      <data key="d0">PERSON</data>
      <data key="d1">Ranade and Joshi are researchers mentioned in the context of intelligence analysis using LLMs</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KLEIN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Klein et al. are researchers who defined sensemaking as a motivated, continuous effort to understand connections</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LEWIS ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Lewis et al. are researchers who have significantly contributed to the field of Artificial Intelligence and Machine Learning, particularly in the development of retrieval-augmented generation (RAG) and memory structures in agentic systems. They are referenced in the context of RAG and are known for their work on External Memory and RAG methods.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,0c932f7def033fa2b1bf210fbb771e7d,6bdf681c0bd9e401ac72344a6a0ae479,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DANG">
      <data key="d0">PERSON</data>
      <data key="d1">Dang is a researcher associated with query-focused summarization (QFS)</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BAUMEL ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Baumel et al. are researchers associated with query-focused abstractive summarization</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LASKAR ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Laskar et al. are researchers known for their contributions to the field of query-focused abstractive summarization. In 2022, they were referenced for their significant work on summarization tasks, highlighting their expertise and influence in this specialized area of artificial intelligence and machine learning.</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YAO ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Yao et al. are researchers who have made significant contributions to the field of Artificial Intelligence and Machine Learning. They are particularly noted for their work on Chain-of-Thought-based planning and reasoning methods. Their research extends to various domains, including web navigation, prompting techniques, and search-guided language model work. Additionally, Yao et al. have been involved in query-focused abstractive summarization, showcasing their diverse expertise in AI and ML. Their work is frequently referenced, highlighting their influence and the impact of their contributions on the broader research community.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,0c932f7def033fa2b1bf210fbb771e7d,93cb0d0456e0822b5fe30a3e627405f8,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GOODWIN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Goodwin et al. are researchers associated with the application of the transformer architecture to summarization tasks.</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIU AND LAPATA">
      <data key="d0">PERSON</data>
      <data key="d1">Liu and Lapata are researchers recognized for their significant contributions to summarization tasks, particularly in 2019. They are noted for their work involving the application of the transformer architecture to these tasks, highlighting their expertise in leveraging advanced neural network models to improve summarization techniques.</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GPT">
      <data key="d0">MODEL</data>
      <data key="d1">GPT, developed by OpenAI, is a Foundation Model designed for general-purpose agentic tasks. It is a series of large language models renowned for their ability to perform various summarization tasks. These models are highly referenced in the AI and ML community for their versatility and effectiveness in processing and generating human-like text, making them integral tools for a wide range of applications.</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="LLAMA">
      <data key="d0">MODEL</data>
      <data key="d1">LLAMA is a series of large language models renowned for their ability to perform various summarization tasks. These models are frequently referenced for their effectiveness in summarizing content, showcasing their utility in a range of applications within the field of natural language processing.</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="GEMINI">
      <data key="d0">MODEL</data>
      <data key="d1">Gemini is a family of highly capable multimodal models, known for their proficiency in performing various summarization tasks. This series of large language models is frequently referenced for their exceptional ability to handle and summarize diverse types of data, showcasing their versatility and advanced capabilities in the field of artificial intelligence and machine learning.</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7,ac21ebe9a9d70d691c717f961d3f10c8</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="ACHIAM ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Achiam et al. are authors referenced for their work on GPT in 2023</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="BROWN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Brown et al. are notable authors referenced for their seminal work on GPT in 2020. They have significantly contributed to the understanding of in-context learning in language models. Their research is frequently cited in discussions about the rise of language models that exhibit strong reasoning capabilities and general adaptability.</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,93cb0d0456e0822b5fe30a3e627405f8,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="TOUVRON ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Touvron et al. are notable authors referenced for their significant contributions to the development of Llama in 2023. Their work is particularly recognized in the context of the rise of advanced language models that exhibit strong reasoning capabilities and general adaptability.</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="ANIL ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Anil et al. are authors referenced for their work on Gemini in 2023</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="KURATOV ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Kuratov et al. are authors referenced for their work on summarization and LLM context windows in 2024</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="LIU ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Liu et al. are authors referenced for their significant contributions in the field of Artificial Intelligence and Machine Learning in 2023. Their work encompasses a variety of critical areas, including summarization and the optimization of large language model (LLM) context windows. Additionally, they have delved into the intricate balance between exploration and exploitation, a fundamental concept in reinforcement learning. Furthermore, Liu et al. have explored innovative approaches to combining search algorithms with language model agents, showcasing their versatility and depth in advancing AI methodologies.</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,6bdf681c0bd9e401ac72344a6a0ae479,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="RAG">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">RAG (Retrieval-Augmented Generation) is a method that enhances the capacity of language models to generate informed, contextually precise responses by incorporating retrieved documents into their outputs. It is used to improve model performance through retrieval and generation, making it a valuable tool in various applications, including query-focused summarization, although it has been noted as inadequate for some specific summarization tasks. RAG is also a skill covered by the synthetic post-training dataset created by AgentInstruct and serves as a building block in agent frameworks like LangChain and ADAS. This technique involves retrieving and adding text chunks to the context window, thereby enriching the generated responses with relevant information.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,26b2dad01a219bc034ac7d6a32d07582,4884e8429ca1e567dadf5e22b4b68274,64476a39d7d8b87b399e3bd3cead79c7,6bdf681c0bd9e401ac72344a6a0ae479,8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2,ac21ebe9a9d70d691c717f961d3f10c8,b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="NEWMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Newman is referenced for their work on the modularity of graphs in 2006</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="LOUVAIN">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Louvain is a community detection algorithm used to partition graphs into modular communities</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="BLONDEL ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Blondel et al. are authors referenced for their work on the Louvain algorithm in 2008</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="TRAAG ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Traag et al. are authors referenced for their work on the Leiden algorithm in 2019</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="HOTPOTQA">
      <data key="d0">DATASET</data>
      <data key="d1">HOTPOTQA is a comprehensive benchmark dataset for open-domain question answering, specifically targeting explicit fact retrieval and reasoning over multiple supporting documents. It contains 113,000 Wikipedia-based question-answer pairs crafted by crowdworkers. This multi-hop question-answering benchmark requires retrieval over two or more Wikipedia passages and is used to evaluate reasoning and acting strategies in language models. HotPotQA is employed in various empirical evaluations, including the performance of the LATS algorithm in decision-making, reasoning, and interactive question-answering tasks. It is also used to compare the cost and performance of different methods, evaluate entity extraction with varying chunk sizes, and measure model performance using metrics such as Exact Match (EM). Additionally, HotPotQA involves interleaving Thought, Action, and Observation steps to solve questions, demonstrating the effectiveness of internal reasoning and external retrieval strategies.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,42de130f5b6144472a86a4c8260a87c7,48e423e2baf2ed485872756f5b4d87d8,4930fce6da868f894757a9da465807ba,594449768ae2dea9b2efbe677075096b,64476a39d7d8b87b399e3bd3cead79c7,8180bf20b7577f3eee40df5991e2886d,93cb0d0456e0822b5fe30a3e627405f8,99d90aededb61e04241516ed9ec656cc,b8dd0300033963bb4a3e1bad37f8e7b9,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="YANG ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Yang et al. are authors referenced for their work on the HotPotQA dataset in 2018. Their contributions are significant in the context of the HotPotQA benchmark, which is utilized in the empirical evaluation of LATS.</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="GPT-4-TURBO">
      <data key="d0">MODEL</data>
      <data key="d1">GPT-4-Turbo is a version of OpenAI's language model known for its large context size of 128k tokens, which is particularly useful in the evaluation of context window sizes. Additionally, GPT-4-Turbo is employed for entity extraction tasks within the HotPotQA dataset, showcasing its versatility and advanced capabilities in handling extensive and complex language processing tasks.</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="MAP-REDUCE">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">MAP-REDUCE is a versatile method employed in various summarization tasks within the field of Artificial Intelligence and Machine Learning. It is used to shuffle and chunk source texts, facilitating different stages of summarization. Specifically, Map-Reduce is applied for global summarization of source texts, ensuring a comprehensive overview of the entire corpus. Additionally, it is utilized for query-focused summarization, tailoring the summarization process to specific queries. This technique is integral to efficiently processing and summarizing large volumes of text data.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,64476a39d7d8b87b399e3bd3cead79c7,ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="PODCAST TRANSCRIPTS">
      <data key="d0">DATASET</data>
      <data key="d1">The "PODCAST TRANSCRIPTS" dataset comprises compiled transcripts of podcast conversations between Kevin Scott, Microsoft CTO, and other technology leaders. This dataset is utilized for evaluation purposes and serves as one of the real-world datasets used to generate activity-centered sense-making questions.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba,64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="NEWS ARTICLES">
      <data key="d0">DATASET</data>
      <data key="d1">The "NEWS ARTICLES" dataset is a benchmark collection comprising news articles published from September 2013 to December 2023 across various categories. This dataset is utilized for evaluation purposes and serves as one of the real-world datasets employed to generate activity-centered sense-making questions.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba,64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="NAIVE RAG">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">NAIVE RAG is a baseline approach for summarizing and indexing data. While it serves as a fundamental technique, it is outperformed by global summarization approaches in terms of comprehensiveness and diversity.</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="GLOBAL MAP-REDUCE">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Global Map-Reduce is a summarization technique compared against Naive RAG</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="GRAPH RAG APPROACH">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Graph RAG Approach is a high-level data flow and pipeline for global summarization</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="TEXT CHUNKS">
      <data key="d0">DATA UNIT</data>
      <data key="d1">Text Chunks are segments of source documents used for processing in the Graph RAG approach</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="ELEMENT INSTANCES">
      <data key="d0">DATA UNIT</data>
      <data key="d1">Element Instances are graph nodes and edges extracted from text chunks. They represent the initial extracted descriptions of entities, relationships, and claims from source texts.</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="NAMED ENTITIES">
      <data key="d0">DATA UNIT</data>
      <data key="d1">Named Entities are broad classes of entities such as people, places, and organizations that are extracted from text. These specific categories are crucial for various natural language processing tasks, enabling the identification and classification of key elements within textual data.</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,e66ed885a08f92cc69f4895302c33047</data>
      <data key="d3">DATA UNIT</data>
    </node>
    <node id="SUMMARIZATION TASKS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="LLM CONTEXT WINDOWS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="QUERY-FOCUSED SUMMARIZATION">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="GLOBAL SUMMARIZATION">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="GRAPH MODULARITY">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="ENTITY EXTRACTION">
      <data key="d0" />
      <data key="d1">ENTITY EXTRACTION is the process of extracting entities from text for graph indexing.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="SENSE-MAKING QUESTIONS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="KNOWLEDGE GRAPH">
      <data key="d0">DATA UNIT</data>
      <data key="d1">A Knowledge Graph is a structured representation of knowledge used in the Graph RAG approach. It serves as a structured representation of information, utilized in various advanced RAG systems for retrieval and reasoning.</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="COMMUNITY DESCRIPTIONS">
      <data key="d0">DATA UNIT</data>
      <data key="d1">Community Descriptions are summaries of graph communities used in the Graph RAG approach</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="ACTIVITY-CENTERED SENSE-MAKING QUESTIONS">
      <data key="d0">DATA UNIT</data>
      <data key="d1">Activity-Centered Sense-Making Questions are generated from real-world datasets for evaluation</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="COMPREHENSIVENESS">
      <data key="d0">QUALITY</data>
      <data key="d1">Comprehensiveness is a metric used to evaluate the completeness and detail of answers in summarization tasks. It assesses how thoroughly an answer covers all aspects and relevant information of the question. Comprehensiveness is a target quality for evaluating summarization approaches, and its effectiveness can be indicated by metrics such as Graph RAG's win rate in comprehensiveness.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,64476a39d7d8b87b399e3bd3cead79c7,edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="DIVERSITY">
      <data key="d0">QUALITY</data>
      <data key="d1">DIVERSITY is a metric used to evaluate the variety and richness of answers in providing different perspectives and insights on a question. It is a target quality for evaluating summarization approaches, ensuring that the generated data and instructions encompass a wide range of information or perspectives. This attribute is crucial in summarization tasks, as indicated by Graph RAG's win rate in diversity, and is one of the attributes that agentic flows aim to ensure in the generated data and instructions.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,64476a39d7d8b87b399e3bd3cead79c7,edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="EMPOWERMENT">
      <data key="d0">QUALITY</data>
      <data key="d1">EMPOWERMENT is a metric used to evaluate the effectiveness of answers in summarization tasks. It assesses how well an answer helps the reader understand and make informed judgments about the topic. Empowerment is a target quality for evaluating summarization approaches, referring to the ability of a system to help users reach an informed understanding, often by providing specific examples, quotes, and citations.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,64476a39d7d8b87b399e3bd3cead79c7,edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="SOURCE TEXTS">
      <data key="d0">DATA UNIT</data>
      <data key="d1">Source Texts are the original documents used for summarization in the Graph RAG approach. These texts serve as the foundational materials in the Graph RAG method, providing the essential content that is summarized and analyzed.</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="HIERARCHICAL LEVEL">
      <data key="d0">QUALITY</data>
      <data key="d1">Hierarchical Level is a variable in the evaluation of community summaries</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="TOKEN COSTS">
      <data key="d0">QUALITY</data>
      <data key="d1">Token Costs are a metric used to evaluate the efficiency of summarization approaches</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="GRAPH RAG PIPELINE">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Graph RAG Pipeline is the implementation of the Graph RAG approach</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="LLM PROMPTS">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">LLM Prompts are used to extract elements of a graph index from text chunks</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="FEW-SHOT EXAMPLES">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Few-Shot Examples are specialized examples used in Large Language Model (LLM) prompts for in-context learning. These examples are provided to the LLM to enhance its performance in domains requiring specialized knowledge, such as science, medicine, and law.</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="LLM">
      <data key="d0" />
      <data key="d1">A Large Language Model (LLM) is a sophisticated AI tool used in various applications within the AI and ML community. It is employed to hypothesize other APIs present in the library during the Content Transformation Flow, assess the comprehensiveness, diversity, empowerment, and directness of generated answers, and power agents that can optionally use tools such as search APIs, code interpreters, or calculators. Additionally, LLMs are utilized to generate community summaries and global answers based on user queries, extract descriptions of entities, relationships, and claims from source texts, and create summaries of these elements. They are also integral to various Retrieval-Augmented Generation (RAG) approaches, processing and generating text based on retrieved information, generating questions, and evaluating answers.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,427e98b00e49b6a8f8649054122dd45b,4930fce6da868f894757a9da465807ba,64476a39d7d8b87b399e3bd3cead79c7,c8e8019de153e439d6a79dcf209b943b,e66ed885a08f92cc69f4895302c33047,edab4014b8f55e5b25bd7f396314be1f,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="EVALUATION">
      <data key="d0" />
      <data key="d1">Evaluation is the third operation in LATS where a scalar value is assigned to each new child node to quantify the agent&#8217;s progress in task completion. It involves the process of assessing the quality and effectiveness of generated questions and answers, as well as assigning a scalar value to each new child node for selection and backpropagation.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,26b2dad01a219bc034ac7d6a32d07582,64476a39d7d8b87b399e3bd3cead79c7,86f77e15d41cbd0cb33f635ccb2cb66b,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="COVARIATES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Covariates are additional variables or attributes associated with extracted node instances, such as claims linked to detected entities</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="LOGIT BIAS">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Logit bias is a technique used to force the LLM to make a yes/no decision during the entity extraction process</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="GLEANINGS">
      <data key="d0">PROCESS</data>
      <data key="d1">Gleanings refer to multiple rounds of extraction to encourage the LLM to detect any additional entities it may have missed in prior rounds</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="ELEMENT SUMMARIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Element summaries are single blocks of descriptive text for each graph element, created by further summarizing instance-level summaries</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="GRAPH ELEMENT">
      <data key="d0">CONCEPT</data>
      <data key="d1">Graph elements include entity nodes, relationship edges, and claim covariates in a graph structure</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="ENTITY GRAPH">
      <data key="d0">CONCEPT</data>
      <data key="d1">An entity graph is a graph structure where nodes represent entities and edges represent relationships between them</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="KNOWLEDGE GRAPHS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Knowledge graphs are graph structures that rely on concise and consistent knowledge triples (subject, predicate, object) for reasoning tasks</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="GRAPH COMMUNITIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Graph communities, as utilized in the Graph RAG method, are groups of nodes within a graph that exhibit stronger connections to each other than to nodes outside the group. These communities are identified using community detection algorithms and are employed at different levels to effectively answer user queries.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="COMMUNITY SUMMARIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Community summaries are a type of self-memory used in Graph RAG for generation-augmented retrieval, facilitating future generation cycles. They are generated descriptions of elements within a community, used to create a final answer in a multi-stage process based on hierarchical community structure. These report-like summaries provide an understanding of the global structure and semantics of the dataset. Community summaries are generated at different levels of a graph community hierarchy and are particularly focused on root-level communities in the entity-based graph index.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba,ac21ebe9a9d70d691c717f961d3f10c8,e66ed885a08f92cc69f4895302c33047,edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="MULTIHOP-RAG">
      <data key="d0">DATASET</data>
      <data key="d1">MULTIHOP-RAG is a benchmark dataset used for evaluating open-domain question answering systems. It includes news articles across various categories and is also utilized in the context of graph indexing and community detection.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba,e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="OPENORD">
      <data key="d0">TOOL</data>
      <data key="d1">OpenORD is a tool used for node layout in graph visualization</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="FORCE ATLAS 2">
      <data key="d0">TOOL</data>
      <data key="d1">Force Atlas 2 is a tool used for node layout in graph visualization</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="HIERARCHICAL CLUSTERING">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Hierarchical clustering is a technique used to reveal internal structure within root-level communities in a graph</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="ROOT COMMUNITIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Root communities are the top-level communities in a hierarchical clustering of a graph</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="LEAF-LEVEL COMMUNITIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Leaf-level communities are the bottom-level communities in a hierarchical clustering of a graph. They represent the most granular divisions within a hierarchical community structure, where element summaries, including nodes, edges, and covariates, are prioritized. These summaries are added to the LLM context window until the token limit is reached, ensuring detailed and comprehensive representation of the community's structure and relationships.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba,e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="TOKEN LIMIT">
      <data key="d0">CONCEPT</data>
      <data key="d1">Token limit refers to the maximum number of tokens that can be processed in a single context window by the LLM</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="BROWN ET AL., 2020">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A reference to the work by Brown et al. in 2020, which discusses examples provided to the LLM for in-context learning</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="SCIENCE">
      <data key="d0">DOMAIN</data>
      <data key="d1">SCIENCE is a specialized domain tested by Meta Agent Search using the GPQA benchmark, where models are evaluated on their ability to understand and interpret scientific information. It benefits from few-shot examples for improving the performance of large language models (LLMs). However, the knowledge embedded in foundational models (FMs) is often not sufficient to solve challenging questions within this domain.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b,e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="MEDICINE">
      <data key="d0">DOMAIN</data>
      <data key="d1">A specialized domain that benefits from few-shot examples for LLM performance improvement</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="LAW">
      <data key="d0">DOMAIN</data>
      <data key="d1">A specialized domain that benefits from few-shot examples for LLM performance improvement</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="CLAIMS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Claims are statements linked to detected entities, including attributes like subject, object, type, description, source text span, and start and end dates</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="FIGURE 2">
      <data key="d0">VISUALIZATION</data>
      <data key="d1">FIGURE 2 is a figure referenced in the text that illustrates the impact of using larger chunk sizes without a drop in quality or the forced introduction of noise. Additionally, Figure 2 provides an overview of the six operations in LATS, depicting the sequence of operations performed.</data>
      <data key="d2">c234cb83764b899335af0950677ad024,e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="FORTUNATO, 2010">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A survey by Fortunato in 2010, referenced in the context of community detection algorithms</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="JIN ET AL., 2021">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A survey by Jin et al. in 2021, referenced in the context of community detection algorithms</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="TRAAG ET AL., 2019">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A reference to the work by Traag et al. in 2019, which discusses the Leiden algorithm for community detection</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="TANG AND YANG, 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A reference to the work by Tang and Yang in 2024, which discusses the MultiHop-RAG dataset</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="MARTIN ET AL., 2011">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A reference to the work by Martin et al. in 2011, which discusses the OpenORD tool for node layout</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="JACOMY ET AL., 2014">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A reference to the work by Jacomy et al. in 2014, which discusses the Force Atlas 2 tool for node layout</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="ROOT-LEVEL COMMUNITIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Root-level communities refer to the primary divisions within a hierarchical community structure, revealing internal structure within these communities.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="HIGHER-LEVEL COMMUNITIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Higher-level communities are broader divisions within a hierarchical community structure, where sub-communities are ranked and summarized to fit within the context window if element summaries exceed the token limit.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="GLOBAL ANSWER">
      <data key="d0">CONCEPT</data>
      <data key="d1">The global answer is the final response generated from community summaries to answer a user query, ensuring a balance of summary detail and scope.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="USER QUERY">
      <data key="d0">CONCEPT</data>
      <data key="d1">A user query is a question posed by the user that the system aims to answer using community summaries and hierarchical community structure.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="TECH JOURNALIST">
      <data key="d0">USER</data>
      <data key="d1">A tech journalist is a potential user looking for insights and trends in the tech industry, particularly regarding the role of policy and regulation.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="EDUCATOR">
      <data key="d0">USER</data>
      <data key="d1">An educator is a potential user incorporating current affairs into curricula, particularly focusing on health and wellness.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="MT-BENCH">
      <data key="d0">DATASET</data>
      <data key="d1">MT-BENCH is a benchmark dataset designed for open-domain question answering, specifically targeting explicit fact retrieval. It assesses the competence of chat assistants in multi-turn conversations, with GPT-4 serving as the evaluator. The benchmark evaluates model responses to both first-turn and second-turn queries, with GPT-4 providing scores ranging from 1 to 10 for each turn. MT-BENCH is used to evaluate the performance of various models, including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4. This project, developed by Lm-sys, was published in 2023.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba,3d1f6634f93f8a4c296dc8df7e59859e,4930fce6da868f894757a9da465807ba,86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="RAG SYSTEMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Retrieval-Augmented Generation (RAG) systems are used for generating answers to questions by retrieving relevant information from large datasets.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="DATA SENSEMAKING">
      <data key="d0">PROCESS</data>
      <data key="d1">Data sensemaking is the process through which people inspect, engage with, and contextualize data within the broader scope of real-world activities.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="KOESTEN ET AL.">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study by Koesten et al. (2021) on data sensemaking behaviors.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="XU AND LAPATA">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study by Xu and Lapata (2021) on methods for extracting latent summarization queries from source texts.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="KEVIN SCOTT">
      <data key="d0">PERSON</data>
      <data key="d1">Kevin Scott is the CTO of Microsoft and a participant in the podcast conversations included in the podcast transcripts dataset.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="TANG AND YANG">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study by Tang and Yang (2024) on the MultiHop-RAG benchmark dataset.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="TECH LEADERS">
      <data key="d0">PERSON</data>
      <data key="d1">Tech leaders are participants in the podcast conversations included in the podcast transcripts dataset, discussing various technology-related topics.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="PRIVACY LAWS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Privacy laws are regulations discussed by guests in the podcast transcripts dataset, focusing on their impact on technology development.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="INNOVATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Innovation is a topic discussed by guests in the podcast transcripts dataset, particularly in relation to ethical considerations.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="ETHICAL CONSIDERATIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Ethical considerations are discussed by guests in the podcast transcripts dataset, especially in the context of balancing innovation and ethics.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="POLICY AND REGULATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Policy and regulation are topics discussed in the podcast transcripts dataset, focusing on their role in the tech industry.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="COLLABORATIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Collaborations between tech companies and governments are discussed by guests in the podcast transcripts dataset.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="HEALTH EDUCATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Health education is a topic of interest for educators using news articles to teach about health and wellness.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="PREVENTIVE MEDICINE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Preventive medicine is a concept addressed in news articles, relevant to health education curricula.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="WELLNESS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Wellness is a concept addressed in news articles, relevant to health education curricula.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="PUBLIC HEALTH">
      <data key="d0">CONCEPT</data>
      <data key="d1">Public health priorities are insights gleaned from news coverage, relevant to health education.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="HEALTH LITERACY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Health literacy is an important aspect highlighted by educators using news articles to teach about health and wellness.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="DATASET">
      <data key="d0">DATA/RESOURCE</data>
      <data key="d1">The term "DATASET" refers to a collection of data used for analysis and evaluation in the study. Specifically, the dataset is utilized for evaluating Graph RAG, which implies its role in assessing the performance and effectiveness of this particular graph-based retrieval-augmented generation method.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="TEXT SUMMARIZATION (TS)">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">A method applying map-reduce approach directly to source texts for summarization</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="SEMANTIC SEARCH (SS)">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">A naive RAG approach where text chunks are retrieved and added to the context window until the token limit is reached</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="CONTEXT WINDOW">
      <data key="d0">CONCEPT/TECHNIQUE</data>
      <data key="d1">The size of the text window used for generating answers</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="PODCAST DATASET">
      <data key="d0">DATA/RESOURCE</data>
      <data key="d1">The "PODCAST DATASET" is a dataset utilized in studies related to summarization techniques. It is indexed with a context window size of 600 tokens and 1 gleaning. The dataset comprises 8564 nodes and 20691 edges, making it a substantial resource for evaluating various summarization methods.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="NEWS DATASET">
      <data key="d0">DATA/RESOURCE</data>
      <data key="d1">The NEWS DATASET is a dataset used in the study, indexed with a context window size of 600 tokens and 0 gleanings. It consists of 15,754 nodes and 19,520 edges, and is utilized for evaluating summarization techniques.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="DIRECTNESS">
      <data key="d0">METRIC</data>
      <data key="d1">DIRECTNESS is a control metric used to evaluate how specifically and clearly an answer addresses the question. It measures the straightforwardness of answers in summarization tasks, ensuring that responses are direct and relevant to the query posed.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="LLM EVALUATOR">
      <data key="d0">MODEL/TOOL</data>
      <data key="d1">A Large Language Model used to assess the quality of answers based on specific metrics</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="PUBLIC FIGURES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Individuals repeatedly mentioned in various entertainment articles due to their significant contributions and influence</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="ACTIVITY-CENTERED APPROACH">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">An approach used to automate the generation of questions based on a short description of a dataset</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="QUESTION GENERATION">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of generating questions that require understanding of the entire corpus</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="C0">
      <data key="d0">CONCEPT</data>
      <data key="d1">C0 is a condition representing root-level community summaries in the Graph RAG method. These root-level community summaries are utilized in Graph RAG to answer user queries, providing a foundational overview of the community structure and relationships within the AI and ML landscape.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="C1">
      <data key="d0">CONCEPT</data>
      <data key="d1">C1 is a condition representing intermediate-level community summaries in the Graph RAG method. These summaries are utilized to answer user queries, providing a structured and detailed overview of the community's organizational structures and relationships within the AI and ML landscape.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="C2">
      <data key="d0">CONCEPT</data>
      <data key="d1">C2 is a condition representing intermediate-level community summaries in the Graph RAG method. These intermediate-level community summaries are utilized in Graph RAG to answer user queries, providing a structured and insightful overview of the community's organizational structures and relationships.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="C3">
      <data key="d0">CONCEPT</data>
      <data key="d1">C3 is a condition representing low-level community summaries in the Graph RAG method. These low-level community summaries are utilized in Graph RAG to answer user queries.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="RELATIONSHIP EXTRACTION">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of extracting relationships between entities from text for graph indexing</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="TOKEN">
      <data key="d0">CONCEPT</data>
      <data key="d1">TOKEN refers to a unit of text used in the context window for generating answers. Tokens are the basic elements of natural language, often words, used in language models.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="QUESTION">
      <data key="d0">CONCEPT</data>
      <data key="d1">A "QUESTION" is a problem or query presented to the student for which they need to provide an answer. It serves multiple purposes, such as being a query provided to the LLM evaluator to assess the quality of answers, or a specific query that needs to be answered, like "Which magazine was started first, Arthur&#8217;s Magazine or First for Women?" The question is used to evaluate the student's response, making it a critical component in both educational and evaluative contexts.</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50,26b2dad01a219bc034ac7d6a32d07582,357f3442ba581c9d2bdf84d90509056f,5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="ANSWER">
      <data key="d0">CONCEPT</data>
      <data key="d1">ANSWER is a response generated by the LLM to address a given question. It represents the output generated from the final code solution and is produced by the cot_module. The answer is the final solution to the original problem after integrating sub-solutions.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="ASSESSMENT">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of evaluating the quality of answers based on specific metrics</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="STOCHASTICITY">
      <data key="d0">CONCEPT</data>
      <data key="d1">The randomness in the behavior of LLMs during evaluation</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="MEAN SCORES">
      <data key="d0">CONCEPT</data>
      <data key="d1">The average scores used to account for the stochasticity of LLMs in evaluations</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="ENTERTAINMENT INDUSTRY">
      <data key="d0">CONCEPT</data>
      <data key="d1">A sector encompassing film, television, music, sports, and digital media</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="ACTORS AND DIRECTORS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Public figures in the entertainment industry known for their roles in film and television</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="MUSICIANS AND EXECUTIVES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Public figures in the entertainment industry known for their contributions to music and management</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="ATHLETES AND COACHES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Public figures in the entertainment industry known for their roles in sports</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="INFLUENCERS AND ENTREPRENEURS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Public figures in the entertainment industry known for their influence and business ventures</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="GRAPH INDEXING">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="DIRECTORS">
      <data key="d0">PROFESSION</data>
      <data key="d1">Individuals involved in directing films, television shows, or other media productions</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="PUBLIC FIGURES IN CONTROVERSY">
      <data key="d0">CATEGORY</data>
      <data key="d1">Individuals who are frequently mentioned in the media due to their involvement in controversies</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="MUSICIANS">
      <data key="d0">PROFESSION</data>
      <data key="d1">Individuals who create, perform, or produce music</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="EXECUTIVES">
      <data key="d0">PROFESSION</data>
      <data key="d1">Individuals in high-level management positions within organizations, particularly in the entertainment industry</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="ATHLETES">
      <data key="d0">PROFESSION</data>
      <data key="d1">Individuals who compete in sports at a professional level</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="COACHES">
      <data key="d0">PROFESSION</data>
      <data key="d1">Individuals who train and guide athletes or sports teams</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="INFLUENCERS">
      <data key="d0">PROFESSION</data>
      <data key="d1">Individuals who have the power to affect the purchasing decisions of others because of their authority, knowledge, position, or relationship with their audience</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="ENTREPRENEURS">
      <data key="d0">PROFESSION</data>
      <data key="d1">Individuals who start and run businesses, often within the entertainment and digital media sectors</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="TAYLOR SWIFT">
      <data key="d0">PERSON</data>
      <data key="d1">A musician and public figure frequently mentioned in entertainment articles for her professional achievements and personal life</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="TRAVIS KELCE">
      <data key="d0">PERSON</data>
      <data key="d1">An athlete and public figure frequently mentioned in entertainment articles for his professional achievements and personal life</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="BRITNEY SPEARS">
      <data key="d0">PERSON</data>
      <data key="d1">A musician and public figure frequently mentioned in entertainment articles for her professional achievements and personal life</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="JUSTIN TIMBERLAKE">
      <data key="d0">PERSON</data>
      <data key="d1">A musician and public figure frequently mentioned in entertainment articles for his professional achievements and personal life</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="NA&#207;VE RAG">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">NA&#207;VE RAG is a basic approach to retrieval-augmented generation that converts documents to text, splits them into chunks, and embeds these chunks into a vector space for context retrieval. It is also a method used to generate lists of public figures, focusing on a smaller number of individuals and their personal lives.</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="ENTERTAINMENT ARTICLES">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Articles that cover various aspects of the entertainment industry, including news about public figures, trends, and cultural narratives</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="CULTURAL NARRATIVES">
      <data key="d0">CATEGORY</data>
      <data key="d1">Stories and themes that shape and reflect the values, beliefs, and experiences of a culture, often influenced by public figures in entertainment</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="FILM">
      <data key="d0">CATEGORY</data>
      <data key="d1">A sector of the entertainment industry focused on the production and distribution of movies</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="TELEVISION">
      <data key="d0">CATEGORY</data>
      <data key="d1">A sector of the entertainment industry focused on the production and distribution of TV shows</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="MUSIC">
      <data key="d0">CATEGORY</data>
      <data key="d1">A sector of the entertainment industry focused on the creation, performance, and distribution of music</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="DIGITAL MEDIA">
      <data key="d0">CATEGORY</data>
      <data key="d1">A sector of the entertainment industry that includes online content, social media, and other digital platforms</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="SPORTS">
      <data key="d0">CATEGORY</data>
      <data key="d1">A sector of the entertainment industry focused on athletic competitions and events</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="GAMING">
      <data key="d0">CATEGORY</data>
      <data key="d1">A sector of the entertainment industry focused on video games and interactive entertainment</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="SOCIAL DISCUSSIONS">
      <data key="d0">CATEGORY</data>
      <data key="d1">Public conversations and debates often influenced by the actions and statements of public figures</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="PUBLIC DISCOURSE">
      <data key="d0">CATEGORY</data>
      <data key="d1">The exchange of ideas and opinions in the public sphere, often shaped by media coverage and public figures</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="MEDIA COVERAGE">
      <data key="d0">CATEGORY</data>
      <data key="d1">The reporting and analysis of events, trends, and public figures by various media outlets</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="PROFESSIONAL ACHIEVEMENTS">
      <data key="d0">CATEGORY</data>
      <data key="d1">Notable accomplishments in one's career, often highlighted in media coverage of public figures</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="PERSONAL LIVES">
      <data key="d0">CATEGORY</data>
      <data key="d1">Aspects of public figures' private lives that are often covered by the media and of interest to the public</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="CULTURAL IMPACT">
      <data key="d0">CATEGORY</data>
      <data key="d1">The influence that public figures and their activities have on culture and society</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="ECONOMIC IMPACT">
      <data key="d0">CATEGORY</data>
      <data key="d1">The financial effects that public figures and their activities have on the economy</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="MEDIA REACTIONS">
      <data key="d0">CATEGORY</data>
      <data key="d1">The responses and opinions expressed by media outlets regarding public figures and their activities</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="CONTEXT WINDOW SIZE">
      <data key="d0">PARAMETER</data>
      <data key="d1">Context window size refers to the number of tokens used in the context for language model tasks, tested at 8k, 16k, 32k, and 64k sizes</data>
      <data key="d2">ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="KURATOV ET AL., 2024">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study by Kuratov et al. in 2024, discussing the potential for information to be lost in the middle of longer contexts</data>
      <data key="d2">ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="LIU ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">"LIU ET AL., 2023" is a paper that discusses DyLAN and its application in using Foundation Models (FMs) to score the response quality of nodes in each layer to prune the connections. Additionally, the study by Liu et al. in 2023 addresses the potential for information to be lost in the middle of longer contexts, highlighting the challenges and considerations in maintaining information integrity across extended sequences.</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="TS">
      <data key="d0">CONDITION</data>
      <data key="d1">TS is a condition representing global text summarization without a graph index in the Graph RAG method</data>
      <data key="d2">ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="GPT-4">
      <data key="d0">MODEL</data>
      <data key="d1">GPT-4 is a powerful and advanced language model developed by OpenAI, as described in the technical report by Ryan Greenblatt and others. It is utilized in a wide range of applications and evaluations within the AI and ML community. Specifically, GPT-4 is employed by the meta agent in Meta Agent Search and is used to generate high-quality data for AgentInstruct. It serves as a benchmark for evaluating the performance of other models, such as Orca-3, and is used in the evaluation of datasets like Orca-Bench, MIRAGE, and InfoBench.

GPT-4 is instrumental in various exact match/span extraction problems, summarization, and hallucination detection tasks. It is also used to evaluate the performance of different prompting methods, including LATS and Reflexion, achieving state-of-the-art pass@1 accuracy for programming tasks on HumanEval. Additionally, GPT-4 is used to assess the transferability of agents discovered from GPT-3.5 and to set the state of the art for HumanEval when combined with LATS, achieving a 92.7 Pass@1 rate.

Furthermore, GPT-4 is employed to extract the option selected by the model in multiple-choice questions and serves as a baseline for scoring the performance of other models on the Orca-Bench dataset. Overall, GPT-4 plays a crucial role in advancing the capabilities and evaluation standards within the AI and ML landscape.</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a,0cf2e43f324fa4175b9b00b90e5e90ba,103d98395c393552cc954c89d4e59f50,1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,2d4672dfb7bd4283f0b5f23ab4f26653,5819b66e04fd77fa705574edc49395bb,6109537356a2ce2339f77c827aa3668e,6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,8ee9617c145e19fa95f1f9349bfbe69b,93cb0d0456e0822b5fe30a3e627405f8,99d90aededb61e04241516ed9ec656cc,ab04427ae0415a1c812a35cf8d3ee1a2,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bc26e68b0b2783ba912b9e5606d9eb0b,bd4eb9459bc29b4c2da4658914fd4635,ede7063998065122cf7a7152979c1909,f8e7ed806916bf15245bcb4d52570c26,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="HALLUCINATION JUDGE">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">HALLUCINATION JUDGE is a process where a judge determines if there is any hallucination in a generated summary. This process involves using a specific prompt template to ensure accuracy and consistency in identifying hallucinations within the text.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="KOESTEN, L.">
      <data key="d0">PERSON</data>
      <data key="d1">Koesten, L. is an author of the paper "Talking datasets&#8211;understanding data sensemaking behaviours."</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb,df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="GREGORY, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Gregory, K. is an author of the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="GROTH, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Groth, P. is an author of the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="SIMPERL, E.">
      <data key="d0">PERSON</data>
      <data key="d1">Simperl, E. is an author of the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="INTERNATIONAL JOURNAL OF HUMAN-COMPUTER STUDIES">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Talking datasets&#8211;understanding data sensemaking behaviours" was published</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="KURATOV, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Kuratov, Y. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="BULATOV, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Bulatov, A. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="ANOKHIN, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Anokhin, P. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="SOROKIN, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Sorokin, D. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="SOROKIN, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Sorokin, A. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="BURTSEV, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Burtsev, M. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="LANGCHAIN">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">LangChain is an open-source agent framework developed by LangChainAI, designed to support various graph databases and graph-based RAG (Retrieval-Augmented Generation) applications. It provides functions for ADAS (Advanced Driver Assistance Systems) and can be used to build upon existing building blocks like RAG and search engine tools. LangChain is also the organization behind the LangChain graphs project, which focuses on building context-aware reasoning applications.</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,6109537356a2ce2339f77c827aa3668e,6bdf681c0bd9e401ac72344a6a0ae479,df50c95dff7da074cbb2f68e88686f88,edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="LASKAR, M. T. R.">
      <data key="d0">PERSON</data>
      <data key="d1">Laskar, M. T. R. is an author of the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="HOQUE, E.">
      <data key="d0">PERSON</data>
      <data key="d1">Hoque, E. is an author of the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="HUANG, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Huang, J. is an author of the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="ADVANCES IN ARTIFICIAL INTELLIGENCE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The conference where the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models" was presented</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="GENERATION-AUGMENTED RETRIEVAL (GAR)">
      <data key="d0">METHOD</data>
      <data key="d1">GAR is a method that combines retrieval and generation processes to enhance the performance of information retrieval systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="MODULAR RAG">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">Modular RAG includes patterns for iterative and dynamic cycles of interleaved retrieval and generation to improve upon Na&#239;ve RAG</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="SELF-MEMORY (SELFMEM)">
      <data key="d0">CONCEPT</data>
      <data key="d1">Self-memory is a concept used in generation-augmented retrieval to store and utilize past generated content for future retrieval and generation cycles</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="ITERATIVE RETRIEVAL-GENERATION (ITER-RETGEN)">
      <data key="d0">METHOD</data>
      <data key="d1">Iter-RetGen is a strategy for iterative retrieval and generation, used in advanced RAG systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="FEDERATED RETRIEVAL-GENERATION (FEB4RAG)">
      <data key="d0">METHOD</data>
      <data key="d1">FeB4RAG is a federated strategy for retrieval and generation, used in advanced RAG systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="MULTI-DOCUMENT SUMMARIZATION">
      <data key="d0">TASK</data>
      <data key="d1">Multi-document summarization is the process of creating a summary from multiple documents, often using advanced RAG systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="MULTI-HOP QUESTION ANSWERING">
      <data key="d0">TASK</data>
      <data key="d1">Multi-hop question answering involves answering questions that require reasoning over multiple pieces of information, often using advanced RAG systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="HIERARCHICAL INDEX">
      <data key="d0">TOOL/STRUCTURE</data>
      <data key="d1">A hierarchical index is a structure used to organize text chunks by clustering their vector embeddings, facilitating efficient retrieval and summarization</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="LLAMAINDEX">
      <data key="d0">SOFTWARE/LIBRARY</data>
      <data key="d1">LlamaIndex is an organization known for its development of the LlamaIndex Knowledge Graph Index project. This project includes a library that supports various graph databases and graph-based Retrieval-Augmented Generation (RAG) applications, making it a significant player in the field of knowledge graph indexing and graph database support.</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="NEO4J">
      <data key="d0">SOFTWARE/TOOL</data>
      <data key="d1">Neo4J is a graph database format used for creating and reasoning over knowledge graphs. It is supported by LangChain and LlamaIndex, which facilitate its application in knowledge graph creation and reasoning. Additionally, Neo4J is the organization behind "Project NaLLM," further showcasing its commitment to advancing technologies in the realm of knowledge graphs.</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8,df50c95dff7da074cbb2f68e88686f88,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="NEBULAGRAPH">
      <data key="d0">SOFTWARE/TOOL</data>
      <data key="d1">NebulaGraph is a graph database supported by LangChain and LlamaIndex for creating and reasoning over knowledge graphs. The organization behind NebulaGraph is also responsible for the project "Nebulagraph launches industry-first graph RAG: Retrieval-augmented generation with LLM based on knowledge graphs," showcasing their innovative approach in leveraging large language models (LLMs) for advanced data retrieval and generation tasks.</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="SENSEMAKING">
      <data key="d0">ACTIVITY</data>
      <data key="d1">Sensemaking is the activity of understanding and making sense of information, often facilitated by iterative question answering and summarization methods like Graph RAG</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="SCALABILITY">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Scalability refers to the ability of a system to handle increasing amounts of work or data efficiently, as demonstrated by Graph RAG's reduced context token requirements</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="TUNING ELEMENT EXTRACTION PROMPTS">
      <data key="d0">METHOD</data>
      <data key="d1">Tuning element extraction prompts is a method to improve the retention of specific details in the Graph RAG index</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="RAPTOR">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">RAPTOR is a method that generates a hierarchical index of text chunks by clustering their vector embeddings</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="TREE OF CLARIFICATIONS">
      <data key="d0">TOOL/STRUCTURE</data>
      <data key="d1">A tree of clarifications is a structure used to answer multiple interpretations of ambiguous questions</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="KAPING">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">KAPING is an advanced RAG method where the index is a knowledge graph</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="G-RETRIEVER">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">G-Retriever is a method where subsets of the graph structure are the objects of enquiry</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="GRAPH-TOOLFORMER">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">Graph-ToolFormer is a method where derived graph metrics are the objects of enquiry</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="SURGE">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">SURGE is a method where narrative outputs are strongly grounded in the facts of retrieved subgraphs</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="FABULA">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">FABULA is a method where retrieved event-plot subgraphs are serialized using narrative templates</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="NALLM">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">NaLLM is a system that can create and reason over knowledge graphs in Neo4J format</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="GRAPHRAG">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">GraphRAG is a system designed to create and reason over knowledge graphs in the NebulaGraph format.</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="CAIRE-COVID">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">CAiRE-COVID is a system that combines various concepts for multi-document summarization</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="ITRG">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">ITRG is a system for multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="IR-COT">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">IR-CoT is a system for multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="DSP">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">DSP is a system for multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="TRAJANOSKA ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on using LLMs for knowledge graph creation and completion</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="YAO ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on using LLMs for knowledge graph completion</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="BAN ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on using LLMs for the extraction of causal graphs from source texts</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="ZHANG ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on using LLMs for the extraction of causal graphs from source texts</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="BAEK ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on advanced RAG methods where the index is a knowledge graph</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="HE ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on advanced RAG methods where subsets of the graph structure are the objects of enquiry</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="ZHANG, 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on advanced RAG methods where derived graph metrics are the objects of enquiry</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="KANG ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on advanced RAG methods where narrative outputs are strongly grounded in the facts of retrieved subgraphs</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="RANADE AND JOSHI, 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on advanced RAG methods where retrieved event-plot subgraphs are serialized using narrative templates</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="WANG ET AL., 2023B">
      <data key="d0">PUBLICATION</data>
      <data key="d1">"Wang et al., 2023b is a publication that introduces and discusses the Self-Consistency with Chain-of-Thought (COT-SC) technique. This study is significant for its exploration of advanced Retrieval-Augmented Generation (RAG) methods, which support both the creation and traversal of text-relationship graphs for multi-hop question answering. The publication is a key reference for understanding the COT-SC method."</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="NEO4J, 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on systems that can create and reason over knowledge graphs in Neo4J format</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="NEBULAGRAPH, 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on systems that can create and reason over knowledge graphs in NebulaGraph format</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="RAM ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on RAG approaches and systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="GAO ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">"GAO ET AL., 2023" is a publication that discusses the GSM-Hard dataset and conducts a study on Na&#239;ve RAG and advanced RAG systems.</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="CHENG ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on self-memory for generation-augmented retrieval</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="MAO ET AL., 2020">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on generation-augmented retrieval</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="SHAO ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on iterative retrieval-generation strategies</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="WANG ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">"WANG ET AL., 2024" is a publication that discusses the effectiveness of Meta Agent Search in various domains and also explores federated retrieval-generation strategies.</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="SU ET AL., 2020">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on multi-document summarization</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="FENG ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="TRIVEDI ET AL., 2022">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="KHATTAB ET AL., 2022">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="SARTHI ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on generating a hierarchical index of text chunks by clustering their vector embeddings</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="KIM ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on generating a tree of clarifications to answer multiple interpretations of ambiguous questions</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="PODCAST INTERMEDIATE-LEVEL SUMMARIES">
      <data key="d0">OUTPUT/RESULT</data>
      <data key="d1">Podcast intermediate-level summaries are summaries generated for podcasts at an intermediate level of detail</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="NEWS LOW-LEVEL COMMUNITY SUMMARIES">
      <data key="d0">OUTPUT/RESULT</data>
      <data key="d1">News low-level community summaries are summaries generated for news articles at a low level of detail</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="TABLE 3">
      <data key="d0">DATA/RESULT</data>
      <data key="d1">Table 3 illustrates the scalability advantages of Graph RAG compared to source text summarization</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="ROOT-LEVEL COMMUNITY SUMMARIES">
      <data key="d0">OUTPUT/RESULT</data>
      <data key="d1">Root-level community summaries are summaries generated at the root level of detail</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="ITERATIVE QUESTION ANSWERING">
      <data key="d0">TASK</data>
      <data key="d1">Iterative question answering is a process characterized by repeated cycles of asking and answering questions to facilitate sensemaking</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="AD-HOC LLM USE">
      <data key="d0">METHOD</data>
      <data key="d1">Ad-hoc LLM use refers to the spontaneous use of large language models to analyze reasoning and provide specific examples, quotes, and citations</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="RAG APPROACHES AND SYSTEMS">
      <data key="d0">CONCEPT</data>
      <data key="d1">RAG approaches and systems involve retrieving relevant information from external data sources and adding it to the context window of an LLM along with the original query</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="PRE-RETRIEVAL STRATEGIES">
      <data key="d0">METHOD</data>
      <data key="d1">Pre-retrieval strategies are techniques used before the retrieval process to enhance the performance of RAG systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="POST-RETRIEVAL STRATEGIES">
      <data key="d0">METHOD</data>
      <data key="d1">Post-retrieval strategies are techniques used after the retrieval process to enhance the performance of RAG systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="COMMUNITY ANSWERS">
      <data key="d0">OUTPUT/RESULT</data>
      <data key="d1">Community answers are generated from community summaries in a parallel generation process</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="MULTI-HOP QUESTION ANSWERING (ITRG)">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">ITRG is a system for multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="MULTI-HOP QUESTION ANSWERING (IR-COT)">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">IR-CoT is a system for multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="MULTI-HOP QUESTION ANSWERING (DSP)">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">DSP is a system for multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="HIERARCHICAL INDEX OF TEXT CHUNKS">
      <data key="d0">TOOL/STRUCTURE</data>
      <data key="d1">A hierarchical index of text chunks is a structure used to organize text chunks by clustering their vector embeddings</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="TEXT-RELATIONSHIP GRAPHS">
      <data key="d0">TOOL/STRUCTURE</data>
      <data key="d1">Text-relationship graphs are structures that support both creation and traversal for multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="GRAPH DATABASES">
      <data key="d0">TOOL/STRUCTURE</data>
      <data key="d1">Graph databases are databases that use graph structures for semantic queries, supported by LangChain and LlamaIndex</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="GRAPH-BASED RAG APPLICATIONS">
      <data key="d0">TOOL/STRUCTURE</data>
      <data key="d1">Graph-based RAG applications are systems that create and reason over knowledge graphs in various formats. These applications utilize graph structures for retrieval-augmented generation, enabling them to efficiently manage and leverage complex relationships within data. By integrating knowledge graphs, these systems enhance the retrieval and generation processes, making them powerful tools for various AI and ML tasks.</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="SENSEMAKING ACTIVITY">
      <data key="d0">ACTIVITY</data>
      <data key="d1">Sensemaking activity involves understanding and making sense of information, often facilitated by iterative question answering and summarization methods like Graph RAG</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="QUOTES">
      <data key="d0">DATA/RESULT</data>
      <data key="d1">Quotes are specific excerpts from texts used to support reasoning and understanding in LLM analyses</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="CITATIONS">
      <data key="d0">DATA/RESULT</data>
      <data key="d1">Citations are references to sources used to support reasoning and understanding in LLM analyses</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="EXAMPLES">
      <data key="d0">DATA/RESULT</data>
      <data key="d1">Examples are specific instances used to illustrate points and support reasoning in LLM analyses</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="INDEX">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Index is a publication mentioned in the context of libraries and graph-based RAG applications</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="NEBULA-GRAPH">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Nebula-Graph is another format used for creating and reasoning over knowledge graphs</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="SELF-CHECKGPT">
      <data key="d0">TOOL</data>
      <data key="d1">SelfCheckGPT is a tool used for comparing fabrication rates in analysis</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="GRAPH INDEX">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Graph index is a method used in Graph RAG for building a data index that supports global summarization</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="QFS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">QFS stands for Query-Focused Summarization, a method used to support human sensemaking over text corpora</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="ALONSO GUEVARA FERN&#193;NDEZ">
      <data key="d0">PERSON</data>
      <data key="d1">Alonso Guevara Fern&#225;ndez is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="AMBER HOAK">
      <data key="d0">PERSON</data>
      <data key="d1">Amber Hoak is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="ANDR&#201;S MORALES ESQUIVEL">
      <data key="d0">PERSON</data>
      <data key="d1">Andr&#233;s Morales Esquivel is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="BEN CUTLER">
      <data key="d0">PERSON</data>
      <data key="d1">Ben Cutler is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="BILLIE RINALDI">
      <data key="d0">PERSON</data>
      <data key="d1">Billie Rinaldi is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="CHRIS SANCHEZ">
      <data key="d0">PERSON</data>
      <data key="d1">Chris Sanchez is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="CHRIS TREVINO">
      <data key="d0">PERSON</data>
      <data key="d1">Chris Trevino is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="CHRISTINE CAGGIANO">
      <data key="d0">PERSON</data>
      <data key="d1">Christine Caggiano is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="DAVID TITTSWORTH">
      <data key="d0">PERSON</data>
      <data key="d1">David Tittsworth is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="DAYENNE DE SOUZA">
      <data key="d0">PERSON</data>
      <data key="d1">Dayenne de Souza is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="DOUGLAS ORBAKER">
      <data key="d0">PERSON</data>
      <data key="d1">Douglas Orbaker is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="ED CLARK">
      <data key="d0">PERSON</data>
      <data key="d1">Ed Clark is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="GABRIEL NIEVES-PONCE">
      <data key="d0">PERSON</data>
      <data key="d1">Gabriel Nieves-Ponce is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="GAUDY BLANCO MENESES">
      <data key="d0">PERSON</data>
      <data key="d1">Gaudy Blanco Meneses is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="KATE LYTVYNETS">
      <data key="d0">PERSON</data>
      <data key="d1">Kate Lytvynets is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="KATY SMITH">
      <data key="d0">PERSON</data>
      <data key="d1">Katy Smith is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="M&#211;NICA CARVAJAL">
      <data key="d0">PERSON</data>
      <data key="d1">M&#243;nica Carvajal is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="NATHAN EVANS">
      <data key="d0">PERSON</data>
      <data key="d1">Nathan Evans is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="RICHARD ORTEGA">
      <data key="d0">PERSON</data>
      <data key="d1">Richard Ortega is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="RODRIGO RACANICCI">
      <data key="d0">PERSON</data>
      <data key="d1">Rodrigo Racanicci is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="SARAH SMITH">
      <data key="d0">PERSON</data>
      <data key="d1">Sarah Smith is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="SHANE SOLOMON">
      <data key="d0">PERSON</data>
      <data key="d1">Shane Solomon is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="GPT-4 TECHNICAL REPORT">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The GPT-4 Technical Report is a document published by OpenAI, detailing the capabilities and features of GPT-4. While there is a discrepancy in the publication year, with some sources citing 2023 and others 2024, the report remains a comprehensive resource on the advancements and functionalities of the GPT-4 model.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,3d1f6634f93f8a4c296dc8df7e59859e,ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="KNOWLEDGE-AUGMENTED LANGUAGE MODEL PROMPTING">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">A method for zero-shot knowledge graph question answering</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="QUERY TOOLS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Tools used for querying data and discovering causal relationships</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="QUERY FOCUSED ABSTRACTIVE SUMMARIZATION">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">A method that incorporates query relevance, multi-document coverage, and summary length constraints into seq2seq models</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="FAST UNFOLDING OF COMMUNITIES">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">A method for detecting communities in large networks</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="SENSEMAKING QUESTIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Sensemaking questions are a class of questions used to evaluate the performance of Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="CORPORA">
      <data key="d0">DATA</data>
      <data key="d1">Corpora refer to collections of text data used in the evaluation of Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="USER QUERIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">User queries are questions or requests made by users to retrieve information from the graph index</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="COMMUNITY HIERARCHY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Community hierarchy refers to the hierarchical structure of communities in the graph index</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="MAP-REDUCE SUMMARIZATION">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Map-Reduce summarization is a method used for global summarization of source texts</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="PYTHON">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Python is a Turing Complete programming language widely utilized in various domains, including ADAS (Advanced Driver Assistance Systems) research. It is also employed in datasets like MBPP (Mostly Basic Python Problems) for writing short functions. Additionally, Python serves as the programming language for implementing Graph RAG (Recurrent Attention Graph) approaches, showcasing its versatility and importance in the field of Artificial Intelligence and Machine Learning.</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba,ac21ebe9a9d70d691c717f961d3f10c8,fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="ARXIV">
      <data key="d0">PUBLICATION</data>
      <data key="d1">arXiv is a prominent repository and platform where a wide array of influential papers in the fields of Artificial Intelligence and Machine Learning are published. Notable works hosted on arXiv include "A survey on the memory mechanism of large language model based agents," "Metagpt: Meta programming for multi-agent collaborative framework," "Discovering preference optimization algorithms with and for large language models," "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization," and "Summary length constraints into seq2seq models." Additionally, arXiv features papers such as "Fabula: Intelligence report generation using retrieval-augmented narrative construction," "Lost in the middle: How language models use long contexts," "Raptor: Recursive abstractive processing for tree-organized retrieval," and the GPT-4 technical report.

Other significant publications on arXiv include "Automated Design of Agentic Systems," "Evaluating large language models trained on code" (published in 2021), "Learning to reinforcement learn," "Tool learning with large language models: A survey," "Voyager: An open-ended embodied agent with large language models," "The prompt report: A systematic survey of prompting techniques," "Mastering diverse domains through world models," and "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm." This extensive collection underscores arXiv's critical role in disseminating cutting-edge research and fostering advancements in AI and ML.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e,68e5573b596d253a03047b1e41988598,7a48515e86161237c03c9a8373197126,aa79049289e6532592eec17b9e76adfb,ac21ebe9a9d70d691c717f961d3f10c8,c3d0436082aada237ee4bee645f16059,cc802d9b841fde55e9c0c2ba0ef7869d,df50c95dff7da074cbb2f68e88686f88</data>
    </node>
    <node id="J. ACHIAM">
      <data key="d0">PERSON</data>
      <data key="d1">J. Achiam is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="S. ADLER">
      <data key="d0">PERSON</data>
      <data key="d1">S. Adler is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="S. AGARWAL">
      <data key="d0">PERSON</data>
      <data key="d1">S. Agarwal is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="L. AHMAD">
      <data key="d0">PERSON</data>
      <data key="d1">L. Ahmad is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="I. AKKAYA">
      <data key="d0">PERSON</data>
      <data key="d1">I. Akkaya is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="F. L. ALEMAN">
      <data key="d0">PERSON</data>
      <data key="d1">F. L. Aleman is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="D. ALMEIDA">
      <data key="d0">PERSON</data>
      <data key="d1">D. Almeida is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="J. ALTENSCHMIDT">
      <data key="d0">PERSON</data>
      <data key="d1">J. Altenschmidt is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="S. ALTMAN">
      <data key="d0">PERSON</data>
      <data key="d1">S. Altman is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="S. ANADKAT">
      <data key="d0">PERSON</data>
      <data key="d1">S. Anadkat is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="R. ANIL">
      <data key="d0">PERSON</data>
      <data key="d1">R. Anil is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="S. BORGEAUD">
      <data key="d0">PERSON</data>
      <data key="d1">S. Borgeaud is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="Y. WU">
      <data key="d0">PERSON</data>
      <data key="d1">Y. Wu is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="J.-B. ALAYRAC">
      <data key="d0">PERSON</data>
      <data key="d1">J.-B. Alayrac is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="J. YU">
      <data key="d0">PERSON</data>
      <data key="d1">J. Yu is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="R. SORICUT">
      <data key="d0">PERSON</data>
      <data key="d1">R. Soricut is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="J. SCHALKWYK">
      <data key="d0">PERSON</data>
      <data key="d1">J. Schalkwyk is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="A. M. DAI">
      <data key="d0">PERSON</data>
      <data key="d1">A. M. Dai is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="A. HAUTH">
      <data key="d0">PERSON</data>
      <data key="d1">A. Hauth is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="J. BAEK">
      <data key="d0">PERSON</data>
      <data key="d1">J. Baek is an author of the paper on knowledge-augmented language model prompting</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="A. F. AJI">
      <data key="d0">PERSON</data>
      <data key="d1">A. F. Aji is an author of the paper on knowledge-augmented language model prompting</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="A. SAFFARI">
      <data key="d0">PERSON</data>
      <data key="d1">A. Saffari is an author of the paper on knowledge-augmented language model prompting</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="T. BAN">
      <data key="d0">PERSON</data>
      <data key="d1">T. Ban is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="L. CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">L. Chen is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="X. WANG">
      <data key="d0">PERSON</data>
      <data key="d1">X. Wang is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="H. CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">H. Chen is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="T. BAUMEL">
      <data key="d0">PERSON</data>
      <data key="d1">T. Baumel is an author of the paper on query focused abstractive summarization</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="M. EYAL">
      <data key="d0">PERSON</data>
      <data key="d1">M. Eyal is an author of the paper on query focused abstractive summarization</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="M. ELHADAD">
      <data key="d0">PERSON</data>
      <data key="d1">M. Elhadad is an author of the paper on query focused abstractive summarization</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="V. D. BLONDEL">
      <data key="d0">PERSON</data>
      <data key="d1">V. D. Blondel is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="J.-L. GUILLAUME">
      <data key="d0">PERSON</data>
      <data key="d1">J.-L. Guillaume is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="R. LAMBIOTTE">
      <data key="d0">PERSON</data>
      <data key="d1">R. Lambiotte is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="E. LEFEBVRE">
      <data key="d0">PERSON</data>
      <data key="d1">E. Lefebvre is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="BLONDEL, V. D.">
      <data key="d0">PERSON</data>
      <data key="d1">Blondel, V. D. is an author of the paper "Fast unfolding of communities in large networks"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
    </node>
    <node id="GUILLAUME, J.-L.">
      <data key="d0">PERSON</data>
      <data key="d1">Guillaume, J.-L. is an author of the paper "Fast unfolding of communities in large networks"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
    </node>
    <node id="LAMBIOTTE, R.">
      <data key="d0">PERSON</data>
      <data key="d1">Lambiotte, R. is an author of the paper "Fast unfolding of communities in large networks"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
    </node>
    <node id="LEFEBVRE, E.">
      <data key="d0">PERSON</data>
      <data key="d1">Lefebvre, E. is an author of the paper "Fast unfolding of communities in large networks"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
    </node>
    <node id="JOURNAL OF STATISTICAL MECHANICS: THEORY AND EXPERIMENT">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Fast unfolding of communities in large networks" was published</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
    </node>
    <node id="BROWN, T.">
      <data key="d0">PERSON</data>
      <data key="d1">Brown, T. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MANN, B.">
      <data key="d0">PERSON</data>
      <data key="d1">Mann, B. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RYDER, N.">
      <data key="d0">PERSON</data>
      <data key="d1">Ryder, N. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SUBBIAH, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Subbiah, M. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KAPLAN, J. D.">
      <data key="d0">PERSON</data>
      <data key="d1">Kaplan, J. D. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DHARIWAL, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Dhariwal, P. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NEELAKANTAN, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Neelakantan, A. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHYAM, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Shyam, P. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SASTRY, G.">
      <data key="d0">PERSON</data>
      <data key="d1">Sastry, G. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ASKELL, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Askell, A. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Advances in Neural Information Processing Systems (NeurIPS) is a prominent conference and journal in the field of artificial intelligence and machine learning. It is renowned for being the venue where groundbreaking research papers are presented and published. Notable papers presented at NeurIPS include "Imagenet classification with deep convolutional neural networks," "Retrieval-augmented generation for knowledge-intensive NLP tasks," "Thought Cloning: Learning to think while acting by imitating human thinking," "Judging llm-as-a-judge with mt-bench and chatbot arena," "Language models are few-shot learners," "Reflexion: Language agents with verbal reinforcement learning," "Direct preference optimization: Your language model is secretly a reward model," and "Self-refine: Iterative refinement with self-feedback." Additionally, the journal published the influential paper "Chain-of-thought prompting elicits reasoning in large language models." NeurIPS serves as a critical platform for the dissemination of innovative research and the advancement of knowledge in AI and ML.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e,aa79049289e6532592eec17b9e76adfb,d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="CHENG, X.">
      <data key="d0">PERSON</data>
      <data key="d1">Cheng, X. is an author of the paper "Lift yourself up: Retrieval-augmented text generation with self-memory"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LUO, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Luo, D. is an author of the paper "Lift yourself up: Retrieval-augmented text generation with self-memory"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHEN, X.">
      <data key="d0">PERSON</data>
      <data key="d1">Chen, X. is an author of the paper "Lift yourself up: Retrieval-augmented text generation with self-memory"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIU, L.">
      <data key="d0">PERSON</data>
      <data key="d1">Liu, L. is an author of the paper "Lift yourself up: Retrieval-augmented text generation with self-memory"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHAO, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhao, D. is an author of the paper "Lift yourself up: Retrieval-augmented text generation with self-memory"Zhao, D. is an author of the paper "Retrieval-generation synergy augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YAN, R.">
      <data key="d0">PERSON</data>
      <data key="d1">Yan, R. is an author of the paper "Lift yourself up: Retrieval-augmented text generation with self-memory"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DANG, H. T.">
      <data key="d0">PERSON</data>
      <data key="d1">Dang, H. T. is an author of the paper "Duc 2005: Evaluation of question-focused summarization systems"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PROCEEDINGS OF THE WORKSHOP ON TASK-FOCUSED SUMMARIZATION AND QUESTION ANSWERING">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The workshop where the paper "Duc 2005: Evaluation of question-focused summarization systems" was presented</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="ES, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Es, S. is an author of the paper "Ragas: Automated evaluation of retrieval augmented generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAMES, J.">
      <data key="d0">PERSON</data>
      <data key="d1">James, J. is an author of the paper "Ragas: Automated evaluation of retrieval augmented generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ESPINOSA-ANKE, L.">
      <data key="d0">PERSON</data>
      <data key="d1">Espinosa-Anke, L. is an author of the paper "Ragas: Automated evaluation of retrieval augmented generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SCHOCKAERT, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Schockaert, S. is an author of the paper "Ragas: Automated evaluation of retrieval augmented generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FENG, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Feng, Z. is an author of the paper "Retrieval-generation synergy augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FENG, X.">
      <data key="d0">PERSON</data>
      <data key="d1">Feng, X. is an author of the paper "Retrieval-generation synergy augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YANG, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Yang, M. is an author of the paper "Retrieval-generation synergy augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QIN, B.">
      <data key="d0">PERSON</data>
      <data key="d1">Qin, B. is an author of the paper "Retrieval-generation synergy augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FORTUNATO, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Fortunato, S. is an author of the paper "Community detection in graphs"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PHYSICS REPORTS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Community detection in graphs" was published</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="GAO, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Gao, Y. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIONG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Xiong, Y. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GAO, X.">
      <data key="d0">PERSON</data>
      <data key="d1">Gao, X. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIA, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Jia, K. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PAN, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Pan, J. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BI, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Bi, Y. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAI, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Dai, Y. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SUN, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Sun, J. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WANG, H.">
      <data key="d0">PERSON</data>
      <data key="d1">Wang, H. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GOODWIN, T. R.">
      <data key="d0">PERSON</data>
      <data key="d1">Goodwin, T. R. is an author of the paper "Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAVERY, M. E.">
      <data key="d0">PERSON</data>
      <data key="d1">Savery, M. E. is an author of the paper "Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DEMNER-FUSHMAN, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Demner-Fushman, D. is an author of the paper "Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PROCEEDINGS OF COLING. INTERNATIONAL CONFERENCE ON COMPUTATIONAL LINGUISTICS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The conference where the paper "Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization" was presented</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="HE, X.">
      <data key="d0">PERSON</data>
      <data key="d1">He, X. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TIAN, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Tian, Y. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SUN, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Sun, Y. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHAWLA, N. V.">
      <data key="d0">PERSON</data>
      <data key="d1">Chawla, N. V. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LAURENT, T.">
      <data key="d0">PERSON</data>
      <data key="d1">Laurent, T. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LECUN, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">LeCun, Y. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BRESSON, X.">
      <data key="d0">PERSON</data>
      <data key="d1">Bresson, X. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HOOI, B.">
      <data key="d0">PERSON</data>
      <data key="d1">Hooi, B. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JACOMY, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Jacomy, M. is an author of the paper "Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VENTURINI, T.">
      <data key="d0">PERSON</data>
      <data key="d1">Venturini, T. is an author of the paper "Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HEYMANN, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Heymann, S. is an author of the paper "Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BASTIAN, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Bastian, M. is an author of the paper "Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PLOS ONE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software" was published</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="JIN, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Jin, D. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YU, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Yu, Z. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIAO, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Jiao, P. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PAN, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Pan, S. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HE, D.">
      <data key="d0">PERSON</data>
      <data key="d1">He, D. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WU, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Wu, J. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PHILIP, S. Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Philip, S. Y. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHANG, W.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhang, W. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "A survey of community detection approaches: From statistical modeling to deep learning" was published</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="KANG, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Kang, M. is an author of the paper "Knowledge graph-augmented language models for knowledge-grounded dialogue generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KWAK, J. M.">
      <data key="d0">PERSON</data>
      <data key="d1">Kwak, J. M. is an author of the paper "Knowledge graph-augmented language models for knowledge-grounded dialogue generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BAEK, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Baek, J. is an author of the paper "Knowledge graph-augmented language models for knowledge-grounded dialogue generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HWANG, S. J.">
      <data key="d0">PERSON</data>
      <data key="d1">Hwang, S. J. is an author of the paper "Knowledge graph-augmented language models for knowledge-grounded dialogue generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KHATTAB, O.">
      <data key="d0">PERSON</data>
      <data key="d1">Khattab, O. is an author of the paper "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SANTHANAM, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Santhanam, K. is an author of the paper "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LI, X. L.">
      <data key="d0">PERSON</data>
      <data key="d1">Li, X. L. is an author of the paper "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HALL, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Hall, D. is an author of the paper "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIANG, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Liang, P. is an author of the paper "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP" and also contributed to the paper "Lost in the middle: How language models use long contexts."</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="POTTS, C.">
      <data key="d0">PERSON</data>
      <data key="d1">Potts, C. is an author of the paper "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZAHARIA, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Zaharia, M. is an author of the paper "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KIM, G.">
      <data key="d0">PERSON</data>
      <data key="d1">Kim, G. is an author of the paper "Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KIM, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Kim, S. is an author of the paper "Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JEON, B.">
      <data key="d0">PERSON</data>
      <data key="d1">Jeon, B. is an author of the paper "Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PARK, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Park, J. is an author of the paper "Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KANG, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Kang, J. is an author of the paper "Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KLEIN, G.">
      <data key="d0">PERSON</data>
      <data key="d1">Klein, G. is an author of the paper "Making sense of sensemaking 1: Alternative perspectives"Klein, G. isKlein, G. is an author of the paper "Making sense of sensemaking 2: A macrocognitive model"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MOON, B.">
      <data key="d0">PERSON</data>
      <data key="d1">Moon, B. is an author of the paper "Making sense of sensemaking 1: Alternative perspectives"Moon, B. is an author of the paper "Making sense of sensemaking 2: A macrocognitive model"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HOFFMAN, R. R.">
      <data key="d0">PERSON</data>
      <data key="d1">Hoffman, R. R. is an author of the paper "Making sense of sensemaking 2: A macrocognitive model"Hoffman, R. R. is an author of the paper "Making sense of sensemaking 1: Alternative perspectives"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IEEE INTELLIGENT SYSTEMS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Making sense of sensemaking 1: Alternative perspectives" was publishedThe journal where the paper "Making sense of sensemaking 2: A macrocognitive model" was published</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="COMPUTATIONAL LINGUISTICS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Domain adaptation with pre-trained transformers for query-focused abstractive text summarization" was published</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
    </node>
    <node id="LEWIS, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Lewis, P. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PEREZ, E.">
      <data key="d0">PERSON</data>
      <data key="d1">Perez, E. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PIKTUS, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Piktus, A. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PETRONI, F.">
      <data key="d0">PERSON</data>
      <data key="d1">Petroni, F. is an author of the paper "Lost in the middle: How language models use long contexts"Petroni, F. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KARPUKHIN, V.">
      <data key="d0">PERSON</data>
      <data key="d1">Karpukhin, V. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GOYAL, N.">
      <data key="d0">PERSON</data>
      <data key="d1">Goyal, N. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="K&#220;TTLER, H.">
      <data key="d0">PERSON</data>
      <data key="d1">K&#252;ttler, H. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LEWIS, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Lewis, M. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIH, W.-T.">
      <data key="d0">PERSON</data>
      <data key="d1">Yih, W.-T. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ROCKT&#196;SCHEL, T.">
      <data key="d0">PERSON</data>
      <data key="d1">Rockt&#228;schel, T. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIU, N. F.">
      <data key="d0">PERSON</data>
      <data key="d1">Liu, N. F. is an author of the paper "Lost in the middle: How language models use long contexts"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIN, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Lin, K. is an author of the paper "Lost in the middle: How language models use long contexts"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HEWITT, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Hewitt, J. is an author of the paper "Lost in the middle: How language models use long contexts"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PARANJAPE, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Paranjape, A. is an author of the paper "Lost in the middle: How language models use long contexts"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BEVILACQUA, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Bevilacqua, M. is an author of the paper "Lost in the middle: How language models use long contexts"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIU, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Liu, Y. is an author of the paper "Hierarchical transformers for multi-document summarization"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LAPATA, M.">
      <data key="d0">PERSON</data>
      <data key="d1">LAPATA, M. is an author known for contributing to the field of text summarization. Notably, Lapata, M. has co-authored the papers "Hierarchical transformers for multi-document summarization" and "Text summarization with latent queries," both of which explore advanced methodologies in summarizing textual information.</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MANAKUL, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Manakul, P. is an author of the paper "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIUSIE, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Liusie, A. is an author of the paper "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GALES, M. J.">
      <data key="d0">PERSON</data>
      <data key="d1">Gales, M. J. is an author of the paper "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MAO, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Mao, Y. is an author of the paper "Generation-augmented retrieval for open-domain question answering"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HE, P.">
      <data key="d0">PERSON</data>
      <data key="d1">He, P. is an author of the paper "Generation-augmented retrieval for open-domain question answering"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIU, X.">
      <data key="d0">PERSON</data>
      <data key="d1">Liu, X. is an author of the paper "Generation-augmented retrieval for open-domain question answering"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHEN, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Shen, Y. is an author of the paper "Generation-augmented retrieval for open-domain question answering"Shen, Y. is an author of the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GAO, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Gao, J. is an author of the paper "Generation-augmented retrieval for open-domain question answering"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HAN, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Han, J. is an author of the paper "Generation-augmented retrieval for open-domain question answering"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHEN, W.">
      <data key="d0">PERSON</data>
      <data key="d1">Chen, W. is an author of the papers "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy" and "Generation-augmented retrieval for open-domain question answering."</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SPIE CONFERENCE ON VISUALIZATION AND DATA ANALYSIS (VDA)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The conference where the paper "Openord: An open-source toolbox for large graph layout" was presented</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="MARTIN, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Martin, S. is an author of the paper "Openord: An open-source toolbox for large graph layout"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BROWN, W. M.">
      <data key="d0">PERSON</data>
      <data key="d1">Brown, W. M. is an author of the paper "Openord: An open-source toolbox for large graph layout"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KLAVANS, R.">
      <data key="d0">PERSON</data>
      <data key="d1">Klavans, R. is an author of the paper "Openord: An open-source toolbox for large graph layout"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BOYACK, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Boyack, K. is an author of the paper "Openord: An open-source toolbox for large graph layout"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NEWMAN, M. E.">
      <data key="d0">PERSON</data>
      <data key="d1">Newman, M. E. is an author of the paper "Modularity and community structure in networks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Modularity and community structure in networks" was published</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">JOURNAL</data>
    </node>
    <node id="RAM, O.">
      <data key="d0">PERSON</data>
      <data key="d1">Ram, O. is an author of the paper "In-context retrieval-augmented language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LEVINE, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Levine, Y. is an author of the paper "In-context retrieval-augmented language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DALMEDIGOS, I.">
      <data key="d0">PERSON</data>
      <data key="d1">Dalmedigos, I. is an author of the paper "In-context retrieval-augmented language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MUHLGAY, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Muhlgay, D. is an author of the paper "In-context retrieval-augmented language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHASHUA, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Shashua, A. is an author of the paper "In-context retrieval-augmented language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LEYTON-BROWN, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Leyton-Brown, K. is an author of the paper "In-context retrieval-augmented language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHOHAM, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Shoham, Y. is an author of the paper "In-context retrieval-augmented language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TRANSACTIONS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "In-context retrieval-augmented language models" was published</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">JOURNAL</data>
    </node>
    <node id="RANADE, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Ranade, P. is an author of the paper "Fabula: Intelligence report generation using retrieval-augmented narrative construction"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOSHI, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Joshi, A. is an author of the paper "Fabula: Intelligence report generation using retrieval-augmented narrative construction"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SARTHI, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Sarthi, P. is an author of the paper "Raptor: Recursive abstractive processing for tree-organized retrieval"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ABDULLAH, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Abdullah, S. is an author of the paper "Raptor: Recursive abstractive processing for tree-organized retrieval"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TULI, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Tuli, A. is an author of the paper "Raptor: Recursive abstractive processing for tree-organized retrieval"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KHANNA, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Khanna, S. is an author of the paper "Raptor: Recursive abstractive processing for tree-organized retrieval"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GOLDIE, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Goldie, A. is an author of the paper "Raptor: Recursive abstractive processing for tree-organized retrieval"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MANNING, C. D.">
      <data key="d0">PERSON</data>
      <data key="d1">Manning, C. D. is an author of the paper "HotpotQA: A dataset for diverse, explainable multi-hop question answering" and also contributed to the paper "Raptor: Recursive abstractive processing for tree-organized retrieval."</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SCOTT, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Scott, K. is the host of the podcast "Behind the Tech"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHAO, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Shao, Z. is an author of the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GONG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Gong, Y. is an author of the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HUANG, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Huang, M. is an author of the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DUAN, N.">
      <data key="d0">PERSON</data>
      <data key="d1">Duan, N. is an author of the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SU, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Su, D. is an author of the paper titled "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management." This work focuses on developing a system for managing scholarly information related to COVID-19 through question answering and multi-document summarization techniques.</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XU, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Xu, Y. is an author of the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management" and the paper "Text summarization with latent queries."</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YU, T.">
      <data key="d0">PERSON</data>
      <data key="d1">Yu, T. is an author of the paper titled "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management." This work focuses on developing a system designed to manage scholarly information related to COVID-19 by utilizing question answering and multi-document summarization techniques.</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SIDDIQUE, F. B.">
      <data key="d0">PERSON</data>
      <data key="d1">Siddique, F. B. is an author of the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management."</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BAREZI, E. J.">
      <data key="d0">PERSON</data>
      <data key="d1">Barezi, E. J. is an author of the paper titled "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management."</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FUNG, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Fung, P. is an author of the paper titled "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management." This work focuses on developing a system for managing scholarly information related to COVID-19 through question answering and multi-document summarization techniques.</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CANADIAN AI 2020">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models" was presented</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
    </node>
    <node id="SPRINGER">
      <data key="d0">PUBLISHER</data>
      <data key="d1">Springer is a prominent publisher known for its significant contributions to the academic and scientific community. Notably, Springer is the publisher of the book "Why Greatness Cannot Be Planned: The Myth of the Objective," which delves into the complexities of achieving greatness and challenges conventional wisdom on goal-setting. Additionally, Springer is responsible for publishing the proceedings of the 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020, highlighting its role in disseminating cutting-edge research in the field of Artificial Intelligence. Through these publications, Springer continues to influence and shape the discourse in various academic disciplines.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,df50c95dff7da074cbb2f68e88686f88</data>
    </node>
    <node id="TANG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Tang, Y. is an author of the paper "MultiHop-RAG: Benchmarking retrieval-augmented generation for multi-hop queries"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YANG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Yang, Y. is an author of the paper "MultiHop-RAG: Benchmarking retrieval-augmented generation for multi-hop queries"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TOUVRON, H.">
      <data key="d0">PERSON</data>
      <data key="d1">Touvron, H. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MARTIN, L.">
      <data key="d0">PERSON</data>
      <data key="d1">Martin, L. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="STONE, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Stone, K. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALBERT, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Albert, P. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALMAHAIRI, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Almahairi, A. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BABAEI, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Babaei, Y. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BASHLYKOV, N.">
      <data key="d0">PERSON</data>
      <data key="d1">Bashlykov, N. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BATRA, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Batra, S. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BHARGAVA, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Bhargava, P. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BHOSALE, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Bhosale, S. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TRAAG, V. A.">
      <data key="d0">PERSON</data>
      <data key="d1">Traag, V. A. is an author of the paper "From Louvain to Leiden: guaranteeing well-connected communities"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WALTMAN, L.">
      <data key="d0">PERSON</data>
      <data key="d1">Waltman, L. is an author of the paper "From Louvain to Leiden: guaranteeing well-connected communities"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VAN ECK, N. J.">
      <data key="d0">PERSON</data>
      <data key="d1">Van Eck, N. J. is an author of the paper "From Louvain to Leiden: guaranteeing well-connected communities"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TRAJANOSKA, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Trajanoska, M. is an author of the paper "Enhancing knowledge graph construction using large language models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="STOJANOV, R.">
      <data key="d0">PERSON</data>
      <data key="d1">Stojanov, R. is an author of the paper "Enhancing knowledge graph construction using large language models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TRAJANOV, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Trajanov, D. is an author of the paper "Enhancing knowledge graph construction using large language models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TRIVEDI, H.">
      <data key="d0">PERSON</data>
      <data key="d1">Trivedi, H. is an author of the paper "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BALASUBRAMANIAN, N.">
      <data key="d0">PERSON</data>
      <data key="d1">Balasubramanian, N. is an author of the paper "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KHOT, T.">
      <data key="d0">PERSON</data>
      <data key="d1">Khot, T. is an author of the paper "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SABHARWAL, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Sabharwal, A. is an author of the paper "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WANG, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Wang, J. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIANG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Liang, Y. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MENG, F.">
      <data key="d0">PERSON</data>
      <data key="d1">Meng, F. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SUN, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Sun, Z. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHI, H.">
      <data key="d0">PERSON</data>
      <data key="d1">Shi, H. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LI, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Li, Z. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"Li, Z. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XU, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Xu, J. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QU, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Qu, J. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHOU, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhou, J. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WANG, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Wang, S. is an author of the paper "Feb4rag: Evaluating federated search in the context of retrieval augmented generation"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KHRAMTSOVA, E.">
      <data key="d0">PERSON</data>
      <data key="d1">Khramtsova, E. is an author of the paper "Feb4rag: Evaluating federated search in the context of retrieval augmented generation"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHUANG, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhuang, S. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"Zhuang, S. is an author of the paper "Feb4rag: Evaluating federated search in the context of retrieval augmented generation"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZUCCON, G.">
      <data key="d0">PERSON</data>
      <data key="d1">Zuccon, G. is an author of the paper "Feb4rag: Evaluating federated search in the context of retrieval augmented generation"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WANG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Wang, Y. is an author of the paper "Knowledge graph prompting for multi-document question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIPKA, N.">
      <data key="d0">PERSON</data>
      <data key="d1">Lipka, N. is an author of the paper "Knowledge graph prompting for multi-document question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ROSSI, R. A.">
      <data key="d0">PERSON</data>
      <data key="d1">Rossi, R. A. is an author of the paper "Knowledge graph prompting for multi-document question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SIU, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Siu, A. is an author of the paper "Knowledge graph prompting for multi-document question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHANG, R.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhang, R. is an author of the paper "Knowledge graph prompting for multi-document question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DERR, T.">
      <data key="d0">PERSON</data>
      <data key="d1">Derr, T. is an author of the paper "Knowledge graph prompting for multi-document question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YANG, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Yang, Z. is an author of the paper "HotpotQA: A dataset for diverse, explainable multi-hop question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QI, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Qi, P. is an author of the paper "HotpotQA: A dataset for diverse, explainable multi-hop question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHANG, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhang, S. is an author of the paper "HotpotQA: A dataset for diverse, explainable multi-hop question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BENGIO, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Bengio, Y. is an author of the paper "HotpotQA: A dataset for diverse, explainable multi-hop question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="COHEN, W. W.">
      <data key="d0">PERSON</data>
      <data key="d1">Cohen, W. W. is an author of the paper "HotpotQA: A dataset for diverse, explainable multi-hop question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SALAKHUTDINOV, R.">
      <data key="d0">PERSON</data>
      <data key="d1">Salakhutdinov, R. is an author of the paper "HotpotQA: A dataset for diverse, explainable multi-hop question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YAO, J.-G.">
      <data key="d0">PERSON</data>
      <data key="d1">Yao, J.-g. is an author of the paper "Recent advances in document summarization"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WAN, X.">
      <data key="d0">PERSON</data>
      <data key="d1">Wan, X. is an author of the paper "Recent advances in document summarization"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIAO, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Xiao, J. is an author of the paper "Recent advances in document summarization"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YAO, L.">
      <data key="d0">PERSON</data>
      <data key="d1">Yao, L. is an author of the paper "Causal graph discovery with retrieval-augmented generation based large language models"Yao, L. is an author of the paper "Exploring large language models for knowledge graph completion"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PENG, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Peng, J. is an author of the paper "Exploring large language models for knowledge graph completion"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MAO, C.">
      <data key="d0">PERSON</data>
      <data key="d1">Mao, C. is an author of the paper "Exploring large language models for knowledge graph completion"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LUO, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Luo, Y. is an author of the paper "Exploring large language models for knowledge graph completion"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHANG, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhang, J. is an author of the paper "Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHANG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhang, Y. is an author of the paper "Causal graph discovery with retrieval-augmented generation based large language models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GAN, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Gan, Y. is an author of the paper "Causal graph discovery with retrieval-augmented generation based large language models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WANG, C.">
      <data key="d0">PERSON</data>
      <data key="d1">Wang, C. is an author of the paper "Causal graph discovery with retrieval-augmented generation based large language models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHENG, L.">
      <data key="d0">PERSON</data>
      <data key="d1">Zheng, L. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHIANG, W.-L.">
      <data key="d0">PERSON</data>
      <data key="d1">Chiang, W.-L. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHENG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Sheng, Y. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WU, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Wu, Z. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHUANG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhuang, Y. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIN, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Lin, Z. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LI, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Li, D. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XING, E.">
      <data key="d0">PERSON</data>
      <data key="d1">Xing, E. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LANGUAGE AGENT TREE SEARCH (LATS)">
      <data key="d0">FRAMEWORK/TECHNOLOGY</data>
      <data key="d1">Language Agent Tree Search (LATS) is a general framework that integrates the capabilities of language models (LMs) in reasoning, acting, and planning. It leverages Monte Carlo Tree Search and LM-powered value functions and self-reflections to enable proficient exploration and enhanced decision-making. LATS addresses the shortcomings of previous techniques such as Chain of Thought (CoT), Tree of Thought (ToT), and ReAct by unifying these essential components within language models.</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,93cb0d0456e0822b5fe30a3e627405f8,9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="ANDY ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Andy Zhou is one of the authors of the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" and is affiliated with the University of Illinois Urbana-Champaign and Lapis Labs.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="KAI YAN">
      <data key="d0">PERSON</data>
      <data key="d1">Kai Yan is one of the authors of the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" and is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="MICHAL SHLAPENTOKH-ROTHMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Michal Shlapentokh-Rothman is one of the authors of the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" and is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="HAOHAN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Haohan Wang is one of the authors of the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" and is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="YU-XIONG WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yu-Xiong Wang is one of the authors of the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" and is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">The University of Illinois Urbana-Champaign is an educational institution where several authors of the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" are affiliated.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="LAPIS LABS">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Lapis Labs is an organization affiliated with Andy Zhou, one of the authors of the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models".</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="GPT-3.5">
      <data key="d0">MODEL</data>
      <data key="d1">GPT-3.5 is a version of OpenAI's language model, released in 2022, that has been extensively used in various experiments and studies. It has been employed to evaluate agentic systems, discovered agents, and baselines in Meta Agent Search. Additionally, GPT-3.5 serves as a baseline for transferring discovered agents to GPT-4 and has been used to assess performance on tasks such as HumanEval. In the context of WebShop, GPT-3.5 has been utilized for acting-based prompting methods and evaluated on 50 instructions, demonstrating high performance when used in conjunction with LATS. It has also been involved in experiments for the Game of 24 and in the experimental evaluation of LATS, showing gradient-free performance comparable to gradient-based fine-tuning for web navigation on WebShop. Furthermore, GPT-3.5 has been used to evaluate different prompting methods, including LATS, ReAct, Reflexion, and others, on various benchmarks such as MBPP. Despite its capabilities, GPT-3.5 has been outperformed by Orca-3 on multiple benchmarks.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,594449768ae2dea9b2efbe677075096b,7de66b94cf868b37b1df51dc545c415f,93cb0d0456e0822b5fe30a3e627405f8,99d90aededb61e04241516ed9ec656cc,b88745a13b69cecbc0ee9c3af41389bf,b8dd0300033963bb4a3e1bad37f8e7b9,bc26e68b0b2783ba912b9e5606d9eb0b,f8e7ed806916bf15245bcb4d52570c26,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="MONTE CARLO TREE SEARCH (MCTS)">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Monte Carlo Tree Search (MCTS) is a heuristic search algorithm widely used in various decision-making environments. It constructs a decision tree where each node represents a state and each edge signifies an action. MCTS operates over multiple episodes, expanding the tree by exploring numerous child states and selecting the optimal ones based on the Upper Confidence bounds applied to Trees (UCT) value. This algorithm is also integrated into Language Agent-based Training Systems (LATS) to enhance the decision-making capabilities of language models, thereby improving their exploration and decision-making processes in model-based reinforcement learning contexts.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8,9bb90746134619cad9a3e649b8b35f24,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="REACT">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">REACT is a prompting method designed to enhance the reasoning and acting capabilities of language models by incorporating interactions with external environments, such as games or APIs. It constructs an action space that includes permissible actions and reasoning traces, using observations from the environment to improve performance. ReAct has been evaluated in various studies, including those involving HotPotQA and WebShop, where it demonstrated competitive performance against imitation learning and reinforcement learning techniques. Despite its success, ReAct is considered simpler and less adaptive to environmental conditions compared to more advanced methods like LATS.</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,48e423e2baf2ed485872756f5b4d87d8,594449768ae2dea9b2efbe677075096b,8180bf20b7577f3eee40df5991e2886d,93cb0d0456e0822b5fe30a3e627405f8,99d90aededb61e04241516ed9ec656cc,9bb90746134619cad9a3e649b8b35f24,c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="HUMANEVAL">
      <data key="d0">BENCHMARK</data>
      <data key="d1">HumanEval is a benchmark and dataset used to evaluate the programming capabilities and functional correctness of language models, such as GPT-3.5 and GPT-4. It consists of 164 handwritten programming problems designed to assess the models' ability to synthesize programs from natural language descriptions. HumanEval measures the Pass@1 accuracy, which indicates the model's performance in generating correct solutions on the first attempt. Notably, the LATS algorithm achieved state-of-the-art results with a 92.7% Pass@1 rate when used with GPT-4. The dataset is also utilized in experiments involving a maximum of k=8 trajectories and a sampling size of n=5, further highlighting its role in evaluating reasoning and acting capabilities in language models.</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,48e423e2baf2ed485872756f5b4d87d8,93cb0d0456e0822b5fe30a3e627405f8,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="WEBSHOP">
      <data key="d0">BENCHMARK</data>
      <data key="d1">WebShop is an interactive web-based environment designed to evaluate agents on grounded language understanding and decision-making, simulating an e-commerce shopping task. It serves as a benchmark for assessing reasoning, acting, and planning strategies in language models, particularly in the context of LATS evaluation. WebShop's action space includes various actions such as searching, selecting products, and purchasing items. The platform comprises a website with 1.18 million real-world products and 12,000 human instructions, where agents must navigate the site through various commands to purchase an item matching a user specification. It is used to evaluate web navigation capabilities and has demonstrated significant performance improvements with GPT-3.5. WebShop is also a tool for scalable real-world web interaction with grounded language agents, making it a critical environment for experiments with algorithms like LATS.</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,594449768ae2dea9b2efbe677075096b,785ad59c6a37896a4676ec5c1689735f,8180bf20b7577f3eee40df5991e2886d,93cb0d0456e0822b5fe30a3e627405f8,99d90aededb61e04241516ed9ec656cc,b8dd0300033963bb4a3e1bad37f8e7b9,f8e7ed806916bf15245bcb4d52570c26,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="PROCEEDINGS OF THE 41ST INTERNATIONAL CONFERENCE ON MACHINE LEARNING">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The conference where the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" was presented.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="PMLR 235">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The volume of the Proceedings of the 41st International Conference on Machine Learning where the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" was published.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="2024">
      <data key="d0">YEAR</data>
      <data key="d1">The year when the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" was published.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="GITHUB">
      <data key="d0">PLATFORM</data>
      <data key="d1">GitHub is the platform where all code, prompts, and experiment results related to the Meta Agent Search algorithm are available. It also hosts the AutoGPT project and the code for Language Agent Tree Search (LATS). Additionally, GitHub is the platform where the repository containing all agents from the experiment is hosted, as well as the full framework code.</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b,34d0bb2211fc795fe1096442e086a2b3,449db721e37968e073e3579b59e023b2,93cb0d0456e0822b5fe30a3e627405f8,d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="HTTPS://GITHUB.COM/LAPISROCKS/LANGUAGEAGENTTREESEARCH">
      <data key="d0">URL</data>
      <data key="d1">The URL where the code for Language Agent Tree Search (LATS) is available on GitHub.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="WOOLDRIDGE">
      <data key="d0">PERSON</data>
      <data key="d1">Wooldridge is referenced in the context of general autonomous agents capable of reasoning and decision-making in a variety of environments.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="JENNINGS">
      <data key="d0">PERSON</data>
      <data key="d1">Jennings is referenced in the context of general autonomous agents capable of reasoning and decision-making in a variety of environments.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="CHOWDHERY ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Chowdhery et al. are referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="OPENAI">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">OpenAI is a prominent organization in the field of artificial intelligence, known for its significant contributions to the development of advanced language models. It is the entity behind the creation of ChatGPT, Simple Evals, and the highly influential GPT-3.5 and GPT-4 models. OpenAI has also developed the GPT-4o-2024-05-13 and GPT-3.5-turbo-0125 models, as well as the GPT Foundation Model. The organization is recognized for its work on the GPT-4 technical report, highlighting its role in advancing AI technology with strong reasoning and general adaptability capabilities.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1b1399c76420a477c0c97893d258ae69,2901d5e2711fa4f32d39cd8eea36cd71,2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447,93cb0d0456e0822b5fe30a3e627405f8,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="NALLAPATI ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Nallapati et al. are referenced in the context of language models excelling in standard natural language processing tasks such as summarization.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="BOWMAN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Bowman et al. are referenced in the context of language models excelling in standard natural language processing tasks such as language inference.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="COBBE ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Cobbe et al. are notable for their contributions to the field of artificial intelligence, particularly in the development and adaptation of language models for tasks that require advanced common-sense reasoning and quantitative skills. They are also the authors of the GSM8K dataset, which is a significant resource in this domain.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="SAPAROV AND HE">
      <data key="d0">PERSON</data>
      <data key="d1">Saparov and He are referenced in the context of language models being adapted to tasks requiring advanced common-sense reasoning or quantitative skills.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="DENG ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Deng et al. are referenced in the context of language models performing in complex environments such as web navigation.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="SCHICK ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Schick et al. are authors who have significantly contributed to the research on tool use in agentic systems. They are referenced in the context of language models performing in complex environments, particularly focusing on the application and development of the Tool Use method. Their work is pivotal in understanding how language models can effectively interact with and utilize tools within various environments, advancing the field of artificial intelligence and machine learning.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,93cb0d0456e0822b5fe30a3e627405f8,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="FAN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Fan et al. are referenced in the context of language models performing in complex environments such as open-ended games.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="GAO ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Gao et al. are referenced in the context of prompting techniques that augment language models with feedback or observations from an external environment. Additionally, Gao et al. are the authors of the GSM-Hard dataset.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="SHINN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Shinn et al. are notable authors in the field of Artificial Intelligence and Machine Learning, particularly recognized for their contributions to research on self-reflection in agentic systems. They are referenced in the context of prompting techniques that enhance language models by incorporating feedback or observations from an external environment. Additionally, Shinn et al. are the authors of the Reflection method, which is a significant approach within this domain.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,93cb0d0456e0822b5fe30a3e627405f8,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="SLOMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Sloman is referenced in the context of the limitations of reflexive methods in language models compared to humans' deliberate and thoughtful decision-making characteristics.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="EVANS">
      <data key="d0">PERSON</data>
      <data key="d1">Evans is referenced in the context of the limitations of reflexive methods in language models compared to humans' deliberate and thoughtful decision-making characteristics.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="XIE ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Xie et al. are authors who have significantly contributed to the understanding of beam search in language models. Their work is particularly referenced in the context of recent search-guided language model research, which addresses the challenges of planning and exploring multiple reasoning paths.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="HAO ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Hao et al. are authors who have significantly contributed to the development of techniques such as RAP. They are also referenced in the context of recent search-guided language model work, which addresses the issues of planning and multiple reasoning paths. Their work is influential in advancing methodologies that enhance the capabilities of language models in handling complex reasoning and planning tasks.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="PMLR">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">PMLR, or Proceedings of Machine Learning Research, is the organization that hosted both the International Conference on Machine Learning and the Conference on Robot Learning. Additionally, PMLR published the Proceedings of the 41st International Conference on Machine Learning, where the paper on LATS was presented.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="LATS">
      <data key="d0">FRAMEWORK</data>
      <data key="d1">LATS (Language Agent Tree Search) is a comprehensive framework designed to unify reasoning, acting, and planning in language models. It leverages Monte Carlo Tree Search to enhance performance across various domains by sampling nodes and using trajectories. LATS combines internal reasoning and external retrieval strategies, significantly improving performance on tasks such as HotPotQA, programming challenges, and the Game of 24 by incorporating self-consistency scores. Additionally, it excels in WebShop by surpassing RL-based training, utilizing a preconstructed action space of search and click commands, browser feedback, and reflections for observation. Through interactions with an environment, LATS enhances autonomous decision-making and interpretability, making it a versatile and high-performing method in the AI and ML landscape.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,42de130f5b6144472a86a4c8260a87c7,48e423e2baf2ed485872756f5b4d87d8,4ae237a491bc8a84cc720e40c59a7464,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,b8dd0300033963bb4a3e1bad37f8e7b9,c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="CHAIN-OF-THOUGHT (COT) PROMPTING">
      <data key="d0">METHOD</data>
      <data key="d1">Chain-of-Thought (CoT) prompting is a technique designed to handle scenarios where the direct mapping from input to output is complex, such as mathematical queries or challenging questions. This method involves decomposing complex inputs into sequential steps by creating intermediate thoughts that act as stepping stones between the input and the output. While effective in breaking down intricate tasks, it often suffers from error propagation.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="SELF-CONSISTENCY">
      <data key="d0">METHOD</data>
      <data key="d1">SELF-CONSISTENCY is a heuristic in the field of Artificial Intelligence and Machine Learning that enhances the accuracy of actions sampled multiple times at the same state. This method employs majority voting over sampled chains to mitigate error propagation in Chain-of-Thought prompting, thereby improving the reliability and consistency of the decision-making process.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="LEAST-TO-MOST PROMPTING">
      <data key="d0">METHOD</data>
      <data key="d1">Least-to-most prompting is a multi-step decomposition method aimed at improving Chain-of-Thought prompting</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="TREE-OF-THOUGHT (TOT) PROMPTING">
      <data key="d0">METHOD</data>
      <data key="d1">Tree-of-Thought (ToT) prompting extends Chain-of-Thought (CoT) prompting by exploring multiple reasoning paths over thoughts. It frames problems as a search over a tree, where each node represents a partial solution state. ToT prompting uses search algorithms such as depth-first or breadth-first search, guided by a language model-generated heuristic, to systematically explore the tree and improve the reasoning process.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="REASONING VIA PLANNING (RAP)">
      <data key="d0">METHOD</data>
      <data key="d1">Reasoning via Planning uses Monte Carlo Tree Search with rollouts simulated by language models to improve reasoning tasks</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="SELF-REFINE">
      <data key="d0">METHOD</data>
      <data key="d1">SELF-REFINE (Madaan et al., 2024) is a state-of-the-art, hand-designed agent used as a baseline in Meta Agent Search and various other experimental evaluations, including ARC. This manually designed agent method is employed for a range of tasks such as Math, Reading Comprehension, Multi-task, and Science. SELF-REFINE allows iterative self-reflection to correct mistakes made in previous attempts, enabling up to five refinement iterations with an early stop if the critic deems the answer correct. This technique leverages self-improvement to enhance reasoning, decision-making, and language model performance in acting tasks. By performing iterations of refinement on the novelty and correctness of proposals, SELF-REFINE serves as a powerful tool for improving the performance of models through self-reflection and refinement.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,1b1399c76420a477c0c97893d258ae69,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="REFLEXION">
      <data key="d0">METHOD</data>
      <data key="d1">Reflexion is a prompting method designed to enhance the reasoning and decision-making capabilities of language models by leveraging environmental feedback and self-improvement techniques. It is similar to the ReAct method and is particularly focused on decision-making tasks where reverting between iterations is feasible. Reflexion has been evaluated in experiments involving benchmarks such as HotPotQA and HumanEval, and it is used in applications like WebShop. However, its semantic feedback is noted to be less helpful in complex environments like WebShop. Compared to LATS, Reflexion is considered a simpler prompting method, which is highlighted in its limitations.</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,48e423e2baf2ed485872756f5b4d87d8,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,9bb90746134619cad9a3e649b8b35f24,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="YANG ET AL., 2018">
      <data key="d0">REFERENCE</data>
      <data key="d1">Yang et al., 2018 is a reference for the HotPotQA dataset used in evaluating LATS</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="SILVER ET AL., 2017">
      <data key="d0">REFERENCE</data>
      <data key="d1">"SILVER ET AL., 2017" is a reference to a study or paper that highlights the success of Monte Carlo Tree Search in model-based reinforcement learning. This work is also noted for its contributions to the development and application of learned heuristics within the field.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="YAO ET AL., 2023B">
      <data key="d0">REFERENCE</data>
      <data key="d1">"Yao et al., 2023b" is a paper published in 2023 that discusses the ReAct method and its application in language models. It is referenced for setting the maximum depth in HotPotQA experiments and evaluates the performance of the ReAct method on WebShop. Additionally, the publication is noted for being a reference point where the ReAct method is outperformed by LATS.</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="YAO ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">"Yao et al., 2022" is a publication that discusses the IL (Imitation Learning) and IL+RL (Imitation Learning combined with Reinforcement Learning) methods and their performance on WebShop. This study is also a reference for the WebShop dataset used in evaluating LATS (Learning and Transfer for Sequential Decision Making).</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="CHEN ET AL., 2021">
      <data key="d0">REFERENCE</data>
      <data key="d1">"Chen et al., 2021" is a significant publication in the field of Artificial Intelligence and Machine Learning, particularly in the context of evaluating programming tasks. This paper introduced the HumanEval dataset, which is used to assess the correctness of synthesized programs in Python. It also discusses safety concerns related to executing untrusted model-generated code. The study is notable for its reference to the state-of-the-art performance achieved by LATS using GPT-4 on the HumanEval dataset.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,99d90aededb61e04241516ed9ec656cc,dc55f071b95dec721a9820d39cdb3ccd,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="COBBE ET AL., 2021">
      <data key="d0">REFERENCE</data>
      <data key="d1">Cobbe et al., 2021 is a publication that discusses the GSM8K dataset and serves as a reference for reasoning in language models, particularly focusing on the decomposition of complex inputs into sequential steps.</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="WEI ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">"WEI ET AL., 2022" is a seminal paper published in 2022 by Wei et al. that introduces and discusses the Chain-of-Thought (CoT) method and its application in language models. This publication is a key reference for the CoT technique and its variants, including the ReAct prompting methods. The study has been widely cited in the context of enhancing language model performance through structured reasoning processes.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="KOJIMA ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">Kojima et al., 2022 is a reference for a variant of Chain-of-Thought prompting</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="WANG ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">"Wang et al., 2022" is a paper that discusses the CoT-SC (Chain of Thought with Self-Consistency) method and its application in language models. This study is a significant reference in the field of self-consistency and Chain-of-Thought prompting, providing valuable insights into enhancing the performance and reliability of language models through these techniques.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="GUO ET AL., 2018">
      <data key="d0">REFERENCE</data>
      <data key="d1">Guo et al., 2018 is a reference for the issue of error propagation in Chain-of-Thought prompting</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="CHEN ET AL., 2023B">
      <data key="d0">REFERENCE</data>
      <data key="d1">"Chen et al., 2023b" is a scholarly paper that delves into the concept of AgentVerse and its application in optimizing role definition within prompts. Additionally, this paper serves as a reference for addressing the issue of error propagation in Chain-of-Thought prompting.</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd,f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="YAO ET AL., 2023A">
      <data key="d0">REFERENCE</data>
      <data key="d1">"Yao et al., 2023a" is a paper published in 2023 that discusses the Tree of Thoughts (ToT) method and its application in language models. This publication introduced the ToT search method and serves as a reference for Tree-of-Thought prompting and search algorithms in Chain-of-Thought prompting.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="HAO ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">"Hao et al., 2023" is a paper that discusses the RAP (ReAct Prompting) method and its application in language models. The publication introduces the RAP search method and explores its performance, particularly in the context of Reasoning and Planning. It is cited in relation to standard Monte Carlo Tree Search (MCTS) and RAP, both of which rely on internal dynamics models. The study by Hao et al. is a significant contribution to the field, providing insights into Reasoning via Planning using MCTS.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,c234cb83764b899335af0950677ad024,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="BESTA ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">Besta et al., 2023 is a reference for search algorithms in Chain-of-Thought prompting</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="AHN ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">Ahn et al., 2022 is a reference for using language models as high-level controllers in robotics</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="HUANG ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">Huang et al., 2022 is a reference for using language models as high-level controllers in robotics</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="DRIESS ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">Driess et al., 2023 is a reference for using language models as high-level controllers in robotics</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="BAKER ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">Baker et al., 2022 is a reference for adapting language model agents to complex multimodal games</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="WANG ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">Wang et al., 2023 is a reference for adapting language model agents to complex multimodal games</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="GUSS ET AL., 2019">
      <data key="d0">REFERENCE</data>
      <data key="d1">Guss et al., 2019 is a reference for the game Minecraft used in adapting language model agents</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="FAN ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">Fan et al., 2022 is a reference for the game Minecraft used in adapting language model agents</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="LIU ET AL., 2018">
      <data key="d0">REFERENCE</data>
      <data key="d1">Liu et al., 2018 is a reference for using language models in text-based environments</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="SHRIDHAR ET AL., 2020">
      <data key="d0">REFERENCE</data>
      <data key="d1">Shridhar et al., 2020 is a reference for using language models in text-based environments</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="LIU ET AL., 2024">
      <data key="d0">REFERENCE</data>
      <data key="d1">"LIU ET AL., 2024" is a publication by Liu et al. in 2024 that is related to EoH. This work serves as a reference for using language models in text-based environments, highlighting its significance in the field of Artificial Intelligence and Machine Learning.</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb,f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="MADAAN ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">Madaan et al., 2023 is a reference to a study or paper that is related to the self-reflection method, specifically known as the self-refine method.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="SHINN ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">"Shinn et al., 2023" is a publication that introduced the Reflexion prompting method and discusses its application in language models. The paper is referenced in the context of self-reflection and improving generated agents, and it is also associated with the Self-Refine technique. This study highlights the performance and effectiveness of the Reflexion method in enhancing language model capabilities.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,282313a8340c6792e8c35f53ed157cd0,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="LMS FOR ACTING">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="SEARCH ALGORITHMS">
      <data key="d0">METHOD</data>
      <data key="d1">Search algorithms are methods used to explore and find solutions in various tasks, including language models and interactive environments. They are techniques employed to construct trajectories and incorporate external feedback in Learning and Teaching Systems (LATS). Additionally, search algorithms are utilized to explore multiple branches of outcomes and determine the best course of action in language models.</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="NODES">
      <data key="d0">COMPONENT</data>
      <data key="d1">NODES are components in search algorithms that store and retrieve external feedback, contributing to value assignment heuristics. Additionally, nodes refer to the sampled points in the search space used in the evaluation of LATS on the Game of 24.</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="PROMPTS">
      <data key="d0">COMPONENT</data>
      <data key="d1">PROMPTS are components in search algorithms that store and retrieve external feedback, contributing to value assignment heuristics. Additionally, they are specific instructions or few-shot input-output examples provided along with the input to improve reasoning in language models. These dual functionalities highlight the versatility of prompts in both algorithmic processes and enhancing the performance of language models.</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="EXTERNAL FEEDBACK">
      <data key="d0">CONCEPT</data>
      <data key="d1">External feedback is information from the environment that is incorporated into search algorithms to improve performance. It is also used in LATS (Learning Automata-based Search Techniques) to enhance performance and efficiency.</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="INTERNAL REASONING PERFORMANCE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Internal reasoning performance refers to the ability of a model to reason and solve tasks without external feedback</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="PRETRAINED LMS">
      <data key="d0">MODEL</data>
      <data key="d1">Pretrained language models are models that have been trained on large datasets and are repurposed in LATS for value functions and self-reflections</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="LM-POWERED VALUE FUNCTIONS">
      <data key="d0">COMPONENT</data>
      <data key="d1">LM-powered value functions are components in LATS that use language models to assign values and guide exploration</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="SELF-REFLECTIONS">
      <data key="d0">COMPONENT</data>
      <data key="d1">Self-reflections are components in LATS that allow language models to reflect on their actions and improve exploration</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="IN-CONTEXT LEARNING">
      <data key="d0">CONCEPT</data>
      <data key="d1">IN-CONTEXT LEARNING is a method where the agent learns from trial and error using context. It is the ability of language models to learn and adapt to new tasks based on the context provided. This approach leverages the capabilities of language models to learn from the context without requiring additional training.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="ENVIRONMENTAL CONDITIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Environmental conditions refer to the external factors and feedback that influence the performance of language models in LATS</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="REASONING">
      <data key="d0">CONCEPT</data>
      <data key="d1">Reasoning involves decomposing complex inputs into sequential intermediate steps towards a final answer and is the process of thinking about something in a logical way to form a conclusion or judgment. It is a key component in LATS (Logical and Analytical Thinking Systems). Reasoning is one of the domains where experiments were conducted and is also one of the skills covered by the synthetic post-training dataset created by AgentInstruct.</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,b88745a13b69cecbc0ee9c3af41389bf,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="ACTING">
      <data key="d0">CONCEPT</data>
      <data key="d1">ACTING involves decision-making and performing tasks in interactive environments using language models. It refers to the execution of actions based on decisions made by language models, which is a key component in Language and Action Task Systems (LATS).</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="PLANNING">
      <data key="d0">CONCEPT</data>
      <data key="d1">Planning in LATS involves organizing information, planning future actions, or injecting internal knowledge to formalize decisions. It encompasses creating a sequence of actions to achieve a specific goal, which is a key component in LATS. Additionally, planning involves creating strategies and trajectories to achieve goals in language models and interactive environments. It also refers to the use of a search algorithm to determine the best course of action in language models.</data>
      <data key="d2">c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="AUTONOMOUS REASONING">
      <data key="d0">CONCEPT</data>
      <data key="d1">Autonomous reasoning refers to the ability of language models to reason and make decisions independently</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="DECISION-MAKING">
      <data key="d0">CONCEPT</data>
      <data key="d1">Decision-making is the cognitive process of selecting a course of action from multiple alternatives, particularly in interactive environments. This process is enhanced by Language and Action Transformation Systems (LATS) and involves reasoning and planning. In the context of language models, decision-making refers to choosing actions and strategies based on the model's understanding and interpretation of the given data.</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="INTERACTIVE QUESTION-ANSWERING (QA)">
      <data key="d0">TASK</data>
      <data key="d1">Interactive question-answering is a task where language models answer questions based on interaction with the environment</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="WEB NAVIGATION">
      <data key="d0">TASK</data>
      <data key="d1">Web navigation is a task where language models interact with web environments to retrieve information and perform actions</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="MATH">
      <data key="d0">TASK</data>
      <data key="d1">"MATH" is a domain tested by Meta Agent Search using the MGSM benchmark, which evaluates the ability of AI systems to solve mathematical problems. It is a task where language models solve mathematical problems and equations, and it is one of the skills covered by the synthetic post-training dataset created by AgentInstruct. In this domain, Foundation Models (FMs) possess adequate knowledge to solve questions, with errors mainly arising from hallucinations or calculation mistakes. The evaluation of math capabilities in language models is highlighted in the document's section on evaluation.</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,81c504ffbcc5ed882e234802135295ba,86f77e15d41cbd0cb33f635ccb2cb66b,b88745a13b69cecbc0ee9c3af41389bf,bc26e68b0b2783ba912b9e5606d9eb0b,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="PROGRAMMING">
      <data key="d0">TASK</data>
      <data key="d1">Programming is a domain that requires reasoning and acting, involving the writing and debugging of code to solve problems. It is often evaluated through datasets like HumanEval and MBPP. Additionally, programming is one of the tasks used in experiments with the LATS algorithm, highlighting its significance in the development and testing of language models.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,42de130f5b6144472a86a4c8260a87c7,f8e7ed806916bf15245bcb4d52570c26,fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="COT">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Chain of Thought (CoT) is a reasoning method where the foundational model (FM) is prompted to think step by step before answering a question. This technique is particularly useful in scenarios where the direct mapping from input to output is complex. CoT enhances performance on questions requiring reasoning by leveraging the agent's existing knowledge. It has been evaluated in experiments involving benchmarks like HotPotQA and is used as a base prompting framework in environments without feedback, such as reasoning tasks. Additionally, CoT is employed in the Game of 24 with LATS and demonstrates the performance of models when answering directly without using Retrieval-Augmented Generation (RAG).</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,594449768ae2dea9b2efbe677075096b,97457e990eb6e3c88c11c862f9e3265b,99d90aededb61e04241516ed9ec656cc,ab04427ae0415a1c812a35cf8d3ee1a2,c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="ADAPLANNER">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">AdaPlanner is a technique that incorporates both positive and negative feedback to enhance reasoning and decision-making</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="HUANG ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Huang et al. are authors referenced in the context of multi-objective Advanced Driver Assistance Systems (ADAS). They have also suggested that language models cannot self-correct their internal reasoning, making it critical to use external feedback.</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="EXTERNAL TOOLS">
      <data key="d0">RESOURCE</data>
      <data key="d1">External tools are resources or utilities that agents can access to perform tasks, such as search engines, code execution, and database queries. These tools or APIs are used by agents in reasoning tasks to perform specific actions. External tools, including APIs, search engines, calculators, and other models, are employed to enhance the reasoning and practical abilities of language models.</data>
      <data key="d2">c234cb83764b899335af0950677ad024,c3d0436082aada237ee4bee645f16059,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="TREE-BASED SEARCH">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Tree-based search is a method where multiple branches of outcomes are explored during search, widely used in planning and reinforcement learning algorithms</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="MCTS">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Monte Carlo Tree Search (MCTS) is a principled, tree-based search algorithm that has demonstrated significant performance gains in various studies. It requires an environment model to undo previous steps and form a searching tree. MCTS is notably used in LATS (Language and Tree Search) to fully unlock the potential of language models, providing a more effective search compared to other variants like A* and DFS.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,594449768ae2dea9b2efbe677075096b,c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="LM">
      <data key="d0">MODEL</data>
      <data key="d1">Language Model (LM) is a pre-trained model parameterized to generate outputs corresponding to answers or task completions based on given inputs</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="YA0 ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Yao et al. are authors who have contributed to the development of techniques like ReAct and ToT</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="SELF-REFLECTION">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">SELF-REFLECTION is a technique used in agentic systems to enhance performance by reflecting on past actions and proposing superior alternatives. This method involves a process where the agent reviews its actions, analyzes feedback, and implements improvements to its task-solving capabilities. In the context of LATS (Language Agent Task Solving), self-reflection provides additional semantic signals that aid in better decision-making and reasoning. The process is also characterized by the meta agent's critical thinking and debugging to refine its architecture and implementation. Furthermore, self-reflection includes the review and analysis of one's own code to identify errors and areas for improvement, leveraging language model-generated feedback to optimize reasoning and decision-making.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,282313a8340c6792e8c35f53ed157cd0,594449768ae2dea9b2efbe677075096b,785ad59c6a37896a4676ec5c1689735f,c3d0436082aada237ee4bee645f16059,c95e02c0dca4a4a36b701cbc7dd14da6,d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="EXTERNAL MEMORY">
      <data key="d0">RESOURCE</data>
      <data key="d1">External Memory is a method used for improving the performance of models through the use of external memory resources. It is particularly utilized to store past text context, which can be referenced for future updates of the solution in language models. This approach enhances the model's ability to retain and utilize historical data, thereby improving its overall efficiency and accuracy in processing and generating language.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="REINFORCEMENT LEARNING">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Reinforcement Learning is a type of machine learning algorithm known for its effective exploration-exploitation trade-off, making it particularly useful in planning algorithms. It is also employed in search algorithms to explore the search space in Advanced Driver Assistance Systems (ADAS). This method allows systems to learn optimal behaviors through interactions with their environment, thereby improving decision-making processes in complex scenarios.</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="VODOPIVEC ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Vodopivec et al. are authors who have contributed to the understanding of tree-based search in reinforcement learning</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="WEI ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Wei et al. are authors who have significantly contributed to the development and research of Chain-of-Thought (CoT) prompting, planning, and reasoning. They are recognized for their work on the Chain-of-Thought agent and the Chain-of-Thought method, which are pivotal in advancing the understanding and application of structured reasoning processes in artificial intelligence.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b,c3d0436082aada237ee4bee645f16059,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="SWIECHOWSKI ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Swiechowski et al. are authors who have contributed to the understanding of tree-based search in planning algorithms</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="LAVALLE">
      <data key="d0">AUTHOR</data>
      <data key="d1">LaValle is an author who has contributed to the understanding of tree-based search in planning algorithms</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="HAFNER ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Hafner et al. are authors who have contributed to the understanding of tree-based search in reinforcement learning</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="DU ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Du et al. are authors who have significantly contributed to the understanding of tree-based search in reinforcement learning. They are also the authors of the LLM Debate agent and the LLM Debate method, showcasing their expertise in developing advanced methodologies and agents within the field of artificial intelligence and machine learning.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="WU ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Wu et al. are authors who have significantly contributed to the understanding of tree-based search in reinforcement learning. Additionally, they have developed a method for assigning FM modules in agentic systems with different roles, enabling these modules to collaborate effectively. Their work spans critical areas in AI and ML, showcasing their expertise in both theoretical and practical aspects of the field.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="SHICK ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Schick et al. are authors who have contributed to the understanding of using external tools to enhance language models</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="SHEN ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Shen et al. are authors who have contributed to the understanding of using external tools to enhance language models</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="SURIS ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Suris et al. are authors who have contributed to the understanding of using external tools to enhance language models</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="LM TASKS">
      <data key="d0">TASK</data>
      <data key="d1">"LM TASKS refer to tasks involving language models, which can conveniently reset to any step by simply copy-pasting historical text input. These tasks encompass a variety of functions that language models can perform, including reasoning, decision-making, and planning."</data>
      <data key="d2">c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">TASK</data>
    </node>
    <node id="RAP">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">RAP (Reasoning and Planning) is a multifaceted method that integrates ReAct prompting with search algorithms to enhance decision-making and reasoning in language models. It has been evaluated in various experiments, including HotPotQA and Game of 24, demonstrating competitive performance in reasoning tasks. RAP enhances performance by sampling and exploring multiple outputs, and it is compared to LATS in terms of sample complexity and performance. Additionally, RAP incorporates planning and search algorithms to improve reasoning and decision-making capabilities. Despite its strengths, RAP has limitations in flexibility, sensibility, and adaptability. It also includes configurations for performance and token consumption, and in some contexts, it relies on internal dynamics models to facilitate simulation.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,42de130f5b6144472a86a4c8260a87c7,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,9bb90746134619cad9a3e649b8b35f24,c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6,faa2bd677c7f052136479e0175da3e5b,fb9cb0c0984d44c3da881886ed637e55</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="TOT">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">ToT (Tree of Thoughts) is a prompting method that incorporates planning and search algorithms to enhance reasoning and decision-making in language models. It uses LM-based heuristics to prune branches with low values and is evaluated in experiments involving HotPotQA, Game of 24, and other benchmarks. Compared to other methods like LATS, ToT demonstrates improvements in sample complexity and performance by sampling and exploring multiple outputs to improve performance on reasoning tasks.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,42de130f5b6144472a86a4c8260a87c7,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,c95e02c0dca4a4a36b701cbc7dd14da6,faa2bd677c7f052136479e0175da3e5b,fb9cb0c0984d44c3da881886ed637e55</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="BEAM SEARCH">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Beam search is a technique that uses self-improvement to enhance reasoning and decision-making in language models</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="LM INTERNAL REASONING">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">LM internal reasoning refers to the reasoning capabilities of language models without external inputs</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="PROMPT IO">
      <data key="d0">RESOURCE</data>
      <data key="d1">Prompt IO refers to the process where an input prompt is transformed into an output by a language model</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">RESOURCE</data>
    </node>
    <node id="INPUT X">
      <data key="d0">RESOURCE</data>
      <data key="d1">"Input X refers to the initial input provided to a language model for reasoning or decision-making. It is the initial data or query that the language model processes to generate responses or perform tasks."</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24,c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">RESOURCE</data>
    </node>
    <node id="OUTPUT Y">
      <data key="d0">RESOURCE</data>
      <data key="d1">Output Y refers to the final output generated by a language model based on the given input. It is the final result produced by the language model after processing the input and any intermediate steps.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24,c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">RESOURCE</data>
    </node>
    <node id="P&#920;(X)">
      <data key="d0">MODEL</data>
      <data key="d1">P&#952;(X) refers to the pre-trained language model parameterized by &#952; used to generate outputs based on given inputs</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="AUTOREGRESSIVE DECODING">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Autoregressive decoding is a method where the language model generates text sequentially, predicting the next token based on previous tokens</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="LANGUAGE MODEL (LM)">
      <data key="d0">MODEL</data>
      <data key="d1">A language model (LM) is a type of model used to generate outputs based on given inputs. It is used in various prompting techniques like CoT, ToT, and ReAct to improve reasoning and decision-making tasks.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="UPPER CONFIDENCE BOUNDS APPLIED TO TREES (UCT)">
      <data key="d0">METRIC</data>
      <data key="d1">Upper Confidence bounds applied to Trees (UCT) is a metric used in MCTS to select the best child state for expansion. It is calculated based on the value function, the number of visits to a node, and an exploration weight.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="PROMPT">
      <data key="d0">TECHNIQUE/METHOD</data>
      <data key="d1">A prompt is a specific instruction or few-shot input-output example provided along with the input to improve reasoning in language models. It is a structured format generated by the FM Module by concatenating input Info objects. In the context of HotPotQA, a prompt is used to guide the question answering process. Additionally, the prompt instructs the meta agent on how to format its output, including sections for thought process, agent name, and code implementation.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0,9bb90746134619cad9a3e649b8b35f24,b8dd0300033963bb4a3e1bad37f8e7b9,d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="THOUGHT Z">
      <data key="d0">CONCEPT</data>
      <data key="d1">Thought z is an intermediate language sequence created during the Chain-of-thought (CoT) prompting process to bridge the gap between input x and output y.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="DEPTH-FIRST SEARCH (DFS)">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Depth-first search (DFS) is a search algorithm used in Tree-of-thought (ToT) prompting to systematically explore the tree of reasoning paths.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="BREADTH-FIRST SEARCH (BFS)">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Breadth-first search (BFS) is a search algorithm used in Tree-of-thought (ToT) prompting to systematically explore the tree of reasoning paths.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="OBSERVATION O">
      <data key="d0">CONCEPT</data>
      <data key="d1">Observation o is the feedback or data received from the external environment during the ReAct process, used to improve reasoning and acting.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="ACTION A">
      <data key="d0">CONCEPT</data>
      <data key="d1">Action a is a permissible action added to the reasoning traces in the ReAct process, generated based on observations from the environment.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="RAP (HAO ET AL., 2023)">
      <data key="d0">TECHNIQUE/METHOD</data>
      <data key="d1">RAP is a reasoning-based method that relies on the internal representations of the language model and is used in planning strategies. It has limitations in flexibility, sensibility, and adaptability.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="ATARI">
      <data key="d0">ENVIRONMENT</data>
      <data key="d1">Atari is a decision-making environment where Monte Carlo Tree Search (MCTS) has been successfully applied.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="GO">
      <data key="d0">ENVIRONMENT</data>
      <data key="d1">Go is a decision-making environment where Monte Carlo Tree Search (MCTS) has been successfully applied.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="KOCSIS AND SZEPESV&#193;RI (2006)">
      <data key="d0">PERSON</data>
      <data key="d1">Kocsis and Szepesv&#225;ri are the researchers who developed the Upper Confidence bounds applied to Trees (UCT) metric used in MCTS.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="SILVER ET AL. (2016)">
      <data key="d0">PERSON</data>
      <data key="d1">Silver et al. are the researchers who demonstrated the success of Monte Carlo Tree Search (MCTS) in the game of Go.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="YE ET AL. (2021)">
      <data key="d0">PERSON</data>
      <data key="d1">Ye et al. are the researchers who demonstrated the success of Monte Carlo Tree Search (MCTS) in the Atari environment.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="LM AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">An LM Agent is initialized with a language model to leverage useful language representations for sequential reasoning or decision-making tasks</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="ENVIRONMENT">
      <data key="d0">CONTEXT</data>
      <data key="d1">The environment plays a crucial role in the functioning of the LATS algorithm by providing the context and rewards necessary for its operation. It serves as the medium through which observations are delivered to the agent and actions are received from the agent, facilitating a feedback loop essential for the learning process.</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="P&#920;">
      <data key="d0">MODEL</data>
      <data key="d1">P&#952; is a language model utilized for reasoning about a given state and providing a score. It serves as a base decision-maker, agent, state evaluator, and feedback generator in the LATS framework.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="UCT ALGORITHM">
      <data key="d0">ALGORITHM/TECHNIQUE</data>
      <data key="d1">The UCT (Upper Confidence bounds applied to Trees) algorithm is used to balance exploration and exploitation in the selection operation of LATS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="SELECTION">
      <data key="d0">OPERATION</data>
      <data key="d1">Selection is the first operation in LATS where the algorithm identifies a segment of the current tree most suitable for subsequent expansion. This process involves choosing nodes based on their assigned scalar values, ensuring that the most promising segments are targeted for further development.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="EXPANSION">
      <data key="d0">OPERATION</data>
      <data key="d1">"EXPANSION" is a text modification task that involves adding more information to a piece of text to make it more comprehensive. Additionally, in the context of LATS (likely a specific algorithm or framework), expansion is the second operation where the tree is expanded by sampling actions from P&#952; and adding new child nodes to the tree.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="SIMULATION">
      <data key="d0">OPERATION</data>
      <data key="d1">SIMULATION is an operation in LATS where the algorithm simulates the selected node until a terminal node is reached. This process involves expanding the currently selected node until a terminal state is achieved, ensuring a comprehensive exploration of potential outcomes within the algorithm's framework.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="BACKPROPAGATION">
      <data key="d0">OPERATION</data>
      <data key="d1">Backpropagation is an operation in LATS where the resulting value from the simulation is used to update the value of nodes along the path. This process involves updating the values of the tree based on the outcome of a trajectory, ensuring that the values of nodes are adjusted accordingly.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="REFLECTION">
      <data key="d0">OPERATION</data>
      <data key="d1">REFLECTION is an action to review and analyze past actions or decisions, aimed at improving future performance. It involves thinking about past actions and decisions to enhance decision-making and model performance through self-reflection and refinement. In the context of LATS (Learning and Adaptive Trajectory Systems), reflection is an operation where a reflection is generated if the trajectory fails, providing additional context for future trials. This process of reviewing and analyzing outcomes is integral to refining decision-making and improving overall performance.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,0b6b4880e77d40e284702da16be4ef64,4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7,c234cb83764b899335af0950677ad024,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="LONG-TERM MEMORY STRUCTURE">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">The "LONG-TERM MEMORY STRUCTURE" is a framework designed to store information over an extended period. It functions as an external structure where the expanded tree is stored in LATS (Long-term Associative Tree Storage). This structure is crucial for maintaining and organizing data that needs to be retained and accessed over long durations, ensuring that information is preserved and can be efficiently retrieved when necessary.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="TASK">
      <data key="d0">CONTEXT</data>
      <data key="d1">A task in LATS is an activity or assignment that agents are designed to perform or solve. It is successfully completed when the series of operations result in a solution or a computational limit is reached.</data>
      <data key="d2">c234cb83764b899335af0950677ad024,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="TRAJECTORY">
      <data key="d0">CONCEPT</data>
      <data key="d1">TRAJECTORY in the context of LATS (Learning and Task Scheduling) refers to the sequence of actions and observations sampled from P&#952; to construct the optimal path for task completion. It encompasses the path or sequence of actions taken during the interaction, the sequence of states from the root to the terminal state in the search tree, and the path taken through the state space in the LATS algorithm.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,48e423e2baf2ed485872756f5b4d87d8,5d356b8ff719763a38cecff22c4e17b7,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="FEEDBACK">
      <data key="d0">CONCEPT</data>
      <data key="d1">Feedback in LATS is the response from the environment to the agent's actions, used to guide the search algorithm. It is provided by critics and experts to refine answers in the Meta Agent Search process. Additionally, feedback serves as the evaluation of the generated code based on correct and wrong examples, offering information about the performance and correctness of the code. The feedback provided by the critic_module plays a crucial role in this evaluative process.</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,449db721e37968e073e3579b59e023b2,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="REASONING TASKS">
      <data key="d0">TASK</data>
      <data key="d1">Reasoning tasks are a type of task in LATS where the action space might be limited to a few external tools or APIs</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="DECISION-MAKING TASKS">
      <data key="d0">TASK</data>
      <data key="d1">Decision-making tasks are a type of task in LATS where actions might consist of commands on a website</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="SAMPLING">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Sampling in LATS involves selecting a diverse set of candidates at each step to mitigate the stochastic nature of LM text generation</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="SEARCH ALGORITHM">
      <data key="d0">ALGORITHM/TECHNIQUE</data>
      <data key="d1">A search algorithm in the context of LATS (Learning and Adaptive Trajectory Search) controls the problem-solving process with planning to find the most promising trajectory. It is an algorithm used to explore and find the most promising regions of a tree, which is essential in various applications such as Meta Agent Search, where it is employed to explore and discover new designs. Additionally, the search algorithm defines how ADAS (Advanced Driver Assistance Systems) algorithms explore the search space, ensuring efficient and effective navigation through potential solutions.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="PSEUDOCODE">
      <data key="d0">DOCUMENTATION</data>
      <data key="d1">The pseudocode of the LATS algorithm, referred to as "PSEUDOCODE," is comprehensively detailed in the Appendix. It outlines the operations and processes involved in the LATS algorithm, providing a complete and thorough guide for understanding its implementation.</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="SEC. A">
      <data key="d0">DOCUMENTATION</data>
      <data key="d1">Section A in the Appendix contains the full pseudocode of the proposed algorithm, LATS.</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="EVANS, 2010">
      <data key="d0">REFERENCE</data>
      <data key="d1">Evans, 2010 is a reference cited in the context of sampling diverse candidates for complex decision-making tasks</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="MODERN LMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Modern LMs (Language Models) provide useful language representations that facilitate planning in LATS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="EXPLORATION WEIGHT">
      <data key="d0">CONCEPT</data>
      <data key="d1">Exploration weight is a parameter used in the Monte Carlo Tree Search (MCTS) algorithm to balance exploration and exploitation. It plays a crucial role in the selection formula, significantly affecting the performance of the search in HotPotQA.</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="PARENT NODE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Parent node is a node in the tree structure from which child nodes are derived in MCTS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="EPISODE">
      <data key="d0">CONCEPT</data>
      <data key="d1">An episode refers to a complete sequence of actions and observations in a task, ending in a terminal state</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="RETURN">
      <data key="d0">CONCEPT</data>
      <data key="d1">Return is the reward or feedback used for updating the value function in MCTS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="VALUE FUNCTION">
      <data key="d0">CONCEPT</data>
      <data key="d1">The Value Function is a function that assigns a value to a state based on certain criteria. It is utilized in various applications, including WebShop and the Game of 24, to evaluate the performance of actions. In the context of the LATS algorithm, the value function (pV) is used to evaluate states and incorporates self-consistency as an additional heuristic for reasoning tasks. Additionally, the value function is employed in Monte Carlo Tree Search (MCTS) to estimate the expected return of a state.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,48e423e2baf2ed485872756f5b4d87d8,594449768ae2dea9b2efbe677075096b,b8dd0300033963bb4a3e1bad37f8e7b9,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="VOLD(S)">
      <data key="d0">CONCEPT</data>
      <data key="d1">Vold(s) is the old value function before it is updated with the new return in MCTS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="N(S)">
      <data key="d0">CONCEPT</data>
      <data key="d1">N(s) is the number of times a state has been visited in MCTS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="RESET">
      <data key="d0">CONCEPT</data>
      <data key="d1">Reset refers to the ability to revert to a previous state or step in LM tasks</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="ACTION SPACE">
      <data key="d0">CONCEPT</data>
      <data key="d1">ACTION SPACE refers to the set of all possible actions an agent can take in a given environment. In the context of the LATS algorithm, the action space (A) specifically denotes the set of possible actions that can be executed. This concept is fundamental in understanding the range of behaviors an agent can exhibit within a defined system or algorithm.</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="PERMISSIBLE ACTIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Permissible actions are the actions that an agent is allowed to take in a given environment</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="REASONING TRACES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Reasoning traces are the language-based thoughts used to formalize decisions in LATS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="OBSERVATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">OBSERVATION refers to the environmental feedback or information received after performing an action in various contexts, such as question answering tasks or web searches. In the HotPotQA framework, observation is specifically the action where the result of an action is analyzed. It involves noting the system's response to a user's action, providing crucial feedback that helps in understanding the outcome of the agent's actions.</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f,5d356b8ff719763a38cecff22c4e17b7,785ad59c6a37896a4676ec5c1689735f,b8dd0300033963bb4a3e1bad37f8e7b9,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="POLICY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Policy is a strategy or rule followed by an agent to decide actions based on observations and past actions</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="COMMANDS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Commands are specific actions taken by an agent in decision-making tasks, such as interacting with a website</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="STOCHASTIC NATURE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Stochastic nature refers to the randomness inherent in LM text generation</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="COMPUTATIONAL LIMIT">
      <data key="d0">CONCEPT</data>
      <data key="d1">Computational limit is the maximum amount of computational resources allocated for completing a task in LATS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="SCALAR VALUE">
      <data key="d0">CONCEPT</data>
      <data key="d1">SCALAR VALUE is a numeric value assigned to each new child node for selection and backpropagation. In the context of LATS (Learning Agent Task Scheduler), a scalar value is used to quantify the agent&#8217;s progress in task completion. This numerical value plays a crucial role in evaluating and guiding the agent's performance, ensuring efficient task management and optimization.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="HEURISTIC">
      <data key="d0">CONCEPT</data>
      <data key="d1">HEURISTIC is a method used to guide the search algorithm towards the most promising regions of the tree. Specifically, in the context of LATS (Learning Automata-based Tree Search), heuristic plays a crucial role in efficiently navigating the search space to identify optimal or near-optimal solutions.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="SELF-GENERATED LM SCORE">
      <data key="d0">CONCEPT</data>
      <data key="d1">The SELF-GENERATED LM SCORE is a score generated by the language model to evaluate a state. It serves as a component of the value function in LATS (Language-Aware Task Solver), and is generated by the language model itself.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="AGENT">
      <data key="d0">ACTOR</data>
      <data key="d1">An Agent is a system or entity designed to perform tasks and make decisions in a given environment, often using reasoning and planning. It is powered by a Large Language Model (LLM) and can optionally utilize tools such as search APIs, code interpreters, or calculators. Each agent has a specific role and set of instructions defined as part of the underlying LLM system message. Agents can receive a piece of text and generate a list of questions or modify the text based on predefined tasks such as paraphrasing, expansion, or simplification. Additionally, agents have been discovered by Meta Agent Search to outperform state-of-the-art hand-designed baselines.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,1d8835c0ce90e56be22873bcf2740a5d,bc26e68b0b2783ba912b9e5606d9eb0b,c3d0436082aada237ee4bee645f16059,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="TASK COMPLETION">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of finishing a given task successfully</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </node>
    <node id="SELF-CONSISTENCY SCORE">
      <data key="d0">METRIC</data>
      <data key="d1">The Self-Consistency Score is a metric used in LATS (Learning and Teaching System) to improve performance in the Game of 24. It is based on the consistency of actions sampled multiple times at the same state, ensuring that the actions taken are reliable and repeatable, thereby enhancing the overall effectiveness of the system.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="ENVIRONMENTAL FEEDBACK">
      <data key="d0">FEEDBACK</data>
      <data key="d1">Feedback obtained from the environment to improve value assignment</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </node>
    <node id="TERMINAL STATE">
      <data key="d0">STATE</data>
      <data key="d1">"TERMINAL STATE" refers to a state where the task is either completed successfully or not. In the context of the LATS algorithm, a terminal state (st) is specifically defined as the state where the process ends.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,48e423e2baf2ed485872756f5b4d87d8</data>
      <data key="d3">STATE</data>
    </node>
    <node id="REWARD">
      <data key="d0">METRIC</data>
      <data key="d1">"REWARD" is a value that reflects the outcome of a simulation and serves as feedback from the environment in the LATS algorithm. In the context of WebShop, reward is a metric calculated based on the number of attributes satisfied by the selected item.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,48e423e2baf2ed485872756f5b4d87d8,b8dd0300033963bb4a3e1bad37f8e7b9</data>
      <data key="d3">METRIC</data>
    </node>
    <node id="UCT FORMULA">
      <data key="d0">FORMULA</data>
      <data key="d1">A formula used to guide the selection of the next node</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
      <data key="d3">FORMULA</data>
    </node>
    <node id="MEMORY">
      <data key="d0">STORAGE</data>
      <data key="d1">A storage for failed trajectories and corresponding reflections</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
      <data key="d3">STORAGE</data>
    </node>
    <node id="EXACT MATCH (EM)">
      <data key="d0">METRIC</data>
      <data key="d1">A metric used to evaluate the accuracy of reasoning</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
      <data key="d3">METRIC</data>
    </node>
    <node id="AUSTIN ET AL.">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper related to programming</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="CAMPBELL ET AL., 2002">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper related to programmed heuristics</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="HYPERPARAMETER">
      <data key="d0">METRIC</data>
      <data key="d1">A parameter whose value is set before the learning process begins</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </node>
    <node id="PROGRAMMED HEURISTICS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Heuristics that are manually designed and implemented</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </node>
    <node id="LEARNED HEURISTICS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Heuristics that are learned through machine learning techniques</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </node>
    <node id="NODE">
      <data key="d0">CONCEPT</data>
      <data key="d1">A point in the search tree representing a state</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </node>
    <node id="DEPTH LEVEL">
      <data key="d0">METRIC</data>
      <data key="d1">The level of a node in the search tree, indicating its distance from the root</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </node>
    <node id="COT-SC">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">COT-SC (Chain of Thought with Self-Consistency) is a state-of-the-art, manually designed agent method developed by Wang et al. (2023b). It is used as a baseline in Meta Agent Search and serves as a comparison point in the evaluation of various agentic systems. COT-SC is employed for a range of tasks including Math, Reading Comprehension, Multi-task, and Science. This method is particularly noted for its planning and reasoning capabilities, where it samples five answers and performs an ensemble using either majority voting or an FM query. Additionally, COT-SC enhances reasoning capabilities in language models and is evaluated in experiments involving benchmarks such as HotPotQA. It is also compared to LATS in terms of efficiency and is assessed with different configurations for performance and token consumption.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71,42de130f5b6144472a86a4c8260a87c7,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b,faa2bd677c7f052136479e0175da3e5b,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="AUSTIN ET AL., 2022">
      <data key="d0">PUBLICATION</data>
      <data key="d1">"AUSTIN ET AL., 2022" is a paper by Austin et al. published in 2022 that is referenced in the context of evaluating LATS (Language-Agnostic Task Solvers) on programming tasks. This publication introduced the MBPP (Mostly Basic Python Problems) dataset, which is used for evaluating the correctness of synthesized programs in Python.</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="GAME OF 24">
      <data key="d0">BENCHMARK</data>
      <data key="d1">"Game of 24" is a benchmark used to evaluate reasoning and acting strategies in language models, specifically referenced in the context of LATS (Language and Action Task Suite) evaluation. It is a mathematical reasoning challenge where the goal is to use basic arithmetic operations to construct the number 24 from four given numbers. This task is utilized to test the reasoning ability of agents, making it one of the tasks used in experiments with the LATS algorithm.</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,594449768ae2dea9b2efbe677075096b,b8dd0300033963bb4a3e1bad37f8e7b9,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="MBPP">
      <data key="d0">DATASET</data>
      <data key="d1">MBPP (Mostly Basic Python Problems) is a dataset used to measure the correctness of synthesized programs in Python from natural language docstrings. It serves as a benchmark containing 974 short Python functions, specifically designed to evaluate program synthesis techniques. This dataset is instrumental in assessing the performance and accuracy of AI models in generating Python code from natural language descriptions, making it a valuable resource in the field of program synthesis and AI-driven code generation.</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc,fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="IL">
      <data key="d0">METHOD/ALGORITHM</data>
      <data key="d1">IL (Imitation Learning) is a method used to train models based on expert demonstrations</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="RL">
      <data key="d0">METHOD/ALGORITHM</data>
      <data key="d1">RL (Reinforcement Learning) is a method used to train models based on reward signals</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="FINE-TUNING">
      <data key="d0">METHOD/ALGORITHM</data>
      <data key="d1">Fine-tuning is a method used to improve model performance by training on specific datasets. It is mentioned in the context of enhancing performance in WebShop, as referenced in Furuta et al., 2024. Additionally, fine-tuning involves further training AI models using synthetic datasets created by AgentInstruct.</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="EXPERT">
      <data key="d0">HUMAN</data>
      <data key="d1">EXPERT refers to human performance benchmarks and metrics used for comparison with model performance on WebShop.</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="TAB. 2">
      <data key="d0">DATASET</data>
      <data key="d1">Tab. 2 is a table that shows the performance of internal reasoning and external retrieval strategies on HotPotQA</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="TAB. 3">
      <data key="d0">DATASET</data>
      <data key="d1">Tab. 3 is a table that shows the performance of various methods on HotPotQA in different settings</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="TAB. 4">
      <data key="d0">DATASET</data>
      <data key="d1">Tab. 4 is a table that shows the performance of various methods on programming tasks using HumanEval and MBPP datasets</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="TAB. 5">
      <data key="d0">DATASET</data>
      <data key="d1">Tab. 5 is a table that shows the GPT-3.5 Pass@1 accuracy on MBPP for various prompting methods</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="APPENDIX SEC. D">
      <data key="d0">DATASET</data>
      <data key="d1">Appendix Sec. D provides additional details on the evaluation of LATS and other methods on programming tasks</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="CHEN ET AL., 2023A">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Chen et al., 2023a is a publication that discusses the use of an LM to generate a synthetic test suite for evaluating programming tasks</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="FURUTA ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">"Furuta et al., 2024" is a publication that discusses fine-tuning methods and their performance on WebShop. This study or paper by Furuta et al. in 2024 provides insights into the effectiveness of fine-tuning techniques within the context of WebShop, contributing valuable knowledge to the field of Artificial Intelligence and Machine Learning.</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="DFS">
      <data key="d0">ALGORITHM</data>
      <data key="d1">DFS (Depth-First Search) is a search algorithm variant compared with MCTS (Monte Carlo Tree Search) in LATS (Learning and Adaptive Tree Search), used to observe the effects of different search strategies. DFS is mentioned as an alternative search algorithm to MCTS, highlighting its role in comparative studies within the AI and ML community.</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="ZHUANG ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">"Zhuang et al., 2023" refers to a study or paper authored by Zhuang and colleagues in the year 2023. This work is specifically related to search algorithms, including A* and Depth-First Search (DFS). The reference is cited within discussions or contexts that involve these particular search algorithms, indicating its relevance and contribution to the field of algorithmic search methods.</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="IMPROVEMENT LEARNING (IL)">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">IL (Improvement Learning) is a method used in WebShop for training agents, mentioned in comparison with other methods like RL-based training</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="REINFORCEMENT LEARNING (RL)">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">RL (Reinforcement Learning) is a method used in WebShop for training agents, mentioned in comparison with other methods like IL and LATS</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="PROMPTING">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">Prompting refers to methods used to guide the behavior of models like GPT-3.5 in WebShop, including techniques like ReAct and Reflexion</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="SEARCH AND CLICK COMMANDS">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">Search and click commands are part of the preconstructed action space used in WebShop for agent navigation</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="BROWSER FEEDBACK">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">Browser feedback is used in WebShop as part of the observation mechanism for agents</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="REFLECTIONS">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">Reflections are used in WebShop as part of the observation mechanism for agents, providing feedback for decision-making</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="SUCCESS RATE (SR)">
      <data key="d0">METRIC</data>
      <data key="d1">Success Rate (SR) is a metric used in WebShop to indicate the frequency with which the chosen product fulfills all given conditions</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="AVERAGE SCORE">
      <data key="d0">METRIC</data>
      <data key="d1">Average Score is a metric used in WebShop to reflect the percentage of user-specified attributes met by the selected product</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="MCTS (MONTE CARLO TREE SEARCH)">
      <data key="d0">ALGORITHM</data>
      <data key="d1">MCTS (Monte Carlo Tree Search) is a search algorithm used in LATS, providing principled search and observed performance gains over other variants like A* and DFS</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="A*">
      <data key="d0">ALGORITHM</data>
      <data key="d1">A* is a search algorithm that is mentioned as a variant compared to Monte Carlo Tree Search (MCTS). It is specifically noted in the context of LATS, where it is used to observe the effects of different search strategies.</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="TOKEN CONSUMPTION">
      <data key="d0">METRIC</data>
      <data key="d1">Token consumption is a metric used to measure the number of tokens utilized by different methods. In the context of the ablation study of LATS, token consumption specifically serves to evaluate the efficiency of various components. This metric is crucial for understanding and optimizing the performance of different approaches within the AI and ML landscape.</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="TRAJECTORIES">
      <data key="d0">METRIC</data>
      <data key="d1">TRAJECTORIES refer to the paths constructed by search algorithms in LATS to enhance decision-making. They are labeled sequences of environmental observations, thoughts, and actions in a question answering task. In the context of HumanEval and HotPotQA experiments, trajectories denote the number of paths sampled, with a maximum of k=8. Additionally, they are the sampled paths in the search space used in the evaluation of LATS on Game of 24 and HotPotQA.</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f,48e423e2baf2ed485872756f5b4d87d8,594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="LM-BASED HEURISTIC">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">A heuristic used in ToT that prunes branches with low values, removing selection and backpropagation operations</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="LANGUAGE AGENT TREE SEARCH">
      <data key="d0">FRAMEWORK</data>
      <data key="d1">Language Agent Tree Search (LATS) is a framework that unifies reasoning, acting, and planning for enhanced language model (LM) problem-solving. This method integrates these three critical components to improve the overall performance and capabilities of language models, making them more effective in addressing complex tasks.</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad,785ad59c6a37896a4676ec5c1689735f,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="LM AGENTS">
      <data key="d0">AGENT</data>
      <data key="d1">LM agents are language model agents that benefit from the improvements in MCTS and LATS</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="REVERSION PROPERTY">
      <data key="d0">CONCEPT</data>
      <data key="d1">The reversion property allows reverting to earlier states in decision-making environments, which is assumed by LATS</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="SYSTEM-2 LM APPROACHES">
      <data key="d0">CONCEPT</data>
      <data key="d1">SYSTEM-2 LM APPROACHES refer to advanced language model techniques that involve high-level linguistic reasoning and planning. These approaches emphasize several rounds of decision-making and reflection, rather than relying solely on autoregressive generation. By incorporating reasoning and planning, such as those seen in techniques like LATS, System-2 LM approaches aim to enhance the depth and accuracy of language model outputs.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="PERFORMANCE">
      <data key="d0">METRIC</data>
      <data key="d1">Performance is a metric used in the evaluation function to assess an agent's effectiveness. It is also employed to evaluate the effectiveness of different methods in various studies. Additionally, performance is the primary objective considered in the optimization of Advanced Driver Assistance Systems (ADAS) in the referenced paper.</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="SAMPLE COMPLEXITY">
      <data key="d0">METRIC</data>
      <data key="d1">Sample complexity is a metric used to measure the asymptotic token cost of different methods</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="NODES EXPANDED">
      <data key="d0">METRIC</data>
      <data key="d1">Nodes expanded is a metric used to measure the number of nodes expanded by different methods upon success</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="COMPUTATIONAL COST">
      <data key="d0">METRIC</data>
      <data key="d1">Computational cost is a metric used to measure the computational resources required by different methods</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="INFERENCE-TIME COMPUTE COSTS">
      <data key="d0">METRIC</data>
      <data key="d1">Inference-time compute costs refer to the computational costs incurred during the inference phase of language models</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="TAB. 8">
      <data key="d0">REFERENCE</data>
      <data key="d1">TAB. 8 is a reference to a table in the document that shows performance comparisons. Specifically, Table 8 presents the results for HotPotQA, a dataset used for evaluating question-answering systems.</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="TAB. 9">
      <data key="d0">REFERENCE</data>
      <data key="d1">Tab. 9 is a reference to a table in the document that shows sample complexity and token consumption comparisons</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="TAB. 10">
      <data key="d0">REFERENCE</data>
      <data key="d1">Tab. 10 is a reference to a table in the document that shows the cost of different methods on HotPotQA</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="CONCLUSION">
      <data key="d0">SECTION</data>
      <data key="d1">The conclusion section summarizes the contributions and findings of the study</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="LIMITATIONS AND FUTURE DIRECTIONS">
      <data key="d0">SECTION</data>
      <data key="d1">The limitations and future directions section discusses the constraints and potential future work for LATS</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="LANGUAGE MODELS">
      <data key="d0">AGENT</data>
      <data key="d1">Language models (LMs) are AI models capable of understanding and generating human language, used in various decision-making tasks. They are also employed for reasoning, acting, and planning in the LATS algorithm.</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="PROMPTING TECHNIQUES">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Prompting Techniques are methods used for improving the performance of models through effective prompting. These techniques are employed to guide language models in generating responses, utilizing strategies such as ReAct and Reflexion. By leveraging these methods, the performance and accuracy of language models can be significantly enhanced, ensuring more relevant and contextually appropriate outputs.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="TRAJECTORY CONSTRUCTION">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Trajectory construction is the process of creating paths or sequences of actions in LATS to improve decision-making</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="INTERACTION">
      <data key="d0">CONCEPT</data>
      <data key="d1">INTERACTION refers to the communication and engagement between language models or agents and their environments or users. It is a crucial component in Language and Task Systems (LATS) and plays a significant role in real-world applications by facilitating effective exchanges and responses.</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="AUTONOMOUS DECISION-MAKING">
      <data key="d0">CONCEPT</data>
      <data key="d1">Autonomous decision-making is the ability of language models to make decisions without human intervention, enabled by LATS</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="COMPUTATIONAL BUDGET">
      <data key="d0">CONCEPT</data>
      <data key="d1">Computational budget refers to the limit on computational resources available for a task, relevant to the efficiency of LATS</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="GROUND-TRUTH FEEDBACK">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Ground-truth feedback is accurate information used to guide and improve the performance of language models</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="SYSTEM-1 LM APPROACHES">
      <data key="d0">CONCEPT</data>
      <data key="d1">System-1 LM approaches refer to simpler, more intuitive language model techniques that do not involve complex reasoning or planning</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="DANIEL CAMPOS">
      <data key="d0">PERSON</data>
      <data key="d1">Daniel Campos provided useful feedback on earlier versions of the paper.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="NSF GRANT 2106825">
      <data key="d0">FUNDING</data>
      <data key="d1">A grant from the National Science Foundation that supported the work.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="NIFA AWARD 2020-67021-32799">
      <data key="d0">FUNDING</data>
      <data key="d1">An award from the National Institute of Food and Agriculture that supported the work.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="JUMP ARCHES ENDOWMENT">
      <data key="d0">FUNDING</data>
      <data key="d1">An endowment through the Health Care Engineering Systems Center at Illinois and the OSF Foundation that supported the work.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="IBM-ILLINOIS DISCOVERY ACCELERATOR INSTITUTE">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">An institute that supported the work.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="NVIDIA GPUS">
      <data key="d0">HARDWARE</data>
      <data key="d1">NVIDIA GPUs were used at NCSA Delta through allocations from the ACCESS program.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="NCSA DELTA">
      <data key="d0">FACILITY</data>
      <data key="d1">A facility where NVIDIA GPUs were used for the work.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="ACCESS PROGRAM">
      <data key="d0">PROGRAM</data>
      <data key="d1">A program that provided allocations for using NVIDIA GPUs at NCSA Delta.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="MICHAEL AHN">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Ahn is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="ANTHONY BROHAN">
      <data key="d0">PERSON</data>
      <data key="d1">Anthony Brohan is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NOAH BROWN">
      <data key="d0">PERSON</data>
      <data key="d1">Noah Brown is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored the paper "Do as I can, not as I say: Grounding language in robotic affordances," which explores the integration of language understanding with robotic capabilities. Additionally, he has contributed to the paper "Inner monologue: Embodied reasoning through planning with language models," which delves into the use of language models for planning and reasoning in embodied systems. Through these works, Noah Brown has significantly advanced the understanding of how language can be grounded in physical actions and how language models can be utilized for complex planning tasks in AI.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YEVGEN CHEBOTAR">
      <data key="d0">PERSON</data>
      <data key="d1">Yevgen Chebotar is an author known for his contributions to the field of Artificial Intelligence and Machine Learning, particularly in the context of robotics and language models. He has co-authored the paper "Do as I can, not as I say: Grounding language in robotic affordances," which explores the integration of language understanding with robotic capabilities. Additionally, he has contributed to the paper "Inner monologue: Embodied reasoning through planning with language models," which delves into the use of language models for planning and reasoning in embodied systems. Through these works, Yevgen Chebotar has significantly advanced the understanding of how language can be grounded in robotic actions and how language models can be utilized for complex planning tasks.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="OMAR CORTES">
      <data key="d0">PERSON</data>
      <data key="d1">Omar Cortes is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BYRON DAVID">
      <data key="d0">PERSON</data>
      <data key="d1">Byron David is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHELSEA FINN">
      <data key="d0">PERSON</data>
      <data key="d1">Chelsea Finn is an author of the paper "Direct preference optimization: Your language model is secretly a reward model" and also an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHUYUAN FU">
      <data key="d0">PERSON</data>
      <data key="d1">Chuyuan Fu is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances" and also contributed to the paper "Language to rewards for robotic skill synthesis."</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KEERTHANA GOPALAKRISHNAN">
      <data key="d0">PERSON</data>
      <data key="d1">Keerthana Gopalakrishnan is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KAROL HAUSMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Karol Hausman is a notable author in the field of Artificial Intelligence and Machine Learning, contributing significantly to the understanding of language grounding in robotics and embodied reasoning. Hausman has co-authored influential papers such as "Do as I can, not as I say: Grounding language in robotic affordances," which explores the integration of language with robotic capabilities, and "Inner monologue: Embodied reasoning through planning with language models," which delves into the use of language models for planning and reasoning in embodied systems. These works highlight Hausman's expertise in bridging the gap between linguistic commands and robotic actions, as well as advancing the role of language models in AI-driven planning and decision-making processes.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALEX HERZOG">
      <data key="d0">PERSON</data>
      <data key="d1">Alex Herzog is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DANIEL HO">
      <data key="d0">PERSON</data>
      <data key="d1">Daniel Ho is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JASMINE HSU">
      <data key="d0">PERSON</data>
      <data key="d1">Jasmine Hsu is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JULIAN IBARZ">
      <data key="d0">PERSON</data>
      <data key="d1">Julian Ibarz is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BRIAN ICHTER">
      <data key="d0">PERSON</data>
      <data key="d1">Brian Ichter is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances." He is also an author of the paper "Inner monologue: Embodied reasoning through planning with language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALEX IRPAN">
      <data key="d0">PERSON</data>
      <data key="d1">Alex Irpan is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ROSARIO JAUREGUI RUANO">
      <data key="d0">PERSON</data>
      <data key="d1">Rosario Jauregui Ruano is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KYLE JEFFREY">
      <data key="d0">PERSON</data>
      <data key="d1">Kyle Jeffrey is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SALLY JESMONTH">
      <data key="d0">PERSON</data>
      <data key="d1">Sally Jesmonth is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NIKHIL J JOSHI">
      <data key="d0">PERSON</data>
      <data key="d1">Nikhil J Joshi is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RYAN JULIAN">
      <data key="d0">PERSON</data>
      <data key="d1">Ryan Julian is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DMITRY KALASHNIKOV">
      <data key="d0">PERSON</data>
      <data key="d1">Dmitry Kalashnikov is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUHENG KUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yuheng Kuang is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KUANG-HUEI LEE">
      <data key="d0">PERSON</data>
      <data key="d1">Kuang-Huei Lee is an author of several influential papers in the field of Artificial Intelligence and Machine Learning. His notable works include "Do as I can, not as I say: Grounding language in robotic affordances," "Language to rewards for robotic skill synthesis," and "Multimodal web navigation with instruction-finetuned foundation models." These contributions highlight his expertise in grounding language in robotic affordances, synthesizing robotic skills through language, and enhancing web navigation using multimodal instruction-finetuned models.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SERGEY LEVINE">
      <data key="d0">PERSON</data>
      <data key="d1">Sergey Levine is an author of several influential papers in the field of Artificial Intelligence and Machine Learning. His notable works include "Do as I can, not as I say: Grounding language in robotic affordances," which explores the integration of language with robotic capabilities, "Inner monologue: Embodied reasoning through planning with language models," which delves into the use of language models for planning and reasoning, and "The false promise of imitating proprietary LLMs," which critically examines the limitations of imitating proprietary large language models. These contributions highlight his significant role in advancing the understanding and application of AI and ML technologies.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YAO LU">
      <data key="d0">PERSON</data>
      <data key="d1">Yao Lu is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LINDA LUU">
      <data key="d0">PERSON</data>
      <data key="d1">Linda Luu is an author known for her contributions to the field of Artificial Intelligence and Machine Learning. She has co-authored the paper "Do as I can, not as I say: Grounding language in robotic affordances," which explores the integration of language understanding with robotic capabilities. Additionally, she has contributed to the paper "Inner monologue: Embodied reasoning through planning with language models," which delves into the use of language models for planning and reasoning in embodied systems. Her work focuses on the intersection of language processing and robotic functionalities, highlighting her role in advancing research in these areas.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CAROLINA PARADA">
      <data key="d0">PERSON</data>
      <data key="d1">Carolina Parada is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PETER PASTOR">
      <data key="d0">PERSON</data>
      <data key="d1">Peter Pastor is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JORNELL QUIAMBAO">
      <data key="d0">PERSON</data>
      <data key="d1">Jornell Quiambao is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KANISHKA RAO">
      <data key="d0">PERSON</data>
      <data key="d1">Kanishka Rao is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAREK RETTINGHOUSE">
      <data key="d0">PERSON</data>
      <data key="d1">Jarek Rettinghouse is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DIEGO REYES">
      <data key="d0">PERSON</data>
      <data key="d1">Diego Reyes is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PIERRE SERMANET">
      <data key="d0">PERSON</data>
      <data key="d1">Pierre Sermanet is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances" and the paper "Inner monologue: Embodied reasoning through planning with language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NICOLAS SIEVERS">
      <data key="d0">PERSON</data>
      <data key="d1">Nicolas Sievers is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CLAYTON TAN">
      <data key="d0">PERSON</data>
      <data key="d1">Clayton Tan is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALEXANDER TOSHEV">
      <data key="d0">PERSON</data>
      <data key="d1">Alexander Toshev is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VINCENT VANHOUCKE">
      <data key="d0">PERSON</data>
      <data key="d1">Vincent Vanhoucke is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FEI XIA">
      <data key="d0">PERSON</data>
      <data key="d1">Fei Xia is an author of several notable papers in the field of Artificial Intelligence and Machine Learning. These include "Chain-of-thought prompting elicits reasoning in large language models," "Do as I can, not as I say: Grounding language in robotic affordances," and "PaLM-E: An embodied multimodal language model," which was presented at ICML in 2023.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TED XIAO">
      <data key="d0">PERSON</data>
      <data key="d1">Ted Xiao is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored significant papers such as "Do as I can, not as I say: Grounding language in robotic affordances" and "Inner monologue: Embodied reasoning through planning with language models." These works highlight his focus on integrating language models with robotic systems and embodied reasoning, showcasing his expertise in grounding language in practical, robotic applications and planning through language models.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PENG XU">
      <data key="d0">PERSON</data>
      <data key="d1">Peng Xu is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SICHUN XU">
      <data key="d0">PERSON</data>
      <data key="d1">Sichun Xu is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MENGYUAN YAN">
      <data key="d0">PERSON</data>
      <data key="d1">Mengyuan Yan is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANDY ZENG">
      <data key="d0">PERSON</data>
      <data key="d1">Andy Zeng is an author of the papers "Do as I can, not as I say: Grounding language in robotic affordances" and "Inner monologue: Embodied reasoning through planning with language models." These works highlight his contributions to the field of Artificial Intelligence and Machine Learning, particularly in the areas of robotic affordances and the integration of language models for embodied reasoning and planning.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JACOB AUSTIN">
      <data key="d0">PERSON</data>
      <data key="d1">Jacob Austin is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He co-authored the paper "PaLM: Scaling language modeling with pathways," which was published in the Journal of Machine Learning Research (JMLR) in 2023. Additionally, he is credited with authoring the paper "Program synthesis with large language models." These works highlight his involvement in advancing language modeling and program synthesis using large language models, marking him as a significant contributor to contemporary AI research.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AUGUSTUS ODENA">
      <data key="d0">PERSON</data>
      <data key="d1">Augustus Odena is an author of the paper "Program synthesis with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MAXWELL NYE">
      <data key="d0">PERSON</data>
      <data key="d1">Maxwell Nye is an author of the paper "Program synthesis with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MAARTEN BOSMA">
      <data key="d0">PERSON</data>
      <data key="d1">Maarten Bosma is an author mentioned in the text, known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored several significant papers, including "Chain-of-thought prompting elicits reasoning in large language models," "PaLM: Scaling language modeling with pathways" published in JMLR in 2023, and "Program synthesis with large language models." His work focuses on advancing the capabilities of large language models and their applications in reasoning and program synthesis.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HENRYK MICHALEWSKI">
      <data key="d0">PERSON</data>
      <data key="d1">Henryk Michalewski is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He co-authored the paper "PaLM: Scaling language modeling with pathways," which was published in the Journal of Machine Learning Research (JMLR) in 2023. Additionally, he has contributed to the paper "Program synthesis with large language models," showcasing his expertise in leveraging large language models for advanced AI applications.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAVID DOHAN">
      <data key="d0">PERSON</data>
      <data key="d1">David Dohan is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He co-authored the paper "PaLM: Scaling language modeling with pathways," which was published in the Journal of Machine Learning Research (JMLR) in 2023. Additionally, he has contributed to the paper "Program synthesis with large language models." His work focuses on advancing language modeling and program synthesis, highlighting his significant role in the development of large language models and their applications.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ELLEN JIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Ellen Jiang is an author of the paper "Program synthesis with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CARRIE CAI">
      <data key="d0">PERSON</data>
      <data key="d1">Carrie Cai is an author of the paper "Program synthesis with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MICHAEL TERRY">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Terry is an author of the paper "Program synthesis with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QUOC LE">
      <data key="d0">PERSON</data>
      <data key="d1">Quoc Le is an author mentioned in the text, known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored significant papers such as "Least-to-most prompting enables complex reasoning in large language models" and "Program synthesis with large language models." These works highlight his involvement in advancing the capabilities of large language models, particularly in the areas of complex reasoning and program synthesis.</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,4ae237a491bc8a84cc720e40c59a7464,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHARLES SUTTON">
      <data key="d0">PERSON</data>
      <data key="d1">Charles Sutton is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He co-authored the paper "PaLM: Scaling language modeling with pathways," which was published in the Journal of Machine Learning Research (JMLR) in 2023. Additionally, he is an author of the paper "Program synthesis with large language models." His work focuses on advancing language modeling and program synthesis, highlighting his significant role in the AI and ML research community.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BOWEN BAKER">
      <data key="d0">PERSON</data>
      <data key="d1">Bowen Baker is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ILGE AKKAYA">
      <data key="d0">PERSON</data>
      <data key="d1">Ilge Akkaya is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PETER ZHOKHOV">
      <data key="d0">PERSON</data>
      <data key="d1">Peter Zhokhov is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOOST HUIZINGA">
      <data key="d0">PERSON</data>
      <data key="d1">Joost Huizinga is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIE TANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jie Tang is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ADRIEN ECOFFET">
      <data key="d0">PERSON</data>
      <data key="d1">Adrien Ecoffet is an influential figure in the field of Artificial Intelligence and Machine Learning, known for his contributions to advancing the understanding of AI safety and learning methodologies. He is an author of the paper "Open questions in creating safe open-ended AI: Tensions between control and creativity," which explores the critical balance between maintaining control over AI systems and fostering their creative potential. Additionally, he has co-authored the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos," which delves into innovative techniques for training AI by leveraging vast amounts of unlabeled video data. Through his research, Adrien Ecoffet significantly contributes to the ongoing discourse on AI safety and the development of more autonomous and capable AI systems.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BRANDON HOUGHTON">
      <data key="d0">PERSON</data>
      <data key="d1">Brandon Houghton is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored the paper "MineRL: A large-scale dataset of Minecraft demonstrations," which focuses on creating a comprehensive dataset for AI research using Minecraft gameplay. Additionally, he has contributed to the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos," which explores the potential of using video pretraining to enhance machine learning models by leveraging unlabeled online video content. Through these works, Brandon Houghton has significantly impacted the development of innovative methods for training AI systems.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RAUL SAMPEDRO">
      <data key="d0">PERSON</data>
      <data key="d1">Raul Sampedro is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JEFF CLUNE">
      <data key="d0">PERSON</data>
      <data key="d1">Jeff Clune is a prominent researcher in the field of Artificial Intelligence and Machine Learning, particularly known for his work on the Automated Design of Agentic Systems. He is affiliated with the University of British Columbia, the Vector Institute, and holds a prestigious Canada CIFAR AI Chair. Jeff Clune has made significant contributions to the academic community through his extensive authorship of influential papers. His notable works include "Designing neural networks through neuroevolution," "Intelligent go-explore: Standing on the shoulders of giant foundation models," "OMNI: Open-endedness via models of human notions of interestingness," "Thought Cloning: Learning to think while acting by imitating human thinking," and "Video pretraining (VPT): Learning to act by watching unlabeled online videos." Additionally, he has authored "Poet: open-ended coevolution of environments and their optimized solutions," "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions," "Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artificial intelligence," and "Open questions in creating safe open-ended AI: Tensions between control and creativity." Jeff Clune's work is instrumental in advancing the understanding and development of AI and ML, making him a key figure in the field.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,1b1399c76420a477c0c97893d258ae69,2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,4ae237a491bc8a84cc720e40c59a7464,6109537356a2ce2339f77c827aa3668e,c3d0436082aada237ee4bee645f16059,cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MACIEJ BESTA">
      <data key="d0">PERSON</data>
      <data key="d1">Maciej Besta is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NILS BLACH">
      <data key="d0">PERSON</data>
      <data key="d1">Nils Blach is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALES KUBICEK">
      <data key="d0">PERSON</data>
      <data key="d1">Ales Kubicek is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ROBERT GERSTENBERGER">
      <data key="d0">PERSON</data>
      <data key="d1">Robert Gerstenberger is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LUKAS GIANINAZZI">
      <data key="d0">PERSON</data>
      <data key="d1">Lukas Gianinazzi is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOANNA GAJDA">
      <data key="d0">PERSON</data>
      <data key="d1">Joanna Gajda is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TOMASZ LEHMANN">
      <data key="d0">PERSON</data>
      <data key="d1">Tomasz Lehmann is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MICHAL PODSTAWSKI">
      <data key="d0">PERSON</data>
      <data key="d1">Michal Podstawski is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HUBERT NIEWIADOMSKI">
      <data key="d0">PERSON</data>
      <data key="d1">Hubert Niewiadomski is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PIOTR NYCZYK">
      <data key="d0">PERSON</data>
      <data key="d1">Piotr Nyczyk is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TORSTEN HOEFLER">
      <data key="d0">PERSON</data>
      <data key="d1">Torsten Hoefler is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAMUEL R BOWMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Samuel R Bowman is an author of the paper "A large annotated corpus for learning natural language inference."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GABOR ANGELI">
      <data key="d0">PERSON</data>
      <data key="d1">Gabor Angeli is an author of the paper "A large annotated corpus for learning natural language inference."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHRISTOPHER POTTS">
      <data key="d0">PERSON</data>
      <data key="d1">Christopher Potts is an author of the paper "A large annotated corpus for learning natural language inference."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHRISTOPHER D MANNING">
      <data key="d0">PERSON</data>
      <data key="d1">Christopher D. Manning is an influential author in the field of Artificial Intelligence and Machine Learning. He has contributed significantly to the academic community with his work, including authoring the paper "A large annotated corpus for learning natural language inference" and the paper "Direct preference optimization: Your language model is secretly a reward model." His research focuses on advancing natural language processing and understanding, making him a key player in the AI and ML landscape.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,4ae237a491bc8a84cc720e40c59a7464,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TOM B. BROWN">
      <data key="d0">PERSON</data>
      <data key="d1">Tom B. Brown is an author of the paper "Language models are few-shot learners."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BENJAMIN MANN">
      <data key="d0">PERSON</data>
      <data key="d1">Benjamin Mann is an author of the paper "Language models are few-shot learners."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NICK RYDER">
      <data key="d0">PERSON</data>
      <data key="d1">Nick Ryder is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored significant papers, including "Evaluating large language models trained on code," published on arXiv in 2021, and "Language models are few-shot learners." His work focuses on the evaluation and capabilities of large language models, particularly in the context of code and few-shot learning scenarios.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MELANIE SUBBIAH">
      <data key="d0">PERSON</data>
      <data key="d1">Melanie Subbiah is an author of the paper "Language models are few-shot learners."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="JARED KAPLAN">
      <data key="d0">PERSON</data>
      <data key="d1">Jared Kaplan is an influential figure in the field of Artificial Intelligence and Machine Learning, known for his contributions to the development and evaluation of large language models. He is an author of the paper "Evaluating Large Language Models Trained on Code," which was published on arXiv in 2021. Additionally, he co-authored the significant paper "Language Models are Few-Shot Learners," further establishing his expertise and impact in the domain of AI and ML research.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126,7de66b94cf868b37b1df51dc545c415f</data>
    </node>
    <node id="PRAFULLA DHARIWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Prafulla Dhariwal is an author of the paper "Language models are few-shot learners."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="ARVIND NEELAKANTAN">
      <data key="d0">PERSON</data>
      <data key="d1">Arvind Neelakantan is an author of the paper "Language models are few-shot learners."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="PRANAV SHYAM">
      <data key="d0">PERSON</data>
      <data key="d1">Pranav Shyam is an author of the paper "Language models are few-shot learners."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="GIRISH SASTRY">
      <data key="d0">PERSON</data>
      <data key="d1">Girish Sastry is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He co-authored the paper "Evaluating large language models trained on code," which was published on arXiv in 2021. Additionally, he is an author of the influential paper "Language models are few-shot learners." These works highlight his expertise in evaluating and understanding the capabilities of large language models, particularly in the context of code and few-shot learning scenarios.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="AMANDA ASKELL">
      <data key="d0">PERSON</data>
      <data key="d1">Amanda Askell is an author known for her contributions to the field of Artificial Intelligence and Machine Learning. She has co-authored significant papers, including "Language models are few-shot learners" and "Constitutional AI: Harmlessness from AI Feedback." Her work focuses on advancing the understanding and development of AI systems, particularly in the areas of language models and AI safety.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7de66b94cf868b37b1df51dc545c415f</data>
    </node>
    <node id="SANDHINI AGARWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Sandhini Agarwal is an author of the paper "Language models are few-shot learners."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="ARIEL HERBERT-VOSS">
      <data key="d0">PERSON</data>
      <data key="d1">Ariel Herbert-Voss is a notable figure in the field of Artificial Intelligence and Machine Learning, recognized for her contributions to the development and evaluation of large language models. She is an author of the influential paper "Evaluating large language models trained on code," which was published on arXiv in 2021. Additionally, she co-authored the significant work "Language models are few-shot learners," further establishing her expertise and impact in the domain. Through her research, Ariel Herbert-Voss has contributed to advancing the understanding and capabilities of language models, particularly in their application to code and few-shot learning scenarios.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="GRETCHEN KRUEGER">
      <data key="d0">PERSON</data>
      <data key="d1">Gretchen Krueger is an author known for her contributions to the field of Artificial Intelligence and Machine Learning. She co-authored the paper "Evaluating large language models trained on code," which was published on arXiv in 2021. Additionally, she contributed to the influential paper "Language models are few-shot learners." Her work focuses on the capabilities and evaluation of large language models, particularly in the context of code and few-shot learning scenarios.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="AMODEI">
      <data key="d0">PERSON</data>
      <data key="d1">Amodei is an author of the paper "Language models are few-shot learners" presented at NeurIPS in 2020</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="NEURIPS">
      <data key="d0">CONFERENCE</data>
      <data key="d1">NeurIPS, also known as the Conference on Neural Information Processing Systems, is a prominent conference in the field of artificial intelligence and machine learning. It serves as a platform for presenting groundbreaking research and innovative papers. Notable papers presented at NeurIPS include "Language models are few-shot learners" in 2020, "LLaMA: Open and efficient foundation language models," "AdaPlanner: Adaptive planning from feedback with language models," "Self-refine: Iterative refinement with self-feedback," "Learning universal policies via text-guided video generation," and "Large language models are zero-shot reasoners." This conference is a key event for researchers and practitioners to share advancements and collaborate on cutting-edge AI and ML technologies.</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,68e5573b596d253a03047b1e41988598,7a48515e86161237c03c9a8373197126,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="MURRAY CAMPBELL">
      <data key="d0">PERSON</data>
      <data key="d1">Murray Campbell is an author of the paper "Deep blue" published in Artificial Intelligence in 2002</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="A JOSEPH HOANE JR">
      <data key="d0">PERSON</data>
      <data key="d1">A Joseph Hoane Jr is an author of the paper "Deep blue" published in Artificial Intelligence in 2002</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="FENG-HSIUNG HSU">
      <data key="d0">PERSON</data>
      <data key="d1">Feng-hsiung Hsu is an author of the paper "Deep blue" published in Artificial Intelligence in 2002</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ARTIFICIAL INTELLIGENCE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Artificial Intelligence is the journal where the paper "Deep blue" was published in 2002</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="BEI CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Bei Chen is an author of the paper "CodeT: Code generation with generated tests" presented at ICLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="FENGJI ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Fengji Zhang is an author of the paper "CodeT: Code generation with generated tests" presented at ICLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ANH NGUYEN">
      <data key="d0">PERSON</data>
      <data key="d1">Anh Nguyen is an author of the Phi-3 technical report and also contributed to the paper "CodeT: Code generation with generated tests," which was presented at the International Conference on Learning Representations (ICLR) in 2023.</data>
      <data key="d2">7a48515e86161237c03c9a8373197126,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="DAOGUANG ZAN">
      <data key="d0">PERSON</data>
      <data key="d1">Daoguang Zan is an author of the paper "CodeT: Code generation with generated tests" presented at ICLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ZEQI LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Zeqi Lin is an accomplished author in the field of Artificial Intelligence and Machine Learning. Notably, Lin contributed to the Phi-3 technical report and co-authored the paper "CodeT: Code generation with generated tests," which was presented at the International Conference on Learning Representations (ICLR) in 2023. Lin's work demonstrates a strong focus on advancing code generation techniques, highlighting their expertise and influence within the AI and ML research community.</data>
      <data key="d2">7a48515e86161237c03c9a8373197126,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="JIAN-GUANG LOU">
      <data key="d0">PERSON</data>
      <data key="d1">Jian-Guang Lou is an author of the paper "CodeT: Code generation with generated tests" presented at ICLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="WEIZHU CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Weizhu Chen is an accomplished author in the field of Artificial Intelligence and Machine Learning. He has contributed to several significant works, including the Phi-3 technical report, the paper "Agieval: A human-centric benchmark for evaluating foundation models," and the paper "CodeT: Code generation with generated tests," which was presented at the International Conference on Learning Representations (ICLR) in 2023.</data>
      <data key="d2">7a48515e86161237c03c9a8373197126,dd9a46950237e49ef9b1c7ef08e08d42,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ICLR">
      <data key="d0">CONFERENCE</data>
      <data key="d1">ICLR, the International Conference on Learning Representations, is a prominent conference in the field of artificial intelligence and machine learning. In 2023, several significant papers were presented at ICLR, including "CodeT: Code generation with generated tests," "ALFWorld: Aligning text and embodied environments for interactive learning," "ToolLLM: Facilitating large language models to master 16000+ real-world APIs," "Reinforcement learning on web interfaces using workflow-guided exploration," "Multimodal web navigation with instruction-finetuned foundation models," and "Large language models cannot self-correct reasoning yet." These presentations highlight ICLR's role as a key platform for disseminating cutting-edge research and advancements in AI and ML.</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,68e5573b596d253a03047b1e41988598,7a48515e86161237c03c9a8373197126,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="MARK CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Mark Chen is an author of the paper "Evaluating Large Language Models Trained on Code" published on arXiv in 2021. Additionally, he is also an author of the paper "Training Verifiers to Solve Math Word Problems," which was published on arXiv in the same year.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126,7de66b94cf868b37b1df51dc545c415f,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JERRY TWOREK">
      <data key="d0">PERSON</data>
      <data key="d1">Jerry Tworek is an author of multiple influential papers in the field of Artificial Intelligence and Machine Learning. Notably, he co-authored the paper "Evaluating Large Language Models Trained on Code," which was published on arXiv in 2021. Additionally, he contributed to the paper "Training Verifiers to Solve Math Word Problems," also published on arXiv in 2021. His work focuses on the evaluation and training of AI models, particularly in the context of code and mathematical problem-solving.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126,7de66b94cf868b37b1df51dc545c415f,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HEEWOO JUN">
      <data key="d0">PERSON</data>
      <data key="d1">Heewoo Jun is an author of multiple research papers, including "Evaluating Large Language Models Trained on Code" and "Training Verifiers to Solve Math Word Problems," both published on arXiv in 2021.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126,7de66b94cf868b37b1df51dc545c415f,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QIMING YUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Qiming Yuan is an author of the paper "Evaluating Large Language Models Trained on Code," which was published on arXiv in 2021.</data>
      <data key="d2">7a48515e86161237c03c9a8373197126,7de66b94cf868b37b1df51dc545c415f</data>
    </node>
    <node id="HENRIQUE PONDE">
      <data key="d0">PERSON</data>
      <data key="d1">Henrique Ponde is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="HARRISON EDWARDS">
      <data key="d0">PERSON</data>
      <data key="d1">Harrison Edwards is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="YURA BURDA">
      <data key="d0">PERSON</data>
      <data key="d1">Yura Burda is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="NICHOLAS JOSEPH">
      <data key="d0">PERSON</data>
      <data key="d1">Nicholas Joseph is an author of the paper "Evaluating Large Language Models Trained on Code," which was published on arXiv in 2021.</data>
      <data key="d2">7a48515e86161237c03c9a8373197126,7de66b94cf868b37b1df51dc545c415f</data>
    </node>
    <node id="GREG BROCKMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Greg Brockman is an author of the paper "Evaluating Large Language Models Trained on Code," which was published on arXiv in 2021.</data>
      <data key="d2">7a48515e86161237c03c9a8373197126,7de66b94cf868b37b1df51dc545c415f</data>
    </node>
    <node id="ALEX RAY">
      <data key="d0">PERSON</data>
      <data key="d1">Alex Ray is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="RAUL PURI">
      <data key="d0">PERSON</data>
      <data key="d1">Raul Puri is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MICHAEL PETROV">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Petrov is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="HEIDY KHLAAF">
      <data key="d0">PERSON</data>
      <data key="d1">Heidy Khlaaf is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="PAMELA MISHKIN">
      <data key="d0">PERSON</data>
      <data key="d1">Pamela Mishkin is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="BROOKE CHAN">
      <data key="d0">PERSON</data>
      <data key="d1">Brooke Chan is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SCOTT GRAY">
      <data key="d0">PERSON</data>
      <data key="d1">Scott Gray is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MIKHAIL PAVLOV">
      <data key="d0">PERSON</data>
      <data key="d1">Mikhail Pavlov is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ALETHEA POWER">
      <data key="d0">PERSON</data>
      <data key="d1">Alethea Power is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="LUKASZ KAISER">
      <data key="d0">PERSON</data>
      <data key="d1">Lukasz Kaiser is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021, as well as the paper "Training verifiers to solve math word problems" also published on arXiv in 2021.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MOHAMMAD BAVARIAN">
      <data key="d0">PERSON</data>
      <data key="d1">Mohammad Bavarian is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021, as well as the paper "Training verifiers to solve math word problems" also published on arXiv in 2021.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CLEMENS WINTER">
      <data key="d0">PERSON</data>
      <data key="d1">Clemens Winter is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="PHILIPPE TILLET">
      <data key="d0">PERSON</data>
      <data key="d1">Philippe Tillet is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="FELIPE PETROSKI SUCH">
      <data key="d0">PERSON</data>
      <data key="d1">Felipe Petroski Such is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="DAVID W. CUMMINGS">
      <data key="d0">PERSON</data>
      <data key="d1">David W. Cummings is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MATTHIAS PLAPPERT">
      <data key="d0">PERSON</data>
      <data key="d1">Matthias Plappert is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021. He is also an author of the paper "Training verifiers to solve math word problems," which was published on arXiv in the same year.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FOTIOS CHANTZIS">
      <data key="d0">PERSON</data>
      <data key="d1">Fotios Chantzis is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ELIZABETH BARNES">
      <data key="d0">PERSON</data>
      <data key="d1">Elizabeth Barnes is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="WILLIAM H. GUSS">
      <data key="d0">PERSON</data>
      <data key="d1">William H. Guss is a notable figure in the field of Artificial Intelligence and Machine Learning, recognized for his contributions to research and development. He is an author of the paper "Evaluating large language models trained on code," which was published on arXiv in 2021. Additionally, he has contributed to the paper "MineRL: A large-scale dataset of Minecraft demonstrations," showcasing his involvement in creating significant datasets and advancing the understanding of AI through practical applications. His work spans across evaluating the capabilities of large language models and leveraging gaming environments like Minecraft to gather valuable data for AI research.</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ALEX NICHOL">
      <data key="d0">PERSON</data>
      <data key="d1">Alex Nichol is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="IGOR BABUSCHKIN">
      <data key="d0">PERSON</data>
      <data key="d1">Igor Babuschkin is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SUCHIR BALAJI">
      <data key="d0">PERSON</data>
      <data key="d1">Suchir Balaji is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored the paper "Evaluating large language models trained on code," which was published on arXiv in 2021. Additionally, he has contributed to the paper "WebGPT: Browser-assisted question-answering with human feedback." These works highlight his involvement in advancing the understanding and application of large language models and enhancing AI-driven question-answering systems through human feedback.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SHANTANU JAIN">
      <data key="d0">PERSON</data>
      <data key="d1">Shantanu Jain is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He co-authored the paper "Evaluating large language models trained on code," which was published on arXiv in 2021. Additionally, he contributed to the paper "WebGPT: Browser-assisted question-answering with human feedback." These works highlight his involvement in advancing the understanding and application of large language models and enhancing question-answering systems through human feedback.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ANDREW CARR">
      <data key="d0">PERSON</data>
      <data key="d1">Andrew Carr is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="JAN LEIKE">
      <data key="d0">PERSON</data>
      <data key="d1">Jan Leike is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="JOSHUA ACHIAM">
      <data key="d0">PERSON</data>
      <data key="d1">Joshua Achiam is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="VEDANT MISRA">
      <data key="d0">PERSON</data>
      <data key="d1">Vedant Misra is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021Vedant Misra is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="EVAN MORIKAWA">
      <data key="d0">PERSON</data>
      <data key="d1">Evan Morikawa is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ALEC RADFORD">
      <data key="d0">PERSON</data>
      <data key="d1">Alec Radford is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MATTHEW M. KNIGHT">
      <data key="d0">PERSON</data>
      <data key="d1">Matthew M. Knight is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MILES BRUNDAGE">
      <data key="d0">PERSON</data>
      <data key="d1">Miles Brundage is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MIRA MURATI">
      <data key="d0">PERSON</data>
      <data key="d1">Mira Murati is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="KATIE MAYER">
      <data key="d0">PERSON</data>
      <data key="d1">Katie Mayer is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="PETER WELINDER">
      <data key="d0">PERSON</data>
      <data key="d1">Peter Welinder is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="BOB MCGREW">
      <data key="d0">PERSON</data>
      <data key="d1">Bob McGrew is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="DARIO AMODEI">
      <data key="d0">PERSON</data>
      <data key="d1">Dario Amodei is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SAM MCCANDLISH">
      <data key="d0">PERSON</data>
      <data key="d1">Sam McCandlish is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ILYA SUTSKEVER">
      <data key="d0">PERSON</data>
      <data key="d1">Ilya Sutskever is a prominent figure in the field of Artificial Intelligence and Machine Learning, known for his significant contributions to the development and evaluation of advanced models. He is an author of several influential papers, including "Evaluating large language models trained on code" published on arXiv in 2021, "Imagenet classification with deep convolutional neural networks," "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm," "Mastering the game of Go with deep neural networks and tree search," and "RL^2: Fast reinforcement learning via slow reinforcement learning." His work spans various aspects of AI, from deep learning and neural networks to reinforcement learning, showcasing his extensive expertise and impact on the field.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,2d4672dfb7bd4283f0b5f23ab4f26653,6109537356a2ce2339f77c827aa3668e,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="WOJCIECH ZAREMBA">
      <data key="d0">PERSON</data>
      <data key="d1">Wojciech Zaremba is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="WENHU CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Wenhu Chen is an author of the paper "Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks" published in TMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="XUEGUANG MA">
      <data key="d0">PERSON</data>
      <data key="d1">Xueguang Ma is an author of the paper "Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks" published in TMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="XINYI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Xinyi Wang is an author of the paper "Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks" published in TMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="WILLIAM W. COHEN">
      <data key="d0">PERSON</data>
      <data key="d1">William W. Cohen is an author of the paper "Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks" published in TMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="TMLR">
      <data key="d0">PUBLICATION</data>
      <data key="d1">TMLR is the journal where the paper "Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks" was published in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="AAKANKSHA CHOWDHERY">
      <data key="d0">PERSON</data>
      <data key="d1">Aakanksha Chowdhery is an author of several influential papers in the field of Artificial Intelligence and Machine Learning. Her notable works include "Challenging big-bench tasks and whether chain-of-thought can solve them," "PaLM: Scaling language modeling with pathways" published in JMLR in 2023, and "Self-consistency improves chain of thought reasoning in language models."</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,7a48515e86161237c03c9a8373197126,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHARAN NARANG">
      <data key="d0">PERSON</data>
      <data key="d1">Sharan Narang is an author mentioned in the text, known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored the paper "PaLM: Scaling language modeling with pathways," which was published in the Journal of Machine Learning Research (JMLR) in 2023. Additionally, he is an author of the paper "Self-consistency improves chain of thought reasoning in language models," further highlighting his involvement in advancing language modeling techniques.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,7a48515e86161237c03c9a8373197126,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="JACOB DEVLIN">
      <data key="d0">PERSON</data>
      <data key="d1">Jacob Devlin is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="GAURAV MISHRA">
      <data key="d0">PERSON</data>
      <data key="d1">Gaurav Mishra is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ADAM ROBERTS">
      <data key="d0">PERSON</data>
      <data key="d1">Adam Roberts is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="PAUL BARHAM">
      <data key="d0">PERSON</data>
      <data key="d1">Paul Barham is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="HYUNG WON CHUNG">
      <data key="d0">PERSON</data>
      <data key="d1">Hyung Won Chung is an author of several notable papers in the field of Artificial Intelligence and Machine Learning. These include "Challenging big-bench tasks and whether chain-of-thought can solve them," "Language models are multilingual chain-of-thought reasoners," and "PaLM: Scaling language modeling with pathways," which was published in the Journal of Machine Learning Research (JMLR) in 2023.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,7a48515e86161237c03c9a8373197126,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="SEBASTIAN GEHRMANN">
      <data key="d0">PERSON</data>
      <data key="d1">Sebastian Gehrmann is an author of the paper "Challenging big-bench tasks and whether chain-of-thought can solve them." Additionally, he co-authored the paper "PaLM: Scaling language modeling with pathways," which was published in the Journal of Machine Learning Research (JMLR) in 2023.</data>
      <data key="d2">7a48515e86161237c03c9a8373197126,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JOSHUA MAYNEZ">
      <data key="d0">PERSON</data>
      <data key="d1">Joshua Maynez is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ABHISHEK RAO">
      <data key="d0">PERSON</data>
      <data key="d1">Abhishek Rao is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="PARKER BARNES">
      <data key="d0">PERSON</data>
      <data key="d1">Parker Barnes is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="YI TAY">
      <data key="d0">PERSON</data>
      <data key="d1">Yi Tay is an author of several influential papers in the field of Artificial Intelligence and Machine Learning. Notably, Yi Tay contributed to the paper "Challenging big-bench tasks and whether chain-of-thought can solve them," which explores the efficacy of chain-of-thought reasoning in addressing complex AI tasks. Additionally, Yi Tay co-authored "Language models are multilingual chain-of-thought reasoners," highlighting the capabilities of language models in multilingual contexts. Furthermore, Yi Tay is an author of the paper "PaLM: Scaling language modeling with pathways," published in the Journal of Machine Learning Research (JMLR) in 2023, which discusses advancements in scaling language models using the Pathways framework.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,7a48515e86161237c03c9a8373197126,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="NOAM SHAZEER">
      <data key="d0">PERSON</data>
      <data key="d1">Noam Shazeer is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="VINODKUMAR PRABHAKARAN">
      <data key="d0">PERSON</data>
      <data key="d1">Vinodkumar Prabhakaran is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="EMILY REIF">
      <data key="d0">PERSON</data>
      <data key="d1">Emily Reif is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="NAN DU">
      <data key="d0">PERSON</data>
      <data key="d1">Nan Du is an author known for contributing to significant advancements in language modeling. In 2023, Nan Du co-authored the paper "PaLM: Scaling language modeling with pathways," which was published in the Journal of Machine Learning Research (JMLR). Additionally, Nan Du is also credited as an author of the paper "React: Synergizing reasoning and acting in language models." These works highlight Nan Du's involvement in cutting-edge research aimed at enhancing the capabilities of language models through innovative approaches.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="BEN HUTCHINSON">
      <data key="d0">PERSON</data>
      <data key="d1">Ben Hutchinson is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="REINER POPE">
      <data key="d0">PERSON</data>
      <data key="d1">Reiner Pope is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="JAMES BRADBURY">
      <data key="d0">PERSON</data>
      <data key="d1">James Bradbury is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MICHAEL ISARD">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Isard is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="GUY GUR-ARI">
      <data key="d0">PERSON</data>
      <data key="d1">Guy Gur-Ari is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="PENGCHENG YIN">
      <data key="d0">PERSON</data>
      <data key="d1">Pengcheng Yin is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="TOJU DUKE">
      <data key="d0">PERSON</data>
      <data key="d1">Toju Duke is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ANSELM LEVSKAYA">
      <data key="d0">PERSON</data>
      <data key="d1">Anselm Levskaya is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SANJAY GHEMAWAT">
      <data key="d0">PERSON</data>
      <data key="d1">Sanjay Ghemawat is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SUNIPA DEV">
      <data key="d0">PERSON</data>
      <data key="d1">Sunipa Dev is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="XAVIER GARCIA">
      <data key="d0">PERSON</data>
      <data key="d1">Xavier Garcia is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="KEVIN ROBINSON">
      <data key="d0">PERSON</data>
      <data key="d1">Kevin Robinson is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="LIAM FEDUS">
      <data key="d0">PERSON</data>
      <data key="d1">Liam Fedus is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="DENNY ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Denny Zhou is an author extensively mentioned in the text, known for his significant contributions to the field of large language models and their reasoning capabilities. He has authored several influential papers, including "Instruction-following evaluation for large language models," "Challenging big-bench tasks and whether chain-of-thought can solve them," "Language models are multilingual chain-of-thought reasoners," "Large language models cannot self-correct reasoning yet," and "PaLM: Scaling language modeling with pathways," which was published in JMLR in 2023. Additionally, he has contributed to the papers "Take a step back: Evoking reasoning via abstraction in large language models," "Self-consistency improves chain of thought reasoning in language models," and "Chain-of-thought prompting elicits reasoning in large language models." His work primarily focuses on enhancing the reasoning abilities of large language models through various innovative approaches.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,68e5573b596d253a03047b1e41988598,7a48515e86161237c03c9a8373197126,8180bf20b7577f3eee40df5991e2886d,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="DAPHNE IPPOLITO">
      <data key="d0">PERSON</data>
      <data key="d1">Daphne Ippolito is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="DAVID LUAN">
      <data key="d0">PERSON</data>
      <data key="d1">David Luan is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="HYEONTAEK LIM">
      <data key="d0">PERSON</data>
      <data key="d1">Hyeontaek Lim is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="BARRET ZOPH">
      <data key="d0">PERSON</data>
      <data key="d1">Barret Zoph is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ALEXANDER SPIRIDONOV">
      <data key="d0">PERSON</data>
      <data key="d1">Alexander Spiridonov is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="RYAN SEPASSI">
      <data key="d0">PERSON</data>
      <data key="d1">Ryan Sepassi is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SHIVANI AGRAWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Shivani Agrawal is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MARK OMERNICK">
      <data key="d0">PERSON</data>
      <data key="d1">Mark Omernick is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ANDREW M. DAI">
      <data key="d0">PERSON</data>
      <data key="d1">Andrew M. Dai is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="THANUMALAYAN SANKARANARAYANA PILLAI">
      <data key="d0">PERSON</data>
      <data key="d1">Thanumalayan Sankaranarayana Pillai is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MARIE PELLAT">
      <data key="d0">PERSON</data>
      <data key="d1">Marie Pellat is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="AITOR LEWKOWYCZ">
      <data key="d0">PERSON</data>
      <data key="d1">Aitor Lewkowycz is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ERICA MOREIRA">
      <data key="d0">PERSON</data>
      <data key="d1">Erica Moreira is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="REWON CHILD">
      <data key="d0">PERSON</data>
      <data key="d1">Rewon Child is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="OLEKSANDR POLOZOV">
      <data key="d0">PERSON</data>
      <data key="d1">Oleksandr Polozov is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="KATHERINE LEE">
      <data key="d0">PERSON</data>
      <data key="d1">Katherine Lee is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ZONGWEI ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Zongwei Zhou is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="XUEZHI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Xuezhi Wang is an author mentioned in the text and has contributed to several significant papers in the field of Artificial Intelligence and Machine Learning. Notably, Xuezhi Wang is an author of the paper "Language models are multilingual chain-of-thought reasoners." Additionally, Wang co-authored the paper "PaLM: Scaling language modeling with pathways," which was published in the Journal of Machine Learning Research (JMLR) in 2023. Furthermore, Xuezhi Wang has contributed to the papers "Self-consistency improves chain of thought reasoning in language models" and "Chain-of-thought prompting elicits reasoning in large language models."</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,7a48515e86161237c03c9a8373197126,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="BRENNAN SAETA">
      <data key="d0">PERSON</data>
      <data key="d1">Brennan Saeta is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MARK DIAZ">
      <data key="d0">PERSON</data>
      <data key="d1">Mark Diaz is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ORHAN FIRAT">
      <data key="d0">PERSON</data>
      <data key="d1">Orhan Firat is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MICHELE CATASTA">
      <data key="d0">PERSON</data>
      <data key="d1">Michele Catasta is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="JASON WEI">
      <data key="d0">PERSON</data>
      <data key="d1">Jason Wei is an author mentioned in the text and has contributed significantly to the field of language models and reasoning. He is an author of several influential papers, including "Challenging big-bench tasks and whether chain-of-thought can solve them," "Language models are multilingual chain-of-thought reasoners," "PaLM: Scaling language modeling with pathways" published in JMLR in 2023, "Self-consistency improves chain of thought reasoning in language models," and "Chain-of-thought prompting elicits reasoning in large language models." His work primarily focuses on enhancing the reasoning capabilities of language models through innovative techniques such as chain-of-thought prompting and self-consistency.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,7a48515e86161237c03c9a8373197126,8180bf20b7577f3eee40df5991e2886d,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="KATHY MEIER-HELLSTERN">
      <data key="d0">PERSON</data>
      <data key="d1">Kathy Meier-Hellstern is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="DOUGLAS ECK">
      <data key="d0">PERSON</data>
      <data key="d1">Douglas Eck is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="JEFF DEAN">
      <data key="d0">PERSON</data>
      <data key="d1">Jeff Dean is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SLAV PETROV">
      <data key="d0">PERSON</data>
      <data key="d1">Slav Petrov is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="NOAH FIEDEL">
      <data key="d0">PERSON</data>
      <data key="d1">Noah Fiedel is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="JMLR">
      <data key="d0">PUBLICATION</data>
      <data key="d1">JMLR is the journal where the paper "PaLM: Scaling language modeling with pathways" was published in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="KARL COBBE">
      <data key="d0">PERSON</data>
      <data key="d1">Karl Cobbe is an author of the paper "Training verifiers to solve math word problems," which was published on arXiv in 2021.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="VINEET KOSARAJU">
      <data key="d0">PERSON</data>
      <data key="d1">Vineet Kosaraju is an author of the paper "Training verifiers to solve math word problems," which was published on arXiv in 2021. Additionally, Vineet Kosaraju is also an author of the paper "WebGPT: Browser-assisted question-answering with human feedback."</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,1b1399c76420a477c0c97893d258ae69,7a48515e86161237c03c9a8373197126,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="JACOB HILTON">
      <data key="d0">PERSON</data>
      <data key="d1">Jacob Hilton is an author of the paper "Training verifiers to solve math word problems," which was published on arXiv in 2021. Additionally, he is an author of the paper "WebGPT: Browser-assisted question-answering with human feedback."</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,1b1399c76420a477c0c97893d258ae69,7a48515e86161237c03c9a8373197126,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="REIICHIRO NAKANO">
      <data key="d0">PERSON</data>
      <data key="d1">Reiichiro Nakano is an author of the paper "Training verifiers to solve math word problems," which was published on arXiv in 2021. Additionally, he is an author of the paper "WebGPT: Browser-assisted question-answering with human feedback."</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,1b1399c76420a477c0c97893d258ae69,7a48515e86161237c03c9a8373197126,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CHRISTOPHER HESSE">
      <data key="d0">PERSON</data>
      <data key="d1">Christopher Hesse is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He co-authored the paper "Training verifiers to solve math word problems," which was published on arXiv in 2021. Additionally, he contributed to the paper "WebGPT: Browser-assisted question-answering with human feedback." These works highlight his involvement in advancing AI methodologies for problem-solving and enhancing question-answering systems through human feedback.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="JOHN SCHULMAN">
      <data key="d0">PERSON</data>
      <data key="d1">John Schulman is an influential figure in the field of Artificial Intelligence and Machine Learning. He is the author of the paper "RL^2: Fast reinforcement learning via slow reinforcement learning," which explores advanced methodologies in reinforcement learning. Additionally, he co-authored the paper "Training verifiers to solve math word problems," published on arXiv in 2021, contributing to the development of AI systems capable of solving complex mathematical problems. His work significantly impacts the AI and ML landscape, particularly in the areas of reinforcement learning and problem-solving algorithms.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="XIANG DENG">
      <data key="d0">PERSON</data>
      <data key="d1">Xiang Deng is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="YU GU">
      <data key="d0">PERSON</data>
      <data key="d1">Yu Gu is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="BOYUAN ZHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Boyuan Zheng is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SHIJIE CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Shijie Chen is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SAMUEL STEVENS">
      <data key="d0">PERSON</data>
      <data key="d1">Samuel Stevens is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="BOSHI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Boshi Wang is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="HUAN SUN">
      <data key="d0">PERSON</data>
      <data key="d1">Huan Sun is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="YU SU">
      <data key="d0">PERSON</data>
      <data key="d1">Yu Su is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="NEURIPS DATASETS AND BENCHMARKS TRACK">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The NeurIPS Datasets and Benchmarks Track is a conference that serves as a platform for presenting significant advancements in the field of Artificial Intelligence and Machine Learning. In 2023, the conference featured the presentation of the paper "Mind2Web: Towards a generalist agent for the web." Additionally, it is the venue where the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge" was presented. This track is instrumental in showcasing cutting-edge research and developments that contribute to the progress of AI and ML technologies.</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="DANNY DRIESS">
      <data key="d0">PERSON</data>
      <data key="d1">Danny Driess is an author of the paper "PaLM-E: An embodied multimodal language model" presented at ICML in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MEHDI S. M. SAJJADI">
      <data key="d0">PERSON</data>
      <data key="d1">Mehdi S. M. Sajjadi is an author of the paper "PaLM-E: An embodied multimodal language model" presented at ICML in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="COREY LYNCH">
      <data key="d0">PERSON</data>
      <data key="d1">Corey Lynch is an author of the paper "PaLM-E: An embodied multimodal language model" presented at ICML in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MENGJIAO YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Mengjiao Yang is an author of the paper "Learning universal policies via text-guided video generation"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="BO DAI">
      <data key="d0">PERSON</data>
      <data key="d1">Bo Dai is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored significant papers such as "AdaPlanner: Adaptive planning from feedback with language models" and "Learning universal policies via text-guided video generation." These works highlight his expertise in leveraging language models for adaptive planning and in developing universal policies through innovative text-guided video generation techniques.</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="HANJUN DAI">
      <data key="d0">PERSON</data>
      <data key="d1">Hanjun Dai is an author of the paper "Learning universal policies via text-guided video generation"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="OFIR NACHUM">
      <data key="d0">PERSON</data>
      <data key="d1">Ofir Nachum is an author of the paper "Multimodal web navigation with instruction-finetuned foundation models"Ofir Nachum is an author of the paper "Learning universal policies via text-guided video generation"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOSHUA B. TENENBAUM">
      <data key="d0">PERSON</data>
      <data key="d1">Joshua B. Tenenbaum is an influential figure in the field of Artificial Intelligence and Machine Learning, known for his contributions to advancing the understanding and capabilities of language models and policy learning. He is an author of the paper "Improving factuality and reasoning in language models through multiagent debate," which explores methods to enhance the accuracy and logical consistency of language models by employing multiagent debate techniques. Additionally, Tenenbaum has contributed to the paper "Learning universal policies via text-guided video generation," which investigates the development of universal policies through the innovative approach of generating video content guided by textual descriptions. His work significantly impacts the AI and ML community, particularly in the areas of language processing and policy learning.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="DALE SCHUURMANS">
      <data key="d0">PERSON</data>
      <data key="d1">Dale Schuurmans is an author mentioned in the text and has contributed to several significant papers in the field of Artificial Intelligence and Machine Learning. Notably, he is an author of the paper "Learning universal policies via text-guided video generation." Additionally, he has co-authored the papers "Self-consistency improves chain of thought reasoning in language models" and "Chain-of-thought prompting elicits reasoning in large language models."</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="PIETER ABBEEL">
      <data key="d0">PERSON</data>
      <data key="d1">Pieter Abbeel is a prominent author in the field of Artificial Intelligence and Machine Learning. He has contributed to several influential papers, including "Managing Extreme AI Risks Amid Rapid Progress," "Learning universal policies via text-guided video generation," "RL^2: Fast reinforcement learning via slow reinforcement learning," and "The false promise of imitating proprietary LLMs." His work spans various critical areas within AI and ML, highlighting his significant role in advancing the understanding and development of these technologies.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,68e5573b596d253a03047b1e41988598,7de66b94cf868b37b1df51dc545c415f,8180bf20b7577f3eee40df5991e2886d,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="JONATHAN ST BT EVANS">
      <data key="d0">PERSON</data>
      <data key="d1">Jonathan St BT Evans is the author of the paper "Intuition and reasoning: A dual-process perspective"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PSYCHOLOGICAL INQUIRY">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Psychological Inquiry is the journal where the paper "Intuition and reasoning: A dual-process perspective" was published</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="LINXI FAN">
      <data key="d0">PERSON</data>
      <data key="d1">Linxi Fan is an author mentioned in the text, known for contributing to several significant papers in the field of Artificial Intelligence and Machine Learning. These papers include "Eureka: Human-level reward design via coding large language models," "MineDojo: Building open-ended embodied agents with internet-scale knowledge," and "Voyager: An open-ended embodied agent with large language models."</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,34d0bb2211fc795fe1096442e086a2b3,68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="GUANZHI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Guanzhi Wang is an author mentioned in the text and has contributed to several significant papers in the field of Artificial Intelligence and Machine Learning. These papers include "Eureka: Human-level reward design via coding large language models," "MineDojo: Building open-ended embodied agents with internet-scale knowledge," and "Voyager: An open-ended embodied agent with large language models."</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,34d0bb2211fc795fe1096442e086a2b3,68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YUNFAN JIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yunfan Jiang is an author mentioned in the text, known for contributing to the field of Artificial Intelligence and Machine Learning. Yunfan Jiang has co-authored significant papers, including "MineDojo: Building open-ended embodied agents with internet-scale knowledge" and "Voyager: An open-ended embodied agent with large language models." These works highlight Jiang's involvement in developing advanced AI systems that leverage extensive internet-scale knowledge and large language models to create open-ended embodied agents.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="AJAY MANDLEKAR">
      <data key="d0">PERSON</data>
      <data key="d1">Ajay Mandlekar is an author mentioned in the text, known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored significant papers such as "MineDojo: Building open-ended embodied agents with internet-scale knowledge" and "Voyager: An open-ended embodied agent with large language models." These works highlight his focus on developing advanced embodied agents that leverage extensive knowledge and large language models, contributing to the advancement of AI and ML research.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YUNCONG YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yuncong Yang is an author of the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="HAOYI ZHU">
      <data key="d0">PERSON</data>
      <data key="d1">Haoyi Zhu is an author of the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="ANDREW TANG">
      <data key="d0">PERSON</data>
      <data key="d1">Andrew Tang is an author of the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="DE-AN HUANG">
      <data key="d0">PERSON</data>
      <data key="d1">De-An Huang is an author of notable papers in the field of Artificial Intelligence and Machine Learning. Among his contributions are the paper "Eureka: Human-level reward design via coding large language models" and "MineDojo: Building open-ended embodied agents with internet-scale knowledge." These works highlight his involvement in advancing AI through innovative approaches to reward design and the development of embodied agents utilizing extensive internet-scale knowledge.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="YUKE ZHU">
      <data key="d0">PERSON</data>
      <data key="d1">Yuke Zhu is an author mentioned in the text and has contributed to several significant papers in the field of Artificial Intelligence and Machine Learning. These papers include "Eureka: Human-level reward design via coding large language models," "MineDojo: Building open-ended embodied agents with internet-scale knowledge," and "Voyager: An open-ended embodied agent with large language models."</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,34d0bb2211fc795fe1096442e086a2b3,68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ANIMA ANANDKUMAR">
      <data key="d0">PERSON</data>
      <data key="d1">Anima Anandkumar is an author mentioned in the text and has contributed to several significant papers in the field of Artificial Intelligence and Machine Learning. She is an author of the paper "Eureka: Human-level reward design via coding large language models," which explores advanced reward design using large language models. Additionally, she has co-authored "MineDojo: Building open-ended embodied agents with internet-scale knowledge," focusing on the development of agents with extensive knowledge from the internet. Furthermore, she is an author of "Voyager: An open-ended embodied agent with large language models," which delves into the creation of sophisticated embodied agents utilizing large language models.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,34d0bb2211fc795fe1096442e086a2b3,68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="HIROKI FURUTA">
      <data key="d0">PERSON</data>
      <data key="d1">Hiroki Furuta is an author of the paper "Multimodal web navigation with instruction-finetuned foundation models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUTAKA MATSUO">
      <data key="d0">PERSON</data>
      <data key="d1">Yutaka Matsuo is an author of the paper "Large language models are zero-shot reasoners"Yutaka Matsuo is an author of the paper "Multimodal web navigation with instruction-finetuned foundation models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHIXIANG SHANE GU">
      <data key="d0">PERSON</data>
      <data key="d1">Shixiang Shane Gu is an author of the paper "Multimodal web navigation with instruction-finetuned foundation models"Shixiang Shane Gu is an author of the paper "Large language models are zero-shot reasoners"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IZZEDDIN GUR">
      <data key="d0">PERSON</data>
      <data key="d1">Izzeddin Gur is an author of the paper "Multimodal web navigation with instruction-finetuned foundation models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LUYU GAO">
      <data key="d0">PERSON</data>
      <data key="d1">Luyu Gao is an author of the paper "PAL: Program-aided language models" and also contributed to the paper "Self-refine: Iterative refinement with self-feedback."</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AMAN MADAAN">
      <data key="d0">PERSON</data>
      <data key="d1">Aman Madaan is an author of the paper "PAL: Program-aided language models" and also contributed to the paper "Self-refine: Iterative refinement with self-feedback."</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHUYAN ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Shuyan Zhou is an author of the paper "PAL: Program-aided language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="URI ALON">
      <data key="d0">PERSON</data>
      <data key="d1">Uri Alon is an author of the paper "PAL: Program-aided language models" and also contributed to the paper "Self-refine: Iterative refinement with self-feedback."</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PENGFEI LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Pengfei Liu is an author of multiple influential papers in the field of Artificial Intelligence and Machine Learning. Notably, Liu has contributed to the paper titled "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization," which explores the performance and evaluation metrics of large language models in the context of summarization tasks. Additionally, Liu is also an author of the paper "PAL: Program-aided language models," which delves into the integration of programmatic aids to enhance the capabilities of language models. These contributions highlight Pengfei Liu's active role in advancing the understanding and development of sophisticated AI and ML methodologies.</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIMING YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yiming Yang is an author of the papers "PAL: Program-aided language models" and "Self-refine: Iterative refinement with self-feedback."</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653,6109537356a2ce2339f77c827aa3668e,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAMIE CALLAN">
      <data key="d0">PERSON</data>
      <data key="d1">Jamie Callan is an author of the paper "PAL: Program-aided language models."</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GRAHAM NEUBIG">
      <data key="d0">PERSON</data>
      <data key="d1">Graham Neubig is an author of the paper "PAL: Program-aided language models."</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ICML">
      <data key="d0">CONFERENCE</data>
      <data key="d1">ICML is the conference where the paper "PAL: Program-aided language models" was presentedICML is the conference where the paper "An embodied multi-modal language model" was presentedICML is the conference where the paper "Learning latent dynamics for planning from pixels" was presented</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="JIANXIAN GUO">
      <data key="d0">PERSON</data>
      <data key="d1">Jiaxian Guo is an author of the paper "Long text generation via adversarial training with leaked information"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="SIDI LU">
      <data key="d0">PERSON</data>
      <data key="d1">Sidi Lu is an author of the paper "Long text generation via adversarial training with leaked information"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HAN CAI">
      <data key="d0">PERSON</data>
      <data key="d1">Han Cai is an author of the paper "Long text generation via adversarial training with leaked information"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEINAN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Weinan Zhang is an author of the paper "Long text generation via adversarial training with leaked information"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YONG YU">
      <data key="d0">PERSON</data>
      <data key="d1">Yong Yu is an author of the paper "Long text generation via adversarial training with leaked information"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JUN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jun Wang is an author of the paper "Long text generation via adversarial training with leaked information"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AAAI">
      <data key="d0">CONFERENCE</data>
      <data key="d1">AAAI is the conference where the paper "Long text generation via adversarial training with leaked information" was presented</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="NICHOLAY TOPIN">
      <data key="d0">PERSON</data>
      <data key="d1">Nicholay Topin is an author of the paper "MineRL: A large-scale dataset of Minecraft demonstrations"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PHILLIP WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Phillip Wang is an author of the paper "MineRL: A large-scale dataset of Minecraft demonstrations"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CAYDEN CODEL">
      <data key="d0">PERSON</data>
      <data key="d1">Cayden Codel is an author of the paper "MineRL: A large-scale dataset of Minecraft demonstrations"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MANUELA VELOSO">
      <data key="d0">PERSON</data>
      <data key="d1">Manuela Veloso is an author of the paper "MineRL: A large-scale dataset of Minecraft demonstrations"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RUSLAN SALAKHUTDINOV">
      <data key="d0">PERSON</data>
      <data key="d1">Ruslan Salakhutdinov is an author mentioned in the text and is notably recognized for his contribution as an author of the paper titled "MineRL: A large-scale dataset of Minecraft demonstrations."</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IJCAI">
      <data key="d0">CONFERENCE</data>
      <data key="d1">IJCAI is the conference where the paper "MineRL: A large-scale dataset of Minecraft demonstrations" was presented</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="DANIJAR HAFNER">
      <data key="d0">PERSON</data>
      <data key="d1">Danijar Hafner is an author mentioned in the text, known for his contributions to the field of Artificial Intelligence and Machine Learning. He has authored significant papers, including "Mastering diverse domains through world models" and "Learning latent dynamics for planning from pixels." These works highlight his expertise in developing advanced models for understanding and planning in diverse environments using latent dynamics and pixel-based data.</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TIMOTHY LILLICRAP">
      <data key="d0">PERSON</data>
      <data key="d1">Timothy Lillicrap is an author of the paper "Learning latent dynamics for planning from pixels"Timothy Lillicrap is an author of the paper "Mastering diverse domains through world models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IAN FISCHER">
      <data key="d0">PERSON</data>
      <data key="d1">Ian Fischer is an author of the paper "Learning latent dynamics for planning from pixels"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RUBEN VILLEGAS">
      <data key="d0">PERSON</data>
      <data key="d1">Ruben Villegas is an author of the paper "Learning latent dynamics for planning from pixels"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAVID HA">
      <data key="d0">PERSON</data>
      <data key="d1">David Ha is an author of the paper "Learning latent dynamics for planning from pixels"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HONGLAK LEE">
      <data key="d0">PERSON</data>
      <data key="d1">Honglak Lee is an author of the paper "Learning latent dynamics for planning from pixels"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAMES DAVIDSON">
      <data key="d0">PERSON</data>
      <data key="d1">James Davidson is an author of the paper "Learning latent dynamics for planning from pixels"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JURGIS PASUKONIS">
      <data key="d0">PERSON</data>
      <data key="d1">Jurgis Pasukonis is an author of the paper "Mastering diverse domains through world models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIMMY BA">
      <data key="d0">PERSON</data>
      <data key="d1">Jimmy Ba is an author of the paper "Mastering diverse domains through world models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHIBO HAO">
      <data key="d0">PERSON</data>
      <data key="d1">Shibo Hao is an author of the paper "Reasoning with language model is planning with world model"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YI GU">
      <data key="d0">PERSON</data>
      <data key="d1">Yi Gu is an author of the paper "Reasoning with language model is planning with world model"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HAODI MA">
      <data key="d0">PERSON</data>
      <data key="d1">Haodi Ma is an author of the paper "Reasoning with language model is planning with world model"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOSHUA JIAHUA HONG">
      <data key="d0">PERSON</data>
      <data key="d1">Joshua Jiahua Hong is an author of the paper "Reasoning with language model is planning with world model"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHEN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhen Wang is an author of the paper "Reasoning with language model is planning with world model"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAISY ZHE WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Daisy Zhe Wang is an author of the paper "Reasoning with language model is planning with world model"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHTING HU">
      <data key="d0">PERSON</data>
      <data key="d1">Zhiting Hu is an author of the paper "Reasoning with language model is planning with world model"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="EMNLP">
      <data key="d0">CONFERENCE</data>
      <data key="d1">EMNLP is a prominent conference in the field of Natural Language Processing (NLP) where significant research papers are presented. Notably, it is the venue where the influential paper on HotpotQA was introduced, as well as the paper titled "Reasoning with language model is planning with world model." These contributions highlight EMNLP's role as a key platform for advancing research and knowledge in NLP and related areas.</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="JIE HUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jie Huang is an author of the paper "Large language models cannot self-correct reasoning yet"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XINYUN CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Xinyun Chen is an author of several influential papers in the field of Artificial Intelligence and Machine Learning. These papers include "Large language models as optimizers," "Large language models cannot self-correct reasoning yet," and "Take a step back: Evoking reasoning via abstraction in large language models." Through these works, Xinyun Chen contributes to the understanding and development of large language models, focusing on their optimization capabilities, limitations in self-correcting reasoning, and the potential for enhancing reasoning through abstraction.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,68e5573b596d253a03047b1e41988598,cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SWAROOP MISHRA">
      <data key="d0">PERSON</data>
      <data key="d1">Swaroop Mishra is an author of several influential papers in the field of large language models. His works include "Instruction-following evaluation for large language models," "Large language models cannot self-correct reasoning yet," and "Take a step back: Evoking reasoning via abstraction in large language models." These contributions highlight his focus on evaluating and improving the reasoning capabilities of large language models, addressing their limitations, and exploring methods to enhance their performance through abstraction.</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HUAIXIU STEVEN ZHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Huaixiu Steven Zheng is an author of the papers "Large language models cannot self-correct reasoning yet" and "Take a step back: Evoking reasoning via abstraction in large language models." These works contribute to the understanding of the limitations and potential improvements in reasoning capabilities of large language models, highlighting Zheng's involvement in advancing research in artificial intelligence and machine learning.</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ADAMS WEI YU">
      <data key="d0">PERSON</data>
      <data key="d1">Adams Wei Yu is an author of the paper "Large language models cannot self-correct reasoning yet"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XINYING SONG">
      <data key="d0">PERSON</data>
      <data key="d1">Xinying Song is an author of the paper "Large language models cannot self-correct reasoning yet"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WENLONG HUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Wenlong Huang is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="F. XIA">
      <data key="d0">PERSON</data>
      <data key="d1">F. Xia is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HARRIS CHAN">
      <data key="d0">PERSON</data>
      <data key="d1">Harris Chan is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JACKY LIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jacky Liang is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PETER R. FLORENCE">
      <data key="d0">PERSON</data>
      <data key="d1">Peter R. Florence is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JONATHAN TOMPSON">
      <data key="d0">PERSON</data>
      <data key="d1">Jonathan Tompson is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IGOR MORDATCH">
      <data key="d0">PERSON</data>
      <data key="d1">Igor Mordatch is an author of the paper "Improving factuality and reasoning in language models through multiagent debate" and also contributed to the paper "Inner monologue: Embodied reasoning through planning with language models." His work focuses on enhancing the capabilities of language models, particularly in the areas of factual accuracy, reasoning, and embodied planning.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TOMAS JACKSON">
      <data key="d0">PERSON</data>
      <data key="d1">Tomas Jackson is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIAXIAN GUO">
      <data key="d0">PERSON</data>
      <data key="d1">Jiaxian Guo is an author of the paper "Long text generation via adversarial training with leaked information"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="CORL">
      <data key="d0">CONFERENCE</data>
      <data key="d1">CoRL, or the Conference on Robot Learning, is a prominent conference in the field of robotics and machine learning. It serves as a platform for presenting cutting-edge research and advancements. Notably, CoRL is where the paper on Daydreamer was presented, as well as the paper titled "Inner monologue: Embodied reasoning through planning with language models." These presentations highlight CoRL's role in showcasing significant contributions to the AI and ML community.</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="LEVENTE KOCSIS">
      <data key="d0">PERSON</data>
      <data key="d1">Levente Kocsis is an author of the paper "Bandit based monte-carlo planning"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="CSABA SZEPESVARI">
      <data key="d0">PERSON</data>
      <data key="d1">Csaba Szepesvari is an author of the paper "Bandit based monte-carlo planning"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="ECML">
      <data key="d0">CONFERENCE</data>
      <data key="d1">ECML is the conference where the paper "Bandit based monte-carlo planning" was presented</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="TAKESHI KOJIMA">
      <data key="d0">PERSON</data>
      <data key="d1">Takeshi Kojima is an author of the paper "Large language models are zero-shot reasoners"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="MACHEL REID">
      <data key="d0">PERSON</data>
      <data key="d1">Machel Reid is an author of the paper "Large language models are zero-shot reasoners"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="YUSUKE IWASAWA">
      <data key="d0">PERSON</data>
      <data key="d1">Yusuke Iwasawa is an author of the paper "Large language models are zero-shot reasoners"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="STEVEN M. LAVALLE">
      <data key="d0">PERSON</data>
      <data key="d1">Steven M. LaValle is the author of the paper "Rapidly-exploring random trees: A new tool for path planning"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="THE ANNUAL RESEARCH REPORT">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The Annual Research Report is the publication where the paper "Rapidly-exploring random trees: A new tool for path planning" was published</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="EVAN ZHERAN LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Evan Zheran Liu is an author of the paper "Reinforcement learning on web interfaces using workflow-guided exploration"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="KELVIN GUU">
      <data key="d0">PERSON</data>
      <data key="d1">Kelvin Guu is an author of the paper "Reinforcement learning on web interfaces using workflow-guided exploration"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="PANUPONG PASUPAT">
      <data key="d0">PERSON</data>
      <data key="d1">Panupong Pasupat is an author of the paper "Reinforcement learning on web interfaces using workflow-guided exploration"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="TIANLIN SHI">
      <data key="d0">PERSON</data>
      <data key="d1">Tianlin Shi is an author of the paper "Reinforcement learning on web interfaces using workflow-guided exploration"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="PERCY LIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Percy Liang is an author of several influential papers in the field of Artificial Intelligence and Machine Learning. His works include "Alpaca," "Alpacaeval: An automatic evaluator of instruction-following models," "Generative agents: Interactive simulacra of human behavior," and "Reinforcement learning on web interfaces using workflow-guided exploration." These contributions highlight his significant role in advancing research on instruction-following models, interactive simulations of human behavior, and reinforcement learning in web interfaces.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,3d1f6634f93f8a4c296dc8df7e59859e,68e5573b596d253a03047b1e41988598,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="NIKET TANDON">
      <data key="d0">PERSON</data>
      <data key="d1">Niket Tandon is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="PRAKHAR GUPTA">
      <data key="d0">PERSON</data>
      <data key="d1">Prakhar Gupta is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="SKYLER HALLINAN">
      <data key="d0">PERSON</data>
      <data key="d1">Skyler Hallinan is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="SARAH WIEGREFFE">
      <data key="d0">PERSON</data>
      <data key="d1">Sarah Wiegreffe is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="NOUHA DZIRI">
      <data key="d0">PERSON</data>
      <data key="d1">Nouha Dziri is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="SHRIMAI PRABHUMOYE">
      <data key="d0">PERSON</data>
      <data key="d1">Shrimai Prabhumoye is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="SHASHANK GUPTA">
      <data key="d0">PERSON</data>
      <data key="d1">Shashank Gupta is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="BODHISATTWA PRASAD MAJUMDER">
      <data key="d0">PERSON</data>
      <data key="d1">Bodhisattwa Prasad Majumder is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="KATHERINE HERMANN">
      <data key="d0">PERSON</data>
      <data key="d1">Katherine Hermann is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="SEAN WELLECK">
      <data key="d0">PERSON</data>
      <data key="d1">Sean Welleck is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="AMIR YAZDANBAKHSH">
      <data key="d0">PERSON</data>
      <data key="d1">Amir Yazdanbakhsh is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="PETER CLARK">
      <data key="d0">PERSON</data>
      <data key="d1">Peter Clark is an author of the paper "Self-refine: Iterative refinement with self-feedback" and also contributed to the paper "Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge." These works highlight his involvement in advancing the field of Artificial Intelligence, particularly in the areas of iterative refinement and question answering systems.</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="RAMESH NALLAPATI">
      <data key="d0">PERSON</data>
      <data key="d1">Ramesh Nallapati is an author of the paper "Abstractive text summarization using sequence-to-sequence RNNs and beyond"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="BOWEN ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Bowen Zhou is an author of the paper "Abstractive text summarization using sequence-to-sequence RNNs and beyond" and the paper "Enhancing chat language models by scaling high-quality instructional conversations."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CICERO DOS SANTOS">
      <data key="d0">PERSON</data>
      <data key="d1">Cicero dos Santos is an author of the paper "Abstractive text summarization using sequence-to-sequence RNNs and beyond"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="CAGLAR GULCEHRE">
      <data key="d0">PERSON</data>
      <data key="d1">Caglar Gulcehre is an author of the paper "Abstractive text summarization using sequence-to-sequence RNNs and beyond"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="BING XIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Bing Xiang is an author of the paper "Abstractive text summarization using sequence-to-sequence RNNs and beyond"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="SPECIAL INTEREST GROUP ON NATURAL LANGUAGE LEARNING">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "Abstractive text summarization using sequence-to-sequence RNNs and beyond" was presented</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="YUJIA QIN">
      <data key="d0">PERSON</data>
      <data key="d1">Yujia Qin is an accomplished author in the field of Artificial Intelligence and Machine Learning. She has contributed significantly to the advancement of language models, as evidenced by her authorship of the paper "Enhancing chat language models by scaling high-quality instructional conversations." Additionally, Yujia Qin has made notable strides in facilitating the mastery of real-world applications by large language models, demonstrated through her work on the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs." Her research focuses on improving the capabilities and applications of language models, making her a key player in the AI and ML community.</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="SHIHAO LIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Shihao Liang is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="YINING YE">
      <data key="d0">PERSON</data>
      <data key="d1">Yining Ye is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="KUNLUN ZHU">
      <data key="d0">PERSON</data>
      <data key="d1">Kunlun Zhu is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="LAN YAN">
      <data key="d0">PERSON</data>
      <data key="d1">Lan Yan is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="YAXI LU">
      <data key="d0">PERSON</data>
      <data key="d1">Yaxi Lu is an accomplished author in the field of Artificial Intelligence and Machine Learning. They have contributed to significant research, including the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors," which delves into the dynamics of multi-agent systems and their collaborative potential. Additionally, Yaxi Lu has co-authored "ToolLLM: Facilitating large language models to master 16000+ real-world APIs," a paper that addresses the integration of large language models with a vast array of real-world application programming interfaces (APIs). These contributions highlight Yaxi Lu's expertise in advancing the capabilities and applications of AI and ML technologies.</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e,7de66b94cf868b37b1df51dc545c415f</data>
    </node>
    <node id="YANKAI LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Yankai Lin is an author known for contributing to the field of large language models and their applications. He has co-authored the paper "A survey on large language model based autonomous agents," which explores the capabilities and developments in autonomous agents powered by large language models. Additionally, he has contributed to the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs," which focuses on enhancing the proficiency of large language models in utilizing a vast array of real-world APIs. Through these works, Yankai Lin has made significant contributions to advancing the understanding and practical implementation of large language models in various domains.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="XIN CONG">
      <data key="d0">PERSON</data>
      <data key="d1">Xin Cong is an author of the paper "Communicative agents for software development" and also contributed to the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs." These works highlight Xin Cong's involvement in advancing the field of Artificial Intelligence and Machine Learning, particularly in the areas of communicative agents and the application of large language models to real-world APIs.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="XIANGRU TANG">
      <data key="d0">PERSON</data>
      <data key="d1">Xiangru Tang is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="BILL QIAN">
      <data key="d0">PERSON</data>
      <data key="d1">Bill Qian is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SIHAN ZHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Sihan Zhao is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="RUNCHU TIAN">
      <data key="d0">PERSON</data>
      <data key="d1">Runchu Tian is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="RUOBING XIE">
      <data key="d0">PERSON</data>
      <data key="d1">Ruobing Xie is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="JIE ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Jie Zhou is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="MARK GERSTEIN">
      <data key="d0">PERSON</data>
      <data key="d1">Mark Gerstein is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="DAHAI LI">
      <data key="d0">PERSON</data>
      <data key="d1">Dahai Li is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ZHIYUAN LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Zhiyuan Liu is a prolific author in the field of Artificial Intelligence and Machine Learning. He has contributed to several significant papers, including "Communicative agents for software development," "Enhancing chat language models by scaling high-quality instructional conversations," and "ToolLLM: Facilitating large language models to master 16000+ real-world APIs." His work spans various aspects of AI, focusing on the development and enhancement of language models and their practical applications in software development and real-world API integration.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="MAOSONG SUN">
      <data key="d0">PERSON</data>
      <data key="d1">Maosong Sun is a prolific author in the field of Artificial Intelligence and Machine Learning, contributing to several significant research papers. He has authored the paper "Communicative agents for software development," which explores the role of communicative agents in enhancing software development processes. Additionally, he has contributed to the paper "Enhancing chat language models by scaling high-quality instructional conversations," focusing on improving chat language models through high-quality instructional dialogues. Furthermore, Maosong Sun is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs," which addresses the challenge of enabling large language models to effectively utilize a vast array of real-world APIs. His work demonstrates a broad and impactful engagement with advancing AI and ML technologies.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ABULHAIR SAPAROV">
      <data key="d0">PERSON</data>
      <data key="d1">Abulhair Saparov is an author of the paper "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="HE HE">
      <data key="d0">PERSON</data>
      <data key="d1">He He is an author of the paper "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="TIMO SCHICK">
      <data key="d0">PERSON</data>
      <data key="d1">Timo Schick is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JANE DWIVEDI-YU">
      <data key="d0">PERSON</data>
      <data key="d1">Jane Dwivedi-Yu is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ROBERTO DESSI">
      <data key="d0">PERSON</data>
      <data key="d1">Roberto Dessi is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ROBERTA RAILEANU">
      <data key="d0">PERSON</data>
      <data key="d1">Roberta Raileanu is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MARIA LOMELI">
      <data key="d0">PERSON</data>
      <data key="d1">Maria Lomeli is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="LUKE ZETTLEMOYER">
      <data key="d0">PERSON</data>
      <data key="d1">Luke Zettlemoyer is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="NICOLA CANCEDDA">
      <data key="d0">PERSON</data>
      <data key="d1">Nicola Cancedda is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="THOMAS SCIALOM">
      <data key="d0">PERSON</data>
      <data key="d1">Thomas Scialom is an author mentioned in the text and is notably recognized for his contribution as an author of the paper titled "Toolformer: Language models can teach themselves to use tools."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YONGLIANG SHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Yongliang Shen is an author of the paper "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="KAITAO SONG">
      <data key="d0">PERSON</data>
      <data key="d1">Kaitao Song is an author known for contributing to the field of Artificial Intelligence and Machine Learning. He has co-authored the paper "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face," which explores the integration of ChatGPT with the Hugging Face ecosystem to address various AI tasks. Additionally, he has contributed to the paper "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms," which delves into the use of evolutionary algorithms for the automatic generation of multi-agent systems.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="XU TAN">
      <data key="d0">PERSON</data>
      <data key="d1">Xu Tan is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored the paper "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms," which explores the use of evolutionary algorithms for the automatic generation of multi-agent systems. Additionally, Xu Tan has contributed to the paper "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face," which investigates the integration of ChatGPT with the Hugging Face ecosystem to address various AI tasks. His work demonstrates a focus on innovative approaches to AI and collaborative efforts within the AI research community.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="DONGSHENG LI">
      <data key="d0">PERSON</data>
      <data key="d1">Dongsheng Li is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored the paper "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms," which explores the use of evolutionary algorithms for the automatic generation of multi-agent systems. Additionally, Dongsheng Li has contributed to the paper "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face," which investigates the integration of ChatGPT with other models in the Hugging Face ecosystem to address various AI tasks. His work demonstrates a focus on innovative approaches to AI and collaborative efforts within the research community.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="WEIMING LU">
      <data key="d0">PERSON</data>
      <data key="d1">Weiming Lu is an author of the paper "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="YUETING ZHUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yueting Zhuang is an author of the paper "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="NOAH SHINN">
      <data key="d0">PERSON</data>
      <data key="d1">Noah Shinn is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="FEDERICO CASSANO">
      <data key="d0">PERSON</data>
      <data key="d1">Federico Cassano is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="BECK LABASH">
      <data key="d0">PERSON</data>
      <data key="d1">Beck Labash is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="ASHWIN GOPINATH">
      <data key="d0">PERSON</data>
      <data key="d1">Ashwin Gopinath is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="KARTHIK NARASIMHAN">
      <data key="d0">PERSON</data>
      <data key="d1">Karthik Narasimhan is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SHUNYU YAO">
      <data key="d0">PERSON</data>
      <data key="d1">Shunyu Yao is an author mentioned in the text, known for contributing to the field of Artificial Intelligence and Machine Learning. Shunyu Yao has authored significant papers, including "React: Synergizing reasoning and acting in language models" and "Reflexion: Language agents with verbal reinforcement learning." These works highlight Yao's involvement in advancing language models and verbal reinforcement learning, showcasing their expertise and influence in the AI and ML research community.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="MOHIT SHRIDHAR">
      <data key="d0">PERSON</data>
      <data key="d1">Mohit Shridhar is an author of the paper "ALFWorld: Aligning text and embodied environments for interactive learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="XINGDI YUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Xingdi Yuan is an author of the paper "ALFWorld: Aligning text and embodied environments for interactive learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="MARC-ALEXANDRE COTE">
      <data key="d0">PERSON</data>
      <data key="d1">Marc-Alexandre Cote is an author of the paper "ALFWorld: Aligning text and embodied environments for interactive learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="YONATAN BISK">
      <data key="d0">PERSON</data>
      <data key="d1">Yonatan Bisk is an author of the paper "ALFWorld: Aligning text and embodied environments for interactive learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="ADAM TRISCHLER">
      <data key="d0">PERSON</data>
      <data key="d1">Adam Trischler is an author of the paper "ALFWorld: Aligning text and embodied environments for interactive learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="MATTHEW HAUSKNECHT">
      <data key="d0">PERSON</data>
      <data key="d1">Matthew Hausknecht is an author of the paper "ALFWorld: Aligning text and embodied environments for interactive learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="DAVID SILVER">
      <data key="d0">PERSON</data>
      <data key="d1">David Silver is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"David Silver is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AJA HUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Aja Huang is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Aja Huang is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHRIS J. MADDISON">
      <data key="d0">PERSON</data>
      <data key="d1">Chris J. Maddison is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Chris J. Maddison is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ARTHUR GUEZ">
      <data key="d0">PERSON</data>
      <data key="d1">Arthur Guez is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Arthur Guez is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="L. SIFRE">
      <data key="d0">PERSON</data>
      <data key="d1">L. Sifre is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"L. Sifre is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GEORGE VAN DEN DRIESSCHE">
      <data key="d0">PERSON</data>
      <data key="d1">George van den Driessche is an author of the paper "Mastering the game of Go with deep neural networks and tree search"George van den Driessche is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JULIAN SCHRITTWIESER">
      <data key="d0">PERSON</data>
      <data key="d1">Julian Schrittwieser is an author of the paper "Mastering the game of Go with deep neural networks and tree search"Julian Schrittwieser is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IOANNIS ANTONOGLOU">
      <data key="d0">PERSON</data>
      <data key="d1">Ioannis Antonoglou is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Ioannis Antonoglou is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VEDAVYAS PANNEERSHELVAM">
      <data key="d0">PERSON</data>
      <data key="d1">Vedavyas Panneershelvam is an author of the paper "Mastering the game of Go with deep neural networks and tree search"Vedavyas Panneershelvam is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MARC LANCTOT">
      <data key="d0">PERSON</data>
      <data key="d1">Marc Lanctot is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Marc Lanctot is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SANDER DIELEMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Sander Dieleman is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Sander Dieleman is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DOMINIK GREWE">
      <data key="d0">PERSON</data>
      <data key="d1">Dominik Grewe is an author of the paper "Mastering the game of Go with deep neural networks and tree search"Dominik Grewe is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOHN NHAM">
      <data key="d0">PERSON</data>
      <data key="d1">John Nham is an author of the paper "Mastering the game of Go with deep neural networks and tree search"John Nham is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NAL KALCHBRENNER">
      <data key="d0">PERSON</data>
      <data key="d1">Nal Kalchbrenner is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Nal Kalchbrenner is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TIMOTHY P. LILLICRAP">
      <data key="d0">PERSON</data>
      <data key="d1">Timothy P. Lillicrap is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Timothy P. Lillicrap is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MADELEINE LEACH">
      <data key="d0">PERSON</data>
      <data key="d1">Madeleine Leach is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Madeleine Leach is an author of the paper "Master</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NATURE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Nature is the journal where the groundbreaking paper "Mastering the game of Go with deep neural networks and tree search" was published, showcasing significant advancements in AI through the combination of deep neural networks and tree search techniques. Additionally, Nature is also the journal that published the influential paper "Mathematical discoveries from program search with large language models," highlighting the role of large language models in making substantial mathematical discoveries. These publications underscore Nature's pivotal role in disseminating key research findings in the fields of Artificial Intelligence and Machine Learning.</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="KORAY KAVUKCUOGLU">
      <data key="d0">PERSON</data>
      <data key="d1">Koray Kavukcuoglu is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="THORE GRAEPEL">
      <data key="d0">PERSON</data>
      <data key="d1">Thore Graepel is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="DEMIS HASSABIS">
      <data key="d0">PERSON</data>
      <data key="d1">Demis Hassabis is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="STEVEN A. SLOMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Steven A. Sloman is the author of the paper "The empirical case for two systems of reasoning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="PSYCHOLOGICAL BULLETIN">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Psychological Bulletin is the journal where the paper "The empirical case for two systems of reasoning" was published</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="HAOTIAN SUN">
      <data key="d0">PERSON</data>
      <data key="d1">Haotian Sun is an author of the paper "AdaPlanner: Adaptive planning from feedback with language models"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="YUCHEN ZHUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yuchen Zhuang is an author mentioned in the text, known for contributing to the field of Artificial Intelligence and Machine Learning. Yuchen Zhuang has authored significant papers, including "AdaPlanner: Adaptive planning from feedback with language models" and "ToolChain*: Efficient action space navigation in large language models with A* search." These works highlight Zhuang's involvement in advancing adaptive planning and efficient action space navigation within large language models.</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="LINGKAI KONG">
      <data key="d0">PERSON</data>
      <data key="d1">Lingkai Kong is an author of the paper "AdaPlanner: Adaptive planning from feedback with language models"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="CHAO ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Chao Zhang is an author mentioned in the text, known for contributing to the field of Artificial Intelligence and Machine Learning. Chao Zhang has co-authored significant papers, including "AdaPlanner: Adaptive planning from feedback with language models" and "ToolChain*: Efficient action space navigation in large language models with A* search." These works highlight Chao Zhang's involvement in advancing adaptive planning and efficient action space navigation within large language models, showcasing their expertise and influence in the AI and ML research community.</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="DIDAC SURIS">
      <data key="d0">PERSON</data>
      <data key="d1">Didac Suris is an author of the paper "ViperGPT: Visual inference via Python execution for reasoning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="SACHIT MENON">
      <data key="d0">PERSON</data>
      <data key="d1">Sachit Menon is an author of the paper "ViperGPT: Visual inference via Python execution for reasoning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="CARL VONDRICK">
      <data key="d0">PERSON</data>
      <data key="d1">Carl Vondrick is an author of the paper "ViperGPT: Visual inference via Python execution for reasoning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="ICCV">
      <data key="d0">CONFERENCE</data>
      <data key="d1">ICCV is the conference where the paper "ViperGPT: Visual inference via Python execution for reasoning" was presented</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="MACIEJ SWIECHOWSKI">
      <data key="d0">PERSON</data>
      <data key="d1">Maciej Swiechowski is an author of the paper "Monte Carlo tree search: A review of recent modifications and applications"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="KONRAD GODLEWSKI">
      <data key="d0">PERSON</data>
      <data key="d1">Konrad Godlewski is an author of the paper "Monte Carlo tree search: A review of recent modifications and applications"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="BARTOSZ SAWICKI">
      <data key="d0">PERSON</data>
      <data key="d1">Bartosz Sawicki is an author of the paper "Monte Carlo tree search: A review of recent modifications and applications"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="JACEK MA'NDZIUK">
      <data key="d0">PERSON</data>
      <data key="d1">Jacek Ma'ndziuk is an author of the paper "Monte Carlo tree search: A review of recent modifications and applications"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="ARTIFICIAL INTELLIGENCE REVIEW">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Artificial Intelligence Review is the journal where the paper "Monte Carlo tree search: A review of recent modifications and applications" was published</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="HUGO TOUVRON">
      <data key="d0">PERSON</data>
      <data key="d1">Hugo Touvron is an author mentioned in the text and is notably recognized as an author of the paper "LLaMA: Open and efficient foundation language models."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="LOUIS MARTIN">
      <data key="d0">PERSON</data>
      <data key="d1">Louis Martin is an author mentioned in the text and is notably recognized as an author of the paper "LLaMA: Open and efficient foundation language models."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="KEVIN R. STONE">
      <data key="d0">PERSON</data>
      <data key="d1">Kevin R. Stone is an author mentioned in the text and is notably the author of the paper titled "LLaMA: Open and efficient foundation language models."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="PETER ALBERT">
      <data key="d0">PERSON</data>
      <data key="d1">Peter Albert is an author mentioned in the text and is known for co-authoring the paper "LLaMA: Open and efficient foundation language models."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="AMJAD ALMAHAIRI">
      <data key="d0">PERSON</data>
      <data key="d1">Amjad Almahairi is an author mentioned in the text and is notably recognized as an author of the paper "LLaMA: Open and efficient foundation language models."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YASMINE BABAEI">
      <data key="d0">PERSON</data>
      <data key="d1">Yasmine Babaei is an author mentioned in the text and is known for her contribution as an author of the paper "LLaMA: Open and efficient foundation language models."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="NIKOLAY BASHLYKOV">
      <data key="d0">PERSON</data>
      <data key="d1">Nikolay Bashlykov is an author mentioned in the text and is notably recognized for his contribution as an author of the paper titled "LLaMA: Open and efficient foundation language models."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="SOUMYA BATRA">
      <data key="d0">PERSON</data>
      <data key="d1">Soumya Batra is an author mentioned in the text and is notably recognized for co-authoring the paper titled "LLaMA: Open and efficient foundation language models."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="PRAJJWAL BHARGAVA">
      <data key="d0">PERSON</data>
      <data key="d1">Prajjwal Bhargava is an author mentioned in the text and is notably recognized for co-authoring the paper titled "LLaMA: Open and efficient foundation language models."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="SHRUTI BHOSALE">
      <data key="d0">PERSON</data>
      <data key="d1">Shruti Bhosale is an author mentioned in the text and is notably recognized for her contribution as an author of the paper titled "LLaMA: Open and efficient foundation language models."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="DANIEL M. BIKEL">
      <data key="d0">PERSON</data>
      <data key="d1">Daniel M. Bikel is an author mentioned in the text and is notably recognized for his contribution as an author of the paper "LLaMA: Open and efficient foundation language models."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="LUKAS BLECHER">
      <data key="d0">PERSON</data>
      <data key="d1">Lukas Blecher is an author mentioned in the text and is notably recognized as an author of the paper "LLaMA: Open and efficient foundation language models."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="CRISTIAN CANTON FERRER">
      <data key="d0">PERSON</data>
      <data key="d1">Cristian Canton Ferrer is an author of the paper "LLaMA: Open and efficient foundation language models"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="MOYA CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Moya Chen is an author mentioned in the text and is known for co-authoring the paper "LLaMA: Open and efficient foundation language models."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="GUILLEM CUCURULL">
      <data key="d0">PERSON</data>
      <data key="d1">Guillem Cucurull is an author mentioned in the text and is notably recognized as an author of the paper "LLaMA: Open and efficient foundation language models."</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="CRISTIAN CANT&#211;N FERRER">
      <data key="d0">PERSON</data>
      <data key="d1">Cristian Cant&#243;n Ferrer is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="DAVID ESIOBU">
      <data key="d0">PERSON</data>
      <data key="d1">David Esiobu is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="JUDE FERNANDES">
      <data key="d0">PERSON</data>
      <data key="d1">Jude Fernandes is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="JEREMY FU">
      <data key="d0">PERSON</data>
      <data key="d1">Jeremy Fu is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="WENYIN FU">
      <data key="d0">PERSON</data>
      <data key="d1">Wenyin Fu is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="BRIAN FULLER">
      <data key="d0">PERSON</data>
      <data key="d1">Brian Fuller is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="CYNTHIA GAO">
      <data key="d0">PERSON</data>
      <data key="d1">Cynthia Gao is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="VEDANUJ GOSWAMI">
      <data key="d0">PERSON</data>
      <data key="d1">Vedanuj Goswami is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="NAMAN GOYAL">
      <data key="d0">PERSON</data>
      <data key="d1">Naman Goyal is an author mentioned in the text and is notably recognized for his contribution as an author of the paper titled "Retrieval-augmented generation for knowledge-intensive NLP tasks."</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ANTHONY S. HARTSHORN">
      <data key="d0">PERSON</data>
      <data key="d1">Anthony S. Hartshorn is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="SAGHAR HOSSEINI">
      <data key="d0">PERSON</data>
      <data key="d1">Saghar Hosseini is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="RUI HOU">
      <data key="d0">PERSON</data>
      <data key="d1">Rui Hou is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="HAKAN INAN">
      <data key="d0">PERSON</data>
      <data key="d1">Hakan Inan is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="MARCIN KARDAS">
      <data key="d0">PERSON</data>
      <data key="d1">Marcin Kardas is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="VIKTOR KERKEZ">
      <data key="d0">PERSON</data>
      <data key="d1">Viktor Kerkez is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="MADIAN KHABSA">
      <data key="d0">PERSON</data>
      <data key="d1">Madian Khabsa is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ISABEL M. KLOUMANN">
      <data key="d0">PERSON</data>
      <data key="d1">Isabel M. Kloumann is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="A. V. KORENEV">
      <data key="d0">PERSON</data>
      <data key="d1">A. V. Korenev is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="PUNIT SINGH KOURA">
      <data key="d0">PERSON</data>
      <data key="d1">Punit Singh Koura is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="MARIE-ANNE LACHAUX">
      <data key="d0">PERSON</data>
      <data key="d1">Marie-Anne Lachaux is an author mentioned in the text and is notably recognized as an author of the paper "Mistral 7b."</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="THIBAUT LAVRIL">
      <data key="d0">PERSON</data>
      <data key="d1">Thibaut Lavril is an author mentioned in the text and is notably recognized as an author of the paper titled "Mistral 7b."</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="JENYA LEE">
      <data key="d0">PERSON</data>
      <data key="d1">Jenya Lee is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="DIANA LISKOVICH">
      <data key="d0">PERSON</data>
      <data key="d1">Diana Liskovich is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YINGHAI LU">
      <data key="d0">PERSON</data>
      <data key="d1">Yinghai Lu is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YUNING MAO">
      <data key="d0">PERSON</data>
      <data key="d1">Yuning Mao is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="XAVIER MARTINET">
      <data key="d0">PERSON</data>
      <data key="d1">Xavier Martinet is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="TODOR MIHAYLOV">
      <data key="d0">PERSON</data>
      <data key="d1">Todor Mihaylov is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="PUSHKAR MISHRA">
      <data key="d0">PERSON</data>
      <data key="d1">Pushkar Mishra is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="IGOR MOLYBOG">
      <data key="d0">PERSON</data>
      <data key="d1">Igor Molybog is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YIXIN NIE">
      <data key="d0">PERSON</data>
      <data key="d1">Yixin Nie is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ANDREW POULTON">
      <data key="d0">PERSON</data>
      <data key="d1">Andrew Poulton is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="JEREMY REIZENSTEIN">
      <data key="d0">PERSON</data>
      <data key="d1">Jeremy Reizenstein is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="RASHI RUNGTA">
      <data key="d0">PERSON</data>
      <data key="d1">Rashi Rungta is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="KALYAN SALADI">
      <data key="d0">PERSON</data>
      <data key="d1">Kalyan Saladi is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ALAN SCHELTEN">
      <data key="d0">PERSON</data>
      <data key="d1">Alan Schelten is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="RUAN SILVA">
      <data key="d0">PERSON</data>
      <data key="d1">Ruan Silva is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ERIC MICHAEL SMITH">
      <data key="d0">PERSON</data>
      <data key="d1">Eric Michael Smith is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="R. SUBRAMANIAN">
      <data key="d0">PERSON</data>
      <data key="d1">R. Subramanian is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="XIA TAN">
      <data key="d0">PERSON</data>
      <data key="d1">Xia Tan is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="BINH TANG">
      <data key="d0">PERSON</data>
      <data key="d1">Binh Tang is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ROSS TAYLOR">
      <data key="d0">PERSON</data>
      <data key="d1">Ross Taylor is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ADINA WILLIAMS">
      <data key="d0">PERSON</data>
      <data key="d1">Adina Williams is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="JIAN XIANG KUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Jian Xiang Kuan is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="PUXIN XU">
      <data key="d0">PERSON</data>
      <data key="d1">Puxin Xu is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ZHENGXU YAN">
      <data key="d0">PERSON</data>
      <data key="d1">Zhengxu Yan is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ILIYAN ZAROV">
      <data key="d0">PERSON</data>
      <data key="d1">Iliyan Zarov is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YUCHEN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yuchen Zhang is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ANGELA FAN">
      <data key="d0">PERSON</data>
      <data key="d1">Angela Fan is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="MELANIE KAMBADUR">
      <data key="d0">PERSON</data>
      <data key="d1">Melanie Kambadur is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="AURELIEN RODRIGUEZ">
      <data key="d0">PERSON</data>
      <data key="d1">Aurelien Rodriguez is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ROBERT STOJNIC">
      <data key="d0">PERSON</data>
      <data key="d1">Robert Stojnic is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="SERGEY EDUNOV">
      <data key="d0">PERSON</data>
      <data key="d1">Sergey Edunov is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="LLAMA 2">
      <data key="d0">MODEL</data>
      <data key="d1">Llama 2 is an open foundation and fine-tuned chat model mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="TOM VODOPIVEC">
      <data key="d0">PERSON</data>
      <data key="d1">Tom Vodopivec is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SPYRIDON SAMOTHRAKIS">
      <data key="d0">PERSON</data>
      <data key="d1">Spyridon Samothrakis is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BRANKO STER">
      <data key="d0">PERSON</data>
      <data key="d1">Branko Ster is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUQI XIE">
      <data key="d0">PERSON</data>
      <data key="d1">Yuqi Xie is an author mentioned in the text and is notably recognized as an author of the paper titled "Voyager: An open-ended embodied agent with large language models."</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHAOWEI XIAO">
      <data key="d0">PERSON</data>
      <data key="d1">Chaowei Xiao is an author mentioned in the text and is notably recognized for co-authoring the paper titled "Voyager: An open-ended embodied agent with large language models."</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VOYAGER">
      <data key="d0">TOOL/AGENT</data>
      <data key="d1">Voyager is an open-ended embodied agent with large language models mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">TOOL/AGENT</data>
    </node>
    <node id="ED CHI">
      <data key="d0">PERSON</data>
      <data key="d1">Ed Chi is an author mentioned in the text and is notably the author of the paper titled "Least-to-most prompting enables complex reasoning in large language models."</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MICHAEL WOOLDRIDGE">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Wooldridge is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NICHOLAS R JENNINGS">
      <data key="d0">PERSON</data>
      <data key="d1">Nicholas R Jennings is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THE KNOWLEDGE ENGINEERING REVIEW">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The Knowledge Engineering Review is a publication where the paper by Michael Wooldridge and Nicholas R Jennings was published</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="PHILIPP WU">
      <data key="d0">PERSON</data>
      <data key="d1">Philipp Wu is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALEJANDRO ESCONTRELA">
      <data key="d0">PERSON</data>
      <data key="d1">Alejandro Escontrela is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KEN GOLDBERG">
      <data key="d0">PERSON</data>
      <data key="d1">Ken Goldberg is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAYDREAMER">
      <data key="d0">TOOL/AGENT</data>
      <data key="d1">Daydreamer is a world model for physical robot learning mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">TOOL/AGENT</data>
    </node>
    <node id="YUXI XIE">
      <data key="d0">PERSON</data>
      <data key="d1">Yuxi Xie is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KENJI KAWAGUCHI">
      <data key="d0">PERSON</data>
      <data key="d1">Kenji Kawaguchi is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIRAN ZHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Yiran Zhao is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XU ZHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Xu Zhao is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MIN-YEN KAN">
      <data key="d0">PERSON</data>
      <data key="d1">Min-Yen Kan is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JUNXIAN HE">
      <data key="d0">PERSON</data>
      <data key="d1">Junxian He is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QIZHE XIE">
      <data key="d0">PERSON</data>
      <data key="d1">Qizhe Xie is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHILIN YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhilin Yang is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PENG QI">
      <data key="d0">PERSON</data>
      <data key="d1">Peng Qi is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAIZHENG ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Saizheng Zhang is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YOSHUA BENGIO">
      <data key="d0">PERSON</data>
      <data key="d1">Yoshua Bengio, an influential figure in the field of Artificial Intelligence and Machine Learning, is an author of the paper titled "Managing Extreme AI Risks Amid Rapid Progress." His contributions to the AI and ML community are significant, and he is frequently mentioned in discussions related to advancements and risks in AI technology.</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WILLIAM W COHEN">
      <data key="d0">PERSON</data>
      <data key="d1">William W Cohen is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HOWARD CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Howard Chen is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOHN YANG">
      <data key="d0">PERSON</data>
      <data key="d1">John Yang is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KARTHIK R NARASIMHAN">
      <data key="d0">PERSON</data>
      <data key="d1">Karthik R Narasimhan is an author mentioned in the text and is notably recognized for his contribution as an author of the paper titled "React: Synergizing reasoning and acting in language models."</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DIAN YU">
      <data key="d0">PERSON</data>
      <data key="d1">Dian Yu is an author mentioned in the text and is known for co-authoring the paper titled "React: Synergizing reasoning and acting in language models."</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JEFFREY ZHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Jeffrey Zhao is an author mentioned in the text and is notably the author of the paper titled "React: Synergizing reasoning and acting in language models."</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IZHAK SHAFRAN">
      <data key="d0">PERSON</data>
      <data key="d1">Izhak Shafran is an author mentioned in the text and is notably recognized for his contribution as an author of the paper titled "React: Synergizing reasoning and acting in language models."</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THOMAS L. GRIFFITHS">
      <data key="d0">PERSON</data>
      <data key="d1">Thomas L. Griffiths is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUAN CAO">
      <data key="d0">PERSON</data>
      <data key="d1">Yuan Cao is an author mentioned in the text and is notably recognized as an author of the paper titled "React: Synergizing reasoning and acting in language models."</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TREE OF THOUGHTS">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">Tree of Thoughts is a deliberate problem-solving method with large language models mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">TOOL/PROCESS</data>
    </node>
    <node id="WEIRUI YE">
      <data key="d0">PERSON</data>
      <data key="d1">Weirui Ye is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="SHAOHUAI LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Shaohuai Liu is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="THANARD KURUTACH">
      <data key="d0">PERSON</data>
      <data key="d1">Thanard Kurutach is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YANG GAO">
      <data key="d0">PERSON</data>
      <data key="d1">Yang Gao is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="NATHANAEL SCHARLI">
      <data key="d0">PERSON</data>
      <data key="d1">Nathanael Sch&#228;rli is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="LE HOU">
      <data key="d0">PERSON</data>
      <data key="d1">Le Hou is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="NATHAN SCALES">
      <data key="d0">PERSON</data>
      <data key="d1">Nathan Scales is an author mentioned in the text and is known for his contribution to the paper titled "Challenging big-bench tasks and whether chain-of-thought can solve them."</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="OLIVIER BOUSQUET">
      <data key="d0">PERSON</data>
      <data key="d1">Olivier Bousquet is an author mentioned in the text and is notably recognized for his contribution as an author of the paper titled "Least-to-most prompting enables complex reasoning in large language models."</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="XIANG CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Xiang Chen is an author mentioned in the text and is notably the author of the paper titled "ToolChain*: Efficient action space navigation in large language models with A* search."</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="TONG YU">
      <data key="d0">PERSON</data>
      <data key="d1">Tong Yu is an author mentioned in the text and is known for co-authoring the paper titled "ToolChain*: Efficient action space navigation in large language models with A* search."</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="SAAYAN MITRA">
      <data key="d0">PERSON</data>
      <data key="d1">Saayan Mitra is an author mentioned in the text and is known for authoring the paper titled "ToolChain*: Efficient action space navigation in large language models with A* search."</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="VICTOR BURSZTYN">
      <data key="d0">PERSON</data>
      <data key="d1">Victor Bursztyn is an author mentioned in the text and is notably recognized for his contribution as an author of the paper titled "ToolChain*: Efficient action space navigation in large language models with A* search."</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="RYAN A. ROSSI">
      <data key="d0">PERSON</data>
      <data key="d1">Ryan A. Rossi is an author mentioned in the text and is known for his contribution to the paper titled "ToolChain*: Efficient action space navigation in large language models with A* search."</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="SOMDEB SARKHEL">
      <data key="d0">PERSON</data>
      <data key="d1">Somdeb Sarkhel is an author mentioned in the text and is known for his contribution to the field of Artificial Intelligence and Machine Learning. He is the author of the paper titled "ToolChain*: Efficient action space navigation in large language models with A* search," which highlights his expertise in optimizing action space navigation within large language models using the A* search algorithm.</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="TOOLCHAIN*">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">ToolChain* is a method for efficient action space navigation in large language models with A* search mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ICLR 2022">
      <data key="d0">EVENT</data>
      <data key="d1">The conference where the paper "Least-to-most prompting enables complex reasoning in large language models" was presented</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="ICLR 2023">
      <data key="d0">EVENT</data>
      <data key="d1">The conference where the paper "ToolChain*: Efficient action space navigation in large language models with A* search" was presented</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="MINECRAFT">
      <data key="d0">ENVIRONMENT</data>
      <data key="d1">Minecraft is an environment suggested for future work with planning-based prompting methods like LATS</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="FAN ET AL. 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper by Fan et al. in 2022, related to the use of LATS in environments like Minecraft</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="ALFWORLD">
      <data key="d0">ENVIRONMENT</data>
      <data key="d1">Alfworld is an environment used in experiments with the LATS algorithm</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="TOOLBENCH">
      <data key="d0">TOOL</data>
      <data key="d1">ToolBench is a tool used in experiments with the LATS algorithm</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="QIN ET AL. 2024">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper by Qin et al. in 2024, related to the use of LATS in environments like ToolBench</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="CHEN ET AL. 2021">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper by Chen et al. in 2021, related to the use of LATS in environments like HumanEval</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="YAO ET AL. 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper by Yao et al. in 2022, related to the use of LATS in environments like WebShop</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="SHRIDHAR ET AL. 2020">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper by Shridhar et al. in 2020, related to the use of LATS in environments like Alfworld</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="HAO ET AL. 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper by Hao et al. in 2023, related to the use of LATS in environments like ToolBench</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="LIU ET AL. 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper by Liu et al. in 2023, related to the use of LATS in environments like ToolBench</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="SEC. B">
      <data key="d0">SECTION</data>
      <data key="d1">Section B of the appendix provides further discussion of the limitations of the LATS method</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="SEC. C">
      <data key="d0">SECTION</data>
      <data key="d1">Section C of the appendix presents additional experimental results of the LATS method</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="SEC. D">
      <data key="d0">SECTION</data>
      <data key="d1">Section D of the appendix specifies the environment details in the experiments with the LATS method</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="SEC. E">
      <data key="d0">SECTION</data>
      <data key="d1">Section E of the appendix lists the prompts used for the HotPotQA environment in the experiments with the LATS method</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="SEC. F">
      <data key="d0">SECTION</data>
      <data key="d1">Section F of the appendix lists the prompts used for the Programming environment in the experiments with the LATS method</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="SEC. G">
      <data key="d0">SECTION</data>
      <data key="d1">Section G of the appendix lists the prompts used for the WebShop environment in the experiments with the LATS method</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="MONTE CARLO TREE SEARCH">
      <data key="d0">METHOD</data>
      <data key="d1">Monte Carlo Tree Search is a method used in the LATS algorithm for decision-making tasks</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="DEPTH">
      <data key="d0">PARAMETER</data>
      <data key="d1">Depth is a parameter that defines the maximum number of steps in the search process for HotPotQA</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="LM VALUE FUNCTION">
      <data key="d0">FUNCTION</data>
      <data key="d1">The LM value function scores states based on expected future reward and guides the search process</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="WIKIPEDIA WEB API">
      <data key="d0">TOOL</data>
      <data key="d1">The Wikipedia web API is used for interactive information retrieval in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="CROWDWORKERS">
      <data key="d0">PERSON</data>
      <data key="d1">Crowdworkers are individuals who crafted the question-answer pairs in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="WIKIPEDIA">
      <data key="d0">SOURCE</data>
      <data key="d1">Wikipedia is the source of the documents used in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="HYPERPARAMETERS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Hyperparameters are settings used in the value function for the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="FIG. 3">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 3 shows the results of the HumanEval experiments and the performance of LATS over time</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="FIG. 4">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 4 illustrates how ReAct and LATS work on an example task of HotPotQA</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="ALGORITHM 1">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Algorithm 1 describes the LATS process, including initialization, expansion, simulation, evaluation, selection, and backpropagation</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SAMPLING SIZE">
      <data key="d0">PARAMETER</data>
      <data key="d1">Sampling size is a parameter used in the HumanEval dataset, with a value of n=5</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SELECTION FORMULA">
      <data key="d0">PARAMETER</data>
      <data key="d1">Selection formula is used in the LATS algorithm to choose actions based on exploration weight and value function</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="STATE SPACE">
      <data key="d0">PARAMETER</data>
      <data key="d1">State space refers to the complexity of the environment in which the LATS algorithm operates</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="ROLL-OUTS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Roll-outs refer to the number of iterations (K) in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="CONTEXT">
      <data key="d0">PARAMETER</data>
      <data key="d1">Context (c) is part of the state in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="VISIT COUNTER">
      <data key="d0">PARAMETER</data>
      <data key="d1">Visit counter (N) is used in the LATS algorithm to track the number of visits to each state</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="OBSERVATION SPACE">
      <data key="d0">PARAMETER</data>
      <data key="d1">Observation space (O) is the set of possible observations in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="INITIAL STATE">
      <data key="d0">PARAMETER</data>
      <data key="d1">Initial state (s) is the starting point in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="ACTION GENERATOR">
      <data key="d0">PARAMETER</data>
      <data key="d1">Action generator (p&#952;) is used to sample actions in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="REFLECTION GENERATOR">
      <data key="d0">PARAMETER</data>
      <data key="d1">Reflection generator (pref) is used to generate reflections in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SUCCESS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Success is a condition checked in the reflection phase of the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="EVALUATION OPERATION">
      <data key="d0">PARAMETER</data>
      <data key="d1">Evaluation operation is used to score states in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="ITERATIONS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Iterations refer to the repeated cycles of refining instructions to enhance their complexity and quality in the Instruction Refinement Flow. Additionally, iterations also denote the number of times the LATS algorithm is run.</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SUPPORTING FACTS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Supporting facts are provided by crowdworkers in the HotPotQA dataset to justify answers</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="ENTITY">
      <data key="d0">PARAMETER</data>
      <data key="d1">Entity is a type of question in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="LOCATION">
      <data key="d0">PARAMETER</data>
      <data key="d1">Location is a type of question in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="DATE">
      <data key="d0">PARAMETER</data>
      <data key="d1">Date is a type of question in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="COMPARISON">
      <data key="d0">PARAMETER</data>
      <data key="d1">Comparison is a type of question in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SHARED PROPERTIES">
      <data key="d0">PARAMETER</data>
      <data key="d1">Shared properties are compared between two entities in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SUPPORTING DOCUMENTS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Supporting documents are used to answer questions in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="QUESTION-ANSWER PAIRS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Question-answer pairs are crafted by crowdworkers in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="WIKIPEDIA PARAGRAPHS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Wikipedia paragraphs are used in the HotPotQA benchmark setting</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SUBSET OF 100 QUESTIONS">
      <data key="d0">PARAMETER</data>
      <data key="d1">A randomly selected subset of 100 questions is used in the HotPotQA experiments</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="MAXIMUM DEPTH LIMIT">
      <data key="d0">PARAMETER</data>
      <data key="d1">Maximum depth limit is set to 6 in the HotPotQA experiments</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="VALUE FUNCTION HYPERPARAMETERS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Value function hyperparameters include &#955;=0.5 for the LM score and self-consistency score in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SEARCH [ENTITY]">
      <data key="d0">PARAMETER</data>
      <data key="d1">Search [entity] is an action type in the Wikipedia web API used in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="LOOKUP [STRING]">
      <data key="d0">PARAMETER</data>
      <data key="d1">Lookup [string] is an action type in the Wikipedia web API used in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SEARCH">
      <data key="d0">ACTION/COMMAND</data>
      <data key="d1">SEARCH is an action that involves looking for specific products or information using specific keywords. In the context of HotPotQA, search refers to the action of searching for an entity on Wikipedia, where the search command returns the first 5 sentences from the corresponding entity's wiki page if it exists, or suggests the top-5 similar entities from the Wikipedia search engine. Additionally, search is a process used in exploring and discovering new designs in Advanced Driver Assistance Systems (ADAS). This action is commonly applied in various scenarios, such as looking for specific items or information in a webshop.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7,6bdf681c0bd9e401ac72344a6a0ae479,785ad59c6a37896a4676ec5c1689735f,b8dd0300033963bb4a3e1bad37f8e7b9,fb2b4544aedd793e4d4ec3147320a51c</data>
      <data key="d3">ACTION/COMMAND</data>
    </node>
    <node id="LOOKUP">
      <data key="d0">ACTION/COMMAND</data>
      <data key="d1">LOOKUP is an action in HotPotQA where the next sentence containing a keyword in the current passage is returned. The lookup command specifically returns the next sentence in the page that contains the specified string.</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9,fb2b4544aedd793e4d4ec3147320a51c</data>
      <data key="d3">ACTION/COMMAND</data>
    </node>
    <node id="FINISH">
      <data key="d0">ACTION/COMMAND</data>
      <data key="d1">FINISH is an action in HotPotQA where the final answer is provided, and the task is completed. The finish command completes the current task with the provided answer.</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9,fb2b4544aedd793e4d4ec3147320a51c</data>
      <data key="d3">ACTION/COMMAND</data>
    </node>
    <node id="AMAZON">
      <data key="d0">COMPANY/PLATFORM</data>
      <data key="d1">Amazon is an e-commerce platform from which over 1 million real-world products were scraped for the WebShop environment.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
      <data key="d3">COMPANY/PLATFORM</data>
    </node>
    <node id="TASK SCORE">
      <data key="d0">METRIC</data>
      <data key="d1">Task Score is an evaluation metric in WebShop defined as 100 times the average reward obtained across episodes.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
      <data key="d3">METRIC</data>
    </node>
    <node id="SUCCESS RATE">
      <data key="d0">METRIC</data>
      <data key="d1">Success Rate (SR) is a metric in WebShop defined as the portion of instructions where the reward equals 1. Additionally, it is an evaluation metric in WebShop that measures the portion of successful episodes.</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9,fb2b4544aedd793e4d4ec3147320a51c</data>
      <data key="d3">METRIC</data>
    </node>
    <node id="INTERACTIVE INFORMATION RETRIEVAL">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="FUNCTION SIGNATURE">
      <data key="d0">COMPONENT</data>
      <data key="d1">A function signature is part of a programming problem, including the function name and parameters.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="DOCSTRING">
      <data key="d0">COMPONENT</data>
      <data key="d1">A docstring is a description of a function's purpose and behavior, included in programming problems.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="REFERENCE IMPLEMENTATION">
      <data key="d0">COMPONENT</data>
      <data key="d1">A reference implementation is a sample solution provided for a programming problem.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="UNIT TESTS">
      <data key="d0">COMPONENT</data>
      <data key="d1">UNIT TESTS are tests used to validate the correctness of a function implementation in programming problems. They are designed to ensure that individual units of code, such as functions, work as intended by checking their output against expected results.</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f,fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="NATURAL LANGUAGE DESCRIPTION">
      <data key="d0">COMPONENT</data>
      <data key="d1">A natural language description explains a programming task in plain language, used in datasets like HumanEval and MBPP.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="ALGORITHMS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Algorithms are step-by-step procedures for solving problems, often evaluated in programming tasks.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="BASIC MATHEMATICS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Basic mathematics involves fundamental mathematical operations, often required in programming tasks.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="CROWDSOURCING">
      <data key="d0">PROCESS</data>
      <data key="d1">Crowdsourcing involves gathering input or data from a large group of people, used to create datasets like MBPP.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="QUERY SEARCHES">
      <data key="d0">ACTION/COMMAND</data>
      <data key="d1">Query searches are actions in WebShop that allow agents to search for products based on specified criteria.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="BUTTON CLICKS">
      <data key="d0">ACTION/COMMAND</data>
      <data key="d1">Button clicks are actions in WebShop that allow agents to interact with the web interface, such as selecting products or navigating pages.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="HTML MODE">
      <data key="d0">MODE</data>
      <data key="d1">HTML mode in WebShop provides pixel-level observations with interactive elements for agents to interact with.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="SIMPLE MODE">
      <data key="d0">MODE</data>
      <data key="d1">Simple mode in WebShop converts raw HTML into structured text observations for easier training of agents.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="LEXICAL MATCHING">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Lexical matching is a technique used in WebShop to compare the product purchased by the agent against specified attributes and options.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="SEMANTIC SIMILARITY">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Semantic similarity is a technique used in WebShop to compare the product purchased by the agent against specified attributes and options based on meaning.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="BAUER MEDIA GROUP">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Bauer Media Group is the publisher of the magazine First for Women</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="ARTHUR'S MAGAZINE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Arthur's Magazine was an American literary periodical published in Philadelphia in the 19th century</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="FIRST FOR WOMEN">
      <data key="d0">PUBLICATION</data>
      <data key="d1">FIRST FOR WOMEN is a woman's magazine published by Bauer Media Group in the USA. It was started in 1989 and is part of the discussion regarding the chronological order of magazine launches, specifically in the context of the question, "Which magazine was started first, Arthur&#8217;s Magazine or First for Women?"</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f,b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="GODEY'S LADY'S BOOK">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Godey's Lady's Book was a magazine that Arthur's Magazine was merged into in May 1846</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="QUERY">
      <data key="d0">ACTION</data>
      <data key="d1">Query is an action in WebShop where a search is performed</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="RESULTS">
      <data key="d0">STATE</data>
      <data key="d1">Results is a state in WebShop where search results are displayed</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="ITEM">
      <data key="d0">STATE</data>
      <data key="d1">Item is a state in WebShop where a specific product is selected</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="ITEM-DETAIL">
      <data key="d0">STATE</data>
      <data key="d1">Item-Detail is a state in WebShop where detailed information about a selected item is displayed</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="EPISODE END">
      <data key="d0">STATE</data>
      <data key="d1">Episode End is a state in WebShop where the buying process is completed</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="THOUGHT">
      <data key="d0">ACTION</data>
      <data key="d1">"THOUGHT" refers to a reasoning process about the current situation in a question answering task. In the context of HotPotQA, "thought" is an action where reasoning about the current situation is performed. Additionally, the "thought" section captures the meta agent's reasoning, overall concept, and implementation steps for designing the next function or agent. This comprehensive approach ensures that the reasoning process is well-documented and systematically applied to enhance the performance and accuracy of the task at hand.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0,357f3442ba581c9d2bdf84d90509056f,b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="ACTION">
      <data key="d0">ACTION</data>
      <data key="d1">ACTION in the context of HotPotQA refers to a specific task performed based on the Thought process. It is an operation executed by the user during their interaction with the system to progress in a question-answering task. This can include activities such as searching for information or looking up specific details to answer questions effectively.</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f,5d356b8ff719763a38cecff22c4e17b7,b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="COLORADO OROGENY">
      <data key="d0">ENTITY</data>
      <data key="d1">Colorado Orogeny is a geological event mentioned in a question in HotPotQA</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="HIGH PLAINS">
      <data key="d0">ENTITY</data>
      <data key="d1">High Plains is a region mentioned in a question in HotPotQA</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="VALUE FUNCTION PROMPT">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">The "VALUE FUNCTION PROMPT" is a versatile tool designed to analyze various trajectories and provide evaluative insights. It can be used to assess a purchasing trajectory and assign a correctness score ranging from 1 to 10. Additionally, it is capable of analyzing the trajectories involved in a solution to a question answering task, with a particular focus on thoughts, actions, and observations. This dual functionality makes the "VALUE FUNCTION PROMPT" a valuable asset for evaluating both consumer behavior and the cognitive processes involved in problem-solving tasks.</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f,6f486e20e3102c7a285e357d356417ad</data>
      <data key="d3">INSTRUCTION</data>
    </node>
    <node id="SEARCH[ENTITY]">
      <data key="d0">ACTION</data>
      <data key="d1">An action type that searches for the exact entity on Wikipedia and returns the first paragraph if it exists</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
      <data key="d3">ACTION</data>
    </node>
    <node id="LOOKUP[KEYWORD]">
      <data key="d0">ACTION</data>
      <data key="d1">An action type that returns the next sentence containing the specified keyword in the current passage</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
      <data key="d3">ACTION</data>
    </node>
    <node id="FINISH[ANSWER]">
      <data key="d0">ACTION</data>
      <data key="d1">An action type that returns the answer and finishes the task</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
      <data key="d3">ACTION</data>
    </node>
    <node id="ARTHUR&#8217;S MAGAZINE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">An American literary periodical published in Philadelphia in the 19th century, edited by Timothy Shay Arthur</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="REFLECTION PROMPT">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">The "REFLECTION PROMPT" is a versatile tool used in AI and machine learning to guide an AI Python assistant or an advanced reasoning agent in diagnosing and reflecting on failures. It serves multiple purposes, including analyzing why a function implementation is incorrect based on unit test results, diagnosing failures in previous trials, and devising new plans to mitigate these failures. Additionally, it prompts the agent to analyze the trajectories of solutions to question-answering tasks, focusing on thoughts, actions, and observations. This comprehensive approach helps in refining and improving AI performance by encouraging detailed reflection and strategic planning.</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f,6f486e20e3102c7a285e357d356417ad,785ad59c6a37896a4676ec5c1689735f</data>
      <data key="d3">INSTRUCTION</data>
    </node>
    <node id="PROGRAMMING PROMPTS">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">Prompts related to programming tasks, such as implementing a function or analyzing code</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
      <data key="d3">INSTRUCTION</data>
    </node>
    <node id="HUMANEVAL FUNCTION IMPLEMENTATION EXAMPLE">
      <data key="d0">EXAMPLE</data>
      <data key="d1">An example of a function implementation for evaluating human-like performance in programming tasks</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
      <data key="d3">EXAMPLE</data>
    </node>
    <node id="MINIMUM SUBARRAY SUM">
      <data key="d0">FUNCTION</data>
      <data key="d1">A function that finds the minimum sum of any non-empty sub-array of integers</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
      <data key="d3">FUNCTION</data>
    </node>
    <node id="GODEY&#8217;S LADY&#8217;S BOOK">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A magazine into which Arthur&#8217;s Magazine was merged in May 1846</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
    </node>
    <node id="TIMOTHY SHAY ARTHUR">
      <data key="d0">PERSON</data>
      <data key="d1">The editor of Arthur&#8217;s Magazine</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
    </node>
    <node id="EDGAR A. POE">
      <data key="d0">PERSON</data>
      <data key="d1">A contributor to Arthur&#8217;s Magazine</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
    </node>
    <node id="J.H. INGRAHAM">
      <data key="d0">PERSON</data>
      <data key="d1">A contributor to Arthur&#8217;s Magazine</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
    </node>
    <node id="SARAH JOSEPHA HALE">
      <data key="d0">PERSON</data>
      <data key="d1">A contributor to Arthur&#8217;s Magazine</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
    </node>
    <node id="THOMAS G. SPEAR">
      <data key="d0">PERSON</data>
      <data key="d1">A contributor to Arthur&#8217;s Magazine</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
    </node>
    <node id="AI PYTHON ASSISTANT">
      <data key="d0">TOOL/ROLE</data>
      <data key="d1">An AI assistant designed to help with Python programming tasks, including implementing functions, running unit tests, and reflecting on code</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="FUNCTION IMPLEMENTATION">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">The process of writing and defining a function in Python, including its signature and body</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="UNIT TEST">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">A type of software testing where individual units or components of a software are tested to validate that each unit performs as expected</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="BRIGHT CITRUS DEODORANT">
      <data key="d0">PRODUCT</data>
      <data key="d1">BRIGHT CITRUS DEODORANT by Earth Mama is a natural and safe deodorant specifically designed for sensitive skin, pregnancy, and breastfeeding. It features a bright citrus scent and contains organic calendula, making it gentle and suitable for those with sensitive skin. This deodorant is available in a 3-ounce size, ensuring a convenient and effective option for personal care.</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad,785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="EARTH MAMA">
      <data key="d0">BRAND</data>
      <data key="d1">EARTH MAMA is a brand that produces natural and safe products specifically designed for sensitive skin, pregnancy, and breastfeeding. Among its offerings, Earth Mama is known for producing the Bright Citrus Deodorant, which aligns with its commitment to providing gentle and effective personal care solutions.</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad,785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="GINGER FRESH DEODORANT">
      <data key="d0">PRODUCT</data>
      <data key="d1">GINGER FRESH DEODORANT by Earth Mama is a natural and safe deodorant specifically designed for sensitive skin, including those who are pregnant or breastfeeding. It features a refreshing ginger fresh scent and contains organic calendula, making it gentle and soothing. This deodorant is available in a convenient 3-ounce size, ensuring it is both effective and easy to use.</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad,785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="BARREL AND OAK">
      <data key="d0">BRAND</data>
      <data key="d1">A brand that produces aluminum-free deodorants and other personal care products</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="CEDAR &amp; PATCHOULI DEODORANT">
      <data key="d0">PRODUCT</data>
      <data key="d1">A type of deodorant with a cedar and patchouli blend, suitable for sensitive skin</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="MIN SUM">
      <data key="d0">CONCEPT</data>
      <data key="d1">A variable used in a function to keep track of the minimum sum encountered during iteration</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="CURRENT SUM">
      <data key="d0">CONCEPT</data>
      <data key="d1">A variable used in a function to keep track of the current sum during iteration</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="NUMS">
      <data key="d0">CONCEPT</data>
      <data key="d1">A list of numbers that the function iterates over to calculate sums</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="BASE ACTING/REASONING PROMPT">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A prompt given to an AI Python assistant to guide the implementation of a function based on previous code, unit tests, and self-reflection</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="IMPROVED IMPLEMENTATION">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">The revised version of a function after identifying and fixing errors from the previous implementation</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="TEST CASE GENERATION PROMPT">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A prompt given to an AI coding assistant to write unique and diverse unit tests for functions</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="ACTING PROMPT">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A prompt given to an AI assistant to guide actions in a specific scenario, such as searching for products in a webshop</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="THINK">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">"THINK" refers to the action of reflecting or considering information before making a decision. It involves the process of reasoning about the current situation or next steps, ensuring that all relevant information is taken into account before proceeding. This thoughtful consideration is crucial for making informed and effective decisions.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7,785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="CLICK">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">"CLICK" refers to the action of selecting or interacting with a specific item or option. This action is commonly used to select an item or link, such as a product in a webshop, or to choose an option or button within a user interface.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7,785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="PRICE">
      <data key="d0">CONCEPT</data>
      <data key="d1">The cost of a product, such as deodorant in a webshop</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="SIZE">
      <data key="d0">CONCEPT</data>
      <data key="d1">The dimensions or volume of a product, such as a 3-ounce bottle of deodorant</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="SCENT">
      <data key="d0">CONCEPT</data>
      <data key="d1">The fragrance or smell of a product, such as bright citrus or ginger fresh</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="ENJOY LIFE FOODS">
      <data key="d0">BRAND</data>
      <data key="d1">Enjoy Life Foods is a brand that offers a wide range of allergen-free food products, including dairy-free, nut-free, soy-free, gluten-free, and vegan options. The company is dedicated to providing safe and delicious food choices for individuals with food allergies and dietary restrictions.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,6f486e20e3102c7a285e357d356417ad</data>
      <data key="d3">BRAND</data>
    </node>
    <node id="DAIRY FREE AND APPLE VARIETY PACK OF CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">The "DAIRY FREE AND APPLE VARIETY PACK OF CHIPS" is a product that offers a selection of chips free from dairy, featuring an apple flavor. This variety pack is produced by Enjoy Life Foods, a company known for creating allergy-friendly and gluten-free snacks.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,6f486e20e3102c7a285e357d356417ad</data>
      <data key="d3">PRODUCT</data>
    </node>
    <node id="WEB SHOP">
      <data key="d0">PLATFORM</data>
      <data key="d1">An online shopping platform where users can search for and purchase items</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
      <data key="d3">PLATFORM</data>
    </node>
    <node id="CALMING LAVENDER DEODORANT">
      <data key="d0">PRODUCT</data>
      <data key="d1">Calming Lavender Deodorant by Earth Mama is a natural and safe deodorant for sensitive skin, pregnancy, and breastfeeding, containing organic calendula, available in a 3-ounce size</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="SIMPLY NON-SCENTS DEODORANT">
      <data key="d0">PRODUCT</data>
      <data key="d1">Simply Non-Scents Deodorant by Earth Mama is a natural and safe deodorant for sensitive skin, pregnancy, and breastfeeding, containing organic calendula, available in a 3-ounce size</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="ENJOY LIFE FOODS SOFT BAKED OVALS">
      <data key="d0">PRODUCT</data>
      <data key="d1">"ENJOY LIFE FOODS SOFT BAKED OVALS" are breakfast bars from Enjoy Life Foods that are nut-free, soy-free, dairy-free, non-GMO, gluten-free, and vegan. These bars are available in a variety pack of 4 boxes, making them a convenient and inclusive option for individuals with various dietary restrictions.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="ENJOY LIFE SOFT BAKED CHEWY BARS">
      <data key="d0">PRODUCT</data>
      <data key="d1">ENJOY LIFE SOFT BAKED CHEWY BARS are a specific product from Enjoy Life Foods. These chewy bars are nut-free, soy-free, dairy-free, and gluten-free, catering to various dietary restrictions. They are available in a variety pack of 6 boxes, making them a convenient and inclusive snack option for individuals with food allergies or sensitivities.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="ENJOY LIFE LENTIL CHIPS VARIETY PACK">
      <data key="d0">PRODUCT</data>
      <data key="d1">Enjoy Life Lentil Chips Variety Pack are dairy-free, soy-free, nut-free, non-GMO, vegan, gluten-free chips available in a pack of 24 bags</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="TRAVEL SET (4-PACK)">
      <data key="d0">PRODUCT</data>
      <data key="d1">A travel set of deodorants by Earth Mama, available in a pack of 4</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="3 OUNCE (PACK OF 1)">
      <data key="d0">PRODUCT</data>
      <data key="d1">A 3-ounce bottle of deodorant by Earth Mama, available in a pack of 1</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="3-OUNCE (2-PACK)">
      <data key="d0">PRODUCT</data>
      <data key="d1">A 3-ounce bottle of deodorant by Earth Mama, available in a pack of 2</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="DAIRY FREE CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Dairy-free chips produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
      <data key="d3">PRODUCT</data>
    </node>
    <node id="NUT FREE BARS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Nut-free bars produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="SOY FREE BARS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Soy-free bars produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="GLUTEN FREE BARS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Gluten-free bars produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="VEGAN BARS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Vegan bars produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="NON GMO BARS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Non-GMO bars produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="SOY FREE CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Soy-free chips produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="NUT FREE CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Nut-free chips produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="NON GMO CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Non-GMO chips produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="VEGAN CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Vegan chips produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="GLUTEN FREE CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Gluten-free chips produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="SOFT BAKED OVALS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Breakfast bars from Enjoy Life Foods that are nut-free, soy-free, dairy-free, non-GMO, gluten-free, and vegan</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="SOFT BAKED CHEWY BARS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Chewy bars from Enjoy Life Foods that are nut-free, soy-free, dairy-free, and gluten-free</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="LENTIL CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Lentil chips from Enjoy Life Foods that are dairy-free, soy-free, nut-free, non-GMO, vegan, and gluten-free</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="VARIETY PACK">
      <data key="d0">OPTION</data>
      <data key="d1">An option for the Enjoy Life Lentil Chips that includes multiple flavors</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="0.8 OUNCE (PACK OF 24)">
      <data key="d0">SIZE</data>
      <data key="d1">A size option for the Enjoy Life Lentil Chips variety pack</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="BUY NOW">
      <data key="d0">ACTION</data>
      <data key="d1">An action to purchase a product immediately</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON">
      <data key="d0">PRODUCT</data>
      <data key="d1">A gluten-free, vegetarian product with smoked peppered bacon flavor</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="SMOKED BACON SEA SALT">
      <data key="d0">PRODUCT</data>
      <data key="d1">A 3-pack of smoked bacon sea salt flavors that are gluten-free, non-GMO, and contain no MSG</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="SPICY HOT PEPPER SEA SALT">
      <data key="d0">PRODUCT</data>
      <data key="d1">A 3-pack of spicy hot pepper sea salt flavors that are gluten-free, kosher, non-GMO, and contain no MSG</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="LOUISVILLE VEGAN JERKY">
      <data key="d0">PRODUCT</data>
      <data key="d1">LOUISVILLE VEGAN JERKY offers a 5-flavor variety pack of vegan jerky made from non-GMO soy protein and is gluten-free. The flavors included in this pack are Black Pepper, Buffalo Dill, Pepperoni, Maple Bacon, and Carolina BBQ.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="PREVIOUS TRIAL INSTRUCTION">
      <data key="d0">ACTION</data>
      <data key="d1">An instruction given for a previous search or task</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="ENJOY LIFE LENTIL CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">A specific product from Enjoy Life Foods that includes lentil chips which are dairy-free, soy-free, nut-free, non-GMO, vegan, and gluten-free</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="SMOKED BACON CHIPOTLE">
      <data key="d0">PRODUCT</data>
      <data key="d1">A flavor option in the Smoked Bacon Sea Salt 3-Pack that is gluten-free, non-GMO, and contains no MSG</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="SMOKED BACON AND ONION">
      <data key="d0">PRODUCT</data>
      <data key="d1">A flavor option in the Smoked Bacon Sea Salt 3-Pack that is gluten-free, non-GMO, and contains no MSG</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="GHOST PEPPER">
      <data key="d0">PRODUCT</data>
      <data key="d1">GHOST PEPPER is a type of hot pepper featured as a flavor option in the Spicy Hot Pepper Sea Salt 3-Pack. This product is notable for being gluten-free, kosher, non-GMO, and containing no MSG, making it a suitable choice for a variety of dietary preferences and restrictions.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="JALAPENO">
      <data key="d0">PRODUCT</data>
      <data key="d1">JALAPENO is a type of hot pepper used in the Spicy Hot Pepper Sea Salt 3-Pack. This flavor option is gluten-free, kosher, non-GMO, and contains no MSG, making it a versatile and health-conscious choice for consumers.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="HABANERO">
      <data key="d0">PRODUCT</data>
      <data key="d1">HABANERO is a type of hot pepper used in the Spicy Hot Pepper Sea Salt 3-Pack. This flavor option is notable for being gluten-free, kosher, non-GMO, and containing no MSG, making it a versatile and health-conscious choice for consumers seeking both flavor and dietary considerations.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="BLACK PEPPER">
      <data key="d0">PRODUCT</data>
      <data key="d1">BLACK PEPPER is one of the flavors in the Louisville Vegan Jerky 5-Flavor Variety Pack. This flavor option is non-GMO, soy protein-based, and gluten-free, making it a suitable choice for those with dietary restrictions or preferences.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="BUFFALO DILL">
      <data key="d0">PRODUCT</data>
      <data key="d1">BUFFALO DILL is a flavor option in the Louisville Vegan Jerky 5-Flavor Variety Pack. This particular flavor is non-GMO, soy protein-based, and gluten-free, making it a suitable choice for those with dietary restrictions or preferences. As one of the flavors in the Louisville Vegan Jerky variety pack, BUFFALO DILL contributes to the diverse and inclusive range of options available to consumers seeking plant-based snack alternatives.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="PEPPERONI">
      <data key="d0">PRODUCT</data>
      <data key="d1">PEPPERONI is a flavor option in the Louisville Vegan Jerky 5-Flavor Variety Pack. It is non-GMO, soy protein-based, and gluten-free, making it a popular choice for those seeking plant-based, allergen-friendly snack options.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="MAPLE BACON">
      <data key="d0">PRODUCT</data>
      <data key="d1">MAPLE BACON is a flavor option in the Louisville Vegan Jerky 5-Flavor Variety Pack. It is non-GMO, soy protein-based, and gluten-free, making it a suitable choice for those with dietary restrictions or preferences. This flavor is part of the diverse selection offered in the Louisville Vegan Jerky variety pack.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="CAROLINA BBQ">
      <data key="d0">PRODUCT</data>
      <data key="d1">CAROLINA BBQ is a flavor option in the Louisville Vegan Jerky 5-Flavor Variety Pack. This particular flavor is non-GMO, soy protein-based, and gluten-free, making it a suitable choice for those with dietary restrictions or preferences. As one of the flavors in the Louisville Vegan Jerky variety pack, CAROLINA BBQ offers a unique taste experience within the assortment.</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="NON-GMO">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Non-GMO refers to products that are not genetically modified</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="SPICY HOT PEPPER SEA SALT 3-PACK">
      <data key="d0">PRODUCT</data>
      <data key="d1">A 3-pack of spicy hot pepper sea salt including Ghost Pepper, Jalapeno, and Habanero flavors. It is all-natural, gluten-free, kosher, no MSG, and non-GMO</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="GLUTEN-FREE">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Indicates that the product does not contain gluten</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="KOSHER">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Indicates that the product meets kosher dietary standards</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="NO MSG">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Indicates that the product does not contain monosodium glutamate</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="SOY PROTEIN">
      <data key="d0">INGREDIENT</data>
      <data key="d1">A plant-based protein used in Louisville Vegan Jerky</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="REFINE SEARCH">
      <data key="d0">ACTION</data>
      <data key="d1">The process of narrowing down search results to better match desired criteria</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="VEGETARIAN BACON">
      <data key="d0">PRODUCT</data>
      <data key="d1">A type of bacon alternative made from vegetarian ingredients</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="GLUTEN-FREE AND 4 OUNCE PACK OF 2">
      <data key="d0">CONSTRAINT</data>
      <data key="d1">Specific requirements for the product being searched for</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="STATUS">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">The current state or condition of the system or process</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="FAIL">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Indicates that the attempt or action was unsuccessful</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="PREVIOUS TRIAL">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Refers to the earlier attempt or session in the process</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="SHENGRAN HU">
      <data key="d0">PERSON</data>
      <data key="d1">Shengran Hu is a researcher affiliated with the University of British Columbia and the Vector Institute, specializing in the study of Automated Design of Agentic Systems. Hu is the author of several notable papers, including "Intelligent go-explore: Standing on the shoulders of giant foundation models" and "Thought Cloning: Learning to think while acting by imitating human thinking." Additionally, Hu is associated with the experiment and the repository containing all agents from the experiment, as well as the implementation of baselines and the repository available at https://github.com/ShengranHu/ADAS. Hu is also the creator of the Meta Agent Search algorithm and the author or maintainer of the framework code available on GitHub.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,24d7b89ae9522ae60d2317984951355b,449db721e37968e073e3579b59e023b2,6109537356a2ce2339f77c827aa3668e,97457e990eb6e3c88c11c862f9e3265b,c3d0436082aada237ee4bee645f16059,d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="CONG LU">
      <data key="d0">PERSON</data>
      <data key="d1">Cong Lu is a researcher involved in the study of Automated Design of Agentic Systems and is affiliated with the University of British Columbia and the Vector Institute. Cong Lu is an author of several notable papers, including "Intelligent go-explore: Standing on the shoulders of giant foundation models," "The AI Scientist: Towards fully automated open-ended scientific discovery," and "Varibad: Variational bayes-adaptive deep RL via meta-learning."</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,6109537356a2ce2339f77c827aa3668e,c3d0436082aada237ee4bee645f16059,cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </node>
    <node id="UNIVERSITY OF BRITISH COLUMBIA">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">The University of British Columbia is an educational institution where some of the researchers involved in the study of Automated Design of Agentic Systems are affiliated</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="VECTOR INSTITUTE">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">The Vector Institute is a research institution that has supported the work on the Automated Design of Agentic Systems. Some of the researchers involved in this study are affiliated with the Vector Institute, highlighting its role as a key player in advancing research in this area.</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="CANADA CIFAR AI CHAIR">
      <data key="d0">TITLE</data>
      <data key="d1">The Canada CIFAR AI Chair is a title held by Jeff Clune, one of the researchers involved in the study of Automated Design of Agentic Systems</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)">
      <data key="d0">RESEARCH AREA</data>
      <data key="d1">Automated Design of Agentic Systems (ADAS) is a research area focused on automatically creating powerful agentic system designs, including inventing novel building blocks and/or combining them in new ways</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="META AGENT SEARCH">
      <data key="d0">ALGORITHM</data>
      <data key="d1">META AGENT SEARCH is a comprehensive process and algorithm designed to improve results and reduce costs in agent discovery and evaluation. It involves a meta agent iteratively building on previous discoveries to program new agents, progressively discovering high-performance agents based on an ever-growing archive of previous findings. This method is used to discover agents that outperform state-of-the-art hand-designed baselines across multiple domains such as reading comprehension, math, multi-task problem solving, and science. It also aims to discover generalizable design patterns and agentic systems that can be transferred across different domains.

Meta Agent Search is particularly effective in tailoring agents to specific domains, showcasing its versatility and effectiveness. It is an algorithm in Advanced Driver Assistance Systems (ADAS) that enables the complete design of agentic systems in code space. The algorithm can be configured to prioritize safety during training, creating helpful, harmless, and honest agents. It involves executing untrusted model-generated code, which raises safety concerns, but also demonstrates the approach of defining and searching for agents in the context of ADAS. By adopting foundational models (FMs) as meta agents, it iteratively programs new agents, tests their performance on tasks, and adds them to an archive of discovered agents to inform subsequent iterations.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,449db721e37968e073e3579b59e023b2,4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479,7de66b94cf868b37b1df51dc545c415f,81c504ffbcc5ed882e234802135295ba,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b,c3d0436082aada237ee4bee645f16059,dc55f071b95dec721a9820d39cdb3ccd,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="FOUNDATION MODELS (FMS)">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Foundation Models (FMs) are integral modules within agentic systems, designed to solve tasks that require flexible reasoning and planning. These models are utilized in the control flow of such systems to plan, use tools, and execute multiple, iterative steps of processing. Additionally, Foundation Models are queried by the meta agent within the framework to assist in generating and improving agent architectures.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0,4884e8429ca1e567dadf5e22b4b68274,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="CLAUDE">
      <data key="d0">MODEL</data>
      <data key="d1">Claude is a Foundation Model developed by Anthropic, used for general-purpose agentic tasks</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="CHAIN-OF-THOUGHT">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Chain-of-Thought (COT) is a state-of-the-art, manually designed agent technique introduced by Wei et al. in 2022. It is utilized for generating possible answers, refining them, and ensembling the best answers within the Meta Agent Search algorithm. COT serves as a baseline for comparison in the evaluation of Meta Agent Search and is employed across various tasks, including Math, Reading Comprehension, Multi-task, and Science. Additionally, it functions as a planning and reasoning method, acting as a fundamental building block in agentic systems.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb,bc26e68b0b2783ba912b9e5606d9eb0b,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="TOOLFORMER">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Toolformer is a technique used in agentic systems to enable the use of external tools</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="META AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">META AGENT is a sophisticated system designed to create, program, and improve other agents, particularly within the context of Advanced Driver Assistance Systems (ADAS). It leverages the "gpt-4o-2024-05-13" model for evaluation and optimization, enabling it to generate code solutions for tasks such as the ARC challenge. The meta agent iteratively tests and enhances the performance of the agents it creates, thereby refining the ADAS algorithm's ability to discover new agentic systems. Through a process of self-reflection and debugging, the meta agent continuously improves agent architectures, ensuring optimal performance across various benchmarks.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,282313a8340c6792e8c35f53ed157cd0,4884e8429ca1e567dadf5e22b4b68274,4b43decac6833d1515992f8869ecada7,6bdf681c0bd9e401ac72344a6a0ae479,81c504ffbcc5ed882e234802135295ba,84317ae35cc75d612287186d93461447,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="AGENT ARCHIVE">
      <data key="d0">REPOSITORY</data>
      <data key="d1">Agent Archive is a repository where discovered agents are stored and used to inform the meta agent in subsequent iterations</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="MULTI-STEP PEER REVIEW AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">The Multi-Step Peer Review Agent is an agent discovered during the search in the Reading Comprehension domain. It serves as an example of an agent identified by the Meta Agent Search algorithm. This agent exemplifies the application of advanced search algorithms in the field of reading comprehension, highlighting the potential for automated systems to enhance the peer review process through multi-step evaluations.</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="VERIFIED MULTIMODAL AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">The Verified Multimodal Agent is an advanced computational entity designed to solve problems using visual representations and verification processes. It was discovered during the search in the Math domain, exemplifying the capabilities of the Meta Agent Search algorithm. This agent leverages multimodal data to enhance its problem-solving efficiency, making it a significant development in the field of Artificial Intelligence and Machine Learning.</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,c3d0436082aada237ee4bee645f16059,ef75d2c866bee783577ed9f65707cf13</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="DIVIDE AND CONQUER AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">The Divide and Conquer Agent is an agent discovered during the search in the Reading Comprehension domain. It serves as an example of an agent identified by the Meta Agent Search algorithm.</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="HOG">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">HOG (Histogram of Oriented Gradients) is a hand-designed feature used in computer vision that was eventually replaced by learned features from Convolutional Neural Networks</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="CONVOLUTIONAL NEURAL NETWORKS (CNNS)">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Convolutional Neural Networks (CNNs) are a type of neural network used in computer vision that replaced hand-designed features like HOG</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="NEURAL ARCHITECTURE SEARCH">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Neural Architecture Search (NAS) is a method for automating the design of neural network architectures. It provides insights into neural networks by observing the emerged architecture and is instrumental in creating the best-performing Convolutional Neural Network (CNN) models. NAS is a significant research area related to Advanced Driver Assistance Systems (ADAS), focusing on optimizing neural network architectures to enhance performance and efficiency.</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,7c08d98f503d722d7de13be55375c8cb,7de66b94cf868b37b1df51dc545c415f,81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="AUTOML">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">AutoML (Automated Machine Learning) is a field of research that aims to automate the process of machine learning model development. It involves methods and techniques that optimize the application of machine learning to real-world problems. AutoML is also related to Advanced Driver Assistance Systems (ADAS), highlighting its relevance in various practical applications.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,4884e8429ca1e567dadf5e22b4b68274,7c08d98f503d722d7de13be55375c8cb,81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="AI-GENERATING ALGORITHMS (AI-GAS)">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">AI-Generating Algorithms (AI-GAs) are methods that automatically generate AI systems, demonstrating the superiority of learned AI systems over hand-designed ones. These algorithms represent a significant advancement in the field of artificial intelligence, as they streamline the creation of AI systems by leveraging automated processes, thereby reducing the need for manual design and potentially leading to more efficient and effective AI solutions.</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="SHENGRAN HU'S GITHUB">
      <data key="d0">RESOURCE</data>
      <data key="d1">Shengran Hu's GitHub is a resource where the code related to the Automated Design of Agentic Systems can be found</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">RESOURCE</data>
    </node>
    <node id="ANTHROPIC">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Anthropic is an organization renowned for its contributions to the field of Artificial Intelligence, particularly through the development of advanced language models. They introduced the next generation of Claude, including the Claude 3.5 Sonnet. Additionally, Anthropic is the driving force behind the Claude-Haiku and Claude-Sonnet models, as well as the foundational Claude model. Their work in creating these sophisticated AI models underscores their influential role in the AI and ML landscape.</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,7de66b94cf868b37b1df51dc545c415f,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">ORGANIZATION</data>
    </node>
    <node id="WANG ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Wang et al. are prominent authors in the field of Artificial Intelligence and Machine Learning, known for their significant contributions to research on Foundation Models and agentic systems. They are the authors of the COT-SC agent and have developed a method for creating new skills for embodied agents, known as the COT-SC method. Their work is influential in advancing the capabilities and understanding of intelligent systems.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="ROCKT&#196;SCHEL">
      <data key="d0">AUTHOR</data>
      <data key="d1">Rockt&#228;schel is an author who has contributed to the research on compound agentic systems</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="ZAHARIA ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Zaharia et al. are authors who have contributed to the research on compound agentic systems</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="HU &amp; CLUNE">
      <data key="d0">AUTHOR</data>
      <data key="d1">Hu &amp; Clune are authors who have significantly contributed to the research on Chain-of-Thought-based planning and reasoning methods. Their work focuses on enhancing the capabilities of artificial intelligence in planning and reasoning by leveraging the Chain-of-Thought approach.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="ZHANG ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Zhang et al. are authors referenced in the context of Open-ended Algorithms and have made significant contributions to the research on memory structures in agentic systems. They are also the authors of the External Memory and RAG (Retrieval-Augmented Generation) methods, highlighting their influential role in advancing the understanding and development of memory mechanisms within artificial intelligence and machine learning.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,6bdf681c0bd9e401ac72344a6a0ae479,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="QU ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Qu et al. are authors who have significantly contributed to the research on tool use in agentic systems. They are known for their work on the Tool Use method, which has been influential in advancing the understanding and application of tools within these systems.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="MADAAN ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Madaan et al. are notable authors who have made significant contributions to the research on self-reflection in agentic systems. They are the authors of the Self-Refine agent and the Self-Refine method, which are pivotal in advancing the understanding and implementation of self-reflective capabilities in artificial intelligence systems.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="CLUNE (2019)">
      <data key="d0">AUTHOR</data>
      <data key="d1">"CLUNE (2019)" refers to a publication by Clune in 2019 that discusses AI-Generating Algorithms. In this work, Clune contributes to the research on the history of machine learning, particularly focusing on the transition from hand-designed solutions to learned solutions. This publication is significant in understanding the evolution of AI and machine learning methodologies.</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="DALAL &amp; TRIGGS (2005)">
      <data key="d0">AUTHOR</data>
      <data key="d1">Dalal &amp; Triggs (2005) are authors who have contributed to the research on hand-designed features in computer vision</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="KRIZHEVSKY ET AL. (2012)">
      <data key="d0">AUTHOR</data>
      <data key="d1">"KRIZHEVSKY ET AL. (2012)" refers to a seminal publication by Krizhevsky et al. in 2012 that discusses Convolutional Neural Networks (CNNs). This work is highly influential in the field of Artificial Intelligence and Machine Learning, particularly in the development and application of CNNs. The authors, Krizhevsky and his colleagues, have made significant contributions to the research on CNNs, which have become a foundational technology in various AI applications, including image and video recognition, medical image analysis, and more. Their 2012 paper is often cited as a pivotal moment in the advancement of deep learning techniques.</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="HUTTER ET AL. (2019)">
      <data key="d0">AUTHOR</data>
      <data key="d1">Hutter et al. (2019) is a publication that discusses AutoML methods. The authors, Hutter et al., have made significant contributions to the research on AutoML methods, highlighting their expertise and influence in the field of automated machine learning.</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="ELSKEN">
      <data key="d0">AUTHOR</data>
      <data key="d1">Elsken is an author who has made significant contributions to the field of Neural Architecture Search. Their work is frequently referenced in the context of this specialized area of research, highlighting their influence and expertise in developing methods and frameworks for optimizing neural network architectures.</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="MEMORY STRUCTURES">
      <data key="d0" />
      <data key="d1">Memory structures are components in agentic systems that store information for future use, aiding in reasoning and planning</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="TOOL USE">
      <data key="d0" />
      <data key="d1">Tool Use is a method used for improving the performance of models through the use of external tools. It involves the employment of functions or APIs to perform tasks or solve problems, and the manipulation of tools to achieve goals. In the context of AI, Tool Use refers to the ability of an AI system to utilize available resources or auxiliary systems to solve complex tasks. It is a component of agentic systems that involves using external tools to accomplish tasks. Additionally, Tool Use is one of the skills covered by the synthetic post-training dataset created by AgentInstruct.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,0c212c1467564ad33330b1f655a8e27e,4884e8429ca1e567dadf5e22b4b68274,b88745a13b69cecbc0ee9c3af41389bf,c3d0436082aada237ee4bee645f16059,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="BUILDING BLOCKS">
      <data key="d0">CONCEPT</data>
      <data key="d1">"Building blocks" are fundamental components used to construct agentic systems, such as chain-of-thought, self-reflection, and tool use. These building blocks are essential elements that can also be utilized to seed Advanced Driver Assistance Systems (ADAS). They serve as the foundational components necessary for the development and functionality of various agentic systems.</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479,81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="COMPOUND AGENTIC SYSTEM">
      <data key="d0">CONCEPT</data>
      <data key="d1">A compound agentic system is an agentic system composed of multiple components or building blocks to solve complex tasks</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="MONOLITHIC MODEL QUERY">
      <data key="d0">CONCEPT</data>
      <data key="d1">A monolithic model query is a single, standalone model used to solve a task, as opposed to a compound agentic system</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="APPENDIX F">
      <data key="d0">DOCUMENT SECTION</data>
      <data key="d1">Appendix F contains the detailed code of example agents</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="CNN">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Convolutional Neural Networks (CNNs) are a type of deep learning model used in various AI applications</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="ELSKEN ET AL. (2019)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Elsken et al. in 2019 that discusses Neural Architecture Search</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="SHEN ET AL. (2023)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Shen et al. in 2023 that discusses Neural Architecture Search</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="LLM ALIGNMENT">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">LLM alignment refers to aligning large language models to specific tasks or goals</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="LEARNED LOSS FUNCTIONS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Learned loss functions are loss functions that are optimized through learning rather than manually designed</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="LU ET AL. (2024A)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Lu et al. in 2024 that discusses learned loss functions in LLM alignment</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="DPO">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">DPO is a hand-designed loss function used in LLM alignment</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="RAFAILOV ET AL. (2024)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Rafailov et al. in 2024 that discusses DPO</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="AI SCIENTIST">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">AI Scientist is an automated research pipeline for developing novel machine learning algorithms</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="LU ET AL. (2024B)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Lu et al. in 2024 that discusses the AI Scientist</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="OMNI-EPIC">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">OMNI-EPIC is a system designed to enable the automatic generation of robotics learning environments. It allows FMs (presumably referring to Field Managers or Functional Managers) to create these environments by programming in code. The technique employed by OMNI-EPIC facilitates the generation of learning environments in an open-ended manner, providing a flexible and dynamic platform for robotics education and development.</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb,81c504ffbcc5ed882e234802135295ba,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="FALDOR ET AL. (2024)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Faldor et al. in 2024 that discusses OMNI-EPIC</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="ADAS">
      <data key="d0">RESEARCH AREA</data>
      <data key="d1">ADAS (Automated Design of Agentic Systems) is a proposed research area focused on the automated invention of novel building blocks and the design of powerful agentic systems. This field involves using programming languages as the search space and employs search algorithms to discover agentic systems that optimize an evaluation function. ADAS aims to progressively discover agents that outperform state-of-the-art hand-designed baselines and innovate through the combination of various stepping stones. Additionally, ADAS encompasses learning more components in agents beyond just prompts and provides a repository where detailed implementations of all baselines can be found. The goal is to leverage powerful functional models (FMs) without the need for expensive hardware, thereby advancing the field of agentic systems design.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479,7c08d98f503d722d7de13be55375c8cb,7de66b94cf868b37b1df51dc545c415f,81c504ffbcc5ed882e234802135295ba,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ARC LOGIC PUZZLE TASK">
      <data key="d0">BENCHMARK</data>
      <data key="d1">The ARC logic puzzle task is a benchmark that tests the general intelligence of an AI system</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="CHOLLET (2019)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Chollet in 2019 that discusses the ARC logic puzzle task</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="DROP">
      <data key="d0">BENCHMARK</data>
      <data key="d1">DROP (Discrete Reasoning Over Paragraphs) is a dataset and benchmark used to evaluate reading comprehension abilities of models. Introduced by Dua et al. in 2019, DROP is designed to assess the capability of agents to perform discrete reasoning and comprehend detailed information across multiple paragraphs. It requires models to resolve references in questions and execute discrete operations such as sorting, counting, and addition. DROP is utilized to evaluate the performance of various models, including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4. This benchmark is pivotal in the domain of reading comprehension, providing a ground-truth answer value for exact match/span extraction problems.</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50,10fda605f670bcfccfc13c2ca0dde959,24d7b89ae9522ae60d2317984951355b,81c504ffbcc5ed882e234802135295ba,84317ae35cc75d612287186d93461447,86f77e15d41cbd0cb33f635ccb2cb66b,bb87f82e6a9f1d4da6480ec78a0e3701,bc26e68b0b2783ba912b9e5606d9eb0b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="DUA ET AL. (2019)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Dua et al. in 2019 that discusses the DROP benchmark</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="MGSM">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MGSM (Math Generalization and Symbolic Manipulation) is a comprehensive dataset and benchmark used to evaluate the mathematical problem-solving abilities of agents. It is particularly notable for its application in a multilingual setting, as highlighted by Shi et al. in 2023. MGSM serves as a critical tool for assessing math capabilities across various languages, making it a valuable resource for understanding and improving the performance of AI agents in mathematical tasks. Additionally, MGSM is associated with the Math domain where the Verified Multimodal Agent was discovered, further emphasizing its significance in the field of AI and machine learning.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,81c504ffbcc5ed882e234802135295ba,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="SHI ET AL. (2023)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Shi et al. in 2023 that discusses the MGSM benchmark</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="GSM8K">
      <data key="d0">BENCHMARK</data>
      <data key="d1">GSM8K, also known as Grade School Math 8K, is a high-quality dataset comprising diverse grade school math word problems that typically require between 2 and 8 steps to solve. It serves as a benchmark for evaluating the performance of AI and language models on mathematical problem-solving tasks. GSM8K is utilized to assess the capabilities of various models, including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4. Additionally, it is used to evaluate the transferability of agents discovered through Meta Agent Search. This dataset is instrumental in exact match/span extraction problems and provides a comprehensive measure of a model's proficiency in handling math-based questions.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,103d98395c393552cc954c89d4e59f50,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,6fe27f9eb76cf2ddf712a2cee5783d1c,81c504ffbcc5ed882e234802135295ba,86f77e15d41cbd0cb33f635ccb2cb66b,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
      <data key="d3">BENCHMARK</data>
    </node>
    <node id="COBBE ET AL. (2021)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Cobbe et al. in 2021 that discusses the GSM8K benchmark</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="GSM-HARD">
      <data key="d0">BENCHMARK</data>
      <data key="d1">GSM-HARD is a benchmark and dataset designed for evaluating the performance and transferability of models and agents in more challenging mathematical problem-solving tasks. It is specifically used to assess the capabilities of agents discovered by Meta Agent Search in handling complex math tasks.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">BENCHMARK</data>
    </node>
    <node id="GAO ET AL. (2023)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Gao et al. in 2023 that discusses the GSM-Hard benchmark</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="FERNANDO ET AL. (2024)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Fernando et al. in 2024 that discusses ADAS methods focusing on designing prompts</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="YANG ET AL. (2024)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Yang et al. in 2024 that discusses ADAS methods focusing on designing prompts</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="BOYER &amp; MOORE (1983)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Boyer &amp; Moore in 1983 that discusses Turing Completeness</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="LADHA (2024)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Ladha in 2024 that discusses Turing Completeness</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="FMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Foundation Models (FMs) are large-scale models proficient in coding and possess the knowledge to solve questions in various domains. They are utilized as meta agents in Advanced Driver Assistance Systems (ADAS) and play a crucial role in the Meta Agent Search process.</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,81c504ffbcc5ed882e234802135295ba,bc26e68b0b2783ba912b9e5606d9eb0b,dc55f071b95dec721a9820d39cdb3ccd</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="LU ET AL. (2024C)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Lu et al. in 2024 that discusses open-endedness algorithms leveraging human notions of interestingness</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="ZHANG ET AL. (2024A)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Zhang et al. in 2024 that discusses open-endedness algorithms leveraging human notions of interestingness</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="AGENTIC SYSTEMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Agentic Systems refer to machine learning systems designed to perform tasks autonomously. These systems operate primarily over natural language and are interpretable to humans. They are optimized to improve performance in various domains, particularly effective in mitigating errors such as hallucinations or calculation mistakes. Agentic Systems involve the use of Foundation Models as modules to solve tasks by planning, using tools, and carrying out multiple, iterative steps of processing.</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,449db721e37968e073e3579b59e023b2,4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479,81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="ARC">
      <data key="d0">BENCHMARK</data>
      <data key="d1">ARC (Abstraction and Reasoning Corpus) is a challenging logic puzzle task and dataset used for search and evaluation experiments. It serves as a benchmark for testing the general intelligence of AI systems by evaluating the performance of discovered agents. Developed by AllenAI, the AI2 Reasoning Challenge (ARC) measures the reasoning, commonsense knowledge, and deep comprehension abilities of language models. The dataset involves learning transformation rules from input-output grid examples and is used in various experiments, including those mentioned in Section 4.1 of a specific paper. ARC is utilized to assess the performance of models such as Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4, as well as agents discovered by Meta Agent Search.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,449db721e37968e073e3579b59e023b2,4b43decac6833d1515992f8869ecada7,81c504ffbcc5ed882e234802135295ba,86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="READING COMPREHENSION">
      <data key="d0">BENCHMARK</data>
      <data key="d1">Reading Comprehension is a domain where models are evaluated on their ability to understand, process, and interpret written text. This task is critical for learning and encompasses decoding, fluency, and vocabulary knowledge. It enables scenarios such as question answering, search, and grounded reasoning. Reading comprehension benchmarks, such as the DROP benchmark used by Meta Agent Search, test the ability of AI systems to understand and interpret text. This domain is particularly important for Large Language Models (LLMs) and Small Language Models (SLMs), as they are better suited as reasoning engines rather than mere retrieval systems. Agentic systems can also be applied and evaluated in this domain, highlighting its significance in the field of AI and ML.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,0c212c1467564ad33330b1f655a8e27e,2901d5e2711fa4f32d39cd8eea36cd71,4884e8429ca1e567dadf5e22b4b68274,81c504ffbcc5ed882e234802135295ba,86f77e15d41cbd0cb33f635ccb2cb66b,bc26e68b0b2783ba912b9e5606d9eb0b,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SCIENCE QUESTIONS">
      <data key="d0">BENCHMARK</data>
      <data key="d1">Science questions benchmarks test the ability of AI systems to answer questions related to science</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="MULTI-TASK PROBLEM SOLVING">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MULTI-TASK PROBLEM SOLVING is a domain tested by Meta Agent Search using the MMLU benchmark. Multi-task problem solving benchmarks are designed to evaluate the ability of AI systems to solve a variety of tasks, assessing their versatility and adaptability across different challenges.</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="TRANSFERABILITY">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Transferability refers to the ability of AI systems to apply learned knowledge to different domains. Additionally, it encompasses the capability of agents discovered by Meta Agent Search to perform well across various models and domains. This concept is crucial in the AI and ML landscape as it highlights the adaptability and generalization potential of AI systems, enabling them to function effectively beyond their initial training environments.</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="OPEN-ENDEDNESS ALGORITHMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Open-endedness algorithms encourage the exploration of novel and interesting solutions</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="INTERESTINGNESS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Interestingness refers to the quality of being novel or worthwhile, often used in the context of open-endedness algorithms</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="F1 SCORES">
      <data key="d0">METRIC</data>
      <data key="d1">F1 scores are a measure of a model's accuracy, considering both precision and recall</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="ACCURACY RATES">
      <data key="d0">METRIC</data>
      <data key="d1">Accuracy rates measure the proportion of correct predictions made by a model</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="SEARCH SPACE">
      <data key="d0">COMPONENT</data>
      <data key="d1">The search space defines which agentic systems can be represented and thus discovered in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">COMPONENT</data>
    </node>
    <node id="EVALUATION FUNCTION">
      <data key="d0">COMPONENT</data>
      <data key="d1">The "EVALUATION FUNCTION" is a critical component used to assess the performance of discovered agents. It is designed to evaluate a candidate agent based on target objectives, such as performance metrics. This function can be optimized to reduce costs, ensuring that the agents are not only effective but also efficient in their operations.</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,ef75d2c866bee783577ed9f65707cf13</data>
      <data key="d3">COMPONENT</data>
    </node>
    <node id="PROMPTBREEDER">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">PromptBreeder is a tool designed to automate prompt engineering for agents by adopting Foundation Models (FMs). It focuses on enhancing the reasoning capability of agents through the careful phrasing of instructions within the prompts. While PromptBreeder mutates the text prompts of an agent, it ensures that other components, such as control flow, remain unchanged.</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,dc55f071b95dec721a9820d39cdb3ccd</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="CHASE">
      <data key="d0">PERSON</data>
      <data key="d1">Chase is an author referenced in the context of defining agentic systems involving Foundation Models</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NG">
      <data key="d0">PERSON</data>
      <data key="d1">Ng is an author referenced in the context of defining agentic systems involving Foundation Models</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CLUNE">
      <data key="d0">PERSON</data>
      <data key="d1">CLUNE is an author referenced in the context of discussions beyond the scope of the paper, particularly in research areas related to AI-Generative Algorithms (AI-GAs).</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HUTTER">
      <data key="d0">PERSON</data>
      <data key="d1">Hutter is an author referenced in the context of research areas in AutoML</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FERNANDO">
      <data key="d0">PERSON</data>
      <data key="d1">Fernando is an author referenced in the context of PromptBreeder and other ADAS-related works</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHUGE">
      <data key="d0">PERSON</data>
      <data key="d1">Zhuge is an author referenced in the context of search spaces such as graph structures and reinforcement learning in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Liu is an author referenced in the context of search spaces such as feed-forward networks in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SUTTON">
      <data key="d0">PERSON</data>
      <data key="d1">Sutton is an author referenced in the context of the exploration-exploitation trade-off in search algorithms</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BARTO">
      <data key="d0">PERSON</data>
      <data key="d1">Barto is an author referenced in the context of the exploration-exploitation trade-off in search algorithms</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AI-GAS">
      <data key="d0">RESEARCH AREA</data>
      <data key="d1">AI-GAs (Artificial Intelligence-Generative Algorithms) is a research area related to ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="FEED-FORWARD NETWORKS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Feed-forward networks are a type of search space explored in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="GRAPH STRUCTURES">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Graph structures are a type of search space explored in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="EXPLORATION-EXPLOITATION TRADE-OFF">
      <data key="d0">CONCEPT</data>
      <data key="d1">The exploration-exploitation trade-off is a concept in search algorithms that balances discovering high-performance agentic systems and avoiding local optima</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="ACCURACY RATE">
      <data key="d0">METRIC</data>
      <data key="d1">Accuracy rate is a metric used in the evaluation function to assess an agent's performance on validation data</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="COST">
      <data key="d0">METRIC</data>
      <data key="d1">COST is a metric used in the evaluation function to assess the expense of running an agent. It is also an objective that can be considered in multi-objective Advanced Driver Assistance Systems (ADAS). Additionally, COST refers to the resource-intensive nature of generating synthetic data with multiple agents using Large Language Models (LLMs) and tools.</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="LATENCY">
      <data key="d0">METRIC</data>
      <data key="d1">Latency is a metric used in the evaluation function to assess the response time of an agent. Additionally, latency is an objective that can be considered in multi-objective Advanced Driver Assistance Systems (ADAS). This highlights its importance in both performance evaluation and the optimization of complex systems where timely responses are critical.</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479</data>
    </node>
    <node id="SAFETY">
      <data key="d0">METRIC</data>
      <data key="d1">Safety is a metric used in the evaluation function to assess the risk associated with an agent</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="MATHEMATICS">
      <data key="d0">DOMAIN</data>
      <data key="d1">Mathematics is a domain where agentic systems can be applied and evaluated</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="VALIDATION DATA">
      <data key="d0">DATA</data>
      <data key="d1">Validation data is used to assess the performance of an agent on unseen future data</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="CONTROL FLOW">
      <data key="d0">COMPONENT</data>
      <data key="d1">Control flow is a component of agentic systems that dictates the sequence of operations or steps an agent follows</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="TEXT PROMPTS">
      <data key="d0">COMPONENT</data>
      <data key="d1">Text prompts are components of agentic systems that can be mutated to create new agents</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="PROGRAMMING LANGUAGES">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Programming languages are used as a search space in ADAS to define and discover new agentic systems</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="OPEN-SOURCE AGENT FRAMEWORKS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Open-source agent frameworks like LangChain are used to build upon existing building blocks in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="AI SAFETY">
      <data key="d0">CONCEPT</data>
      <data key="d1">AI safety is a concept that involves ensuring the safe operation of AI systems, which can be enhanced by using readable program code in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="DEBUGGING">
      <data key="d0">PROCESS</data>
      <data key="d1">Debugging is a process that involves identifying and fixing issues in program code, which is made easier by using readable code in Advanced Driver Assistance Systems (ADAS). Additionally, debugging encompasses the analysis of running logs to improve agentic systems, ensuring that both the code and the system's performance are optimized.</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="UNSEEN FUTURE DATA">
      <data key="d0">DATA</data>
      <data key="d1">Unseen future data is data that an agent has not encountered before, used to evaluate its performance</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">DATA</data>
    </node>
    <node id="EXISTING HUMAN EFFORTS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Existing human efforts refer to the prior work and knowledge that can be built upon in ADAS, especially when using programming languages as the search space</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="EXISTING BUILDING BLOCKS">
      <data key="d0">COMPONENT</data>
      <data key="d1">Existing building blocks are components like RAG and search engine tools that can be used in open-source agent frameworks in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">COMPONENT</data>
    </node>
    <node id="SEARCH ENGINE TOOLS">
      <data key="d0">TOOL/COMPONENT</data>
      <data key="d1">SEARCH ENGINE TOOLS are components that can be utilized in open-source agent frameworks such as LangChain within Advanced Driver Assistance Systems (ADAS). These tools represent existing human efforts that serve as foundational building blocks in the development and enhancement of ADAS technologies.</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TOOL/COMPONENT</data>
    </node>
    <node id="FM">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">FM, which stands for Foundation Models, is a versatile model utilized in various advanced methods such as COT (Chain of Thought) and COT-SC (Chain of Thought with Self-Consistency). These models play a crucial role in the development of ADAS (Advanced Driver Assistance Systems) by programming agents. Additionally, Foundation Models serve as meta agents in the Meta Agent Search algorithm, where they are instrumental in programming new agents.</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b,6bdf681c0bd9e401ac72344a6a0ae479,97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="FUNSEARCH">
      <data key="d0">ALGORITHM</data>
      <data key="d1">FUNSEARCH is a tool where Foundation Models write code to discover better optimization algorithms. It is also mentioned as an algorithm similar to Meta Agent Search, where a "forward" function is programmed to define a new agentic system.</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="LLM DEBATE">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">LLM Debate (Du et al., 2023) is a state-of-the-art, manually designed agent method used as a baseline in the evaluation of Meta Agent Search. It is employed for various tasks including Math, Reading Comprehension, Multi-task, and Science. The technique aims to improve the performance of models through debate and discussion, and it enhances refinement in the Meta Agent Search algorithm by incorporating multiple critics.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="QUALITY-DIVERSITY">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Quality-Diversity (Lu et al., 2024c) is a state-of-the-art, hand-designed agent used as a baseline in Meta Agent Search. It is a concept that can be incorporated into the design of search algorithms for Advanced Driver Assistance Systems (ADAS) and is utilized for various tasks such as Math, Reading Comprehension, Multi-task, and Science. Quality-Diversity is a method aimed at improving the performance of models through diversity and quality control, where three iterations are conducted to collect diverse answers based on previously proposed ones. It is a simplified version of Intelligent Go-Explore, producing and ensembling diverse answers to better explore potential solutions. This technique is used in the Meta Agent Search algorithm to explore new agents based on an ever-growing archive of previous discoveries, and it serves as a state-of-the-art hand-designed agent baseline for experiments on the Abstraction and Reasoning Corpus (ARC).</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,6bdf681c0bd9e401ac72344a6a0ae479,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="ROMERA-PAREDES">
      <data key="d0">PERSON</data>
      <data key="d1">Romera-Paredes is an author mentioned in relation to the FunSearch algorithm</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="LU">
      <data key="d0">PERSON</data>
      <data key="d1">Lu is an author mentioned in relation to open-endedness algorithms that leverage human notions of interestingness</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhang is an author mentioned in relation to open-endedness algorithms that leverage human notions of interestingness</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="MADAAN">
      <data key="d0">PERSON</data>
      <data key="d1">Madaan is an author mentioned in relation to self-reflection iterations in the meta agent</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="SHINN">
      <data key="d0">PERSON</data>
      <data key="d1">Shinn is an author mentioned in relation to self-reflection iterations in the meta agent</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="CHOLLET">
      <data key="d0">PERSON</data>
      <data key="d1">Chollet is an author mentioned in relation to the ARC logic puzzle task</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="DUA">
      <data key="d0">PERSON</data>
      <data key="d1">Dua is an author mentioned in relation to the DROP dataset</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="SHI">
      <data key="d0">PERSON</data>
      <data key="d1">Shi is an author mentioned in relation to the MGSM dataset</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="COBBE">
      <data key="d0">PERSON</data>
      <data key="d1">Cobbe is an author mentioned in relation to the GSM8K dataset</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="GAO">
      <data key="d0">PERSON</data>
      <data key="d1">Gao is an author mentioned in relation to the GSM-Hard dataset</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="ARC CHALLENGE">
      <data key="d0">TASK/CHALLENGE</data>
      <data key="d1">The ARC CHALLENGE, also known as the Abstraction and Reasoning Corpus challenge, is designed to evaluate the general intelligence of AI systems. It involves learning transformation rules from input-output grid examples to predict the output grid for a test example. The challenge aims to assess the ability of AI systems to efficiently acquire new skills, thereby providing a measure of their general intelligence.</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="CHAIN-OF-THOUGHT (COT)">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Chain-of-Thought (COT) is a state-of-the-art hand-designed agent baseline used for experiments on the Abstraction and Reasoning Corpus (ARC). It instructs the agent to output the reasoning process before providing an answer, thereby enhancing complex problem-solving capabilities through intermediate steps. This approach aims to improve the agent's performance by making the reasoning process explicit, which is particularly beneficial for tackling intricate problems.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT (COT-SC)">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Self-Consistency with Chain-of-Thought (COT-SC) is a state-of-the-art hand-designed agent baseline used for experiments on the ARC dataset. It operates by generating multiple parallel answers through the Chain-of-Thought (COT) process and then ensembles these answers to produce a more accurate final result.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="LLM-DEBATE">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">LLM-Debate is a state-of-the-art hand-designed agent baseline for experiments on ARC. It enables different Large Language Models (LLMs) to debate with each other, leveraging diverse perspectives to find better answers. In this method, each debate module is assigned a unique role, and the debate lasts for two rounds, facilitating a structured and comprehensive exploration of the topic at hand.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="ABSTRACTION AND REASONING CORPUS (ARC)">
      <data key="d0">DATASET</data>
      <data key="d1">The dataset used in the ARC challenge, consisting of visual input-output grid patterns</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="GREENBLATT, 2024">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a common practice in the field, requiring the agent to write code for the transformation rule instead of answering directly</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="MADAAN ET AL., 2024">
      <data key="d0">REFERENCE</data>
      <data key="d1">"Madaan et al., 2024 is a publication that introduces and discusses the Self-Refine method. This work is cited in the context of self-reflection and improving the generated agent, highlighting its significance in the field of AI and ML."</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,282313a8340c6792e8c35f53ed157cd0,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="DU ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">"Du et al., 2023" is a publication that discusses the LLM-Debate method. This work is a significant reference in the field, as it introduces and elaborates on the LLM-Debate technique. The publication by Du et al. in 2023 is frequently cited in discussions related to this method, highlighting its importance and influence in the ongoing discourse surrounding LLM Debate.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="LU ET AL., 2024C">
      <data key="d0">REFERENCE</data>
      <data key="d1">"LU ET AL., 2024C" is a publication by Lu et al. in 2024 that discusses the Quality-Diversity method. This work is referenced for introducing and elaborating on the Quality-Diversity technique, making it a significant contribution to the field.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="FALDOR ET AL., 2024">
      <data key="d0">REFERENCE</data>
      <data key="d1">"FALDOR ET AL., 2024" is a publication by Faldor et al. that discusses OMNI-EPIC and its application in enabling FMs to create robotics learning environments by programming in code. The paper also references prior works on open-endedness and AI-GAs, contributing to the broader discourse on artificial intelligence and machine learning.</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="LEHMAN &amp; STANLEY, 2011">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to prior works on open-endedness and AI-GAs</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="WANG ET AL., 2019">
      <data key="d0">REFERENCE</data>
      <data key="d1">"WANG ET AL., 2019" is a publication by Wang et al. in 2019 that is related to POET (Paired Open-Ended Trailblazer). This work references prior research on open-endedness and AI-GAs (Artificial Intelligence-Generative Algorithms), contributing to the ongoing discourse in these areas within the AI and ML community.</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="WANG ET AL., 2020">
      <data key="d0">REFERENCE</data>
      <data key="d1">"WANG ET AL., 2020" is a publication by Wang et al. in 2020 that is related to POET (Paired Open-Ended Trailblazer). This work references prior research on open-endedness and AI-GAs (Artificial Intelligence-Generative Algorithms), contributing to the ongoing discourse in these areas within the AI and ML community.</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="ZHANG ET AL., 2024A">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to prior works on open-endedness and AI-GAs</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="APPENDIX C">
      <data key="d0">DOCUMENT SECTION</data>
      <data key="d1">A section in the document providing detailed implementation of the best agent discovered by Meta Agent Search</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="APPENDIX E">
      <data key="d0">DOCUMENT SECTION</data>
      <data key="d1">"APPENDIX E" is a section in the document that provides more details about the baselines used in the study. This section is where readers can find comprehensive information regarding the baselines, offering a deeper understanding of the foundational metrics and comparisons employed in the research.</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="FIGURE 3">
      <data key="d0">VISUALIZATION</data>
      <data key="d1">A figure in the document showing the results of Meta Agent Search on the ARC challenge</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="PUBLIC TRAINING SET (EASY)">
      <data key="d0">DATASET</data>
      <data key="d1">A subset of the ARC dataset with grid dimensions &#8804;5&#215;5 used for training agents</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="VALIDATION SET">
      <data key="d0">DATASET</data>
      <data key="d1">A set of 20 questions sampled from the ARC dataset used for validating agents</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="TEST SET">
      <data key="d0">DATASET</data>
      <data key="d1">A set of 60 questions sampled from the ARC dataset used for testing agents</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="DYNAMIC MEMORY">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Dynamic memory is introduced for doing more refinements in the Meta Agent Search process</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="MULTIPLE CRITICS">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Multiple critics are introduced for enhanced refinement in the best agent discovered by Meta Agent Search</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="META-AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">The meta-agent in Meta Agent Search uses GPT-4 to discover novel agentic systems</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="CRITIC">
      <data key="d0">AGENT</data>
      <data key="d1">Critics are used to provide feedback for refining answers in the Meta Agent Search process</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="EFFICIENCY EXPERT">
      <data key="d0">AGENT</data>
      <data key="d1">EFFICIENCY EXPERT is an expert role assigned to evaluate the efficiency of answers in the Meta Agent Search process. This role involves providing feedback on the efficiency of the code, ensuring that the solutions generated are optimized and effective.</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="READABILITY EXPERT">
      <data key="d0">AGENT</data>
      <data key="d1">READABILITY EXPERT is an expert role assigned to evaluate the readability of answers in the Meta Agent Search process. This role involves providing feedback on the readability of the code, ensuring that the information is clear and accessible.</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="SIMPLICITY EXPERT">
      <data key="d0">AGENT</data>
      <data key="d1">SIMPLICITY EXPERT is an expert role assigned to evaluate the simplicity of answers in the Meta Agent Search process. This role involves providing feedback on the simplicity of the code, ensuring that solutions are straightforward and easy to understand.</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="ENSEMBLE">
      <data key="d0">PROCESS</data>
      <data key="d1">Ensembling is used to combine the best answers in the Meta Agent Search process</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="HUMAN-LIKE FEEDBACK">
      <data key="d0">PROCESS</data>
      <data key="d1">Human-like feedback is simulated to refine answers in the Meta Agent Search process</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="TRANSFORMATION RULE">
      <data key="d0">CONCEPT</data>
      <data key="d1">TRANSFORMATION RULE: A transformation rule is a rule learned from input-output grid examples to predict the output grid for a test example. This rule is integral to the AI system's ability to transform input grid patterns to output grid patterns in the Abstraction and Reasoning Corpus (ARC) challenge. The ARC challenge is a benchmark designed to evaluate the generalization capabilities of AI systems by requiring them to infer transformation rules from a limited set of examples.</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="NUMBER COUNTING">
      <data key="d0">SKILL</data>
      <data key="d1">A capability required by AI systems to efficiently learn from few-shot examples in the ARC challenge</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="GEOMETRY">
      <data key="d0">SKILL</data>
      <data key="d1">A capability required by AI systems to efficiently learn from few-shot examples in the ARC challenge</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="TOPOLOGY">
      <data key="d0">SKILL</data>
      <data key="d1">A capability required by AI systems to efficiently learn from few-shot examples in the ARC challenge</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="TOOL FUNCTIONS">
      <data key="d0">TOOL</data>
      <data key="d1">Functions provided in the framework to evaluate the generated transformation code in the ARC challenge</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="STOCHASTIC SAMPLING OF FMS">
      <data key="d0">PROCESS</data>
      <data key="d1">A process used to reduce variance in the validation and test accuracy of agents in the ARC challenge</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="ITERATION 3">
      <data key="d0">EVENT</data>
      <data key="d1">An iteration in Meta Agent Search where multiple COTs are used to generate possible answers, refine them, and ensemble the best answers</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="ITERATION 5">
      <data key="d0">EVENT</data>
      <data key="d1">"ITERATION 5" is an iteration in Meta Agent Search where the idea of incorporating diverse feedback emerged.</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="ITERATION 11">
      <data key="d0">EVENT</data>
      <data key="d1">"ITERATION 11" is an iteration in Meta Agent Search where the idea of evaluating for various specific traits via experts emerged. During this iteration, the focus was on assessing traits such as efficiency and simplicity, marking a significant step in refining the evaluation process within the Meta Agent Search framework.</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="ITERATION 12">
      <data key="d0">EVENT</data>
      <data key="d1">"ITERATION 12" in Meta Agent Search is notable for being the phase where the concept of simulating human-like feedback first emerged. This iteration marked a significant development in the project, focusing on enhancing the system's ability to mimic human responses and interactions.</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="MMLU">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MMLU (Massive Multitask Language Understanding) is a benchmark introduced by Hendrycks et al. in 2021 for evaluating multi-task problem-solving capabilities of AI models. It assesses a model&#8217;s ability to answer questions across a wide range of subjects and difficulty levels, encompassing 57 academic subjects with approximately 16,000 multiple-choice questions. MMLU is used to evaluate the performance of various language models, including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4. It is also part of the MIRAGE collection and includes tasks in areas such as abstract algebra and college mathematics.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,ab04427ae0415a1c812a35cf8d3ee1a2,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bc26e68b0b2783ba912b9e5606d9eb0b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="GPQA">
      <data key="d0">BENCHMARK</data>
      <data key="d1">GPQA (Graduate-Level Google-Proof Q&amp;A Benchmark) is a challenging dataset consisting of 448 high-quality, difficult multiple-choice questions across the domains of biology, physics, and chemistry, created by domain experts. Published in 2023 by Rein et al., GPQA serves as a benchmark for evaluating the capability of models to solve hard, graduate-level questions in science. It is used to assess the performance of various models, including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4. Additionally, GPQA is relevant in the Reading Comprehension domain, where it has been instrumental in the discovery of the Multi-Step Peer Review Agent and the Divide and Conquer Agent.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,3d1f6634f93f8a4c296dc8df7e59859e,84317ae35cc75d612287186d93461447,86f77e15d41cbd0cb33f635ccb2cb66b,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="STEP-BACK ABSTRACTION">
      <data key="d0">AGENT</data>
      <data key="d1">Step-back Abstraction, as described by Zheng et al. (2023), is a state-of-the-art, hand-designed agent used as a baseline in Meta Agent Search. This manually designed agent is employed for various tasks, including Math, Reading Comprehension, Multi-task, and Science. Additionally, Step-back Abstraction serves as a method for improving the performance of models through abstraction and simplification. It is particularly utilized in experiments focused on Reasoning and Problem-Solving domains, where it acts as a benchmark for evaluating other agents.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,10fda605f670bcfccfc13c2ca0dde959,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="ROLE ASSIGNMENT">
      <data key="d0">AGENT</data>
      <data key="d1">Role Assignment (Xu et al., 2023) is a state-of-the-art, hand-designed agent used as a baseline in Meta Agent Search. It is a manually designed agent employed for various tasks, including Math, Reading Comprehension, Multi-task, and Science. Additionally, Role Assignment is a method used for assigning different roles to modules within an agentic system, enabling them to collaborate effectively. This method is particularly utilized in experiments on Reasoning and Problem-Solving domains, serving as a benchmark for evaluating performance in these areas.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,10fda605f670bcfccfc13c2ca0dde959,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="DUA ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Dua et al. are the authors of the DROP benchmark for evaluating reading comprehension</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="SHI ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Shi et al. are the authors of the MGSM benchmark for evaluating math capability under a multi-lingual setting</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="HENDRYCKS ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Hendrycks et al. are the authors of the MMLU benchmark for evaluating multi-task problem solving</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="REIN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Rein et al. are the authors of the GPQA benchmark for evaluating the capability of solving hard (graduate-level) questions in science</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="ZHENG ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Zheng et al. are the authors of both the Step-back Abstraction agent and the Step-back Abstraction method.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="LU ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Lu et al. are authors referenced in the context of higher-order ADAS and subjective answer evaluations. They are also the authors of the Quality-Diversity agent and the Quality-Diversity method. Their work spans significant contributions to the fields of advanced driver-assistance systems (ADAS) and the development of innovative methods for evaluating and enhancing the diversity and quality of agents in artificial intelligence.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,6bdf681c0bd9e401ac72344a6a0ae479,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="XU ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Xu et al. are the authors of both the Role Assignment agent and the Role Assignment method.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="MECHANISM">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A sophisticated feedback mechanism that refines answers more effectively by incorporating diverse feedback, evaluating for various specific traits, and simulating human-like feedback</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="MEYERSON ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Meyerson et al. are the authors who discussed the concept of crossover in evolution via LLMs</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="EXPERTS">
      <data key="d0">PERSON</data>
      <data key="d1">Individuals who evaluate various specific traits such as efficiency and simplicity</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="F1 SCORE">
      <data key="d0">METRIC</data>
      <data key="d1">A performance metric used to evaluate agents in the Reading Comprehension and Math domains</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="ACCURACY">
      <data key="d0">METRIC</data>
      <data key="d1">ACCURACY is a performance metric used to evaluate agents in the Reading Comprehension and Math domains. It also refers to the potential inaccuracies in synthetic data due to its inability to perfectly replicate real-world data.</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="BOOTSTRAP CONFIDENCE INTERVAL">
      <data key="d0">METRIC</data>
      <data key="d1">The "BOOTSTRAP CONFIDENCE INTERVAL" is a statistical measure reported in the performance comparison of Meta Agent Search and state-of-the-art hand-designed agents. It is a statistical method used to report the test accuracy of agents discovered by Meta Agent Search. This method provides a robust way to estimate the confidence intervals for the performance metrics, ensuring that the reported results are reliable and statistically significant.</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="EXPERIMENT SETTINGS">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">Details about the datasets and experiment settings used in Meta Agent Search, found in Appendix D</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="BASELINES">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">Baselines are standard agents used for comparison in evaluations. They represent the state-of-the-art hand-designed agents employed for comparison in Meta Agent Search.</data>
      <data key="d2">84317ae35cc75d612287186d93461447,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="APPENDIX D">
      <data key="d0">DOCUMENT</data>
      <data key="d1">The section of the document where more details about datasets and experiment settings can be found</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="MULTI-TASK">
      <data key="d0">DOMAIN</data>
      <data key="d1">Multi-task is a domain where models are evaluated on their ability to perform multiple different tasks simultaneously. In this context, the knowledge embedded in foundational models (FMs) is often not sufficient to solve the challenging questions that arise. This highlights the complexity and the need for advanced methodologies in multi-task learning to effectively address and manage the diverse range of tasks involved.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">DOMAIN</data>
    </node>
    <node id="CLAUDE-HAIKU">
      <data key="d0">MODEL</data>
      <data key="d1">Claude-Haiku is a model from Anthropic used to evaluate the performance of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="CLAUDE-SONNET">
      <data key="d0">MODEL</data>
      <data key="d1">Claude-Sonnet is a model from Anthropic used to evaluate the performance of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="STRUCTURED FEEDBACK AND ENSEMBLE AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">The "Structured Feedback and Ensemble Agent" is an advanced agent that leverages structured feedback and ensemble methods to effectively solve tasks. Recognized as one of the top agents discovered by Meta Agent Search, this agent stands out for its innovative approach in the AI and ML landscape. By integrating structured feedback mechanisms with ensemble techniques, it enhances performance and accuracy, making it a significant player in the field.</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,449db721e37968e073e3579b59e023b2</data>
      <data key="d3">AGENT</data>
    </node>
    <node id="HIERARCHICAL COMMITTEE REINFORCEMENT AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">Hierarchical Committee Reinforcement Agent is one of the top agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">AGENT</data>
    </node>
    <node id="DYNAMIC MEMORY AND REFINEMENT AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">Dynamic Memory and Refinement Agent is one of the top agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">AGENT</data>
    </node>
    <node id="SVAMP">
      <data key="d0">DATASET</data>
      <data key="d1">SVAMP is a dataset utilized for evaluating the performance of models in mathematical problem-solving tasks. Specifically, it serves as a math dataset to assess the capabilities of agents discovered through Meta Agent Search.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">DATASET</data>
    </node>
    <node id="ASDIV">
      <data key="d0">DATASET</data>
      <data key="d1">ASDiv is a dataset utilized for evaluating the performance of models in mathematical problem-solving tasks. Specifically, it serves as a math dataset to assess the capabilities of agents discovered through Meta Agent Search.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">DATASET</data>
    </node>
    <node id="PATEL ET AL., 2021">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication that discusses the SVAMP dataset</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="MIAO ET AL., 2020">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication that discusses the ASDiv dataset</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="MULTI-TASK AND SCIENCE DOMAINS">
      <data key="d0">DOMAIN</data>
      <data key="d1">Multi-task and Science domains are areas where Meta Agent Search outperforms baselines, but the gap is smaller due to the challenging nature of the questions</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="HUMAN EFFORTS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Human efforts refer to the existing work and tools that can be leveraged in Advanced Driver Assistance Systems (ADAS). Additionally, human efforts encompass the manual work that can be saved by using Meta Agent Search to develop better task-specific agents.</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,6bdf681c0bd9e401ac72344a6a0ae479</data>
    </node>
    <node id="TASK-SPECIFIC AGENTS">
      <data key="d0">AGENT</data>
      <data key="d1">Task-specific agents are agents tailored to perform specific tasks effectively, discovered through Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="GENERALIZATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Generalization refers to the ability of agents discovered by Meta Agent Search to apply learned knowledge to new, unseen tasks or domains</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="MGSM (MATH)">
      <data key="d0">DOMAIN</data>
      <data key="d1">MGSM (Math) is a domain used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="GSM8K (COBBE ET AL., 2021)">
      <data key="d0">DOMAIN</data>
      <data key="d1">GSM8K (Cobbe et al., 2021) is a math domain used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="GSM-HARD (GAO ET AL., 2023)">
      <data key="d0">DOMAIN</data>
      <data key="d1">GSM-Hard (Gao et al., 2023) is a math domain used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="SVAMP (PATEL ET AL., 2021)">
      <data key="d0">DOMAIN</data>
      <data key="d1">SVAMP (Patel et al., 2021) is a math domain used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="ASDIV (MIAO ET AL., 2020)">
      <data key="d0">DOMAIN</data>
      <data key="d1">ASDiv (Miao et al., 2020) is a math domain used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="MGSM (MATH) DOMAIN">
      <data key="d0">DOMAIN</data>
      <data key="d1">MGSM (Math) domain is used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">DOMAIN</data>
    </node>
    <node id="FOUR POPULAR MATH DOMAINS">
      <data key="d0">DOMAIN</data>
      <data key="d1">Four popular math domains are used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">DOMAIN</data>
    </node>
    <node id="DOMAINS BEYOND MATH">
      <data key="d0">DOMAIN</data>
      <data key="d1">Domains beyond math are used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">DOMAIN</data>
    </node>
    <node id="TOP 3 AGENTS">
      <data key="d0">AGENT</data>
      <data key="d1">Top 3 agents are the best-performing agents discovered by Meta Agent Search, evaluated with GPT-3.5 on ARC and transferred to other models</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">AGENT</data>
    </node>
    <node id="INVENTED BUILDING BLOCKS AND DESIGN PATTERNS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Invented building blocks and design patterns are the components and strategies discovered by Meta Agent Search that contribute to the effectiveness of the agents</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="PERFORMANCE ON ARC">
      <data key="d0">CONCEPT</data>
      <data key="d1">Performance on ARC refers to the evaluation results of agents discovered by Meta Agent Search on the ARC dataset</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="TEST ACCURACY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Test accuracy is a measure of how well agents discovered by Meta Agent Search perform on specific datasets</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="95% BOOTSTRAP CONFIDENCE INTERVAL">
      <data key="d0">METHOD</data>
      <data key="d1">95% bootstrap confidence interval is a statistical method used to report the test accuracy of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">METHOD</data>
    </node>
    <node id="CLAUDE-SONNET (ANTHROPIC, 2024B)">
      <data key="d0">MODEL</data>
      <data key="d1">Claude-Sonnet (Anthropic, 2024b) is a model from Anthropic used to evaluate the performance of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="CLAUDE-HAIKU (ANTHROPIC, 2024A)">
      <data key="d0">MODEL</data>
      <data key="d1">Claude-Haiku (Anthropic, 2024a) is a model from Anthropic used to evaluate the performance of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="GPT-4 (OPENAI, 2024)">
      <data key="d0">MODEL</data>
      <data key="d1">GPT-4 (OpenAI, 2024) is a version of OpenAI's language model used to evaluate the performance of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="GPT-3.5 (OPENAI, 2022)">
      <data key="d0">MODEL</data>
      <data key="d1">GPT-3.5 (OpenAI, 2022) is a version of OpenAI's language model used to evaluate the performance of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="DYNAMIC ROLE-PLAYING ARCHITECTURE">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">Dynamic Role-Playing Architecture is a method used for improving the performance of models through dynamic role-playing and interaction. It has been identified as a top agent in the Math domain and has shown successful transferability to non-math domains. This architecture leverages the concept of dynamic role-playing to enhance model capabilities, making it a versatile tool across various fields.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="STRUCTURED MULTIMODAL FEEDBACK LOOP">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">The Structured Multimodal Feedback Loop is a method utilized for enhancing the performance of models through structured feedback and interaction. Initially recognized as a top agent in the Math domain, this method has demonstrated its versatility by being successfully transferred to non-math domains.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="INTERACTIVE MULTIMODAL FEEDBACK LOOP">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">The Interactive Multimodal Feedback Loop is a method used for improving the performance of models through interactive feedback and interaction. It has been recognized as a top agent in the Math domain and has successfully been transferred to non-math domains, showcasing its versatility and effectiveness across various fields.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="FM MODULES">
      <data key="d0">COMPONENT</data>
      <data key="d1">FM Modules are components in an agentic system that are assigned different roles and enabled to collaborate</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="AI-GENERATING ALGORITHMS">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">AI-Generating Algorithms (AI-GAs) are a field of research focused on learning components in AI systems to replace handcrafted ones. These methods are used for generating AI models and systems automatically, aiming to streamline and enhance the development process by reducing the need for manual intervention. AI-GAs represent a significant advancement in the automation of AI development, enabling more efficient and scalable creation of sophisticated AI systems.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="PATEL ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Patel et al. are the authors of the SVAMP dataset</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="MIAO ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Miao et al. are the authors of the ASDiv dataset</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="CHEN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Chen et al. are the authors of the Prompting Techniques method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="SCHULHOFF ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Schulhoff et al. are the authors of the Prompting Techniques method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="VEMPRALA ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Vemprala et al. are the authors of the method for developing new skills for embodied agents in code</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="NAKANO ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Nakano et al. are the authors of the Tool Use method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="HONG ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Hong et al. are authors referenced in the context of incorporating organizational structure in agents. They are known for developing a method for assigning FM modules in the agentic system with different roles and enabling these agents to collaborate effectively.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,6bdf681c0bd9e401ac72344a6a0ae479</data>
    </node>
    <node id="QIAN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Qian et al. are the authors of the method for assigning FM modules in the agentic system with different roles and enabling them to collaborate</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="RICHARDS">
      <data key="d0">PERSON</data>
      <data key="d1">Richards is the author of the method for enabling the agent to instruct itself for the next action</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="CHAIN-OF-THOUGHT-BASED PLANNING AND REASONING METHODS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="DEVELOPING NEW SKILLS FOR EMBODIED AGENTS IN CODE">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="EXTERNAL MEMORY AND RAG">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES AND ENABLING THEM TO COLLABORATE">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="ENABLING THE AGENT TO INSTRUCT ITSELF FOR THE NEXT ACTION">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="MAML">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">MAML (Model-Agnostic Meta-Learning) is a technique that allows "learning to learn" for better sample efficiency and generalizability</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="META-RL">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Meta-RL (Meta-Reinforcement Learning) is a technique that allows "learning to learn" for continuous learning of multiple tasks</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="POET">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">POET (Paired Open-Ended Trailblazer) is a technique aimed at generating learning environments in an open-ended manner</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="FOUNDATION MODELS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Foundation Models (FMs) are large-scale models used to write code for various applications, including optimization algorithms and reinforcement learning</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="EOH">
      <data key="d0">TOOL</data>
      <data key="d1">EoH is a tool where Foundation Models write code to discover better optimization algorithms</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="DISCOPOP">
      <data key="d0">TOOL</data>
      <data key="d1">DiscoPOP is a tool where Foundation Models program the loss function for preference learning in FM alignment training</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="EUREKA">
      <data key="d0">TOOL</data>
      <data key="d1">EUREKA is a tool designed for human-level reward design through the use of large language models. It enables Foundation Models (FMs) to write reward functions specifically for reinforcement learning applications in robotics. By leveraging advanced language models, EUREKA facilitates the creation of sophisticated reward functions, enhancing the capabilities and performance of reinforcement learning systems in robotic environments.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="LANGUAGE-TO-REWARD">
      <data key="d0">TOOL</data>
      <data key="d1">Language-to-Reward is a tool that enables Foundation Models (FMs) to write reward functions for reinforcement learning in robotics. This innovative tool leverages the capabilities of advanced language models to facilitate the creation of reward functions, which are crucial for guiding the learning process in reinforcement learning algorithms. By utilizing Foundation Models, Language-to-Reward aims to streamline and enhance the development of reward functions, thereby improving the efficiency and effectiveness of reinforcement learning applications in the field of robotics.</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="CLUNE, 2019">
      <data key="d0">PUBLICATION</data>
      <data key="d1">"CLUNE, 2019" refers to a paper authored by Clune in 2019 that discusses the pursuit of Artificial General Intelligence (AGI) and introduces the concept of AI-Generating Algorithms (AI-GA). This publication is significant in the AI and ML community for its exploration of how AI can be used to generate other AI systems, potentially accelerating the development of AGI.</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="HUTTER ET AL., 2019">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Hutter et al. in 2019 related to AutoML</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="ELSKEN ET AL., 2019">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Elsken et al. in 2019 related to Neural Architecture Search</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="HU ET AL., 2021">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Hu et al. in 2021 related to Neural Architecture Search</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="LU ET AL., 2019">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Lu et al. in 2019 related to Neural Architecture Search</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="FINN ET AL., 2017">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Finn et al. in 2017 related to MAML</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="DUAN ET AL., 2017">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Duan et al. in 2017 related to Meta-RL</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="NORMAN &amp; CLUNE, 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Norman &amp; Clune in 2023 related to Meta-RL</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="WANG ET AL., 2016">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Wang et al. in 2016 related to Meta-RL</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="ZINTGRAF ET AL., 2021A">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Zintgraf et al. in 2021 related to Meta-RL</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="ZINTGRAF ET AL., 2021B">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Zintgraf et al. in 2021 related to Meta-RL</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="DHARNA ET AL., 2020">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Dharna et al. in 2020 related to POET</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="ROMERA-PAREDES ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Romera-Paredes et al. in 2024 related to FunSearch</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="LU ET AL., 2024A">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Lu et al. in 2024 related to DiscoPOP</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="RAFAILOV ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Rafailov et al. in 2024 related to FM alignment training</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="MA ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">"MA ET AL., 2023" is a paper by Ma et al. published in 2023 that discusses Eureka and its application in enabling foundation models (FMs) to write reward functions for reinforcement learning in robotics.</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="YU ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">"YU ET AL., 2023" is a publication by Yu et al. in 2023 that discusses the concept of language-to-reward and its application in enabling foundation models (FMs) to write reward functions for reinforcement learning in robotics. This paper explores how natural language can be utilized to define reward functions, thereby facilitating more intuitive and flexible programming of robotic behaviors through reinforcement learning.</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ZHENG ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">"ZHENG ET AL., 2023" is a publication by Zheng et al. in 2023 that discusses and is referenced for the Step-back Abstraction method.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="XU ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">"Xu et al., 2023" is a publication that discusses the benefits of assigning personas or roles to agents. This paper, authored by Xu et al. in 2023, is specifically related to the Role Assignment method and is frequently referenced for its insights on this topic.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="OPRO">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">OPRO adopts FMs to automate prompt engineering for agents, focusing on the phrasing of instructions in the prompt to enhance reasoning capability</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="SELF-DISCOVER">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">Self-Discover adopts FMs to automate prompt engineering for agents, focusing on the phrasing of instructions in the prompt to enhance reasoning capability</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="EVOAGENT">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">EvoAgent optimizes role definition in the prompt, assigning personas or roles to agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="AGENTVERSE">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">AGENTVERSE is a framework that optimizes role definition in prompts by assigning personas or roles to agents. It is detailed in a paper titled "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors," which was presented at The Twelfth International Conference on Learning Representations in 2023. The framework aims to enhance multi-agent collaboration and investigate emergent behaviors within these systems.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="DYLAN">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">DyLAN uses FMs to score the response quality of nodes in each layer to prune the connections in a fully connected feed-forward network</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="DSPY">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">DSPy generates a set of possible nodes and optimizes across the Cartesian product of these nodes while optimizing the few-shot examples for nodes</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="GPT-SWARM">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">GPT-Swarm represents an agentic system in a graph with a predefined set of nodes and uses a Reinforcement Learning algorithm to optimize the possible connections between nodes</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="AGENTOPTIMIZER">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">AgentOptimizer learns the tools used in agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="AGENT SYMBOLIC LEARNING">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">Agent Symbolic Learning learns prompts, tools, and control flow together in agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="AGI">
      <data key="d0">CONCEPT</data>
      <data key="d1">Artificial General Intelligence (AGI) is a concept in AI research that involves creating highly autonomous systems with general intelligence</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="AI-GA">
      <data key="d0">CONCEPT</data>
      <data key="d1">AI-GA refers to AI Generative Algorithms, which could potentially contribute to creating AGI faster than the current manual approach</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="BENGIO ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing the pursuit of AGI and AI-GA</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="BOSTROM, 2002">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing the pursuit of AGI and AI-GA</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ECOFFET ET AL., 2020">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing the pursuit of AGI and AI-GA</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="YUDKOWSKY ET AL., 2008">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing the pursuit of AGI and AI-GA</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ROKON ET AL., 2020">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing safety concerns when executing untrusted model-generated code</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="YEE ET AL., 2010">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing the use of sandbox environments to safely run untrusted model-generated code</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="YANG ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing OPRO and its application in automating prompt engineering for agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="FERNANDO ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing PromptBreeder and its application in automating prompt engineering for agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ZHOU ET AL., 2024A">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing Self-Discover and its application in automating prompt engineering for agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="YUAN ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing EvoAgent and its application in optimizing role definition in the prompt</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="KHATTAB ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing DSPy and its application in generating a set of possible nodes and optimizing across the Cartesian product of these nodes</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ZHUGE ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing GPT-Swarm and its application in representing an agentic system in a graph with a predefined set of nodes</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ZHANG ET AL., 2024B">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing AgentOptimizer and its application in learning the tools used in agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ZHOU ET AL., 2024B">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing Agent Symbolic Learning and its application in learning prompts, tools, and control flow together in agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="BOSTROM">
      <data key="d0">PERSON</data>
      <data key="d1">Bostrom is an author referenced in the context of discussions beyond the scope of the paper</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ECOFFET ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Ecoffet et al. are authors referenced in the context of discussions beyond the scope of the paper</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUDKOWSKY ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Yudkowsky et al. are authors referenced in the context of discussions beyond the scope of the paper</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="API">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">API access to powerful FMs is used to program ADAS algorithms</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="SAFE-ADAS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Safe-ADAS refers to algorithms that conduct ADAS safely, avoiding harmful code and creating honest, helpful agents</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="CONSTITUTIONAL AI">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Constitutional AI is an idea that can be incorporated into Meta Agent Search to ensure the creation of safe agents</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="HIGHER-ORDER ADAS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Higher-order ADAS refers to the concept of improving the meta agent through ADAS, allowing for meta-learning of the meta agent and beyond</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="MULTI-OBJECTIVE ADAS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Multi-objective ADAS refers to integrating multiple objectives like cost, latency, and robustness into ADAS algorithms</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="NOVELTY SEARCH ALGORITHMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Novelty search algorithms focus on exploring new designs and can be incorporated into Meta Agent Search</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="AI-GENERATING">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">AI-generating refers to algorithms that can generate AI, a concept that can be incorporated into ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="OPEN-ENDED ALGORITHMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Open-ended algorithms are designed to explore a wide range of possibilities and can be incorporated into ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="EVALUATION FUNCTIONS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Evaluation functions are used to assess the performance of discovered agents in ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="MULTI-MODAL CAPABILITIES">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Multi-modal capabilities refer to the ability to handle different types of data, such as vision, in FMs</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="META">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Meta is an organization mentioned in the context of open-source research for safe-ADAS. Additionally, Meta is the organization that published the news article titled "Open source AI is the path forward." This highlights Meta's commitment to advancing open-source initiatives within the AI and ML landscape, particularly focusing on the development of safe Advanced Driver Assistance Systems (ADAS).</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">ORGANIZATION</data>
    </node>
    <node id="CALDWELL">
      <data key="d0">PERSON</data>
      <data key="d1">Caldwell is an author referenced in the context of open-source research for safe-ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BAI ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Bai et al. are authors referenced in the context of Constitutional AI</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LANGCHAINAI">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">LangChainAI is the organization behind both the LangChain framework and the LangChain project.</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">ORGANIZATION</data>
    </node>
    <node id="HU ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Hu et al. are authors referenced in the context of multi-objective ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DEB ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Deb et al. are authors referenced in the context of multi-objective search algorithms</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CULLY &amp; DEMIRIS">
      <data key="d0">PERSON</data>
      <data key="d1">Cully &amp; Demiris are authors referenced in the context of Quality-Diversity</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MOURET &amp; CLUNE">
      <data key="d0">PERSON</data>
      <data key="d1">Mouret &amp; Clune are authors referenced in the context of Quality-Diversity</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FALDOR ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Faldor et al. are authors referenced in the context of Open-ended Algorithms</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="STANLEY &amp; LEHMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Stanley &amp; Lehman are authors referenced in the context of Open-ended Algorithms</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="STANLEY ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Stanley et al. are authors referenced in the context of Open-ended Algorithms</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SUTTON &amp; BARTO">
      <data key="d0">PERSON</data>
      <data key="d1">Sutton &amp; Barto are authors referenced in the context of balancing exploration and exploitation</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHOU ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhou et al. are authors referenced in the context of intelligent evaluation functions</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHIANG ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Chiang et al. are authors referenced in the context of subjective answer evaluations</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NATURAL LANGUAGE">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Natural language is the representation used by agentic systems and humans in constructing organizations and society</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="HUMAN ORGANIZATIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Human organizations are structures that can be incorporated into agentic systems to improve their design</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="SOCIETY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Society is the broader context in which human organizations and agentic systems operate</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="COMPLEX DOMAINS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Complex domains refer to real-world applications involving multi-step interaction with complex environments</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="SINGLE-STEP QA TASKS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Single-step QA tasks are the type of tasks on which Meta Agent Search is evaluated in the paper</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="MULTIPLE DOMAINS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Multiple domains refer to the capability of ADAS algorithms to design agents that perform well across various fields</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="META-META AGENT">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Meta-meta agent is a higher-order agent that can be learned through meta-learning in ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="DATA PRIVACY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Data privacy is a priority that can influence the choice of FMs in agentic systems</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="ROBUSTNESS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Robustness is an objective that can be considered in multi-objective ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="EXPLORATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Exploration is a strategy in search algorithms to discover new designs</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="EXPLOITATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Exploitation is a strategy in search algorithms to utilize known good designs</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="SUBJECTIVE ANSWER EVALUATIONS">
      <data key="d0">PROCESS</data>
      <data key="d1">Subjective answer evaluations are tasks that do not have ground-truth answers and require novel evaluation functions in ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="GENERALIST AGENTS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Generalist agents are agents capable of performing well across multiple domains</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="INTERPRETABLE">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Interpretable refers to the quality of being understandable to humans, as in the case of natural language used in agentic systems</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">ATTRIBUTE</data>
    </node>
    <node id="ORGANIZATIONAL STRUCTURE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Organizational structure refers to the arrangement of roles and responsibilities within human companies, which can be incorporated into agents</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="HUMAN COMPANIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Human companies are organizations that can provide a model for the design of agentic systems</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="COMPLEXITY">
      <data key="d0">CONCEPT</data>
      <data key="d1">COMPLEXITY is one of the attributes that the Instruction Refinement Flow aims to enhance in the generated instructions. It refers to the intricate and interconnected nature of systems, which can emerge from human organizations and agentic systems.</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479,f7eb89a70f544664546a510e46d5febd</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="META-LEARNING">
      <data key="d0">PROCESS</data>
      <data key="d1">Meta-learning is the process of learning how to learn, applied to the meta agent and beyond in ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="TRAINING">
      <data key="d0">PROCESS</data>
      <data key="d1">Training is the process of teaching the meta agent to be safe and create helpful, harmless, honest agents</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="HONEST AGENTS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Honest agents are agents that are designed to be truthful and reliable</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="HELPFUL AGENTS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Helpful agents are agents that are designed to assist and provide value</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="HARMLESS AGENTS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Harmless agents are agents that are designed to avoid causing harm</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="NUMERICAL PERFORMANCE RESULTS">
      <data key="d0">DATA</data>
      <data key="d1">Numerical performance results are used to evaluate discovered agents in ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">DATA</data>
    </node>
    <node id="RUNNING LOGS">
      <data key="d0">DATA</data>
      <data key="d1">Running logs contain detailed information on the performance of agents during evaluation</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">DATA</data>
    </node>
    <node id="FAILURE MODES">
      <data key="d0">DATA</data>
      <data key="d1">Failure modes are patterns of errors or issues that occur during the operation of agentic systems</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">DATA</data>
    </node>
    <node id="SUCCESS MODES">
      <data key="d0">DATA</data>
      <data key="d1">Success modes are patterns of correct or optimal performance during the operation of agentic systems</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">DATA</data>
    </node>
    <node id="QA TASKS">
      <data key="d0">TASK</data>
      <data key="d1">QA tasks refer to question-answering tasks used to evaluate Meta Agent Search</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TASK</data>
    </node>
    <node id="REAL-WORLD APPLICATIONS">
      <data key="d0">TASK</data>
      <data key="d1">Real-world applications involve multi-step interactions with complex environments</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TASK</data>
    </node>
    <node id="ENVIRONMENTS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Environments are the settings or contexts in which agents operate and interact</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="INSTRUCTION">
      <data key="d0">DATA</data>
      <data key="d1">Instruction refers to the commands or tasks given to the meta agent in ADAS (Advanced Driver Assistance Systems). These instructions are the tasks or guidelines that agents follow to perform their roles in the agentic flows, ensuring the proper functioning and coordination within the system.</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479,f7eb89a70f544664546a510e46d5febd</data>
      <data key="d3">DATA</data>
    </node>
    <node id="DOMAIN">
      <data key="d0">CONCEPT</data>
      <data key="d1">Domain refers to the specific area or field in which ADAS algorithms operate</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="META AGENT SEARCH ALGORITHM">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Meta Agent Search Algorithm is the specific algorithm used in Meta Agent Search toMeta Agent Search Algorithm is the specific algorithm used in Meta Agent Search to explore new designs</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="QUALITY-DIVERSITY ALGORITHMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Quality-Diversity algorithms are used to explore a wide range of possibilities in ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="BALANCING EXPLORATION AND EXPLOITATION">
      <data key="d0">PROCESS</data>
      <data key="d1">Balancing exploration and exploitation is a strategy in search algorithms to discover new designs while utilizing known good designs</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="DETAILED RUNNING LOGS">
      <data key="d0">DATA</data>
      <data key="d1">Detailed running logs contain rich information on the performance of agents during evaluation</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">DATA</data>
    </node>
    <node id="AS">
      <data key="d0">CONCEPT</data>
      <data key="d1">AS refers to the agentic system, a machine learning system that operates primarily over natural language and is interpretable to humans</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
    </node>
    <node id="HUMAN ORGANIZATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Human organization refers to the structured arrangement of individuals and groups in society</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="HUMAN SOCIETY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Human society refers to the collective of human beings and their social structures and institutions</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="AGENTIC SYSTEM">
      <data key="d0">SYSTEM</data>
      <data key="d1">A machine learning system that operates primarily over natural language and is interpretable to humans</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">SYSTEM</data>
    </node>
    <node id="HONG ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A work that incorporates the organizational structure for human companies in agents</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="PARK ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A work that simulates a human town with agents</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="CANADA CIFAR AI CHAIRS PROGRAM">
      <data key="d0">PROGRAM</data>
      <data key="d1">A program that supported the work on Automated Design of Agentic Systems</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PROGRAM</data>
    </node>
    <node id="SCHMIDT FUTURES">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">An organization that provided grants for the work on Automated Design of Agentic Systems</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">ORGANIZATION</data>
    </node>
    <node id="OPEN PHILANTHROPY">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">An organization that provided grants for the work on Automated Design of Agentic Systems</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">ORGANIZATION</data>
    </node>
    <node id="NSERC DISCOVERY GRANT">
      <data key="d0">GRANT</data>
      <data key="d1">A grant that supported the work on Automated Design of Agentic Systems</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">GRANT</data>
    </node>
    <node id="RAFAEL COSMAN">
      <data key="d0">PERSON</data>
      <data key="d1">A person who made a generous donation to support the work on Automated Design of Agentic Systems</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JENNY ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jenny Zhang is a person acknowledged for insightful discussions and feedback on the work. She is also an author of the paper "OMNI: Open-endedness via models of human notions of interestingness" and the paper "Omni-epic: Open-endedness via models of human notions of interestingness with environments programmed in code."</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,2600a1ed94ad2d3675ea80575c39cbd1,7de66b94cf868b37b1df51dc545c415f,cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RACH PRADHAN">
      <data key="d0">PERSON</data>
      <data key="d1">A person acknowledged for insightful discussions and feedback on the work</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RUIYU GOU">
      <data key="d0">PERSON</data>
      <data key="d1">A person acknowledged for insightful discussions and feedback on the work</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NICHOLAS IOANNIDIS">
      <data key="d0">PERSON</data>
      <data key="d1">A person acknowledged for insightful discussions and feedback on the work</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="EUNJEONG HWANG">
      <data key="d0">PERSON</data>
      <data key="d1">A person acknowledged for insightful discussions and feedback on the work</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUNTAO BAI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAURAV KADAVATH">
      <data key="d0">PERSON</data>
      <data key="d1">Saurav Kadavath is an accomplished author in the field of Artificial Intelligence and Machine Learning. He has contributed to significant research, including the paper "Constitutional AI: Harmlessness from AI Feedback," which explores the development of AI systems that prioritize safety and ethical considerations. Additionally, he has co-authored the paper "Measuring mathematical problem solving with the math dataset," which focuses on evaluating AI's capabilities in solving mathematical problems. His work demonstrates a commitment to advancing AI technologies while addressing critical issues of safety and problem-solving efficacy.</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SANDIPAN KUNDU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JACKSON KERNION">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANDY JONES">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANNA CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANNA GOLDIE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AZALIA MIRHOSEINI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CAMERON MCKINNON">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GEOFFREY HINTON">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANDREW YAO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAWN SONG">
      <data key="d0">PERSON</data>
      <data key="d1">Dawn Song is an author of several influential papers in the field of Artificial Intelligence and Machine Learning. These include "Managing Extreme AI Risks Amid Rapid Progress," "Measuring Massive Multitask Language Understanding," "Measuring Mathematical Problem Solving with the Math Dataset," and "The False Promise of Imitating Proprietary LLMs." Her work spans critical areas such as AI risk management, language understanding, mathematical problem-solving, and the limitations of proprietary language models, highlighting her significant contributions to the AI and ML research community.</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,7de66b94cf868b37b1df51dc545c415f,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TREVOR DARRELL">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUVAL NOAH HARARI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YA-QIN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LAN XUE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHAI SHALEV-SHWARTZ">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="N BOSTROM">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ROBERT S BOYER">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "A Mechanical Proof of the Turing Completeness of Pure LISP"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="J STROTHER MOORE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "A Mechanical Proof of the Turing Completeness of Pure LISP"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TRACEY CALDWELL">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Ethical Hackers: Putting on the White Hat"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HARRISON CHASE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the blog post "What is an Agent?"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BANGHAO CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHAOFENG ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NICOLAS LANGREN&#201;">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHENGXIN ZHU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HENRIQUE PONDE DE OLIVEIRA PINTO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Evaluating Large Language Models Trained on Code"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HARRI EDWARDS">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Evaluating Large Language Models Trained on Code"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YURI BURDA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Evaluating Large Language Models Trained on Code"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEIZE CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Weize Chen is an author known for contributing to the field of multi-agent systems and software development. Chen has co-authored the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors," which delves into the dynamics of multi-agent collaboration and the emergent behaviors that arise from such interactions. Additionally, Chen has contributed to the paper "Communicative agents for software development," highlighting the role of communicative agents in enhancing software development processes. These works underscore Chen's expertise in leveraging agent-based models to address complex problems in collaborative environments and software engineering.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUSHENG SU">
      <data key="d0">PERSON</data>
      <data key="d1">Yusheng Su is an author known for contributing to the field of multi-agent systems and software development. He has co-authored the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors," which delves into the dynamics of multi-agent collaboration and emergent behaviors. Additionally, Yusheng Su has also authored the paper "Communicative agents for software development," focusing on the role of communicative agents in enhancing software development processes. His work significantly contributes to the understanding and advancement of collaborative networks and agent-based systems in the AI and ML landscape.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JINGWEI ZUO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHENG YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Cheng Yang is an author known for contributing to the field of multi-agent systems and software development. Notably, Cheng Yang co-authored the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors," which delves into the dynamics of multi-agent collaboration and the exploration of emergent behaviors within these systems. Additionally, Cheng Yang has also contributed to the paper "Communicative agents for software development," highlighting their expertise in the application of communicative agents to enhance software development processes. Through these works, Cheng Yang has made significant contributions to advancing the understanding and implementation of collaborative and communicative agents in AI and ML.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHENFEI YUAN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHI-MIN CHAN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HEYANG YU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YI-HSIN HUNG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHEN QIAN">
      <data key="d0">PERSON</data>
      <data key="d1">Chen Qian is an author known for contributing to the field of multi-agent systems and software development. They have co-authored the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors," which delves into the dynamics of multi-agent collaboration and emergent behaviors. Additionally, Chen Qian has also co-authored the paper "Communicative agents for software development," focusing on the role of communicative agents in enhancing software development processes. Through these works, Chen Qian has made significant contributions to understanding and advancing the collaborative and communicative capabilities of agents in complex systems.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEI-LIN CHIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Wei-Lin Chiang is an author known for contributing to the field of Artificial Intelligence and Machine Learning. Notably, Chiang has co-authored the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors," which delves into the dynamics of multi-agent systems and their collaborative potential. Additionally, Chiang has also contributed to the paper "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference," which focuses on the evaluation of large language models (LLMs) through human preferences. These works highlight Chiang's involvement in advancing the understanding and application of AI and ML technologies.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIANMIN ZHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Lianmin Zheng is an author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors" and also contributed to the paper "Chatbot arena: An open platform for evaluating LLMs by human preference."</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YING SHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Ying Sheng is an author known for contributing to the field of Artificial Intelligence and Machine Learning. Ying Sheng has co-authored the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors," which delves into the dynamics of multi-agent systems and their collaborative potential. Additionally, Ying Sheng has also contributed to the paper "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference," which focuses on the evaluation of large language models (LLMs) through human preferences. These works highlight Ying Sheng's involvement in advancing AI research, particularly in the areas of multi-agent collaboration and the assessment of language models.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANASTASIOS NIKOLAS ANGEL">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHEN, YUSHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Chen, Yusheng is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SU, JINGWEI">
      <data key="d0">PERSON</data>
      <data key="d1">Su, Jingwei is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZUO, CHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Zuo, Cheng is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YANG, CHENFEI">
      <data key="d0">PERSON</data>
      <data key="d1">Yang, Chenfei is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUAN, CHI-MIN">
      <data key="d0">PERSON</data>
      <data key="d1">Yuan, Chi-Min is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHAN, HEYANG">
      <data key="d0">PERSON</data>
      <data key="d1">Chan, Heyang is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YU, YAXI">
      <data key="d0">PERSON</data>
      <data key="d1">Yu, Yaxi is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HUNG, YI-HSIN">
      <data key="d0">PERSON</data>
      <data key="d1">Hung, Yi-Hsin is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QIAN, CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Qian, Chen is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THE TWELFTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The Twelfth International Conference on Learning Representations, held in 2023, featured the presentation of several notable papers, including "Large language models as optimizers," "OMNI: Open-endedness via models of human notions of interestingness," "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors," "Dspy: Compiling declarative language model calls into state-of-the-art pipelines," and "Eureka: Human-level reward design via coding large language models." This conference serves as a significant platform for the dissemination of cutting-edge research in the field of learning representations.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,1b1399c76420a477c0c97893d258ae69,2600a1ed94ad2d3675ea80575c39cbd1,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="ANASTASIOS NIKOLAS ANGELOPOULOS">
      <data key="d0">PERSON</data>
      <data key="d1">Anastasios Nikolas Angelopoulos is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TIANLE LI">
      <data key="d0">PERSON</data>
      <data key="d1">Tianle Li is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DACHENG LI">
      <data key="d0">PERSON</data>
      <data key="d1">Dacheng Li is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HAO ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Hao Zhang is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BANGHUA ZHU">
      <data key="d0">PERSON</data>
      <data key="d1">Banghua Zhu is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MICHAEL JORDAN">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Jordan is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOSEPH E. GONZALEZ">
      <data key="d0">PERSON</data>
      <data key="d1">Joseph E. Gonzalez is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ION STOICA">
      <data key="d0">PERSON</data>
      <data key="d1">Ion Stoica is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHATBOT ARENA">
      <data key="d0">PAPER</data>
      <data key="d1">Chatbot arena is a paper titled "Chatbot arena: An open platform for evaluating llms by human preference" published in 2024</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="FRAN&#199;OIS CHOLLET">
      <data key="d0">PERSON</data>
      <data key="d1">Fran&#231;ois Chollet is the author of the paper "On the measure of intelligence"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ON THE MEASURE OF INTELLIGENCE">
      <data key="d0">PAPER</data>
      <data key="d1">On the measure of intelligence is a paper authored by Fran&#231;ois Chollet, published as an arXiv preprint in 2019</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="AI-GAS: AI-GENERATING ALGORITHMS">
      <data key="d0">PAPER</data>
      <data key="d1">Ai-gas: Ai-generating algorithms is a paper authored by Jeff Clune, published as an arXiv preprint in 2019</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="TRAINING VERIFIERS TO SOLVE MATH WORD PROBLEMS">
      <data key="d0">PAPER</data>
      <data key="d1">"TRAINING VERIFIERS TO SOLVE MATH WORD PROBLEMS" is a paper authored by Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, and Reiichiro Nakano. It was published as an arXiv preprint in 2021. The paper discusses the training of verifiers to solve math word problems, contributing to the field of Artificial Intelligence and Machine Learning by exploring methods to enhance the problem-solving capabilities of AI systems in mathematical contexts.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="ANTOINE CULLY">
      <data key="d0">PERSON</data>
      <data key="d1">Antoine Cully is an author of the paper "Quality and diversity optimization: A unifying modular framework"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIANNIS DEMIRIS">
      <data key="d0">PERSON</data>
      <data key="d1">Yiannis Demiris is an author of the paper "Quality and diversity optimization: A unifying modular framework"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QUALITY AND DIVERSITY OPTIMIZATION">
      <data key="d0">PAPER</data>
      <data key="d1">Quality and diversity optimization is a paper authored by Antoine Cully and Yiannis Demiris, published in IEEE Transactions on Evolutionary Computation in 2017</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="N. DALAL">
      <data key="d0">PERSON</data>
      <data key="d1">N. Dalal is an author of the paper "Histograms of oriented gradients for human detection"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="B. TRIGGS">
      <data key="d0">PERSON</data>
      <data key="d1">B. Triggs is an author of the paper "Histograms of oriented gradients for human detection"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HISTOGRAMS OF ORIENTED GRADIENTS FOR HUMAN DETECTION">
      <data key="d0">PAPER</data>
      <data key="d1">Histograms of oriented gradients for human detection is a paper authored by N. Dalal and B. Triggs, presented at the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&#8217;05)</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="KALYANMOY DEB">
      <data key="d0">PERSON</data>
      <data key="d1">Kalyanmoy Deb is an author of several influential papers in the field of Artificial Intelligence and Machine Learning. His notable works include "A fast and elitist multiobjective genetic algorithm: Nsga-ii," "Nsga-net: neural architecture search using multi-objective genetic algorithm," and "Revisiting residual networks for adversarial robustness." These contributions highlight his expertise in genetic algorithms, neural architecture search, and adversarial robustness, making him a significant figure in the AI and ML research community.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,1b1399c76420a477c0c97893d258ae69,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AMRIT PRATAP">
      <data key="d0">PERSON</data>
      <data key="d1">Amrit Pratap is an author of the paper "A fast and elitist multiobjective genetic algorithm: Nsga-ii"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAMEER AGARWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Sameer Agarwal is an author of the paper "A fast and elitist multiobjective genetic algorithm: Nsga-ii"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TAMT MEYARIVAN">
      <data key="d0">PERSON</data>
      <data key="d1">TAMT Meyarivan is an author of the paper "A fast and elitist multiobjective genetic algorithm: Nsga-ii"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="A FAST AND ELITIST MULTIOBJECTIVE GENETIC ALGORITHM: NSGA-II">
      <data key="d0">PAPER</data>
      <data key="d1">A fast and elitist multiobjective genetic algorithm: Nsga-ii is a paper authored by Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan, published in IEEE Transactions on Evolutionary Computation in 2002</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="AARON DHARNA">
      <data key="d0">PERSON</data>
      <data key="d1">Aaron Dharna is an author of the paper "Co-generation of game levels and game-playing agents"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JULIAN TOGELIUS">
      <data key="d0">PERSON</data>
      <data key="d1">Julian Togelius is an author of the paper "Co-generation of game levels and game-playing agents"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LISA B SOROS">
      <data key="d0">PERSON</data>
      <data key="d1">Lisa B Soros is an author of the paper "Co-generation of game levels and game-playing agents"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CO-GENERATION OF GAME LEVELS AND GAME-PLAYING AGENTS">
      <data key="d0">PAPER</data>
      <data key="d1">Co-generation of game levels and game-playing agents is a paper authored by Aaron Dharna, Julian Togelius, and Lisa B Soros, presented at the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment in 2020</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="YILUN DU">
      <data key="d0">PERSON</data>
      <data key="d1">Yilun Du is an author of the paper "Improving factuality and reasoning in language models through multiagent debate"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHUANG LI">
      <data key="d0">PERSON</data>
      <data key="d1">Shuang Li is an author of the paper "Improving factuality and reasoning in language models through multiagent debate"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANTONIO TORRALBA">
      <data key="d0">PERSON</data>
      <data key="d1">Antonio Torralba is an author of the paper "Improving factuality and reasoning in language models through multiagent debate"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IMPROVING FACTUALITY AND REASONING IN LANGUAGE MODELS THROUGH MULTIAGENT DEBATE">
      <data key="d0">PAPER</data>
      <data key="d1">Improving factuality and reasoning in language models through multiagent debate is a paper authored by Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch, published as an arXiv preprint in 2023</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="DHEERU DUA">
      <data key="d0">PERSON</data>
      <data key="d1">Dheeru Dua is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIZHONG WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yizhong Wang is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2" and the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs."</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PRADEEP DASIGI">
      <data key="d0">PERSON</data>
      <data key="d1">Pradeep Dasigi is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GABRIEL STANOVSKY">
      <data key="d0">PERSON</data>
      <data key="d1">Gabriel Stanovsky is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAMEER SINGH">
      <data key="d0">PERSON</data>
      <data key="d1">Sameer Singh is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MATT GARDNER">
      <data key="d0">PERSON</data>
      <data key="d1">Matt Gardner is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DROP: A READING COMPREHENSION BENCHMARK REQUIRING DISCRETE REASONING OVER PARAGRAPHS">
      <data key="d0">PAPER</data>
      <data key="d1">"DROP: A READING COMPREHENSION BENCHMARK REQUIRING DISCRETE REASONING OVER PARAGRAPHS" is a paper authored by Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. It was published in 2019 and presented at the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. The paper discusses the DROP benchmark, which is designed for reading comprehension and requires discrete reasoning over paragraphs.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="YAN DUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Yan Duan is an author of the paper "RL^2: Fast reinforcement learning via slow reinforcement learning"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XI CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Xi Chen is an author of the paper "RL^2: Fast reinforcement learning via slow reinforcement learning"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PETER L. BARTLETT">
      <data key="d0">PERSON</data>
      <data key="d1">Peter L. Bartlett is an author of the paper "RL^2: Fast reinforcement learning via slow reinforcement learning"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RL^2: FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING">
      <data key="d0">PAPER</data>
      <data key="d1">RL^2: Fast reinforcement learning via slow reinforcement learning is a paper authored by Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel, presented at the International Conference on Learning Representations in 2017</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="JOEL LEHMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Joel Lehman is a prominent figure in the field of Artificial Intelligence and Machine Learning, known for his extensive contributions to the literature. He is the author of several influential papers, including "Abandoning objectives: Evolution through the search for novelty alone," "Designing neural networks through neuroevolution," "Language model crossover: Variation through few-shot prompting," "OMNI: Open-endedness via models of human notions of interestingness," and "Open questions in creating safe open-ended AI: Tensions between control and creativity." Additionally, he has co-authored papers such as "Poet: open-ended coevolution of environments and their optimized solutions" and "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions." Joel Lehman is also the author of the book "Why greatness cannot be planned: The myth of the objective," which explores the limitations of goal-oriented approaches in achieving greatness. His work often focuses on the themes of open-endedness, creativity, and the balance between control and innovation in AI systems.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,1b1399c76420a477c0c97893d258ae69,2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e,cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="OPEN QUESTIONS IN CREATING SAFE OPEN-ENDED AI: TENSIONS BETWEEN CONTROL AND CREATIVITY">
      <data key="d0">PAPER</data>
      <data key="d1">Open questions in creating safe open-ended AI: Tensions between control and creativity is a paper authored by Adrien Ecoffet, Jeff Clune, and Joel Lehman, presented at the Conference on Artificial Life in 2020</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="THOMAS ELSKEN">
      <data key="d0">PERSON</data>
      <data key="d1">Thomas Elsken is an author of the paper "Neural architecture search: A survey"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAN HENDRIK METZEN">
      <data key="d0">PERSON</data>
      <data key="d1">Jan Hendrik Metzen is an author of the paper "Neural architecture search: A survey"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FRANK HUTTER">
      <data key="d0">PERSON</data>
      <data key="d1">Frank Hutter is an influential figure in the field of Artificial Intelligence and Machine Learning, known for his significant contributions to automated machine learning and neural architecture search. He is an author of the book "Automated Machine Learning: Methods, Systems, Challenges," which delves into the methodologies, systems, and challenges associated with automating the machine learning process. Additionally, Frank Hutter has co-authored the paper "Neural Architecture Search: A Survey," which provides a comprehensive overview of the techniques and advancements in neural architecture search. His work has been instrumental in shaping the landscape of AI and ML, making him a key player in these domains.</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NEURAL ARCHITECTURE SEARCH: A SURVEY">
      <data key="d0">PAPER</data>
      <data key="d1">Neural architecture search: A survey is a paper authored by Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter, published in the Journal of Machine Learning Research in 2019</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="MAXENCE FALDOR">
      <data key="d0">PERSON</data>
      <data key="d1">Maxence Faldor is an author of the paper "Omni-epic: Open-endedness via models of human notions of interestingness with environments programmed in code"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PAL">
      <data key="d0">TOOL/FRAMEWORK</data>
      <data key="d1">Pal is a program-aided language model discussed in the paper by Yiming Yang, Jamie Callan, and Graham Neubig</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="INTERNATIONAL CONFERENCE ON MACHINE LEARNING">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The International Conference on Machine Learning (ICML) is a prominent venue for presenting cutting-edge research in the field of machine learning. Notable papers presented at ICML include "Enhanced POET: Open-ended Reinforcement Learning through Unbounded Invention of Learning Challenges and Their Solutions" and "PAL: Program-aided Language Models." This conference serves as a critical platform for researchers and practitioners to share innovative ideas and advancements, fostering collaboration and driving progress in the AI and ML community.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="RYAN GREENBLATT">
      <data key="d0">PERSON</data>
      <data key="d1">Ryan Greenblatt is the author of the technical report "Getting 50% sota on arc-agi with gpt-4"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="DAN HENDRYCKS">
      <data key="d0">PERSON</data>
      <data key="d1">Dan Hendrycks is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored significant papers such as "Measuring Massive Multitask Language Understanding" and "Measuring Mathematical Problem Solving with the Math Dataset." These works highlight his involvement in advancing the understanding and evaluation of AI capabilities in language processing and mathematical problem-solving.</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="COLLIN BURNS">
      <data key="d0">PERSON</data>
      <data key="d1">Collin Burns is an author of the paper "Measuring massive multitask language understanding" and also contributed to the paper "Measuring mathematical problem solving with the math dataset."</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="STEVEN BASART">
      <data key="d0">PERSON</data>
      <data key="d1">Steven Basart is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored significant papers, including "Measuring Massive Multitask Language Understanding" and "Measuring Mathematical Problem Solving with the Math Dataset." These works highlight his involvement in advancing the understanding and evaluation of complex language models and mathematical problem-solving capabilities within AI systems.</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ANDY ZOU">
      <data key="d0">PERSON</data>
      <data key="d1">Andy Zou is an author of the paper "Measuring massive multitask language understanding"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="MANTAS MAZEIKA">
      <data key="d0">PERSON</data>
      <data key="d1">Mantas Mazeika is an author of the paper "Measuring massive multitask language understanding"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="JACOB STEINHARDT">
      <data key="d0">PERSON</data>
      <data key="d1">Jacob Steinhardt is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored significant papers such as "Measuring Massive Multitask Language Understanding" and "Measuring Mathematical Problem Solving with the Math Dataset." These works highlight his involvement in advancing the understanding and evaluation of complex language and mathematical problem-solving tasks within AI.</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS (ICLR) is a prominent venue where significant advancements in the field of artificial intelligence and machine learning are showcased. Notable papers presented at this conference include "Language models are multilingual chain-of-thought reasoners," "Measuring massive multitask language understanding," and "OMNI: Open-endedness via models of human notions of interestingness." These contributions highlight the conference's role in advancing research on multilingual reasoning, multitask language understanding, and the modeling of human notions of interestingness, underscoring ICLR's influence in shaping the AI and ML landscape.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e,cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </node>
    <node id="SIRUI HONG">
      <data key="d0">PERSON</data>
      <data key="d1">Sirui Hong is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="XIAWU ZHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Xiawu Zheng is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JONATHAN CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Jonathan Chen is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUHENG CHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Yuheng Cheng is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JINLIN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jinlin Wang is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CEYAO ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Ceyao Zhang is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZILI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Zili Wang is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="STEVEN KA SHING YAU">
      <data key="d0">PERSON</data>
      <data key="d1">Steven Ka Shing Yau is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZIJUAN LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Zijuan Lin is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIYANG ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Liyang Zhou is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="METAGPT">
      <data key="d0">TOOL/FRAMEWORK</data>
      <data key="d1">Metagpt is a meta programming framework for multi-agent collaboration discussed in the paper by Sirui Hong et al.</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">TOOL/FRAMEWORK</data>
    </node>
    <node id="RAN CHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Ran Cheng is an author of the paper "Accelerating multi-objective neural architecture search by random-weight evaluation"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHENG HE">
      <data key="d0">PERSON</data>
      <data key="d1">Cheng He is an author of the paper "Accelerating multi-objective neural architecture search by random-weight evaluation"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHICHAO LU">
      <data key="d0">PERSON</data>
      <data key="d1">Zhichao Lu is an author of several influential papers in the field of neural architecture search and algorithm design. His notable works include "Accelerating multi-objective neural architecture search by random-weight evaluation," "Evolution of heuristics: Towards efficient automatic algorithm design using large language model," and "Nsga-net: neural architecture search using multi-objective genetic algorithm." These contributions highlight his expertise in leveraging genetic algorithms and large language models to enhance the efficiency and effectiveness of neural architecture search processes.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JING WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jing Wang is an author of the paper "Accelerating multi-objective neural architecture search by random-weight evaluation"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MIAO ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Miao Zhang is an author of the paper "Accelerating multi-objective neural architecture search by random-weight evaluation"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="COMPLEX &amp; INTELLIGENT SYSTEMS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Accelerating multi-objective neural architecture search by random-weight evaluation" was published</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="SHIHUA HUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Shihua Huang is an author of the paper "Revisiting residual networks for adversarial robustness"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VISHNU NARESH BODDETI">
      <data key="d0">PERSON</data>
      <data key="d1">Vishnu Naresh Boddeti is an author of the paper "Revisiting residual networks for adversarial robustness"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) is a prominent event in the field of computer vision and pattern recognition. It serves as a platform for presenting cutting-edge research and advancements in these domains. Notably, the conference featured the presentation of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network," which delves into innovative architectural designs for deep convolutional neural networks. Additionally, the paper "Revisiting residual networks for adversarial robustness" was also presented at this conference, highlighting significant research on enhancing the robustness of residual networks against adversarial attacks. CVPR continues to be a critical venue for disseminating influential research and fostering collaboration among experts in computer vision and machine learning.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="LARS KOTTHOFF">
      <data key="d0">PERSON</data>
      <data key="d1">Lars Kotthoff is an author of the book "Automated machine learning: methods, systems, challenges"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOAQUIN VANSCHOREN">
      <data key="d0">PERSON</data>
      <data key="d1">Joaquin Vanschoren is an author of the book "Automated machine learning: methods, systems, challenges"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SPRINGER NATURE">
      <data key="d0">PUBLISHER</data>
      <data key="d1">The publisher of the book "Automated machine learning: methods, systems, challenges"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PUBLISHER</data>
    </node>
    <node id="OMAR KHATTAB">
      <data key="d0">PERSON</data>
      <data key="d1">Omar Khattab is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines" and also contributed to the paper "The shift from models to compound AI systems." His work focuses on advancing the field of Artificial Intelligence and Machine Learning by developing innovative methods for integrating and optimizing language model calls and exploring the evolution from individual models to more complex AI systems.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ARNAV SINGHVI">
      <data key="d0">PERSON</data>
      <data key="d1">Arnav Singhvi is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PARIDHI MAHESHWARI">
      <data key="d0">PERSON</data>
      <data key="d1">Paridhi Maheshwari is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHIYUAN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhiyuan Zhang is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KESHAV SANTHANAM">
      <data key="d0">PERSON</data>
      <data key="d1">Keshav Santhanam is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAIFUL HAQ">
      <data key="d0">PERSON</data>
      <data key="d1">Saiful Haq is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ASHUTOSH SHARMA">
      <data key="d0">PERSON</data>
      <data key="d1">Ashutosh Sharma is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THOMAS T JOSHI">
      <data key="d0">PERSON</data>
      <data key="d1">Thomas T Joshi is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HANNA MOAZAM">
      <data key="d0">PERSON</data>
      <data key="d1">Hanna Moazam is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HEATHER MILLER">
      <data key="d0">PERSON</data>
      <data key="d1">Heather Miller is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines" and also contributed to the paper "The shift from models to compound AI systems." Her work focuses on advancing the field of Artificial Intelligence and Machine Learning by exploring innovative methods for integrating declarative language models into sophisticated AI pipelines and examining the evolution from traditional models to more complex AI systems.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALEX KRIZHEVSKY">
      <data key="d0">PERSON</data>
      <data key="d1">Alex Krizhevsky is an author of the paper "Imagenet classification with deep convolutional neural networks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GEOFFREY E HINTON">
      <data key="d0">PERSON</data>
      <data key="d1">Geoffrey E Hinton is an author of the paper "Imagenet classification with deep convolutional neural networks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ABRAHIM LADHA">
      <data key="d0">PERSON</data>
      <data key="d1">Abrahim Ladha is the author of the lecture "Lecture 11: Turing-completeness"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CS 4510 AUTOMATA AND COMPLEXITY">
      <data key="d0">COURSE</data>
      <data key="d1">The course for which the lecture "Lecture 11: Turing-completeness" was given</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">COURSE</data>
    </node>
    <node id="RISHABH SINGHAL">
      <data key="d0">PERSON</data>
      <data key="d1">Rishabh Singhal is the scribe for the lecture "Lecture 11: Turing-completeness"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KENNETH O STANLEY">
      <data key="d0">PERSON</data>
      <data key="d1">Kenneth O. Stanley is a prominent figure in the field of Artificial Intelligence and Machine Learning, known for his significant contributions to the understanding of evolutionary algorithms and neural network design. He is the author of the influential paper "Abandoning objectives: Evolution through the search for novelty alone," which explores the concept of novelty search as an alternative to traditional objective-based approaches in evolutionary computation. Additionally, Stanley has co-authored the paper "Designing neural networks through neuroevolution," which delves into the application of evolutionary algorithms for the development of neural network architectures. He is also the author of the book "Why greatness cannot be planned: The myth of the objective," where he argues against the conventional wisdom of goal-oriented planning in favor of more exploratory and innovative approaches. Through his work, Kenneth O. Stanley has significantly shaped contemporary thought on the evolution of artificial intelligence and the design of complex systems.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="EVOLUTIONARY COMPUTATION">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Abandoning objectives: Evolution through the search for novelty alone" was published</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="PATRICK LEWIS">
      <data key="d0">PERSON</data>
      <data key="d1">Patrick Lewis is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ETHAN PEREZ">
      <data key="d0">PERSON</data>
      <data key="d1">Ethan Perez is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALEKSANDRA PIKTUS">
      <data key="d0">PERSON</data>
      <data key="d1">Aleksandra Piktus is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FABIO PETRONI">
      <data key="d0">PERSON</data>
      <data key="d1">Fabio Petroni is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VLADIMIR KARPUKHIN">
      <data key="d0">PERSON</data>
      <data key="d1">Vladimir Karpukhin is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HEINRICH K&#220;TTLER">
      <data key="d0">PERSON</data>
      <data key="d1">Heinrich K&#252;ttler is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MIKE LEWIS">
      <data key="d0">PERSON</data>
      <data key="d1">Mike Lewis is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEN-TAU YIH">
      <data key="d0">PERSON</data>
      <data key="d1">Wen-tau Yih is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TIM ROCKT&#196;SCHEL">
      <data key="d0">PERSON</data>
      <data key="d1">Tim Rockt&#228;schel is a notable figure in the field of Artificial Intelligence and Machine Learning. He is an author of the influential paper "Retrieval-augmented generation for knowledge-intensive NLP tasks," which addresses advanced techniques in natural language processing. Additionally, he has authored the book "Artificial Intelligence: 10 Things You Should Know," providing valuable insights into the fundamentals and key aspects of AI. Through his contributions to both academic research and public education, Tim Rockt&#228;schel has established himself as a significant contributor to the AI and ML community.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FEI LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Fei Liu is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model" and also contributed to the paper "InfoBench: Evaluating instruction following ability in large language models." These works highlight Fei Liu's involvement in advancing the field of artificial intelligence, particularly in the areas of algorithm design and the evaluation of large language models' capabilities.</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TONG XIALIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Tong Xialiang is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MINGXUAN YUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Mingxuan Yuan is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XI LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Xi Lin is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FU LUO">
      <data key="d0">PERSON</data>
      <data key="d1">Fu Luo is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHENKUN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhenkun Wang is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QINGFU ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Qingfu Zhang is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FORTY-FIRST INTERNATIONAL CONFERENCE ON MACHINE LEARNING">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The FORTY-FIRST INTERNATIONAL CONFERENCE ON MACHINE LEARNING is a prestigious event in the field of machine learning where cutting-edge research is presented. Notably, this conference featured the presentation of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model," which explores advancements in automatic algorithm design leveraging large language models. Additionally, the conference showcased the paper "Offline training of language model agents with functions as learnable weights," highlighting innovative approaches in training language model agents. This conference serves as a significant platform for disseminating influential research and fostering collaboration among experts in the machine learning community.</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="ZIJUN LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Zijun Liu is an author of the paper "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YANZHE ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yanzhe Zhang is an author of the paper "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PENG LI">
      <data key="d0">PERSON</data>
      <data key="d1">Peng Li is an author of the paper "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YANG LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Yang Liu is an author of the paper "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DIYI YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Diyi Yang is an author of the paper "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHRIS LU">
      <data key="d0">PERSON</data>
      <data key="d1">Chris Lu is an author of the paper "The AI Scientist: Towards fully automated open-ended scientific discovery"Chris Lu is an author of the paper "Arbitrary order meta-learning with simple population-based evolution"Chris Lu is an author of the paper "Discovering preference optimization algorithms with and for large language models"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SEBASTIAN TOWERS">
      <data key="d0">PERSON</data>
      <data key="d1">Sebastian Towers is an author of the paper "Arbitrary order meta-learning with simple population-based evolution"Sebastian Towers is an author</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAKOB FOERSTER">
      <data key="d0">PERSON</data>
      <data key="d1">Jakob Foerster is an author of the paper "Arbitrary order meta-learning with simple population-based evolution"Jakob Foerster is an author of the paper "Discovering preference optimization algorithms with and for large language models"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALIFE 2023: GHOST IN THE MACHINE: PROCEEDINGS OF THE 2023 ARTIFICIAL LIFE CONFERENCE">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "Arbitrary order meta-learning with simple population-based evolution" was presented</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="MIT PRESS">
      <data key="d0">PUBLISHER</data>
      <data key="d1">MIT Press is a renowned publisher in the field of Artificial Intelligence and Machine Learning. They are the publisher of the influential book "Reinforcement Learning: An Introduction," which is a key resource for understanding the principles and applications of reinforcement learning. Additionally, MIT Press published the proceedings "ALIFE 2023: Ghost in the Machine: Proceedings of the 2023 Artificial Life Conference," showcasing their commitment to disseminating cutting-edge research and developments in artificial life and related domains.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="SAMUEL HOLT">
      <data key="d0">PERSON</data>
      <data key="d1">Samuel Holt is an author of the paper "Discovering preference optimization algorithms with and for large language models"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="CLAUDIO FANCONI">
      <data key="d0">PERSON</data>
      <data key="d1">Claudio Fanconi is an author of the paper "Discovering preference optimization algorithms with and for large language models"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="ALEX J CHAN">
      <data key="d0">PERSON</data>
      <data key="d1">Alex J Chan is an author of the paper "Discovering preference optimization algorithms with and for large language models"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="MIHAELA VAN DER SCHAAR">
      <data key="d0">PERSON</data>
      <data key="d1">Mihaela van der Schaar is an author of the paper "Discovering preference optimization algorithms with and for large language models"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="ROBERT TJARKO LANGE">
      <data key="d0">PERSON</data>
      <data key="d1">Robert Tjarko Lange is an author of the paper "Discovering preference optimization algorithms with and for large language models"Robert Tjarko Lange is an author of the paper "The AI Scientist: Towards fully automated open-ended scientific discovery"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THE AI SCIENTIST">
      <data key="d0">PAPER</data>
      <data key="d1">The AI Scientist: Towards fully automated open-ended scientific discovery is a paper published as an arXiv preprint in 2024</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="IAN WHALEN">
      <data key="d0">PERSON</data>
      <data key="d1">Ian Whalen is an author of the paper "Nsga-net: neural architecture search using multi-objective genetic algorithm"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VISHNU BODDETI">
      <data key="d0">PERSON</data>
      <data key="d1">Vishnu Boddeti is an author of the paper "Nsga-net: neural architecture search using multi-objective genetic algorithm"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YASHESH DHEBAR">
      <data key="d0">PERSON</data>
      <data key="d1">Yashesh Dhebar is an author of the paper "Nsga-net: neural architecture search using multi-objective genetic algorithm"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ERIK GOODMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Erik Goodman is an author of the paper "Nsga-net: neural architecture search using multi-objective genetic algorithm"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WOLFGANG BANZHAF">
      <data key="d0">PERSON</data>
      <data key="d1">Wolfgang Banzhaf is an author of the paper "Nsga-net: neural architecture search using multi-objective genetic algorithm"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NSGA-NET">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Nsga-net is a neural architecture search tool using a multi-objective genetic algorithm</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE">
      <data key="d0">EVENT</data>
      <data key="d1">The Genetic and Evolutionary Computation Conference is a prominent event in the field of artificial intelligence and machine learning, particularly focusing on genetic and evolutionary computation. It serves as a platform for presenting cutting-edge research, such as the paper "Poet: open-ended coevolution of environments and their optimized solutions" and "Nsga-net: neural architecture search using multi-objective genetic algorithm." This conference is a key venue for researchers and practitioners to share innovative ideas and advancements in the domain.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">EVENT</data>
    </node>
    <node id="YECHENG JASON MA">
      <data key="d0">PERSON</data>
      <data key="d1">Yecheng Jason Ma is an author of the paper "Eureka: Human-level reward design via coding large language models"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WILLIAM LIANG">
      <data key="d0">PERSON</data>
      <data key="d1">William Liang is an author of the paper "Eureka: Human-level reward design via coding large language models"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="OSBERT BASTANI">
      <data key="d0">PERSON</data>
      <data key="d1">Osbert Bastani is an author of the paper "Eureka: Human-level reward design via coding large language models"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DINESH JAYARAMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Dinesh Jayaraman is an author of the paper "Eureka: Human-level reward design via coding large language models"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ELLIOT MEYERSON">
      <data key="d0">PERSON</data>
      <data key="d1">Elliot Meyerson is an author of the paper "Language model crossover: Variation through few-shot prompting"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MARK J NELSON">
      <data key="d0">PERSON</data>
      <data key="d1">Mark J Nelson is an author of the paper "Language model crossover: Variation through few-shot prompting"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HERBIE BRADLEY">
      <data key="d0">PERSON</data>
      <data key="d1">Herbie Bradley is an author of the paper "Language model crossover: Variation through few-shot prompting"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ADAM GAIER">
      <data key="d0">PERSON</data>
      <data key="d1">Adam Gaier is an author of the paper "Language model crossover: Variation through few-shot prompting"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ARASH MORADI">
      <data key="d0">PERSON</data>
      <data key="d1">Arash Moradi is an author of the paper "Language model crossover: Variation through few-shot prompting"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AMY K HOOVER">
      <data key="d0">PERSON</data>
      <data key="d1">Amy K Hoover is an author of the paper "Language model crossover: Variation through few-shot prompting"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LANGUAGE MODEL CROSSOVER">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Language model crossover is a tool for variation through few-shot prompting</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="SHEN-YUN MIAO">
      <data key="d0">PERSON</data>
      <data key="d1">Shen-yun Miao is an author of the paper "A diverse corpus for evaluating and developing english math word problem solvers"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHAO-CHUN LIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Chao-Chun Liang is an author of the paper "A diverse corpus for evaluating and developing english math word problem solvers"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KEH-YIH SU">
      <data key="d0">PERSON</data>
      <data key="d1">Keh-Yih Su is an author of the paper "A diverse corpus for evaluating and developing english math word problem solvers"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="A DIVERSE CORPUS FOR EVALUATING AND DEVELOPING ENGLISH MATH WORD PROBLEM SOLVERS">
      <data key="d0">TOOL/DATASET</data>
      <data key="d1">A diverse corpus for evaluating and developing english math word problem solvers is a dataset for evaluating and developing English math word problem solvers</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/DATASET</data>
    </node>
    <node id="THE 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS">
      <data key="d0">EVENT</data>
      <data key="d1">The conference where the paper "A diverse corpus for evaluating and developing english math word problem solvers" was presented</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">EVENT</data>
    </node>
    <node id="JEAN-BAPTISTE MOURET">
      <data key="d0">PERSON</data>
      <data key="d1">Jean-Baptiste Mouret is an author of the paper "Illuminating search spaces by mapping elites"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ILLUMINATING SEARCH SPACES BY MAPPING ELITES">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Illuminating search spaces by mapping elites is a tool for illuminating search spaces</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="JEFF WU">
      <data key="d0">PERSON</data>
      <data key="d1">Jeff Wu is an author of the paper "Webgpt: Browser-assisted question-answering with human feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LONG OUYANG">
      <data key="d0">PERSON</data>
      <data key="d1">Long Ouyang is an author of the paper "Webgpt: Browser-assisted question-answering with human feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHRISTINA KIM">
      <data key="d0">PERSON</data>
      <data key="d1">Christina Kim is an author of the paper "Webgpt: Browser-assisted question-answering with human feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WILLIAM SAUNDERS">
      <data key="d0">PERSON</data>
      <data key="d1">William Saunders is an author of the paper "Webgpt: Browser-assisted question-answering with human feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEBGPT">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Webgpt is a tool for browser-assisted question-answering with human feedback</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="ANDREW NG">
      <data key="d0">PERSON</data>
      <data key="d1">Andrew Ng is the author of the newsletter issue "Issue 253"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BEN NORMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Ben Norman is an author of the paper "First-explore, then exploit: Meta-learning intelligent exploration"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FIRST-EXPLORE, THEN EXPLOIT">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">First-explore, then exploit is a tool for meta-learning intelligent exploration</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="CHATGPT">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">ChatGPT is a conversational AI model introduced by OpenAI. It serves as a baseline model evaluated on the Orca-Bench dataset, showcasing its capabilities in natural language understanding and generation.</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,bd4eb9459bc29b4c2da4658914fd4635</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="SIMPLE EVALS">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Simple Evals is a tool by OpenAI for evaluating AI models</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="JOON SUNG PARK">
      <data key="d0">PERSON</data>
      <data key="d1">Joon Sung Park is an author of the paper "Generative agents: Interactive simulacra of human behavior"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOSEPH O&#8217;BRIEN">
      <data key="d0">PERSON</data>
      <data key="d1">Joseph O&#8217;Brien is an author of the paper "Generative agents: Interactive simulacra of human behavior"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CARRIE JUN CAI">
      <data key="d0">PERSON</data>
      <data key="d1">Carrie Jun Cai is an author of the paper "Generative agents: Interactive simulacra of human behavior"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MEREDITH RINGEL MORRIS">
      <data key="d0">PERSON</data>
      <data key="d1">Meredith Ringel Morris is an author of the paper "Generative agents: Interactive simulacra of human behavior"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MICHAEL S BERNSTEIN">
      <data key="d0">PERSON</data>
      <data key="d1">Michael S Bernstein is an author of the paper "Generative agents: Interactive simulacra of human behavior"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GENERATIVE AGENTS">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Generative agents are interactive simulacra of human behavior</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="THE 36TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY">
      <data key="d0">EVENT</data>
      <data key="d1">The conference where the paper "Generative agents: Interactive simulacra of human behavior" was presented</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">EVENT</data>
    </node>
    <node id="ARKIL PATEL">
      <data key="d0">PERSON</data>
      <data key="d1">Arkil Patel is an author of the paper "Are NLP models really able to solve simple math word problems?"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SATWIK BHATTAMISHRA">
      <data key="d0">PERSON</data>
      <data key="d1">Satwik Bhattamishra is an author of the paper "Are NLP models really able to solve simple math word problems?"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NAVIN GOYAL">
      <data key="d0">PERSON</data>
      <data key="d1">Navin Goyal is an author of the paper "Are NLP models really able to solve simple math word problems?"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THE 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES">
      <data key="d0">EVENT</data>
      <data key="d1">The conference where the paper "Are NLP models really able to solve simple math word problems?" was presented</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">EVENT</data>
    </node>
    <node id="JUYUAN XU">
      <data key="d0">PERSON</data>
      <data key="d1">Juyuan Xu is an author of the paper "Communicative agents for software development"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="COMMUNICATIVE AGENTS">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Communicative agents are tools for software development</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="ZIHAO XIE">
      <data key="d0">PERSON</data>
      <data key="d1">Zihao Xie is an author of the paper "Scaling large-language-model-based multi-agent collaboration"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIFEI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yifei Wang is an author of the paper "Scaling large-language-model-based multi-agent collaboration"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEI LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Wei Liu is an author of the paper "Scaling large-language-model-based multi-agent collaboration"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUFAN DANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yufan Dang is an author of the paper "Scaling large-language-model-based multi-agent collaboration"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHUOYUN DU">
      <data key="d0">PERSON</data>
      <data key="d1">Zhuoyun Du is an author of the paper "Scaling large-language-model-based multi-agent collaboration"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SCALING LARGE-LANGUAGE-MODEL-BASED MULTI-AGENT COLLABORATION">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Scaling large-language-model-based multi-agent collaboration is a tool for multi-agent collaboration</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
    </node>
    <node id="QIANG WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Qiang Wang is an author of the paper "Tool learning with large language models: A survey"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="DAWEI YIN">
      <data key="d0">PERSON</data>
      <data key="d1">Dawei Yin is an author of the paper "Tool learning with large language models: A survey"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JUN XU">
      <data key="d0">PERSON</data>
      <data key="d1">Jun Xu is an author of the paper "Tool learning with large language models: A survey"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JI-RONG WEN">
      <data key="d0">PERSON</data>
      <data key="d1">Ji-Rong Wen is an author of the papers "A survey on the memory mechanism of large language model based agents" and "Tool learning with large language models: A survey."</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </node>
    <node id="RAFAEL RAFAILOV">
      <data key="d0">PERSON</data>
      <data key="d1">Rafael Rafailov is an author of the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ARCHIT SHARMA">
      <data key="d0">PERSON</data>
      <data key="d1">Archit Sharma is an author of the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ERIC MITCHELL">
      <data key="d0">PERSON</data>
      <data key="d1">Eric Mitchell is an author of the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="STEFANO ERMON">
      <data key="d0">PERSON</data>
      <data key="d1">Stefano Ermon is an author of the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="DAVID REIN">
      <data key="d0">PERSON</data>
      <data key="d1">David Rein is an author of the paper "GPQA: A graduate-level Google-proof Q&amp;A benchmark." This work, titled "Gpqa: A graduate-level google-proof q&amp;a benchmark," focuses on creating a benchmark for question and answer systems that are resistant to simple Google searches, aiming to challenge and advance the capabilities of AI in handling complex, graduate-level queries.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="BETTY LI HOU">
      <data key="d0">PERSON</data>
      <data key="d1">Betty Li Hou is an author of the paper titled "GPQA: A graduate-level Google-proof Q&amp;A benchmark." This work focuses on creating a benchmark for question and answer systems that are resistant to simple Google searches, aiming to challenge and advance the capabilities of AI in handling complex, graduate-level queries.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ASA COOPER STICKLAND">
      <data key="d0">PERSON</data>
      <data key="d1">Asa Cooper Stickland is an author of the paper titled "GPQA: A graduate-level Google-proof Q&amp;A benchmark." This work focuses on creating a benchmark for question and answer systems that are resistant to simple Google searches, aiming to challenge and advance the capabilities of AI in understanding and generating complex, graduate-level responses.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="JACKSON PETTY">
      <data key="d0">PERSON</data>
      <data key="d1">Jackson Petty is an author of the paper titled "GPQA: A graduate-level Google-proof Q&amp;A benchmark." This work, which is also referred to as "Gpqa: A graduate-level google-proof q&amp;a benchmark," focuses on creating a benchmark for question and answer systems that are resistant to simple Google searches, aiming to challenge and advance the capabilities of AI in handling complex, graduate-level queries.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="RICHARD YUANZHE PANG">
      <data key="d0">PERSON</data>
      <data key="d1">Richard Yuanzhe Pang is an author of the paper titled "GPQA: A graduate-level Google-proof Q&amp;A benchmark." This work, also referred to as "Gpqa: A graduate-level google-proof q&amp;a benchmark," focuses on creating a benchmark for question and answer systems that are resistant to simple Google searches, aiming to challenge and advance the capabilities of AI in handling complex, graduate-level queries.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="JULIEN DIRANI">
      <data key="d0">PERSON</data>
      <data key="d1">Julien Dirani is an author of the paper titled "GPQA: A graduate-level Google-proof Q&amp;A benchmark." This work, also referred to as "Gpqa: A graduate-level google-proof q&amp;a benchmark," focuses on creating a benchmark for question and answer systems that are resistant to simple Google searches, aiming to challenge and advance the capabilities of AI in understanding and generating complex, graduate-level responses.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="JULIAN MICHAEL">
      <data key="d0">PERSON</data>
      <data key="d1">Julian Michael is an author of the paper titled "GPQA: A graduate-level Google-proof Q&amp;A benchmark." This work focuses on creating a benchmark for question and answer systems that are resistant to simple web searches, aiming to evaluate the depth and robustness of AI models in handling complex, graduate-level queries.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SAMUEL R. BOWMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Samuel R. Bowman is an author of the paper "GPQA: A graduate-level Google-proof Q&amp;A benchmark." This work, which is also referred to as "Gpqa: A graduate-level google-proof q&amp;a benchmark," focuses on creating a benchmark for question and answer systems that are resistant to simple web searches, aiming to challenge and advance the capabilities of AI in understanding and generating complex, nuanced responses.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="TORAN BRUCE RICHARDS">
      <data key="d0">PERSON</data>
      <data key="d1">Toran Bruce Richards is the creator of the AutoGPT project on GitHub</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="AUTOGPT">
      <data key="d0">TOOL/PROJECT</data>
      <data key="d1">AutoGPT is a project hosted on GitHub by Toran Bruce Richards</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SEVEN DIALS">
      <data key="d0">PUBLISHER</data>
      <data key="d1">Seven Dials is the publisher of the book "Artificial Intelligence: 10 Things You Should Know"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MD OMAR FARUK ROKON">
      <data key="d0">PERSON</data>
      <data key="d1">Md Omar Faruk Rokon is an author of the paper "SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="RISUL ISLAM">
      <data key="d0">PERSON</data>
      <data key="d1">Risul Islam is an author of the paper "SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="AHMAD DARKI">
      <data key="d0">PERSON</data>
      <data key="d1">Ahmad Darki is an author of the paper "SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="EVANGELOS E PAPALEXAKIS">
      <data key="d0">PERSON</data>
      <data key="d1">Evangelos E Papalexakis is an author of the paper "SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MICHALIS FALOUTSOS">
      <data key="d0">PERSON</data>
      <data key="d1">Michalis Faloutsos is an author of the paper "SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="INTERNATIONAL SYMPOSIUM ON RESEARCH IN ATTACKS, INTRUSIONS AND DEFENSES">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub" was presented</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="BERNARDINO ROMERA-PAREDES">
      <data key="d0">PERSON</data>
      <data key="d1">Bernardino Romera-Paredes is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MOHAMMADAMIN BAREKATAIN">
      <data key="d0">PERSON</data>
      <data key="d1">Mohammadamin Barekatain is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ALEXANDER NOVIKOV">
      <data key="d0">PERSON</data>
      <data key="d1">Alexander Novikov is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MATEJ BALOG">
      <data key="d0">PERSON</data>
      <data key="d1">Matej Balog is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="M PAWAN KUMAR">
      <data key="d0">PERSON</data>
      <data key="d1">M Pawan Kumar is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="EMILIEN DUPONT">
      <data key="d0">PERSON</data>
      <data key="d1">Emilien Dupont is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="FRANCISCO JR RUIZ">
      <data key="d0">PERSON</data>
      <data key="d1">Francisco JR Ruiz is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JORDAN S ELLENBERG">
      <data key="d0">PERSON</data>
      <data key="d1">Jordan S Ellenberg is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="PENGMING WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Pengming Wang is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="OMAR FAWZI">
      <data key="d0">PERSON</data>
      <data key="d1">Omar Fawzi is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ERIC HAMBRO">
      <data key="d0">PERSON</data>
      <data key="d1">Eric Hambro is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="NEURAL INFORMATION PROCESSING SYSTEMS">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "Toolformer: Language models can teach themselves to use tools" was presented</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SANDER SCHULHOFF">
      <data key="d0">PERSON</data>
      <data key="d1">Sander Schulhoff is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MICHAEL ILIE">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Ilie is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="NISHANT BALEPUR">
      <data key="d0">PERSON</data>
      <data key="d1">Nishant Balepur is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="KONSTANTINE KAHADZE">
      <data key="d0">PERSON</data>
      <data key="d1">Konstantine Kahadze is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="AMANDA LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Amanda Liu is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="CHENGLEI SI">
      <data key="d0">PERSON</data>
      <data key="d1">Chenglei Si is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="YINHENG LI">
      <data key="d0">PERSON</data>
      <data key="d1">Yinheng Li is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="AAYUSH GUPTA">
      <data key="d0">PERSON</data>
      <data key="d1">Aayush Gupta is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="HYOJUNG HAN">
      <data key="d0">PERSON</data>
      <data key="d1">HyoJung Han is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SEVIEN SCHULHOFF">
      <data key="d0">PERSON</data>
      <data key="d1">Sevien Schulhoff is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="XUAN SHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Xuan Shen is an author of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="YAOHUA WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yaohua Wang is an author of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MING LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Ming Lin is an author of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="YILUN HUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yilun Huang is an author of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="HAO TANG">
      <data key="d0">PERSON</data>
      <data key="d1">Hao Tang is an author of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="XIUYU SUN">
      <data key="d0">PERSON</data>
      <data key="d1">Xiuyu Sun is an author of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="YANZHI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yanzhi Wang is an author of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="FREDA SHI">
      <data key="d0">PERSON</data>
      <data key="d1">Freda Shi is an author of the paper "Language models are multilingual chain-of-thought reasoners"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MIRAC SUZGUN">
      <data key="d0">PERSON</data>
      <data key="d1">Mirac Suzgun is an author known for contributing to the field of Artificial Intelligence and Machine Learning through significant research papers. Notably, Suzgun has co-authored the paper titled "Challenging big-bench tasks and whether chain-of-thought can solve them," which explores the efficacy of chain-of-thought reasoning in addressing complex AI tasks. Additionally, Suzgun has contributed to the paper "Language models are multilingual chain-of-thought reasoners," which investigates the capabilities of language models in multilingual contexts using chain-of-thought reasoning. These works highlight Suzgun's focus on enhancing the understanding and application of chain-of-thought methodologies in AI and ML.</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="MARKUS FREITAG">
      <data key="d0">PERSON</data>
      <data key="d1">Markus Freitag is an author of the paper "Language models are multilingual chain-of-thought reasoners"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SURAJ SRIVATS">
      <data key="d0">PERSON</data>
      <data key="d1">Suraj Srivats is an author of the paper "Language models are multilingual chain-of-thought reasoners"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SOROUSH VOSOUGHI">
      <data key="d0">PERSON</data>
      <data key="d1">Soroush Vosoughi is an author of the paper "Language models are multilingual chain-of-thought reasoners"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SEBASTIAN RUDER">
      <data key="d0">PERSON</data>
      <data key="d1">Sebastian Ruder is an author of the paper "Language models are multilingual chain-of-thought reasoners"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="DIPANJAN DAS">
      <data key="d0">PERSON</data>
      <data key="d1">Dipanjan Das is an author of the paper "Language models are multilingual chain-of-thought reasoners"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="RISTO MIIKKULAINEN">
      <data key="d0">PERSON</data>
      <data key="d1">Risto Miikkulainen is an author of the paper "Designing neural networks through neuroevolution"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="NATURE MACHINE INTELLIGENCE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Nature Machine Intelligence is the journal where the paper "Designing neural networks through neuroevolution" was published</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="RICHARD S SUTTON">
      <data key="d0">PERSON</data>
      <data key="d1">Richard S Sutton is an author of the book "Reinforcement learning: An introduction"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ANDREW G BARTO">
      <data key="d0">PERSON</data>
      <data key="d1">Andrew G Barto is an author of the book "Reinforcement learning: An introduction"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SAI VEMPRALA">
      <data key="d0">PERSON</data>
      <data key="d1">Sai Vemprala is an author of the paper "ChatGPT for robotics: Design principles and model abilities"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ROGERIO BONATTI">
      <data key="d0">PERSON</data>
      <data key="d1">Rogerio Bonatti is an author of the paper "ChatGPT for robotics: Design principles and model abilities"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ARTHUR BUCKER">
      <data key="d0">PERSON</data>
      <data key="d1">Arthur Bucker is an author of the paper "ChatGPT for robotics: Design principles and model abilities"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ASHISH KAPOOR">
      <data key="d0">PERSON</data>
      <data key="d1">Ashish Kapoor is an author of the paper "ChatGPT for robotics: Design principles and model abilities"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JANE X WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jane X Wang is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ZEB KURTH-NELSON">
      <data key="d0">PERSON</data>
      <data key="d1">Zeb Kurth-Nelson is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="DHRUVA TIRUMALA">
      <data key="d0">PERSON</data>
      <data key="d1">Dhruva Tirumala is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="HUBERT SOYER">
      <data key="d0">PERSON</data>
      <data key="d1">Hubert Soyer is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JOEL Z LEIBO">
      <data key="d0">PERSON</data>
      <data key="d1">Joel Z Leibo is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="REMI MUNOS">
      <data key="d0">PERSON</data>
      <data key="d1">Remi Munos is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="CHARLES BLUNDELL">
      <data key="d0">PERSON</data>
      <data key="d1">Charles Blundell is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="DHARSHAN KUMARAN">
      <data key="d0">PERSON</data>
      <data key="d1">Dharshan Kumaran is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MATT BOTVINICK">
      <data key="d0">PERSON</data>
      <data key="d1">Matt Botvinick is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="LEI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Lei Wang is an author of the paper "A survey on large language model based autonomous agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="CHEN MA">
      <data key="d0">PERSON</data>
      <data key="d1">Chen Ma is an author of the paper "A survey on large language model based autonomous agents" and also contributed to the paper "A survey on the memory mechanism of large language model based agents."</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </node>
    <node id="XUEYANG FENG">
      <data key="d0">PERSON</data>
      <data key="d1">Xueyang Feng is an author of the paper "A survey on large language model based autonomous agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ZEYU ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Zeyu Zhang is an author of the papers "A survey on large language model based autonomous agents" and "A survey on the memory mechanism of large language model based agents." These works contribute to the understanding and development of large language model-based systems, focusing on their autonomous capabilities and memory mechanisms.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </node>
    <node id="HAO YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Hao Yang is an author of the paper "A survey on large language model based autonomous agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JINGSEN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jingsen Zhang is an author of the paper "A survey on large language model based autonomous agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ZHIYUAN CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Zhiyuan Chen is an author of the paper "A survey on large language model based autonomous agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JIAKAI TANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jiakai Tang is an author of the paper "A survey on large language model based autonomous agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="XU CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Xu Chen is an author of the paper "A survey on large language model based autonomous agents" and also contributed to the paper "A survey on the memory mechanism of large language model based agents."</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </node>
    <node id="FRONTIERS OF COMPUTER SCIENCE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Frontiers of Computer Science is the journal where the paper "A survey on large language model based autonomous agents" was published</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="RUI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Rui Wang is an author of the papers "Poet: open-ended coevolution of environments and their optimized solutions" and "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="KENNETH O. STANLEY">
      <data key="d0">PERSON</data>
      <data key="d1">Kenneth O. Stanley is an author of the papers "Poet: open-ended coevolution of environments and their optimized solutions" and "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="ASSOCIATION FOR COMPUTING MACHINERY">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">The Association for Computing Machinery is the organization that hosted the Genetic and Evolutionary Computation Conference</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="ADITYA RAWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Aditya Rawal is an author of the paper "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="JIALE ZHI">
      <data key="d0">PERSON</data>
      <data key="d1">Jiale Zhi is an author of the paper "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="YULUN LI">
      <data key="d0">PERSON</data>
      <data key="d1">Yulun Li is an author of the paper "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="QUOC V LE">
      <data key="d0">PERSON</data>
      <data key="d1">Quoc V Le is an author of several influential papers in the field of artificial intelligence and machine learning. His works include "Challenging big-bench tasks and whether chain-of-thought can solve them," "Take a step back: Evoking reasoning via abstraction in large language models," "Self-consistency improves chain of thought reasoning in language models," and "Chain-of-thought prompting elicits reasoning in large language models." These contributions highlight his focus on enhancing reasoning capabilities in large language models through innovative techniques such as chain-of-thought prompting and self-consistency.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ED H. CHI">
      <data key="d0">PERSON</data>
      <data key="d1">Ed H. Chi is an author of the paper "Self-consistency improves chain of thought reasoning in language models"Ed H. Chi is an author of the papers "Self-consistency improves chain of thought reasoning in language models" and "Chain-of-thought prompting elicits reasoning in large language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THE ELEVENTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS">
      <data key="d0">EVENT</data>
      <data key="d1">The Eleventh International Conference on Learning Representations is where the paper "Self-consistency improves chain of thought reasoning in language models" was presentedThe Eleventh International Conference on Learning Representations is where the papers "Self-consistency improves chain of thought reasoning in language models" and "React: Synergizing reasoning and acting in language models" were presented</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">EVENT</data>
    </node>
    <node id="QINGYUN WU">
      <data key="d0">PERSON</data>
      <data key="d1">Qingyun Wu is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation framework" and also contributed to the paper "Offline training of language model agents with functions as learnable weights."</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GAGAN BANSAL">
      <data key="d0">PERSON</data>
      <data key="d1">Gagan Bansal is an author of the paper titled "Autogen: Enabling next-gen LLM applications via multi-agent conversation framework." This work focuses on advancing next-generation large language model (LLM) applications through the use of a multi-agent conversation framework.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIEYU ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jieyu Zhang is an author of the papers "Autogen: Enabling next-gen LLM applications via multi-agent conversation framework," "Autogen: Enabling next-gen LLM applications via multi-agent conversation," and "Offline training of language model agents with functions as learnable weights."</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIRAN WU">
      <data key="d0">PERSON</data>
      <data key="d1">Yiran Wu is an author of the paper titled "Autogen: Enabling next-gen LLM applications via multi-agent conversation framework." This work focuses on advancing next-generation large language model (LLM) applications through the use of a multi-agent conversation framework.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHAOKUN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Shaokun Zhang is an author of several influential papers in the field of Artificial Intelligence and Machine Learning. These papers include "Autogen: Enabling next-gen LLM applications via multi-agent conversation," "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms," and "Offline training of language model agents with functions as learnable weights." Zhang's work focuses on advancing the capabilities of language models and multi-agent systems, contributing significantly to the development of next-generation AI applications.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ERKANG ZHU">
      <data key="d0">PERSON</data>
      <data key="d1">Erkang Zhu is an author of the paper titled "Autogen: Enabling next-gen LLM applications via multi-agent conversation framework." This work focuses on advancing next-generation large language model (LLM) applications through the use of a multi-agent conversation framework.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BEIBIN LI">
      <data key="d0">PERSON</data>
      <data key="d1">Beibin Li is an author of the paper titled "Autogen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework." This work focuses on advancing next-generation large language model (LLM) applications through a multi-agent conversation framework, highlighting Beibin Li's contributions to the field of artificial intelligence and machine learning.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LI JIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Li Jiang is an author of the paper titled "Autogen: Enabling next-gen LLM applications via multi-agent conversation framework." This work focuses on advancing next-generation large language model (LLM) applications through the use of a multi-agent conversation framework.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIAOYUN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Xiaoyun Zhang is an author of the paper titled "Autogen: Enabling next-gen LLM applications via multi-agent conversation framework." This work focuses on advancing next-generation large language model (LLM) applications through the use of a multi-agent conversation framework.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Chi Wang is an author of several influential papers in the field of Artificial Intelligence and Machine Learning. Notably, Chi Wang has contributed to the paper "Autogen: Enabling next-gen LLM applications via multi-agent conversation," which explores the potential of multi-agent conversation frameworks in advancing next-generation language model applications. Additionally, Chi Wang has authored the paper "Offline training of language model agents with functions as learnable weights," which delves into the offline training methodologies for language model agents. Furthermore, Chi Wang has also co-authored the paper "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms," highlighting the use of evolutionary algorithms in the automatic generation of multi-agent systems. These contributions underscore Chi Wang's significant role in advancing research in multi-agent systems and language model applications.</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BENFENG XU">
      <data key="d0">PERSON</data>
      <data key="d1">Benfeng Xu is an author of the paper "Expertprompting: Instructing large language models to be distinguished experts"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AN YANG">
      <data key="d0">PERSON</data>
      <data key="d1">An Yang is an author of the paper "Expertprompting: Instructing large language models to be distinguished experts"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JUNYANG LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Junyang Lin is an author of the paper "Expertprompting: Instructing large language models to be distinguished experts"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QUAN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Quan Wang is an author of the paper "Expertprompting: Instructing large language models to be distinguished experts"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHANG ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Chang Zhou is an author of the paper "Expertprompting: Instructing large language models to be distinguished experts"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YONGDONG ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yongdong Zhang is an author of the paper "Expertprompting: Instructing large language models to be distinguished experts"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHENDONG MAO">
      <data key="d0">PERSON</data>
      <data key="d1">Zhendong Mao is an author of the paper "Expertprompting: Instructing large language models to be distinguished experts"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHENGRUN YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Chengrun Yang is an author of the paper "Large language models as optimizers"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIFENG LU">
      <data key="d0">PERSON</data>
      <data key="d1">Yifeng Lu is an author of the paper "Large language models as optimizers"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HANXIAO LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Hanxiao Liu is an author of the paper "Large language models as optimizers"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BENNET YEE">
      <data key="d0">PERSON</data>
      <data key="d1">Bennet Yee is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAVID SEHR">
      <data key="d0">PERSON</data>
      <data key="d1">David Sehr is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GREGORY DARDYK">
      <data key="d0">PERSON</data>
      <data key="d1">Gregory Dardyk is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="J BRADLEY CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">J Bradley Chen is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ROBERT MUTH">
      <data key="d0">PERSON</data>
      <data key="d1">Robert Muth is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TAVIS ORMANDY">
      <data key="d0">PERSON</data>
      <data key="d1">Tavis Ormandy is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHIKI OKASAKA">
      <data key="d0">PERSON</data>
      <data key="d1">Shiki Okasaka is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NEHA NARULA">
      <data key="d0">PERSON</data>
      <data key="d1">Neha Narula is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NICHOLAS FULLAGAR">
      <data key="d0">PERSON</data>
      <data key="d1">Nicholas Fullagar is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="COMMUNICATIONS OF THE ACM">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Communications of the ACM is the journal where the paper "Native client: A sandbox for portable, untrusted x86 native code" was published</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="WENHAO YU">
      <data key="d0">PERSON</data>
      <data key="d1">Wenhao Yu is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NIMROD GILEADI">
      <data key="d0">PERSON</data>
      <data key="d1">Nimrod Gileadi is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SEAN KIRMANI">
      <data key="d0">PERSON</data>
      <data key="d1">Sean Kirmani is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MONTSERRAT GONZALEZ ARENAS">
      <data key="d0">PERSON</data>
      <data key="d1">Montserrat Gonzalez Arenas is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HAO-TIEN LEWIS CHIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Hao-Tien Lewis Chiang is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TOM EREZ">
      <data key="d0">PERSON</data>
      <data key="d1">Tom Erez is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LEONARD HASENCLEVER">
      <data key="d0">PERSON</data>
      <data key="d1">Leonard Hasenclever is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAN HUMPLIK">
      <data key="d0">PERSON</data>
      <data key="d1">Jan Humplik is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CONFERENCE ON ROBOT LEARNING">
      <data key="d0">EVENT</data>
      <data key="d1">The Conference on Robot Learning is where the paper "Language to rewards for robotic skill synthesis" was presented</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="SIYU YUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Siyu Yuan is an author of the paper "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIANGJIE CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Jiangjie Chen is an author of the paper "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="DEQING YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Deqing Yang is an author of the paper "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="ELIEZER YUDKOWSKY">
      <data key="d0">PERSON</data>
      <data key="d1">Eliezer Yudkowsky is an author of the paper "Artificial Intelligence as a positive and negative factor in global risk"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="GLOBAL CATASTROPHIC RISKS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Global Catastrophic Risks is the journal where the paper "Artificial Intelligence as a positive and negative factor in global risk" was published</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="MATEI ZAHARIA">
      <data key="d0">PERSON</data>
      <data key="d1">Matei Zaharia is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="LINGJIAO CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Lingjiao Chen is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="JARED QUINCY DAVIS">
      <data key="d0">PERSON</data>
      <data key="d1">Jared Quincy Davis is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="CHRIS POTTS">
      <data key="d0">PERSON</data>
      <data key="d1">Chris Potts is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="JAMES ZOU">
      <data key="d0">PERSON</data>
      <data key="d1">James Zou is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="MICHAEL CARBIN">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Carbin is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="JONATHAN FRANKLE">
      <data key="d0">PERSON</data>
      <data key="d1">Jonathan Frankle is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="NAVEEN RAO">
      <data key="d0">PERSON</data>
      <data key="d1">Naveen Rao is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="ALI GHODSI">
      <data key="d0">PERSON</data>
      <data key="d1">Ali Ghodsi is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="BAIR">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">BAIR is the organization that published the blog post "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="KENNETH STANLEY">
      <data key="d0">PERSON</data>
      <data key="d1">Kenneth Stanley is an author of the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </node>
    <node id="JIALE LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Jiale Liu is an author known for contributing to the field of Artificial Intelligence and Machine Learning through significant research papers. Notably, Jiale Liu co-authored the paper "Autogen: Enabling next-gen LLM applications via multi-agent conversation," which explores advancements in next-generation language model applications through the use of multi-agent conversations. Additionally, Jiale Liu contributed to the paper "Offline training of language model agents with functions as learnable weights," focusing on the offline training methodologies for language model agents, emphasizing the role of functions as learnable weights. These contributions highlight Jiale Liu's active involvement in pushing the boundaries of AI and ML research.</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LINXIN SONG">
      <data key="d0">PERSON</data>
      <data key="d1">Linxin Song is an author of the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RANJAY KRISHNA">
      <data key="d0">PERSON</data>
      <data key="d1">Ranjay Krishna is an author of the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIAOHE BO">
      <data key="d0">PERSON</data>
      <data key="d1">Xiaohe Bo is an author of the paper "A survey on the memory mechanism of large language model based agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RUI LI">
      <data key="d0">PERSON</data>
      <data key="d1">Rui Li is an author of the paper "A survey on the memory mechanism of large language model based agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QUANYU DAI">
      <data key="d0">PERSON</data>
      <data key="d1">Quanyu Dai is an author of the paper "A survey on the memory mechanism of large language model based agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIEMING ZHU">
      <data key="d0">PERSON</data>
      <data key="d1">Jieming Zhu is an author of the paper "A survey on the memory mechanism of large language model based agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHENHUA DONG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhenhua Dong is an author of the paper "A survey on the memory mechanism of large language model based agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HENG-TZE CHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Heng-Tze Cheng is an author of the paper "Take a step back: Evoking reasoning via abstraction in large language models"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ED H CHI">
      <data key="d0">PERSON</data>
      <data key="d1">Ed H Chi is an author of the paper "Challenging big-bench tasks and whether chain-of-thought can solve them" and the paper "Take a step back: Evoking reasoning via abstraction in large language models."</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PEI ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Pei Zhou is an author of the paper "Self-discover: Large language models self-compose reasoning structures"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAY PUJARA">
      <data key="d0">PERSON</data>
      <data key="d1">Jay Pujara is an author of the paper "Self-discover: Large language models self-compose reasoning structures"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIANG REN">
      <data key="d0">PERSON</data>
      <data key="d1">Xiang Ren is an author of the paper "Self-discover: Large language models self-compose reasoning structures"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WANGCHUNSHU ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Wangchunshu Zhou is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIXIN OU">
      <data key="d0">PERSON</data>
      <data key="d1">Yixin Ou is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHENGWEI DING">
      <data key="d0">PERSON</data>
      <data key="d1">Shengwei Ding is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LONG LI">
      <data key="d0">PERSON</data>
      <data key="d1">Long Li is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIALONG WU">
      <data key="d0">PERSON</data>
      <data key="d1">Jialong Wu is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TIANNAN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Tiannan Wang is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIAMIN CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Jiamin Chen is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHUAI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Shuai Wang is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIAOHUA XU">
      <data key="d0">PERSON</data>
      <data key="d1">Xiaohua Xu is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NINGYU ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Ningyu Zhang is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MINGCHEN ZHUGE">
      <data key="d0">PERSON</data>
      <data key="d1">Mingchen Zhuge is an author of the paper "GPTSwarm: Language agents as optimizable graphs"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WENYI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Wenyi Wang is an author of the paper "GPTSwarm: Language agents as optimizable graphs"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LOUIS KIRSCH">
      <data key="d0">PERSON</data>
      <data key="d1">Louis Kirsch is an author of the paper "GPTSwarm: Language agents as optimizable graphs"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FRANCESCO FACCIO">
      <data key="d0">PERSON</data>
      <data key="d1">Francesco Faccio is an author of the paper "GPTSwarm: Language agents as optimizable graphs"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DMITRII KHIZBULLIN">
      <data key="d0">PERSON</data>
      <data key="d1">Dmitrii Khizbullin is an author of the paper "Camel: Communicative agents for 'mind' exploration of large language model society" and also contributed to the paper "GPTSwarm: Language agents as optimizable graphs."</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="J&#220;RGEN SCHMIDHUBER">
      <data key="d0">PERSON</data>
      <data key="d1">J&#252;rgen Schmidhuber is an author of the paper "GPTSwarm: Language agents as optimizable graphs"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LUISA ZINTGRAF">
      <data key="d0">PERSON</data>
      <data key="d1">Luisa Zintgraf is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SEBASTIAN SCHULZE">
      <data key="d0">PERSON</data>
      <data key="d1">Sebastian Schulze is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LEO FENG">
      <data key="d0">PERSON</data>
      <data key="d1">Leo Feng is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MAXIMILIAN IGL">
      <data key="d0">PERSON</data>
      <data key="d1">Maximilian Igl is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KYRIACOS SHIARLIS">
      <data key="d0">PERSON</data>
      <data key="d1">Kyriacos Shiarlis is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YARIN GAL">
      <data key="d0">PERSON</data>
      <data key="d1">Yarin Gal is a prominent researcher in the field of Artificial Intelligence and Machine Learning. He has made significant contributions to the domain, including authoring influential papers such as "The curse of recursion: Training on generated data makes models forget" and "Varibad: Variational bayes-adaptive deep RL via meta-learning." His work often explores the intricacies of model training and the application of variational methods in deep reinforcement learning, highlighting his expertise and impact on advancing AI and ML research.</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KATJA HOFMANN">
      <data key="d0">PERSON</data>
      <data key="d1">Katja Hofmann is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHIMON WHITESON">
      <data key="d0">PERSON</data>
      <data key="d1">Shimon Whiteson is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LUISA M ZINTGRAF">
      <data key="d0">PERSON</data>
      <data key="d1">Luisa M Zintgraf is an author of the paper "Exploration in approximate hyper-state space for meta reinforcement learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KRISTIAN HARTIKAINEN">
      <data key="d0">PERSON</data>
      <data key="d1">Kristian Hartikainen is an author of the paper "Exploration in approximate hyper-state space for meta reinforcement learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOURNAL OF MACHINE LEARNING RESEARCH">
      <data key="d0">JOURNAL</data>
      <data key="d1">The journal where the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning" was published</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">JOURNAL</data>
    </node>
    <node id="NAME">
      <data key="d0">SECTION/KEY</data>
      <data key="d1">The "name" section corresponds to the name of the next agent architecture proposed by the meta agent.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="CODE">
      <data key="d0">SECTION/KEY</data>
      <data key="d1">"CODE" refers to the programming code that is generated by the FM_Module based on the initial instruction and task information. This code is evaluated and refined throughout the process to ensure its accuracy and efficiency. Specifically, the "code" section contains the exact Python code for the "forward()" function that the meta agent proposes to implement.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0,449db721e37968e073e3579b59e023b2,84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="IMPLEMENTATION MISTAKES">
      <data key="d0">ISSUE/ERROR</data>
      <data key="d1">Implementation mistakes are errors or issues in the code that the meta agent needs to identify and correct during the self-reflection process.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="IMPROVEMENT">
      <data key="d0">PROCESS</data>
      <data key="d1">Improvement involves refining and optimizing the existing implementation to increase performance or effectiveness without altering the overall design framework.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="RUNTIME ERROR">
      <data key="d0">ISSUE/ERROR</data>
      <data key="d1">A runtime error is an error encountered during the execution of the generated code, prompting the meta agent to debug and correct the code.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="FRAMEWORK">
      <data key="d0">SYSTEM/TOOL</data>
      <data key="d1">The framework is a simple system provided to the meta agent to implement basic functions, such as querying Foundation Models and formatting prompts.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="NAMEDTUPLE INFO OBJECT">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">The namedtuple Info object is used in the framework to encapsulate and combine different types of information, facilitating communication between modules.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="INFO OBJECT">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">The Info object is a namedtuple used to encapsulate various pieces of information, such as FM responses and task descriptions, within the framework.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
      <data key="d3">DATA STRUCTURE</data>
    </node>
    <node id="APPENDIX B">
      <data key="d0">SECTION/DOCUMENT</data>
      <data key="d1">"APPENDIX B" contains the framework code used by the meta agent, the prompts utilized in the evaluations for summarization abilities, and specifies the types of tasks/benchmarks along with the corresponding methods used to extract answers and generate metrics.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0,86f77e15d41cbd0cb33f635ccb2cb66b,8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="APPENDICES C AND D">
      <data key="d0">SECTION/DOCUMENT</data>
      <data key="d1">Appendices C and D contain additional information relevant to the meta agent's operation and evaluation.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="OUTPUT INSTRUCTION AND EXAMPLE">
      <data key="d0">SECTION/DOCUMENT</data>
      <data key="d1">This section provides instructions and examples for the meta agent's output format, including the "thought," "name," and "code" keys.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="ARCHIVE">
      <data key="d0">DATASET/REPOSITORY</data>
      <data key="d1">The archive contains existing methods and architectures that the meta agent compares its proposed architecture against during self-reflection.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="WRONG IMPLEMENTATION EXAMPLES">
      <data key="d0">SECTION/DOCUMENT</data>
      <data key="d1">This section provides examples of potential mistakes the meta agent may make in implementation, used for self-reflection and debugging.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="FM MODULE">
      <data key="d0">TOOL/COMPONENT</data>
      <data key="d1">A module that constructs prompts by concatenating input Info objects into a structured format and generates responses using a specified model</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="INFO">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">INFO is a named tuple used for holding task information, including name, author, content, and iteration index. It serves as an object to encapsulate information about sub-problems and their solutions, providing structured feedback during the refinement process. This data structure is essential for organizing and managing detailed task-related information, facilitating efficient problem-solving and iterative improvements.</data>
      <data key="d2">84317ae35cc75d612287186d93461447,d66dc9ce4a9545b44f7486ea057b5937,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="FORMAT_INST">
      <data key="d0">FUNCTION</data>
      <data key="d1">A lambda function that formats instructions for FM responses in a specific JSON format</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="ROLE_DESC">
      <data key="d0">FUNCTION</data>
      <data key="d1">A lambda function that describes the role of the FM Module</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="GET_JSON_RESPONSE_FROM_GPT">
      <data key="d0">FUNCTION</data>
      <data key="d1">A function to get JSON responses from a GPT model, handling rate limit errors using backoff</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="FM_MODULE">
      <data key="d0">CLASS</data>
      <data key="d1">FM_Module is a versatile base class used in various stages of problem-solving, including decomposition, specialization, and integration. It contains attributes such as output fields, name, role, model, temperature, and ID. This module is employed for a range of tasks, including generating initial candidate solutions by thinking and writing code, providing human-like feedback, expert feedback, and refining code solutions.</data>
      <data key="d2">449db721e37968e073e3579b59e023b2,84317ae35cc75d612287186d93461447,d66dc9ce4a9545b44f7486ea057b5937,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="AGENT SYSTEM">
      <data key="d0">SYSTEM</data>
      <data key="d1">A system that processes task information and returns either a namedtuple Info or a string as the final answer</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="META-AGENT SEARCH">
      <data key="d0">SYSTEM</data>
      <data key="d1">A system that uses a simple framework for agentic tasks, including modules like FM_Module</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="CODE 1">
      <data key="d0">CODE SNIPPET</data>
      <data key="d1">A code snippet showing the simple framework used in Meta-Agent Search</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="CODE 2">
      <data key="d0">CODE SNIPPET</data>
      <data key="d1">A code snippet showing an example of implementing self-reflection using the framework</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="TASK DESCRIPTIONS">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">Descriptions of tasks that are used to facilitate communication between different modules</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="FM RESPONSES">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">Responses generated by the FM Module based on the constructed prompts</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="RESULTS FROM TOOL FUNCTION CALLS">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">Results obtained from calling functions within the tool</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="SYSTEM PROMPT">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">Part of the prompt generated by the FM Module, providing system-level instructions</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="USER PROMPT">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">Part of the prompt generated by the FM Module, providing user-level instructions</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="COT_INITIAL_INSTRUCTION">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">The "COT_INITIAL_INSTRUCTION" serves as an initial instruction used by the cot_module to start the task-solving process. It also functions as an instruction for initial reasoning in the self-reflection process.</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7,d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="COT_REFLECT_INSTRUCTION">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">The entity "COT_REFLECT_INSTRUCTION" refers to an instruction designed to encourage individuals to reflect on their previous attempts and feedback in order to improve their task-solving abilities. This reflective process aims to enhance the quality of solutions by learning from past experiences and incorporating constructive feedback.</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7,d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="CRITIC_INSTRUCTION">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">CRITIC_INSTRUCTION is an entity that involves providing an instruction to review and criticize an answer, or confirm its correctness. It serves as a guideline for offering feedback and making necessary corrections to ensure the accuracy and quality of the response.</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7,d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="COT_MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">A module named FM_Module used for 'Chain-of-Thought' processing, involving thinking and answering</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="CRITIC_MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">A module named FM_Module used for providing feedback and correcting answers</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="N_MAX">
      <data key="d0">PARAMETER</data>
      <data key="d1">The maximum number of attempts allowed, set to 5</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="TASKINFO">
      <data key="d0">DATA</data>
      <data key="d1">TaskInfo is a data structure used to provide initial instructions and context for the code evaluation process. It serves as the data input provided to the agent for solving the task, encompassing the initial input data and the input information or task that needs to be processed and solved.</data>
      <data key="d2">449db721e37968e073e3579b59e023b2,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="THINKING">
      <data key="d0">PROCESS</data>
      <data key="d1">THINKING refers to the thought process or reasoning applied during code evaluation and feedback generation. It involves the cognitive process of solving tasks, encompassing the mental activities required to analyze, interpret, and provide constructive feedback on code.</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="GPT-4O-2024-05-13">
      <data key="d0">MODEL</data>
      <data key="d1">GPT-4O-2024-05-13 is a version of OpenAI's language model specifically utilized by the meta agent. This iteration of the GPT-4 series is designed to enhance the capabilities of the meta agent, leveraging advanced natural language processing to facilitate more sophisticated interactions and decision-making processes.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="GPT-3.5-TURBO-0125">
      <data key="d0">MODEL</data>
      <data key="d1">GPT-3.5-TURBO-0125 is a version of OpenAI's language model used to evaluate discovered agents and baselines, with the aim of reducing compute costs.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="EXACT MATCH">
      <data key="d0">METRIC</data>
      <data key="d1">A metric used to calculate the accuracy rate by comparing the reference solution and the predicted answer</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="INPUT GRID">
      <data key="d0">DATA</data>
      <data key="d1">A rectangular matrix of integers representing colors, used as input in the ARC challenge</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="OUTPUT GRID">
      <data key="d0">DATA</data>
      <data key="d1">A rectangular matrix of integers representing colors, used as output in the ARC challenge</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="EXPERIMENT DETAILS">
      <data key="d0">SECTION</data>
      <data key="d1">Details about the experiments conducted for the ARC challenge</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="DEMONSTRATION EXAMPLES">
      <data key="d0">DATA</data>
      <data key="d1">Examples provided in the ARC challenge to demonstrate the transformation rule</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="TEST EXAMPLE">
      <data key="d0">DATA</data>
      <data key="d1">An example in the ARC challenge where the output grid needs to be predicted</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="EXAMPLE 0">
      <data key="d0">DATA</data>
      <data key="d1">An example task from the ARC challenge involving specific input and output grids</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="BEST AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">The best agent on ARC discovered by Meta Agent Search</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">AGENT</data>
    </node>
    <node id="INITIAL INSTRUCTION">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">The initial instruction given to the FM_Module to generate candidate solutions</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">INSTRUCTION</data>
    </node>
    <node id="INITIAL SOLUTIONS">
      <data key="d0">SOLUTIONS</data>
      <data key="d1">Initial solutions are the candidate solutions generated by the FM_Module</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">SOLUTIONS</data>
    </node>
    <node id="THOUGHTS">
      <data key="d0">DATA</data>
      <data key="d1">Thoughts are the intermediate outputs from the FM_Module, including thinking and code</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">DATA</data>
    </node>
    <node id="CORRECT EXAMPLES">
      <data key="d0">DATA</data>
      <data key="d1">Correct examples are the examples where the generated code produced the correct output</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">DATA</data>
    </node>
    <node id="WRONG EXAMPLES">
      <data key="d0">DATA</data>
      <data key="d1">Wrong examples are the examples where the generated code produced the incorrect output</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">DATA</data>
    </node>
    <node id="NUM_CANDIDATES">
      <data key="d0">PARAMETER</data>
      <data key="d1">Num_candidates is the number of initial candidate solutions to be generated</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">PARAMETER</data>
    </node>
    <node id="TEMPERATURE">
      <data key="d0">PARAMETER</data>
      <data key="d1">Temperature is a parameter used in the FM_Module to control the randomness of the generated solutions</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">PARAMETER</data>
    </node>
    <node id="EXPERIMENT">
      <data key="d0">PROCESS</data>
      <data key="d1">The experiment conducted to discover the best agent on ARC using Meta Agent Search</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
    </node>
    <node id="REPOSITORY">
      <data key="d0">PLATFORM</data>
      <data key="d1">The repository on GitHub containing all agents from the experiment</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
    </node>
    <node id="ENSEMBLE METHODS">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Ensemble methods are techniques used to combine multiple models to improve performance</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
    </node>
    <node id="INITIAL CANDIDATE SOLUTIONS">
      <data key="d0">SOLUTIONS</data>
      <data key="d1">Initial candidate solutions are the first set of solutions generated by the FM_Module</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
    </node>
    <node id="CORRECT_COUNT">
      <data key="d0">DATA</data>
      <data key="d1">Correct_count is the number of correct examples produced by the generated code</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
    </node>
    <node id="INITIAL_INSTRUCTION">
      <data key="d0">DATA</data>
      <data key="d1">Initial instruction is the starting guideline or command given to the system for processing</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="CORRECT_EXAMPLES">
      <data key="d0">DATA</data>
      <data key="d1">Correct examples are instances where the code has produced the correct output</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="WRONG_EXAMPLES">
      <data key="d0">DATA</data>
      <data key="d1">Wrong examples are instances where the code has produced incorrect output</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="INITIAL_SOLUTIONS">
      <data key="d0">DATA</data>
      <data key="d1">Initial solutions are the first set of code solutions generated and evaluated based on initial instructions and feedback</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="HUMAN_LIKE_FEEDBACK_MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">Human-like Feedback Module is a module designed to simulate human-like feedback for code evaluation</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="HUMAN_FEEDBACK_INSTRUCTION">
      <data key="d0">DATA</data>
      <data key="d1">Human feedback instruction is the guideline provided to the Human-like Feedback Module to generate feedback</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="HUMAN_FEEDBACK">
      <data key="d0">DATA</data>
      <data key="d1">Human feedback is the feedback generated by the Human-like Feedback Module simulating human evaluation</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="EXPERT_ROLES">
      <data key="d0">DATA</data>
      <data key="d1">Expert roles are specific roles assigned to expert advisors to provide targeted feedback on code</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="EXPERT_ADVISORS">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">Expert advisors are modules assigned specific roles to evaluate and provide targeted feedback on code</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="EXPERT_INSTRUCTION">
      <data key="d0">DATA</data>
      <data key="d1">Expert instruction is the guideline provided to expert advisors to evaluate code and provide feedback</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="EXPERT_FEEDBACK">
      <data key="d0">DATA</data>
      <data key="d1">Expert feedback is the targeted feedback provided by expert advisors based on their specific roles</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="REFINEMENT_MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">Refinement Module is a module used to iteratively refine code solutions based on structured feedback</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="MAX_REFINEMENT_ITERATIONS">
      <data key="d0">DATA</data>
      <data key="d1">Max refinement iterations is the maximum number of iterations allowed for refining code solutions</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="REFINEMENT_INSTRUCTION">
      <data key="d0">DATA</data>
      <data key="d1">Refinement instruction is the guideline provided to the Refinement Module to refine code solutions</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="REFINEMENT_THINKING">
      <data key="d0">PROCESS</data>
      <data key="d1">Refinement thinking refers to the thought process applied during the refinement of code solutions</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="REFINED_CODE">
      <data key="d0">DATA</data>
      <data key="d1">Refined code is the improved version of the code after undergoing refinement iterations</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="REFINED_SOLUTIONS">
      <data key="d0">DATA</data>
      <data key="d1">Refined solutions are the set of code solutions that have been improved through refinement iterations</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="SORTED_SOLUTIONS">
      <data key="d0">DATA</data>
      <data key="d1">Sorted solutions are the refined solutions sorted based on their performance</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="TOP_SOLUTIONS">
      <data key="d0">DATA</data>
      <data key="d1">Top solutions are the best-performing solutions selected from the sorted solutions</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="FINAL_DECISION_INSTRUCTION">
      <data key="d0">DATA</data>
      <data key="d1">Final decision instruction is the guideline provided to make a final decision on the best code solution</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="FINAL_DECISION_MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">Final Decision Module is a module used to make the final decision on the best code solution</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="FINAL_THOUGHTS">
      <data key="d0">PROCESS</data>
      <data key="d1">Final thoughts refer to the thought process applied during the final decision-making on the best code solution</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="FINAL_CODE">
      <data key="d0">DATA</data>
      <data key="d1">Final code is the final version of the code selected as the best solution</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="VALIDATION_SET">
      <data key="d0">DATA</data>
      <data key="d1">Validation set is a subset of data used to validate the performance of agents</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="TEST_SET">
      <data key="d0">DATA</data>
      <data key="d1">Test set is a subset of data used to test the performance of agents</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="ZERO-SHOT STYLE QUESTIONS">
      <data key="d0">DATA</data>
      <data key="d1">Zero-shot style questions are questions that the agents have not seen during training</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="ONE-SHOT STYLE QUESTIONS">
      <data key="d0">DATA</data>
      <data key="d1">One-shot style questions are questions that the agents have seen once during training</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="DISCOVERED AGENTS">
      <data key="d0">AGENT</data>
      <data key="d1">Discovered agents are agents evaluated using the "gpt-3.5-turbo-0125" model</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="META_AGENT">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="DISCOVERED_AGENTS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="AUTOMATED DESIGN OF AGENTIC SYSTEMS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Automated Design of Agentic Systems is a document detailing the experiment and evaluation process for reasoning and problem-solving domains</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="REASONING AND PROBLEM-SOLVING DOMAINS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="BAHRAIN">
      <data key="d0">LOCATION</data>
      <data key="d1">Bahrain is a country where non-nationals make up more than half of the population, with a significant number of immigrants from South and Southeast Asia</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="INDIANS">
      <data key="d0">NATIONALITY</data>
      <data key="d1">Indians are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="BANGLADESHIS">
      <data key="d0">NATIONALITY</data>
      <data key="d1">Bangladeshis are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="PAKISTANIS">
      <data key="d0">NATIONALITY</data>
      <data key="d1">Pakistanis are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="FILIPINOS">
      <data key="d0">NATIONALITY</data>
      <data key="d1">Filipinos are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="INDONESIANS">
      <data key="d0">NATIONALITY</data>
      <data key="d1">Indonesians are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="BIOLOGY">
      <data key="d0">SUBJECT</data>
      <data key="d1">Biology is one of the domains included in the GPQA benchmark</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="PHYSICS">
      <data key="d0">SUBJECT</data>
      <data key="d1">Physics is one of the domains included in the GPQA benchmark</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="CHEMISTRY">
      <data key="d0">SUBJECT</data>
      <data key="d1">Chemistry is one of the domains included in the GPQA benchmark</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="STEM">
      <data key="d0">SUBJECT</data>
      <data key="d1">STEM is one of the subject areas included in the MMLU benchmark</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="SOCIAL SCIENCES">
      <data key="d0">SUBJECT</data>
      <data key="d1">Social Sciences is one of the subject areas included in the MMLU benchmark</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="HUMANITIES">
      <data key="d0">SUBJECT</data>
      <data key="d1">Humanities is one of the subject areas included in the MMLU benchmark</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="SHI ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Shi et al. in 2023 that discusses the Verified Multimodal Agent</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="REIN ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Rein et al. in 2023 that discusses the Multi-Step Peer Review Agent and Divide and Conquer Agent</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="PROBLEM-SOLVING">
      <data key="d0">DOMAIN</data>
      <data key="d1">Problem-Solving is one of the domains where experiments were conducted</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="EXPERIMENTS">
      <data key="d0">PROCESS</data>
      <data key="d1">Experiments were conducted on Reasoning and Problem-Solving domains</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="PHYSICS EXPERT">
      <data key="d0">ROLE</data>
      <data key="d1">PHYSICS EXPERT is a specialized expert module focused on solving sub-problems related to physics. This role is assigned within the LLM-Debate method, indicating its function in providing expert-level insights and solutions in the domain of physics during large language model debates.</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="CHEMISTRY EXPERT">
      <data key="d0">ROLE</data>
      <data key="d1">The "CHEMISTRY EXPERT" is a specialized expert module focused on solving sub-problems related to chemistry. This role is assigned within the LLM-Debate method, indicating its integration into a structured framework for leveraging large language models to address complex issues in the field of chemistry.</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="PHYSICS CRITIC">
      <data key="d0">ROLE</data>
      <data key="d1">Physics Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="CHEMISTRY CRITIC">
      <data key="d0">ROLE</data>
      <data key="d1">Chemistry Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="BIOLOGY CRITIC">
      <data key="d0">ROLE</data>
      <data key="d1">Biology Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="GENERAL CRITIC">
      <data key="d0">ROLE</data>
      <data key="d1">General Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="BIOLOGY EXPERT">
      <data key="d0">ROLE</data>
      <data key="d1">BIOLOGY EXPERT is a specialized expert module focused on solving sub-problems related to biology. Additionally, Biology Expert is a role assigned in the LLM-Debate method, indicating its integration into structured discussions and problem-solving frameworks within the AI and ML community.</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="SCIENCE GENERALIST">
      <data key="d0">ROLE</data>
      <data key="d1">Science Generalist is a role assigned in the LLM-Debate method</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="FINAL DECISION">
      <data key="d0">ROLE</data>
      <data key="d1">Final Decision is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="DECOMPOSITION MODULE">
      <data key="d0">ROLE</data>
      <data key="d1">The DECOMPOSITION MODULE, also referred to as the FM_Module, is a crucial component designed for breaking down complex problems into more manageable sub-problems. It plays a significant role within the Divide and Conquer Agent framework, facilitating the systematic decomposition of tasks to enhance problem-solving efficiency.</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="SPECIALIZED EXPERT">
      <data key="d0">ROLE</data>
      <data key="d1">The "Specialized Expert" is a role within the Divide and Conquer Agent framework, designed to address specific sub-problems through a module named FM_Module. This module encompasses various roles, including Physics Expert, Chemistry Expert, Biology Expert, and General Expert, each tailored to provide specialized knowledge and solutions in their respective domains.</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="INTEGRATION MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">A module named FM_Module used for integrating solutions to sub-problems into a final answer</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="SUB_PROBLEMS">
      <data key="d0">DATA/OUTPUT</data>
      <data key="d1">The decomposed parts of the main problem generated by the Decomposition Module</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="SUB_SOLUTIONS">
      <data key="d0">DATA/OUTPUT</data>
      <data key="d1">The solutions to the sub-problems generated by the Specialized Experts</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="VISUAL REPRESENTATION MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">A module named FM_Module used for generating visual representations of problems</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="VERIFICATION MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">A module named FM_Module used for verifying the accuracy and relevance of visual representations</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="CHAIN-OF-THOUGHT MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">A module named FM_Module used for solving problems using verified visual aids</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="GPT-4O-MINI">
      <data key="d0">MODEL</data>
      <data key="d1">A newer version of OpenAI's language model that is less expensive and offers better performance than GPT-3.5-Turbo-0125</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="ADAS ALGORITHMS">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Algorithms used in the Automated Design of Agentic Systems</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="GENERAL EXPERT">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">A specialized expert module focused on solving general sub-problems</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="DECOMPOSITION INSTRUCTION">
      <data key="d0">DATA/INPUT</data>
      <data key="d1">The instruction provided to the Decomposition Module to decompose the main task</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="SUB_PROBLEM INSTRUCTION">
      <data key="d0">DATA/INPUT</data>
      <data key="d1">The instruction provided to specialized experts to solve sub-problems step by step</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="INTEGRATION INSTRUCTION">
      <data key="d0">DATA/INPUT</data>
      <data key="d1">The instruction provided to the Integration Module to integrate sub-solutions into a final answer</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="VISUAL INSTRUCTION">
      <data key="d0">DATA/INPUT</data>
      <data key="d1">The instruction provided to the Visual Representation Module to create a visual representation of the problem</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="VERIFICATION INSTRUCTION">
      <data key="d0">DATA/INPUT</data>
      <data key="d1">The instruction provided to the Verification Module to verify the accuracy and relevance of the visual representation</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="COT INSTRUCTION">
      <data key="d0">DATA/INPUT</data>
      <data key="d1">The instruction provided to the Chain-of-Thought Module to solve the problem using the verified visual aid</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="OPENAI API">
      <data key="d0">SERVICE</data>
      <data key="d1">The API service used for querying models like GPT-3.5-Turbo-0125 and GPT-4o-Mini</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="EXPERIMENT COST">
      <data key="d0">DATA/OUTPUT</data>
      <data key="d1">The cost associated with running search and evaluation experiments using the OpenAI API</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="AGENTINSTRUCT">
      <data key="d0">TOOL/FRAMEWORK</data>
      <data key="d1">AgentInstruct is an extensible agentic framework designed to automatically create large amounts of diverse and high-quality synthetic data. It is used to generate approximately 22 million instructions aimed at teaching various skills, enhancing the proficiency of models like Mistral across different difficulties in subjects such as math and reading comprehension. The method leverages agentic flows to produce data for model post-training, reducing the need for human expertise in the data generation process. AgentInstruct defines three distinct flows&#8212;Content Transformation Flow, Seed Instruction Generation Flow, and Instruction Refinement Flow&#8212;to ensure diversity and complexity in the generated data. This approach not only synthesizes a large and varied corpus of data to evaluate baseline models but also focuses on creating demonstration and feedback data using raw documents as input. Additionally, AgentInstruct has been shown to reduce hallucinations while maintaining quality levels comparable to GPT-4, making it a robust solution for Generative Teaching.</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a,6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635,dd9a46950237e49ef9b1c7ef08e08d42,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="GENERATIVE TEACHING">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">Generative Teaching is a multifaceted concept in the field of Artificial Intelligence and Machine Learning, primarily focused on teaching skills rather than merely generating data to meet specific benchmarks. This methodology involves generating abundant amounts of diverse, challenging, and high-quality data to effectively teach a particular skill to an AI model. Additionally, Generative Teaching encompasses the process of using synthetic data created by powerful models to impart new skills or behaviors to another model. This approach leverages the strengths of synthetic data generation to enhance the learning capabilities and performance of AI systems.</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="ARINDAM MITRA">
      <data key="d0">PERSON</data>
      <data key="d1">Arindam Mitra is an author of several significant papers and technical reports in the field of Artificial Intelligence and Machine Learning. He contributed to the Phi-3 technical report and co-authored the paper "Orca-math: Unlocking the potential of SLMs in grade school math." Additionally, he is one of the authors of "Orca: Progressive learning from complex explanation traces of GPT-4" and "Orca 2: Teaching small language models how to reason." Furthermore, Arindam Mitra is also one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows."</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="LUCIANO DEL CORRO">
      <data key="d0">PERSON</data>
      <data key="d1">Luciano Del Corro is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored significant papers such as "Orca 2: Teaching small language models how to reason" and "AgentInstruct: Toward Generative Teaching with Agentic Flows." His work focuses on advancing the capabilities of language models and exploring innovative methods for generative teaching, highlighting his role as a key player in the AI and ML research community.</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="GUOQING ZHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Guoqing Zheng is an author of the paper "Orca 2: Teaching small language models how to reason" and one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows." These contributions highlight Zheng's involvement in advancing the field of artificial intelligence, particularly in the areas of language model reasoning and generative teaching methodologies.</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="SHWETI MAHAJAN">
      <data key="d0">PERSON</data>
      <data key="d1">Shweti Mahajan is an accomplished author in the field of Artificial Intelligence and Machine Learning. She has contributed to significant research, including being an author of the paper "Orca 2: Teaching small language models how to reason" and co-authoring the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows." Her work focuses on advancing the capabilities of language models and exploring innovative methods for generative teaching, highlighting her role as a key player in the AI and ML research community.</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="DANY ROUHANA">
      <data key="d0">PERSON</data>
      <data key="d1">Dany Rouhana is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="ANDRES CODAS">
      <data key="d0">PERSON</data>
      <data key="d1">Andres Codas is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored significant research papers, including "Orca 2: Teaching small language models how to reason" and "AgentInstruct: Toward Generative Teaching with Agentic Flows." His work focuses on advancing the capabilities of language models and exploring innovative methods for generative teaching, highlighting his role as a key player in the AI and ML research community.</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="YADONG LU">
      <data key="d0">PERSON</data>
      <data key="d1">Yadong Lu is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="WEI-GE CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Wei-ge Chen is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="OLGA VROUSGOS">
      <data key="d0">PERSON</data>
      <data key="d1">Olga Vrousgos is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="CORBY ROSSET">
      <data key="d0">PERSON</data>
      <data key="d1">Corby Rosset is a prolific author in the field of Artificial Intelligence and Machine Learning. He has contributed to several significant technical reports and research papers. Notably, he is an author of the Phi-3 technical report. Additionally, he has co-authored influential papers such as "Direct Nash optimization: Teaching language models to self-improve with general preferences," "Orca 2: Teaching small language models how to reason," "Orca-math: Unlocking the potential of SLMs in grade school math," and "AgentInstruct: Toward Generative Teaching with Agentic Flows." His work spans various aspects of AI and ML, focusing on optimizing language models and enhancing their reasoning and mathematical capabilities.</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="FILLIPE SILVA">
      <data key="d0">PERSON</data>
      <data key="d1">Fillipe Silva is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="HAMED KHANPOUR">
      <data key="d0">PERSON</data>
      <data key="d1">Hamed Khanpour is an author of several influential papers in the field of Artificial Intelligence and Machine Learning. His works include "Orca-math: Unlocking the potential of SLMs in grade school math," "Orca 2: Teaching small language models how to reason," and "AgentInstruct: Toward Generative Teaching with Agentic Flows." These contributions highlight his focus on enhancing the reasoning capabilities of small language models and exploring generative teaching methodologies.</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="YASH LARA">
      <data key="d0">PERSON</data>
      <data key="d1">Yash Lara is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="AHMED AWADALLAH">
      <data key="d0">PERSON</data>
      <data key="d1">Ahmed Awadallah is a prolific author in the field of Artificial Intelligence and Machine Learning, contributing to several significant papers. He is an author of "Orca-math: Unlocking the potential of SLMs in grade school math," "Orca: Progressive learning from complex explanation traces of GPT-4," and "Orca 2: Teaching small language models how to reason." Additionally, he has co-authored the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows" and contributed to the Phi-3 technical report. His work spans various aspects of AI and ML, focusing on enhancing the capabilities of small language models and exploring innovative teaching methodologies.</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="SYNTHETIC DATA">
      <data key="d0">CONCEPT/DATA</data>
      <data key="d1">Synthetic data refers to data that is artificially generated rather than obtained by direct measurement. It is used in model training to address concerns such as lack of diversity and the need for human curation. By generating data artificially, synthetic data helps to overcome limitations in real-world data, providing a more diverse and comprehensive dataset for training machine learning models.</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="MISTRAL-7B">
      <data key="d0">MODEL</data>
      <data key="d1">Mistral-7B is a base language model that was fine-tuned and post-trained using a synthetic dataset created by AgentInstruct.</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="ORCA-3">
      <data key="d0">MODEL</data>
      <data key="d1">Orca-3 is a language model fine-tuned on the Mistral-7B-v0.1 model, utilizing a dataset of approximately 25.8 million paired instructions. This 7B model has demonstrated significant improvements across various benchmarks, including reading comprehension, math, and format following, particularly in zero-shot settings. Evaluated on the Orca-Bench dataset, Orca-3 has shown notable enhancements in capabilities during post-training, which involved data generated by AgentInstruct. Overall, Orca-3 represents a substantial advancement over other instruction-tuned models, showcasing its enhanced performance and versatility.</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a,6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="AGIEVAL">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AGIEval is a human-centric benchmark used to evaluate the performance of AI and language models on various tasks pertinent to human cognition and problem-solving. It includes assessments in reading comprehension, math, and standardized exams such as the SAT and LSAT. AGIEval is utilized to measure the capabilities of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4.</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="BBH">
      <data key="d0">BENCHMARK</data>
      <data key="d1">BBH, or Big Bench Hard, is a benchmark used to evaluate the performance of AI models, specifically language models. It includes a set of 23 tasks from the broader Big-Bench benchmark that require complex, multi-step reasoning across various academic subjects. BBH is utilized to assess the capabilities of models such as Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4.</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,b88745a13b69cecbc0ee9c3af41389bf,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="ALPACAEVAL">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AlpacaEval is a benchmark designed to evaluate the performance of chat-based language models, particularly in instruction-following tasks. It consists of 805 instructions and measures win-rates by comparing the outputs of the evaluated models to a reference answer, using GPT-4-turbo for evaluation. The benchmark assesses various models, including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4. Published in 2023, AlpacaEval serves as an automatic evaluator of instruction-following models, providing a comprehensive tool for performance assessment in the AI and ML landscape.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba,3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,b88745a13b69cecbc0ee9c3af41389bf,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="LLAMA-8B-INSTRUCT">
      <data key="d0">MODEL</data>
      <data key="d1">LLAMA-8B-INSTRUCT is a language model that has been outperformed by Orca-3 on multiple benchmarks. Despite its capabilities, LLAMA-8B-INSTRUCT falls short when compared to the performance metrics achieved by Orca-3, highlighting the latter's superior proficiency in various tasks within the AI and ML landscape.</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="GPT-3.5-TURBO">
      <data key="d0">MODEL</data>
      <data key="d1">GPT-3.5-turbo is a version of OpenAI's language model that has been evaluated on various benchmarks, including those for summarization and hallucination rates. It serves as a baseline model for assessing the performance of other models, such as Orca-3 and Orca-3-7B. Despite its capabilities, GPT-3.5-turbo was outperformed by Orca-3 in certain evaluations. Additionally, it has been used in the evaluation of MIRAGE datasets, and its scores for GSM8K are referenced from specific sources.</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="LLMS">
      <data key="d0">CONCEPT</data>
      <data key="d1">LLMs, or Large Language Models, are advanced language models that have been significantly developed using synthetic data.</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="SLMS">
      <data key="d0">CONCEPT</data>
      <data key="d1">SLMS, or Small Language Models, are smaller language models that benefit from synthetic data in their training.</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="RLHF">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">RLHF, or Reinforcement Learning from Human Feedback, is a process used in the training of language models</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="AGENTIC WORKFLOWS">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">Agentic workflows involve using agents to generate high-quality data through reflection, iteration, and tool usage</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="MULTI-AGENT WORKFLOWS">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">Multi-agent workflows involve multiple agents working together to generate new prompts and responses, simulating scenarios and automating data generation</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="MISTRAL-7B-INSTRUCT">
      <data key="d0">MODEL</data>
      <data key="d1">Mistral-7B-Instruct is a language model evaluated on various benchmarks for summarization and hallucination rates. It serves as a baseline for comparison with Orca-3-7B, providing a standard for assessing performance in different tasks. This version of the Mistral-7B model is specifically tailored for these comparative evaluations.</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,8ee9617c145e19fa95f1f9349bfbe69b,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="FOFO">
      <data key="d0">BENCHMARK</data>
      <data key="d1">FOFO, also known as Format Following (FoFo), is a benchmark used to evaluate the performance of language models, including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4. It specifically assesses models on their ability to follow complex, domain-specific formatting guidelines across various real-world domains such as Healthcare, Finance, and Marketing. Evaluated using GPT-4, FOFO scores the format correctness of model responses on a scale ranging from 0 to 1.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba,6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="MIRAGE-RAG">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MIRAGE-RAG is a benchmark used to evaluate the performance of language models</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="SYNTHETIC-DATA-GENERATION-AS-A-SERVICE">
      <data key="d0">SERVICE</data>
      <data key="d1">Synthetic-Data-Generation-As-A-Service is a proposed service for generating data for post-training and fine-tuning of AI models using raw materials</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
      <data key="d3">SERVICE</data>
    </node>
    <node id="CONTENT TRANSFORMATION AGENTS">
      <data key="d0">AGENTS</data>
      <data key="d1">Content Transformation Agents are used in the AgentInstruct methodology to transform raw seeds into diverse instructions</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
      <data key="d3">AGENTS</data>
    </node>
    <node id="REFINEMENT AGENTS">
      <data key="d0">AGENTS</data>
      <data key="d1">Refinement Agents are used in the AgentInstruct methodology to iteratively refine the complexity and quality of seed instructions</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
      <data key="d3">AGENTS</data>
    </node>
    <node id="RAW SEEDS">
      <data key="d0">DATA</data>
      <data key="d1">Raw seeds are unstructured text documents or source code used as input for AgentInstruct to generate diverse data</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
      <data key="d3">DATA</data>
    </node>
    <node id="CONTENT TRANSFORMATION FLOW">
      <data key="d0">PROCESS</data>
      <data key="d1">The "Content Transformation Flow" is a process within the AgentInstruct methodology that converts raw seeds into intermediate representations, simplifying the creation of instructions tailored to specific objectives. This flow involves Content Transformation Agents that transform arbitrary articles into well-crafted pieces, conducive to the formulation of a wide array of reading comprehension question types. Additionally, the Content Transformation Flow synthesizes a list of APIs from a random seed, which can include source code snippets or API descriptions.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,427e98b00e49b6a8f8649054122dd45b,b88745a13b69cecbc0ee9c3af41389bf,f7eb89a70f544664546a510e46d5febd</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="SEED INSTRUCTION CREATION FLOW">
      <data key="d0">PROCESS</data>
      <data key="d1">The "SEED INSTRUCTION CREATION FLOW" is a process integral to the AgentInstruct methodology. It consumes a list of APIs and employs various agents to create a diverse set of tasks, which can range from those requiring single APIs to those necessitating multiple APIs with varying degrees of parameter completeness. This flow transforms seeds into a wide array of instructions, thereby facilitating the creation of comprehensive and versatile task sets.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,b88745a13b69cecbc0ee9c3af41389bf</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="REFINEMENT FLOW">
      <data key="d0">PROCESS</data>
      <data key="d1">REFINEMENT FLOW is a process within the AgentInstruct methodology aimed at increasing the complexity and quality of seed instructions. This is achieved by iteratively suggesting refinements that increase the number of steps required to solve a given task. The primary goal of Refinement Flow is to enhance the intricacy of tasks, thereby improving the overall effectiveness and depth of the instructional process.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,b88745a13b69cecbc0ee9c3af41389bf</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="CREATIVE WRITING">
      <data key="d0">SKILL</data>
      <data key="d1">Creative writing is one of the skills covered by the synthetic post-training dataset created by AgentInstruct</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="DATA FILTERING">
      <data key="d0">PROCESS</data>
      <data key="d1">Data filtering is a process applied by AgentInstruct to ensure the quality of generated data</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="VERIFICATION">
      <data key="d0">PROCESS</data>
      <data key="d1">Verification is a process applied by AgentInstruct to ensure the quality of generated data</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="DEMONSTRATION DATA">
      <data key="d0">DATA</data>
      <data key="d1">Demonstration data is created by AgentInstruct to teach AI models specific skills</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="FEEDBACK DATA">
      <data key="d0">DATA</data>
      <data key="d1">Feedback data is created by AgentInstruct to teach AI models specific skills</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="POST-TRAINING">
      <data key="d0">PROCESS</data>
      <data key="d1">Post-training is a process where AI models are further trained using synthetic datasets created by AgentInstruct</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="CONTINUAL LEARNING">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">Continual learning is the ongoing process of training AI models to improve their performance over time</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="SELF-IMPROVEMENT">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">Self-improvement is the process where AI models enhance their own capabilities using generated prompts and responses</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="WEB DATA">
      <data key="d0">DATA</data>
      <data key="d1">Web data is a type of raw material used as seeds for generating synthetic datasets</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="DOMAIN SPECIFIC DATA">
      <data key="d0">DATA</data>
      <data key="d1">Domain specific data is used as seeds to improve AI models in certain specializations</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="TEXTBOOK CHAPTERS">
      <data key="d0">DATA</data>
      <data key="d1">Textbook chapters are a type of raw seed used in the AgentInstruct methodology</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="WEB ARTICLES">
      <data key="d0">DATA</data>
      <data key="d1">Web articles are a type of raw seed used in the AgentInstruct methodology</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="CODE SNIPPETS">
      <data key="d0">DATA</data>
      <data key="d1">Code snippets are a type of raw seed used in the AgentInstruct methodology</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="AGENTIC FLOWS">
      <data key="d0">PROCESS/TECHNIQUE</data>
      <data key="d1">AGENTIC FLOWS is a technique employed by AgentInstruct for synthetic data generation, which aids in model training and customization. This method automates the data generation process by utilizing raw articles as seeds, promoting diversity and ensuring that the problems generated in different iterations are distinct and cover a broad range of topics.</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SEED INSTRUCTION GENERATION FLOW">
      <data key="d0">PROCESS/TECHNIQUE</data>
      <data key="d1">The "SEED INSTRUCTION GENERATION FLOW" is a process that involves generating seed instructions using various text modification methods. It takes transformed content and generates a set of diverse instructions following a comprehensive taxonomy. This flow compiles a collection of reading comprehension question types and employs multiple agents to generate questions based on predefined types from a given text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,5819b66e04fd77fa705574edc49395bb,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="INSTRUCTION REFINEMENT FLOW">
      <data key="d0">PROCESS/TECHNIQUE</data>
      <data key="d1">The Instruction Refinement Flow iteratively enhances the complexity and quality of instructions generated by the Seed Instruction Flow. This process involves a Suggester-Editor pair, or suggester-editor agents, that modify passage-question pairs to create more complex or unanswerable questions, or to alter the answers. Through this iterative enhancement, the Instruction Refinement Flow aims to increase the sophistication of the generated instructions.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,427e98b00e49b6a8f8649054122dd45b,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SUGGESTER AGENT">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">The Suggester Agent is a tool designed to enhance the complexity and intricacy of initial instructions or passage-question pairs. It proposes various approaches to modify these elements, such as introducing hypothetical studies or adding layers of complexity to questions, making them more challenging, unsolvable, or tricky. This functionality aims to refine and elevate the difficulty level of the tasks, ensuring a more robust and comprehensive evaluation or learning experience.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="EDITOR AGENT">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">The Editor Agent is a tool designed to implement modifications suggested by the Suggester Agent, thereby refining passage-question pairs. Editor agents modify the instructions in accordance with the suggestions made by the Suggester agents, ensuring that the final output is polished and accurate.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="TEXT MODIFICATION">
      <data key="d0">SKILL</data>
      <data key="d1">Text modification involves changing existing text to improve its quality, modify its tone, or fit a specific context or audience.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="OPEN DOMAIN QUESTION ANSWERING">
      <data key="d0">SKILL</data>
      <data key="d1">Open Domain Question Answering involves generating responses to questions over a wide range of topics, without being restricted to a specific domain. It is also a process used to generate math problems for evaluating AI models. This approach allows for the assessment of AI models' ability to understand and respond to diverse and unrestricted queries, making it a crucial component in the development and evaluation of advanced AI and Machine Learning systems.</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="WEB AGENT">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">A web agent is a software program that autonomously performs tasks on the web, such as where to click and how much to scroll.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="BRAIN TEASER">
      <data key="d0">SKILL</data>
      <data key="d1">A brain teaser is a problem or puzzle that typically requires thought to solve, often for amusement but also used for training logical thinking and problem-solving skills.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="ANALYTICAL REASONING">
      <data key="d0">SKILL</data>
      <data key="d1">Analytical reasoning involves the ability to look at information, be it qualitative or quantitative, and discern patterns within the information to draw logical conclusions.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="MULTIPLE CHOICE QUESTIONS">
      <data key="d0">SKILL</data>
      <data key="d1">MULTIPLE CHOICE QUESTIONS are a form of assessment where respondents select the best possible answer from a list of choices. In the context of evaluating models, these questions are used in an open-ended generation setting with an empty system message. GPT-4 is employed to extract the option selected by the model from the model&#8217;s response, ensuring an accurate assessment of the model's performance.</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="DATA TO TEXT">
      <data key="d0">SKILL</data>
      <data key="d1">Data-to-text refers to generating human-readable textual summaries from source data, used for reports, explanations, or narratives from structured data.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="FERMI PROBLEMS">
      <data key="d0">SKILL</data>
      <data key="d1">Fermi problems are estimation problems that seek quick, rough estimates of quantities that can be difficult to measure, often requiring justified guesses or assumptions. Named after physicist Enrico Fermi, these problems are designed to provide rapid, approximate solutions by making educated guesses or assumptions.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="CODING">
      <data key="d0">SKILL</data>
      <data key="d1">"Coding involves writing code following instructions, understanding code, debugging code, and writing test cases. It encompasses the entire process of creating software, from the initial writing of code to the debugging and testing phases, ensuring that the code functions as intended. This comprehensive approach to coding is essential for developing reliable and efficient software applications."</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="TEXT EXTRACTION">
      <data key="d0">SKILL</data>
      <data key="d1">Text extraction is the process of retrieving relevant information from a larger text document, including tasks like named entity recognition, keyword extraction, or extracting specific data fields from unstructured text.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="RAW ARTICLES">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Raw articles are used as seeds in agentic flows to foster diversity and ensure broad coverage of generated problems.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SEED INSTRUCTIONS">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Seed instructions are generated from transformed content and are iteratively refined to boost quality, diversity, and complexity.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SEARCH API">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">A tool that agents can use to perform search operations as part of their tasks.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="CODE INTERPRETER">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">A tool that agents can use to interpret and execute code as part of their tasks.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="CALCULATOR">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">A tool that agents can use to perform calculations as part of their tasks.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="READING COMPREHENSION TESTS">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Tests that present text passages of varying lengths and subjects, followed by questions that assess the reader&#8217;s understanding.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="TABLE 1">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">A table that provides a full list of the 17 different skills implemented in the agentic flows, each having multiple subcategories.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="CASE STUDIES">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Case studies explain how the workflows work for generating data for specific skills such as Reading Comprehension, Text Modification, and Tool Use.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SKILLS">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">The 17 different skills implemented in the agentic flows, including reading comprehension, question answering, coding, retrieval augmented generation, creative writing, tool/API use, and Web control.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SUGGESTIONS">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Suggestions are proposed by Suggester agents to increase the intricacy of the initial instructions.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="EDITING">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Editing is the process performed by Editor agents to modify instructions based on the suggestions from Suggester agents.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="TASKS">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Tasks are the specific activities or problems that agents work on within the agentic flows.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="OBJECTIVES">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Objectives are the goals or targets that the instructions are tailored to achieve in the Content Transformation Flow.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="INTERMEDIATE REPRESENTATION">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">An intermediate representation is created from raw seeds to simplify the creation of instructions tailored to specific objectives.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="TAXONOMY">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">A pre-defined, but extensible, taxonomy is used in the Seed Instruction Generation Flow to introduce diversity in the generated instructions.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="QUALITY">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Quality is one of the attributes that the Instruction Refinement Flow aims to boost in the generated instructions.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="TEXT CLASSIFICATION">
      <data key="d0">PROCESS</data>
      <data key="d1">Text classification is a machine learning task where text documents are automatically classified into predefined categories, used for spam detection, sentiment analysis, and topic labeling among others.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="RETRIEVAL AUGMENTED GENERATION">
      <data key="d0">METHOD</data>
      <data key="d1">Retrieval Augmented Generation (RAG) is a method in natural language processing that combines retrieval-based and generative models to generate responses by first retrieving relevant documents and then using these documents to generate a response.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="CREATIVE CONTENT GENERATION">
      <data key="d0">ACTIVITY</data>
      <data key="d1">Creative content generation involves the creation of original content, often involving elements of novelty, value, and surprise. In AI, this could refer to generating text, music, or images that are new, meaningful, and interesting.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="FEW SHOT REASONING">
      <data key="d0">CONCEPT</data>
      <data key="d1">Few-shot reasoning refers to the ability of a machine learning model to understand new concepts, patterns, or tasks with minimal examples or guidance, mimicking the human ability to learn quickly from few examples.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="CONVERSATION">
      <data key="d0">ACTIVITY</data>
      <data key="d1">Conversation refers to conversational agents or chatbots that interact with humans in a natural, human-like manner.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="AGENTINSTRUCT FLOW">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">AGENTINSTRUCT FLOW is a multifaceted process designed to enhance various capabilities, including reading comprehension, through structured tasks and activities. It enables models to interact with external tools or services via APIs, thereby extending their functionality. Additionally, AgentInstruct Flow involves text modification processes that focus on editing and refining written content to improve its quality and effectiveness or alter its attributes.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d,427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="ARGUMENT PASSAGE GENERATOR">
      <data key="d0">TOOL</data>
      <data key="d1">The ARGUMENT PASSAGE GENERATOR is an agent proficient in crafting passages that articulate arguments, although these passages may sometimes exhibit logical inconsistencies. It functions as a tool within the Content Transformation Flow, generating argument passages from seed articles to aid in the development of reading comprehension materials.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="URIC ACID">
      <data key="d0">SUBSTANCE</data>
      <data key="d1">Uric acid is a chemical produced naturally by the breakdown of purine, a type of dietary protein found in red meat and seafood. Its levels in the body can be influenced by lifestyle choices such as alcohol consumption and physical inactivity. Excessive amounts of uric acid can lead to health complications such as hyperuricemia, which may increase the risk of cardiovascular disease. Conversely, low levels of uric acid can indicate underlying kidney or liver issues.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="HYPERURICEMIA">
      <data key="d0">CONDITION</data>
      <data key="d1">Hyperuricemia is a condition characterized by high levels of uric acid in the blood, typically defined as levels above 6 mg/dL in women and 7 mg/dL in men. It can result from increased production of uric acid or insufficient elimination through urine. Hyperuricemia is also associated with an increased risk of cardiovascular disease.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="HYPOURICEMIA">
      <data key="d0">CONDITION</data>
      <data key="d1">Hypouricemia is a condition characterized by low levels of uric acid in the blood. It is less common and usually asymptomatic but can indicate underlying kidney or liver issues.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="ENRICO FERMI">
      <data key="d0" />
      <data key="d1">Enrico Fermi was a physicist after whom Fermi problems are named.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LSAT LOGICAL REASONING TEST">
      <data key="d0">TEST</data>
      <data key="d1">The LSAT Logical Reasoning test features specialized question categories, including assumption, strengthening/weakening, flaw, and inference questions.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="CARDIOVASCULAR DISEASE">
      <data key="d0">CONDITION</data>
      <data key="d1">Cardiovascular disease is a class of diseases that involve the heart or blood vessels. It is a health condition that may be associated with high levels of uric acid in the blood, and these elevated levels are linked to an increased risk of developing such diseases.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="KIDNEY DISEASES">
      <data key="d0">CONDITION</data>
      <data key="d1">Kidney diseases are health conditions that can be caused by the lack or excess of uric acid in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="LEUKEMIA">
      <data key="d0">CONDITION</data>
      <data key="d1">Leukemia is a disease that can cause an imbalance of uric acid in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="OBESITY">
      <data key="d0">CONDITION</data>
      <data key="d1">Obesity is a health condition that can cause an imbalance of uric acid in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="ANEMIA">
      <data key="d0">CONDITION</data>
      <data key="d1">Anemia is a health condition that can cause an imbalance of uric acid in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="ALCOHOL">
      <data key="d0">SUBSTANCE</data>
      <data key="d1">Alcohol consumption is a lifestyle factor that can contribute to high levels of uric acid in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="PROCESSED FOODS">
      <data key="d0">SUBSTANCE</data>
      <data key="d1">Processed foods are a lifestyle factor that can contribute to high levels of uric acid in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="RED MEAT">
      <data key="d0">SUBSTANCE</data>
      <data key="d1">Red meat is a dietary source of purines, which can contribute to high levels of uric acid in the body. As a type of food that contains uric acid, its consumption can influence uric acid levels, potentially impacting health.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="SEAFOOD">
      <data key="d0">SUBSTANCE</data>
      <data key="d1">Seafood is a dietary source of purines, which can contribute to high levels of uric acid in the body. As a type of food that contains uric acid, its consumption can influence uric acid levels within the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="LABORATORY BLOOD TESTS">
      <data key="d0">TEST</data>
      <data key="d1">Laboratory blood tests are used to diagnose conditions related to uric acid levels in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="URINE TESTS">
      <data key="d0">TEST</data>
      <data key="d1">Urine tests are used to diagnose conditions related to uric acid levels in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="LABORATORY TESTS">
      <data key="d0">PROCESS</data>
      <data key="d1">Laboratory tests are medical tests conducted on blood and urine samples to diagnose conditions such as hyperuricemia and hypouricemia.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="CONTENT TRANSFORMATION AGENT">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">The Content Transformation Agent is a tool that determines which subset of agents to engage in the Seed Instruction Generation Flow based on the content.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="PASSAGE-QUESTION PAIRS">
      <data key="d0">OUTPUT</data>
      <data key="d1">Passage-question pairs are the output of the Seed Instruction Generation Flow, consisting of a passage of text and a corresponding question generated by the agents.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="TEXT MODIFICATION TASKS">
      <data key="d0">TASK</data>
      <data key="d1">Text modification tasks are specific tasks such as paraphrasing, expansion, simplification, and redacting content, which are performed by agents in the AgentInstruct Flow.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="PARAPHRASING AGENT">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">The Paraphrasing Agent is a tool designed to create text modification tasks based on a given input text and specific instructions. It takes a piece of text and generates several paraphrased versions of it, thereby facilitating the process of text modification. This functionality is particularly useful for applications requiring diverse textual outputs from a single source, ensuring variety and adaptability in content creation.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="ALCOHOL CONSUMPTION">
      <data key="d0">LIFESTYLE CHOICE</data>
      <data key="d1">Alcohol consumption is a lifestyle choice that can influence uric acid levels in the body.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="PHYSICAL INACTIVITY">
      <data key="d0">LIFESTYLE CHOICE</data>
      <data key="d1">Physical inactivity is a lifestyle choice that can influence uric acid levels in the body.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="KIDNEY ISSUES">
      <data key="d0">CONDITION</data>
      <data key="d1">Kidney issues can be indicated by low levels of uric acid in the blood, known as hypouricemia.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="LIVER ISSUES">
      <data key="d0">CONDITION</data>
      <data key="d1">Liver issues can be indicated by low levels of uric acid in the blood, known as hypouricemia.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="LITERAL COMPREHENSION QUESTIONS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Literal comprehension questions are a type of reading comprehension question that focuses on understanding the explicit content of a text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="CRITICAL COMPREHENSION QUESTIONS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Critical comprehension questions are a type of reading comprehension question that focuses on evaluating and analyzing the content of a text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="EVALUATIVE COMPREHENSION QUESTIONS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Evaluative comprehension questions are a type of reading comprehension question that focuses on making judgments about the content of a text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="REASONING QUESTIONS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Reasoning questions are a type of reading comprehension question that focuses on logical thinking and drawing conclusions from the content of a text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="IDENTIFYING ASSUMPTIONS QUESTIONS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Identifying assumptions questions are a type of reading comprehension question that focuses on recognizing underlying assumptions in the content of a text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="IDENTIFYING INFORMATION THAT STRENGTHENS/WEAKENS AN ARGUMENT QUESTIONS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">These questions focus on identifying information that either strengthens or weakens an argument presented in the text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="ORDERING EVENTS QUESTIONS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Ordering events questions are a type of reading comprehension question that focuses on arranging events in the correct sequence based on the content of a text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="APPENDIX A">
      <data key="d0">DOCUMENT SECTION</data>
      <data key="d1">Appendix A is a section in a document that lists various types of reading comprehension questions and text modification tasks.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="STRENGTHEN TYPE QUESTION">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">A strengthen type question is a reading comprehension question that asks which information most strengthens an argument presented in the text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="HYPOTHETICAL STUDY">
      <data key="d0">STUDY</data>
      <data key="d1">A hypothetical study is a suggested study that could potentially strengthen an argument, requiring the test-taker to infer its impact on the relationship between uric acid levels and cardiovascular disease.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="GENETIC PREDISPOSITION">
      <data key="d0">CONDITION</data>
      <data key="d1">Genetic predisposition refers to the likelihood of developing a condition based on one's genetic makeup, such as hyperuricemia and its correlation with increased cardiovascular events.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="DISTRACTOR OPTION">
      <data key="d0">QUESTION ELEMENT</data>
      <data key="d1">A distractor option is a misleading answer choice in a multiple-choice question that seems correct but does not directly relate to the causal relationship being tested.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="PARAPHRASING">
      <data key="d0">TEXT MODIFICATION TASK</data>
      <data key="d1">Paraphrasing is a text modification task that involves rephrasing a piece of text while retaining its original meaning. It entails rewriting text using different words and sentence structures while maintaining the original meaning.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="SIMPLIFICATION">
      <data key="d0">TEXT MODIFICATION TASK</data>
      <data key="d1">Simplification is a text modification task that involves making a piece of text easier to understand by reducing its complexity.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="REDACTING">
      <data key="d0">TEXT MODIFICATION TASK</data>
      <data key="d1">Redacting is a text modification task that involves removing or obscuring parts of a text for confidentiality or clarity.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="STYLING">
      <data key="d0">TEXT MODIFICATION TASK</data>
      <data key="d1">Styling is a text modification task that involves changing the appearance or format of a text to improve its presentation.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="CODE SWITCHING">
      <data key="d0">TEXT MODIFICATION TASK</data>
      <data key="d1">"CODE SWITCHING" refers to the practice of alternating between languages or dialects within a text. This phenomenon often reflects the natural patterns of bilingual speakers and can also be employed for creative writing purposes. As a text modification task, code switching involves the deliberate and strategic use of multiple languages or dialects within a single piece of text, showcasing the dynamic interplay between different linguistic systems.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="NATASCHA VAN DER ZWAN">
      <data key="d0">PERSON</data>
      <data key="d1">Natascha van der Zwan is a researcher who identifies three distinct research streams that have approached financialization</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="FINANCIALIZATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Financialization is a broad concept that encompasses the increasing social impact and interconnection of financial discourses, markets, actors, and institutions</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="AMERICAN ANTHROPOLOGICAL ASSOCIATION (AAA)">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">The American Anthropological Association is an organization that hosts the SEA 2017 Annual Meeting and provides a platform for submitting abstracts</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="SEA 2017 ANNUAL MEETING">
      <data key="d0">EVENT</data>
      <data key="d1">The SEA 2017 Annual Meeting is an event held from April 6-8, 2017 at the University of Iowa, Iowa City, USA, with an abstract submission deadline of December 1, 2016</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="SUGGESTER-EDITOR PAIR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">The Suggester-Editor pair is a duo that increases the complexity of generated instructions by providing suggestions and edits</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="API RETRIEVAL AGENT">
      <data key="d0">TOOL/AGENT</data>
      <data key="d1">The API Retrieval Agent iteratively searches for similar code to expand the API list during the Content Transformation Flow</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="VIEW ALL FOOD ITEMS">
      <data key="d0">API</data>
      <data key="d1">The "View All Food Items" API allows clients to obtain a detailed list of food items, complete with nutritional profiles</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="SEARCH FOOD ITEMS">
      <data key="d0">API</data>
      <data key="d1">The "Search Food Items" API allows clients to search for food items by name and retrieve a list of matching items. It requires a query parameter to specify the search term and optionally includes a limit parameter to restrict the number of results returned. This functionality enables users to efficiently find specific food items within a database, enhancing the user experience by providing relevant and concise search results.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="UNIVERSITY OF IOWA">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">The University of Iowa is the location where the SEA 2017 Annual Meeting was held</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="APRIL 6-8, 2017">
      <data key="d0">DATE</data>
      <data key="d1">The dates when the SEA 2017 Annual Meeting took place</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="DECEMBER 1, 2016">
      <data key="d0">DATE</data>
      <data key="d1">The deadline for abstract submissions for the SEA 2017 Annual Meeting</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="FINANCE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Finance is a field that deals with the study of investments, financial systems, and the management of money</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="RANDOM SEED">
      <data key="d0">CONCEPT</data>
      <data key="d1">A random seed is used to create a seed instruction for generating text modification tasks</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="SUGGESTION 1">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">Incorporate a fictional narrative. Use a conversational style with colloquial language and include a humorous element</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="SUGGESTION 2">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">Translate the event details into a poetic format. Maintain accurate information while using rhyming couplets and ensure the tone remains light and engaging</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="SUGGESTION 3">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">Frame the event details as a social media post. Use internet slang and emojis. Keep the message within 280 characters</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="MODIFIED INSTRUCTION 1">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">Rewrite the event details (date, location, abstract deadline) as if you&#8217;re telling a funny story to a friend using casual and colloquial language, while incorporating a fictional narrative that still conveys the necessary information</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="MODIFIED INSTRUCTION 2">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">Transform the event details (date, location, abstract deadline) into a light-hearted poem with rhyming couplets, ensuring that the essential information is accurately conveyed in a poetic format</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="MODIFIED INSTRUCTION 3">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">Craft a social media post that includes the event details (date, location, abstract deadline) using internet slang, emojis, and a casual tone, while keeping the message concise and within 280 characters</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="LIBRARY RECONSTRUCTION">
      <data key="d0">PROCESS</data>
      <data key="d1">Library Reconstruction is a scenario in the Content Transformation Flow where a list of APIs is synthesized from a random seed</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="FOOD ITEMS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Food items are products that can be consumed and are often listed with nutritional profiles such as calorie count, protein, and fat</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="GET FOOD ITEM DETAILS">
      <data key="d0">API</data>
      <data key="d1">Provides detailed information about a specific food item. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="CREATE MEAL PLAN">
      <data key="d0">API</data>
      <data key="d1">Enables the creation of a meal plan based on specified dietary preferences, caloric goals, and the number of meals per day. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="UPDATE FOOD ITEM">
      <data key="d0">API</data>
      <data key="d1">Allows updating the details of an existing food item. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="TRACK USER MEAL">
      <data key="d0">API</data>
      <data key="d1">Enables tracking of user meals. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="GET DIETARY RECOMMENDATIONS">
      <data key="d0">API</data>
      <data key="d1">Provides dietary recommendations based on user preferences and nutritional needs. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="ADD NEW FOOD ITEM">
      <data key="d0">API</data>
      <data key="d1">Allows adding a new food item to the database. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="DELETE FOOD ITEM">
      <data key="d0">API</data>
      <data key="d1">Enables the deletion of a food item from the database. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="GET USER NUTRITIONAL STATS">
      <data key="d0">API</data>
      <data key="d1">Provides nutritional statistics for a user. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="AGENT-INSTRUCT FLOW">
      <data key="d0">PROCESS</data>
      <data key="d1">A process that creates multi-turn conversations and instructions for an AI assistant to follow, including making API calls and concluding processes based on the availability of required parameters and APIs.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="USER">
      <data key="d0">PERSON</data>
      <data key="d1">The USER is a participant in the multi-turn interaction in Orca-Bench. They are actively involved in requesting the creation of meal plans, including vegetarian options, tracking daily meals, seeking new food recommendations, obtaining nutritional summaries, and providing feedback on the meal plans. Additionally, the USER is responsible for requesting updates to the food database, ensuring that their dietary needs and preferences are met comprehensively.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="ASSISTANT">
      <data key="d0">AI</data>
      <data key="d1">ASSISTANT is a versatile AI entity that plays a crucial role in various interactive and functional tasks. As a participant in the multi-turn interaction in Orca-Bench, ASSISTANT provides responses that contribute to the ongoing dialogue. Additionally, ASSISTANT is responsible for creating comprehensive meal plans, tracking meals, offering food recommendations, generating nutritional summaries, and updating the food database based on user requests. This multifaceted role ensures that users receive personalized and accurate dietary guidance, making ASSISTANT an essential tool for managing and optimizing nutritional intake.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="QUINOA SALAD">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">QUINOA SALAD is a food item that the assistant is asked to add to the database, requiring nutritional information. It is also a recipe that the user wants to add to the database.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="CHANA MASALA">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">CHANA MASALA is a food item that the assistant is tasked with updating in the database, requiring its unique identifier. Additionally, it is a food item whose calorie count the user believes is incorrect and wants to update.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="BUTTER CHICKEN">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">BUTTER CHICKEN is a food item that the user wants to remove from the database. The assistant is tasked with removing this item, which requires its unique identifier.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="API_CALL">
      <data key="d0">ACTION</data>
      <data key="d1">An action to create a meal plan with specific dietary preferences, caloric goal, and number of meals</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="KNOWLEDGEPILE">
      <data key="d0">DATA SOURCE</data>
      <data key="d1">A source of unstructured text and code files used in the creation of the AgentInstruct dataset</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="AUTOMATHTEXT">
      <data key="d0">DATA SOURCE</data>
      <data key="d1">A source of unstructured text and code files used in the creation of the AgentInstruct dataset</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="OPENSTAX">
      <data key="d0">DATA SOURCE</data>
      <data key="d1">A source of unstructured text and code files used in the creation of the AgentInstruct dataset</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="APACHE-2.0 LICENSED SOURCE CODE">
      <data key="d0">DATA SOURCE</data>
      <data key="d1">A source of unstructured text and code files used in the creation of the AgentInstruct dataset</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="ORCA-2.5-DATASET">
      <data key="d0">DATASET</data>
      <data key="d1">A dataset consisting of approximately 3.8 million paired instructions sourced from various Orca versions and other publicly available sources</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="MISTRAL-7B-V0.1">
      <data key="d0">MODEL</data>
      <data key="d1">The base model used for finetuning with the AgentInstruct dataset to create Orca-3</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="NVIDIA A100">
      <data key="d0">HARDWARE</data>
      <data key="d1">The hardware used for training the Orca-3 model, consisting of 152 GPUs</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="ADAMW OPTIMIZER">
      <data key="d0">OPTIMIZER</data>
      <data key="d1">The optimizer used for training the Orca-3 model with an initial learning rate of 8e-6</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="ORCA-BENCH">
      <data key="d0">DATASET</data>
      <data key="d1">ORCA-BENCH is a dataset designed to evaluate the performance of various baseline models. It consists of a held-out test set with 100 samples from each of the 17 skills curated using AgentInstruct. The performance of models is scored relative to GPT-4 on a scale from 0 to 10, making it a comprehensive tool for assessing model capabilities across multiple skills.</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="VEGETARIAN MEAL PLAN">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="ORCA-1">
      <data key="d0" />
      <data key="d1">A previous version of the Orca model, which contributed to the Orca-2.5-dataset</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="ORCA-2">
      <data key="d0" />
      <data key="d1">A previous version of the Orca model, which contributed to the Orca-2.5-dataset</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="ORCA-MATH">
      <data key="d0" />
      <data key="d1">ORCA-MATH is a specialized version of the Orca model that focuses on mathematical instructions. It has significantly contributed to the development of the Orca-2.5-dataset. Additionally, ORCA-MATH is the subject of a paper titled "Orca-math: Unlocking the potential of SLMs in grade school math," which was published in 2024. This paper highlights the model's potential in enhancing the understanding and application of mathematical concepts at the grade school level.</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a,3d1f6634f93f8a4c296dc8df7e59859e</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="DAY 2">
      <data key="d0">DAY</data>
      <data key="d1">The second day of the vegetarian meal plan, including breakfast, lunch, and dinner with specific food items and total calories</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="OATMEAL WITH FRUITS">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A breakfast food item included in the vegetarian meal plan for Day 1</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="ALMOND MILK">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A breakfast food item included in the vegetarian meal plan for Day 1</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="CHICKPEA SALAD">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A lunch food item included in the vegetarian meal plan for Day 1</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="WHOLE WHEAT BREAD">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A lunch food item included in the vegetarian meal plan for Day 1</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="MIXED VEGETABLE STIR FRY">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A dinner food item included in the vegetarian meal plan for Day 1</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="BROWN RICE">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A dinner food item included in the vegetarian meal plan for Day 1</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="OPEN DOMAIN QUESTION ANSWERING (ODQA)">
      <data key="d0">SKILL</data>
      <data key="d1">A skill category in the Orca-Bench dataset, consisting of 100 questions from the initial seed instruction phase</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="COMPLEX ODQA">
      <data key="d0">SKILL</data>
      <data key="d1">COMPLEX ODQA is a skill category in the Orca-Bench dataset, consisting of a subset of more intricate questions developed during the refinement phase. This category includes complex questions that were specifically designed to challenge and evaluate advanced understanding and capabilities in open-domain question answering (ODQA).</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="DAY 1">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="ORCA-2.5">
      <data key="d0">MODEL</data>
      <data key="d1">Orca-2.5 is a 7B model that serves as a baseline for comparison with Orca-3. It is evaluated on the Orca-Bench dataset and is a previous version of the Orca language model used for comparison in reading comprehension evaluations.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="MISTRAL-INSTRUCT-7B">
      <data key="d0">MODEL</data>
      <data key="d1">Mistral-Instruct-7B is a baseline model evaluated on the Orca-Bench dataset</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="LLAMA3-8B-INSTRUCT">
      <data key="d0">MODEL</data>
      <data key="d1">LLAMA3-8B-Instruct is a baseline language model that has been evaluated on various benchmarks, specifically focusing on summarization and hallucination rates. This model is designed to provide insights into its performance across different tasks, making it a valuable tool for understanding the capabilities and limitations of language models in these areas.</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="IFEVAL">
      <data key="d0">BENCHMARK</data>
      <data key="d1">IFEval, or Instruction-Following Evaluation, is a benchmark designed to assess a model's ability to follow natural language instructions. It utilizes a set of 500 prompts that cover 25 types of verifiable instructions. The benchmark evaluates the performance of various models, including Orca-3, Orca-2.5, Mistral-7B-Instruct, GPT-3.5-turbo, and GPT-4. IFEval ensures that the model responses adhere to the verifiable instructions provided in the prompts, using code supplied by the authors.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba,86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="INFOBENCH">
      <data key="d0">BENCHMARK</data>
      <data key="d1">INFOBENCH is a benchmark designed to evaluate the instruction-following capability of models using the Decomposed Requirements Following Ratio (DRFR) metric. It is assessed using GPT-4 to determine if the model responses adhere to decomposed instructions. INFOBENCH is utilized to evaluate the performance of various models, including Orca-3, Orca-2.5, Mistral-7B-Instruct, GPT-3.5-turbo, and GPT-4. The findings and methodologies related to INFOBENCH are detailed in the paper titled "InfoBench: Evaluating instruction following ability in large language models," published in 2024.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba,3d1f6634f93f8a4c296dc8df7e59859e,86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="EQBENCH">
      <data key="d0">BENCHMARK</data>
      <data key="d1">EQBENCH is a benchmark designed to generate emotion scores from conversations. It is evaluated using GPT-4 to extract and calibrate these scores. EQBENCH is utilized to assess the performance of various models, including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4.</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="SYSTEM MESSAGE">
      <data key="d0">MESSAGE</data>
      <data key="d1">SYSTEM MESSAGE is a part of the multi-turn interaction in Orca-Bench, crafted by GPT-4. It serves as a predefined message used to guide the GPT-4 model in extracting student responses.</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="TEACHER">
      <data key="d0">ROLE</data>
      <data key="d1">GPT-4 acting as the teacher in the multi-turn interaction in Orca-Bench</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="STUDENT">
      <data key="d0">ROLE</data>
      <data key="d1">The model being evaluated in the multi-turn interaction in Orca-Bench</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="ORCA-3 CHECKPOINT EPOCH 1">
      <data key="d0">MODEL</data>
      <data key="d1">A specific checkpoint of the Orca-3 model evaluated on the Orca-Bench dataset</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="ORCA-3 CHECKPOINT EPOCH 2">
      <data key="d0">MODEL</data>
      <data key="d1">A specific checkpoint of the Orca-3 model evaluated on the Orca-Bench dataset</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="TABLE 2">
      <data key="d0">DATA REPRESENTATION</data>
      <data key="d1">A table encapsulating the average (macro) scores across all assessed dimensions for different models</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="FIGURE 4">
      <data key="d0">DATA REPRESENTATION</data>
      <data key="d1">A figure illustrating the performance comparison between baseline models and Orca-3 checkpoints</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="BENCHMARK RESULTS">
      <data key="d0">RESULTS</data>
      <data key="d1">The section evaluating Orca-3 against 5 baseline models on various benchmarks</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="METRIC-V2">
      <data key="d0">BENCHMARK</data>
      <data key="d1">METRIC-V2 is a benchmark used to evaluate the performance of various models, including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="METRIC-V1">
      <data key="d0">BENCHMARK</data>
      <data key="d1">METRIC-V1 is a benchmark used to evaluate the performance of various models, including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4. It serves as a performance metric that has demonstrated a 28% improvement in one of the benchmarks, highlighting its effectiveness in assessing model capabilities.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="LSAT">
      <data key="d0">EXAM</data>
      <data key="d1">The Law School Admission Test (LSAT) is a standardized test used for law school admissions, known for its difficulty. It is particularly challenging in its reading comprehension sections.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="SAT">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="BIG-BENCH">
      <data key="d0" />
      <data key="d1">Big-Bench is a broader benchmark from which the Big Bench Hard (BBH) tasks are selected.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
      <data key="d3">TOOL/BENCHMARK</data>
    </node>
    <node id="DOMAIN EXPERTS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="GRADE SCHOOL MATH">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="DOMAIN-SPECIFIC FORMATS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="INSTRUCTION-FOLLOWING">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="ALPACA WEB DEMO">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="DRFR">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="MISTRAL">
      <data key="d0" />
      <data key="d1">MISTRAL is a language model family that has demonstrated substantial improvement in reading comprehension capabilities through targeted training with AgentInstruct. This model family serves as a base for fine-tuning with AgentInstruct data, enhancing its performance in understanding and processing text.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="ALLENAI">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">AllenAI is the organization that developed the AI2 Reasoning Challenge (ARC) benchmark.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="AI-HUMAN COLLABORATION">
      <data key="d0">PROCESS</data>
      <data key="d1">AI-Human collaboration is the process used to create the diverse range of real-world formats and instructions for the FoFo benchmark.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="ZERO-SHOT SETTING">
      <data key="d0">PROCESS</data>
      <data key="d1">Zero-shot setting is the evaluation method used for Orca-3 and other baseline models unless mentioned otherwise.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="LAW SCHOOL ADMISSION TESTS (LSATS)">
      <data key="d0">EXAM</data>
      <data key="d1">The Law School Admission Tests (LSATs) are standardized tests used for law school admissions, known for their difficulty in reading comprehension sections.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="BENCHMARKS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="GEMINI PRO">
      <data key="d0">MODEL</data>
      <data key="d1">Gemini Pro is a model used as a benchmark for evaluating the format-following capabilities of other models like Orca-3. Its scores are referenced from the original paper, making it a standard for comparison in the field.</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b,bb87f82e6a9f1d4da6480ec78a0e3701</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="MULTIPLE-CHOICE QUESTIONS FLOWS">
      <data key="d0">PROCESS</data>
      <data key="d1">Multiple-Choice Questions Flows is a process used to generate math problems for evaluating AI models</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="PHI3">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Phi3 is a paper that reported the accuracy scores for GPT-3.5-turbo on the GSM8K benchmark</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="ORCA-3-7B">
      <data key="d0">MODEL</data>
      <data key="d1">Orca-3-7B is a language model that has been evaluated on various benchmarks for summarization and hallucination rates. It is fine-tuned with AgentInstruct data and is based on the Mistral model family. Orca-3-7B has demonstrated significant improvements in multiple benchmarks, including reading comprehension, math, and format following, and has been used in the evaluation of MIRAGE datasets.</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="ORCA-2.5-7B">
      <data key="d0">MODEL</data>
      <data key="d1">Orca-2.5-7B is a language model evaluated on various benchmarks for summarization and hallucination rates. It serves as a baseline for comparison with Orca-3-7B and is also utilized in the evaluation of MIRAGE datasets.</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="AGIEVAL LSAT-RC">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AGIEval LSAT-RC is a benchmark used to evaluate models on the reading comprehension sections of the LSAT</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="AGIEVAL SAT-EN">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AGIEval SAT-EN is a benchmark used to evaluate models on the English sections of the SAT</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="AGIEVAL GAOKAO-ENGLISH">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AGIEval Gaokao-English is a benchmark used to evaluate models on the English sections of the Gaokao</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="AGIEVAL LSAT-LR">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AGIEval LSAT-LR is a benchmark used to evaluate models on the logical reasoning sections of the LSAT</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="AGIEVAL MATH">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AGIEval Math is a benchmark used to evaluate models on math problem-solving tasks</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="AGIEVAL SAT-MATH">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AGIEval SAT-Math is a benchmark used to evaluate models on the math sections of the SAT</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="BBH MULTISTEP-ARITHMETIC-TWO">
      <data key="d0">BENCHMARK</data>
      <data key="d1">BBH Multistep-Arithmetic-Two is a benchmark used to evaluate models on multi-step arithmetic problems</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="MMLU ABSTRACT ALGEBRA">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MMLU Abstract Algebra is a benchmark used to evaluate models on abstract algebra tasks</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="MMLU COLLEGE MATHEMATICS">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MMLU College Mathematics is a benchmark used to evaluate models on college-level mathematics tasks</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="MMLU HIGH-SCHOOL MATHEMATICS">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MMLU High-School Mathematics is a benchmark used to evaluate models on high school-level mathematics tasks</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="FOFO BENCHMARK">
      <data key="d0">BENCHMARK</data>
      <data key="d1">FoFo benchmark is used to evaluate the performance of language models, including Orca-3-7B</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="ACI-BENCH">
      <data key="d0">BENCHMARK</data>
      <data key="d1">ACI-Bench is a dataset designed for benchmarking automatic report generation from doctor-patient conversations</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="INSTRUSUM">
      <data key="d0">BENCHMARK</data>
      <data key="d1">InstruSum is a dataset for evaluating the generation capabilities of language models for instruction-controllable summarization</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="ORCA-SUM">
      <data key="d0">BENCHMARK</data>
      <data key="d1">Orca-Sum is a benchmark created to evaluate language models' ability to follow summarization and grounded data transformation instructions</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="MIRAGE">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MIRAGE is a benchmark designed to evaluate the performance of various models on different tasks by focusing on answering medical questions. It achieves this by referring to information retrieved from a comprehensive medical corpus. This collection of datasets serves as a critical tool for assessing the capabilities of AI and ML models in the medical domain, ensuring that they can effectively interpret and utilize medical information to provide accurate and relevant answers.</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="MMLU-MED">
      <data key="d0">DATASET</data>
      <data key="d1">MMLU-Med is a dataset used in the MIRAGE benchmark for evaluating medical question answering</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="MEDQA-US">
      <data key="d0">DATASET</data>
      <data key="d1">MedQA-US is a dataset used in the MIRAGE benchmark for evaluating medical question answering</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="MEDMCQA">
      <data key="d0">DATASET</data>
      <data key="d1">MedMCQA is a dataset used in the MIRAGE benchmark for evaluating medical question answering</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="PUBMEDQA">
      <data key="d0">DATASET</data>
      <data key="d1">PubMedQA is a dataset used in the MIRAGE benchmark for evaluating medical question answering. It is one of the datasets included in the MIRAGE collection and is considered an effective testbed for assessing models' ability to perform Retrieval-Augmented Generation (RAG).</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="BIOASQ">
      <data key="d0">DATASET</data>
      <data key="d1">BIOASQ is a dataset used in the MIRAGE benchmark for evaluating medical question answering. It is one of the datasets included in the MIRAGE collection, which is designed to assess the performance of systems in the domain of medical information retrieval and question answering.</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="HUGGING FACE">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Hugging Face is an organization that hosts various datasets used for constructing the Orca-Sum benchmark</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="CO-T">
      <data key="d0">SKILL</data>
      <data key="d1">CoT (Chain of Thought) is a skill used by GPT-4 for medical question answering in the MIRAGE benchmark</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="MEDMEDQA">
      <data key="d0">DATASET</data>
      <data key="d1">MedMedQA is one of the datasets included in the MIRAGE collection</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="USMEDMCQA">
      <data key="d0">DATASET</data>
      <data key="d1">USMedMCQA is one of the datasets included in the MIRAGE collection</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="MISTRAL-7B-INSTRUCT-V0.1">
      <data key="d0">MODEL</data>
      <data key="d1">Mistral-7B-Instruct-v0.1 is a model used in the evaluation of MIRAGE datasets</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="MEDRAG">
      <data key="d0">TOOL</data>
      <data key="d1">MedRAG is the retrieval mechanism used across all models on MIRAGE, involving the same retrieval function and number of retrieved documents</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
      <data key="d3">TOOL</data>
    </node>
    <node id="AZURE">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Azure, a cloud computing service provided by Microsoft, is recommended for reviewing transparency notes related to large language models. This highlights Azure's role in ensuring transparency and accountability in the deployment and management of advanced AI systems.</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">ORGANIZATION</data>
    </node>
    <node id="TABLE 8">
      <data key="d0">DOCUMENT SECTION</data>
      <data key="d1">Table 8 shows the evaluation results of RAG skill on MIRAGE datasets with and without leveraging RAG</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="AGENTINSTRUCT RAG FLOW">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">AgentInstruct RAG flow is a technique used to train Orca-3, resulting in substantial performance improvement</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="LIMITATIONS">
      <data key="d0">DOCUMENT SECTION</data>
      <data key="d1">The section discussing the limitations of AgentInstruct and synthetic data generation</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="DATA BIASES">
      <data key="d0">ISSUE</data>
      <data key="d1">Data Biases refer to the biases present in the source data that can be carried over to large language models</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="LACK OF TRANSPARENCY">
      <data key="d0">ISSUE</data>
      <data key="d1">Lack of Transparency refers to the difficulty in understanding the rationale behind specific outputs or decisions of large language models</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="CONTENT HARMS">
      <data key="d0">ISSUE</data>
      <data key="d1">CONTENT HARMS refer to various types of harmful content that large language models can generate. This necessitates awareness and preventive actions to mitigate the potential negative impacts.</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="VALIDATION">
      <data key="d0">ISSUE</data>
      <data key="d1">Validation refers to the difficulty in ensuring synthetic data accurately represents the desired scenarios</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="DEPENDENCY ON SEED DATA">
      <data key="d0">ISSUE</data>
      <data key="d1">Dependency on Seed Data refers to the quality of synthetic data being dependent on the quality of the real data used as seeds</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="EXTENSIBILITY">
      <data key="d0">ISSUE</data>
      <data key="d1">Extensibility refers to the human effort required to create agentic flows for different skills</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="BIAS">
      <data key="d0">ISSUE</data>
      <data key="d1">Bias refers to the potential for synthetic data to reflect and amplify biases present in the original seed data</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="TRANSPARENCY NOTES">
      <data key="d0">DOCUMENTATION</data>
      <data key="d1">Transparency notes from Azure provide information about the rationale behind specific outputs or decisions</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="CONTENT MODERATION SERVICES">
      <data key="d0">SERVICE</data>
      <data key="d1">Services provided by different companies and institutions to moderate and prevent harmful content generated by language models</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="GOVERNMENT AND TECHNOLOGY LEADERS">
      <data key="d0">GROUP</data>
      <data key="d1">Entities hoped to provide better regulations and standards around content harms for AI technologies in the future</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="RESEARCH AND OPEN SOURCE COMMUNITY">
      <data key="d0">GROUP</data>
      <data key="d1">Communities valued for their role in addressing content harms and improving AI technologies</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="HALLUCINATION">
      <data key="d0">CONCEPT/ISSUE</data>
      <data key="d1">The phenomenon where language models fabricate content, making it unreliable for critical decisions or information</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="UNSTRUCTURED DATA SOURCES">
      <data key="d0">DATA TYPE</data>
      <data key="d1">Sources of data that are not organized in a pre-defined manner, used by AgentInstruct for generating synthetic data</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="25M PAIR DATASET">
      <data key="d0">DATASET</data>
      <data key="d1">A dataset comprising 25 million pairs of prompts and responses generated by AgentInstruct for post-training the Orca-3 model</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">DATASET</data>
    </node>
    <node id="MODEL TRAINING">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of training machine learning models, which can benefit from synthetic data generated by AgentInstructModel training is the process of training machine learning models, which can benefit from synthetic data generated by AgentInstruct</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="DOMAIN/TASK SPECIALIZATION">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of customizing models for specific domains or tasks using synthetic dataDomain/task specialization is the process of customizing models for specific domains or tasks using synthetic data</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="MARAH ABDIN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMarah Abdin is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAM ADE JACOBS">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportSam Ade Jacobs is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AMMAR AHMAD AWAN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAmmar Ahmad Awan is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JYOTI ANEJA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJyoti Aneja is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HANY AWADALLA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportHany Awadalla is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NGUYEN BACH">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportNguyen Bach is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AMIT BAHREE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAmit Bahree is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ARASH BAKHTIARI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportArash Bakhtiari is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIANMIN BAO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJianmin Bao is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HARKIRAT BEHL">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportHarkirat Behl is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALON BENHAIM">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAlon Benhaim is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MISHA BILENKO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMisha Bilenko is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOHAN BJORCK">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJohan Bjorck is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="S&#201;BASTIEN BUBECK">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportS&#233;bastien Bubeck is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QIN CAI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportQin Cai is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MARTIN CAI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMartin Cai is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CAIO C&#201;SAR TEODORO MENDES">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportCaio C&#233;sar Teodoro Mendes is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VISHRAV CHAUDHARY">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportVishrav Chaudhary is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DONG CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportDong Chen is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DONGDONG CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportDongdong Chen is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YEN-CHUN CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportYen-Chun Chen is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YI-LING CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportYi-Ling Chen is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PARUL CHOPRA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportParul Chopra is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIYANG DAI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportXiyang Dai is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALLIE DEL GIORNO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAllie Del Giorno is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GUSTAVO DE ROSA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportGustavo de Rosa is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MATTHEW DIXON">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMatthew Dixon is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RONEN ELDAN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportRonen Eldan is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VICTOR FRAGOSO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportVictor Fragoso is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAN ITER">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportDan Iter is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MEI GAO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMei Gao is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MIN GAO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMin Gao is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIANFENG GAO">
      <data key="d0">PERSON</data>
      <data key="d1">Jianfeng Gao is an author of the Phi-3 technical report and the paper "Instruction tuning with GPT-4".</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AMIT GARG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAmit Garg is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ABHISHEK GOSWAMI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAbhishek Goswami is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SURIYA GUNASEKAR">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportSuriya Gunasekar is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="EMMAN HAIDER">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportEmman Haider is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JUNHENG HAO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJunheng Hao is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RUSSELL J. HEWETT">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportRussell J. Hewett is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAMIE HUYNH">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJamie Huynh is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MOJAN JAVAHERIPI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMojan Javaheripi is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIN JIN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportXin Jin is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PIERO KAUFFMANN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportPiero Kauffmann is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NIKOS KARAMPATZIAKIS">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportNikos Karampatziakis is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DONGWOO KIM">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportDongwoo Kim is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MAHOUD KHADEMI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMahoud Khademi is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LEV KURILENKO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportLev Kurilenko is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAMES R. LEE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJames R. Lee is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIN TAT LEE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportYin Tat Lee is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUANZHI LI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportYuanzhi Li is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUNSHENG LI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportYunsheng Li is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHEN LIANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportChen Liang is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LARS LIDEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportLars Liden is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CE LIU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportCe Liu is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MENGCHEN LIU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMengchen Liu is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEISHUNG LIU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportWeishung Liu is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ERIC LIN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportEric Lin is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHONG LUO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportChong Luo is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PIYUSH MADAN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportPiyush Madan is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MATT MAZZOLA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMatt Mazzola is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HARDIK MODI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportHardik Modi is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BRANDON NORICK">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportBrandon Norick is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BARUN PATRA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportBarun Patra is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DANIEL PEREZ-BECKER">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportDaniel Perez-Becker is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THOMAS PORTET">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportThomas Portet is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="REID PRYZANT">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportReid Pryzant is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HEYANG QIN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportHeyang Qin is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MARKO RADMILAC">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMarko Radmilac is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAMBUDHA ROY">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportSambudha Roy is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="OLATUNJI RUWASE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportOlatunji Ruwase is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="OLLI SAARIKIVI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportOlli Saarikivi is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AMIN SAIED">
      <data key="d0">PERSON</data>
      <data key="d1">Amin Saied is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models" and also contributed to the Phi-3 technical report.</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ADIL SALIM">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAdil Salim is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MICHAEL SANTACROCE">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Santacroce is an author of the Phi-3 technical report and has also contributed to the paper titled "Direct Nash optimization: Teaching language models to self-improve with general preferences."</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHITAL SHAH">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportShital Shah is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NING SHANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportNing Shang is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HITESHI SHARMA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportHiteshi Sharma is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SWADHEEN SHUKLA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportSwadheen Shukla is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIA SONG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportXia Song is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MASAHIRO TANAKA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMasahiro Tanaka is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANDREA TUPINI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAndrea Tupini is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportXin Wang is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIJUAN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportLijuan Wang is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHUNYU WANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportChunyu Wang is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YU WANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportYu Wang is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RACHEL WARD">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportRachel Ward is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GUANHUA WANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportGuanhua Wang is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PHILIPP WITTE">
      <data key="d0">PERSON</data>
      <data key="d1">Philipp Witte is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone."</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HAIPING WU">
      <data key="d0">PERSON</data>
      <data key="d1">Haiping Wu is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone." This work highlights the development and capabilities of the Phi-3 language model, which is designed to operate efficiently on mobile devices.</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MICHAEL WYATT">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Wyatt is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone."</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BIN XIAO">
      <data key="d0">PERSON</data>
      <data key="d1">Bin Xiao is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone." This work highlights the development and capabilities of the Phi-3 language model, which is designed to operate efficiently on mobile devices.</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CAN XU">
      <data key="d0">PERSON</data>
      <data key="d1">Can Xu is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone." Additionally, Can Xu has contributed to the paper "Wizardlm: Empowering large language models to follow complex instructions."</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIAHANG XU">
      <data key="d0">PERSON</data>
      <data key="d1">Jiahang Xu is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone."</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEIJIAN XU">
      <data key="d0">PERSON</data>
      <data key="d1">Weijian Xu is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone."</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SONALI YADAV">
      <data key="d0">PERSON</data>
      <data key="d1">Sonali Yadav is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone."</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FAN YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Fan Yang is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone." This work highlights the development and capabilities of the Phi-3 language model, which is designed to operate efficiently on mobile devices.</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIANWEI YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jianwei Yang is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone." This work highlights the development and capabilities of the Phi-3 language model, which is designed to operate efficiently on mobile devices.</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZIYI YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Ziyi Yang is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone."</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIFAN YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yifan Yang is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone."</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DONGHAN YU">
      <data key="d0">PERSON</data>
      <data key="d1">Donghan Yu is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone."</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LU YUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Lu Yuan is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone."</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHENGRUIDONG ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Chengruidong Zhang is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone."</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CYRIL ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Cyril Zhang is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone."</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIANWEN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jianwen Zhang is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone." This work highlights the development and capabilities of the Phi-3 language model, which is designed to operate efficiently on mobile devices.</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LI LYNA ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Li Lyna Zhang is an author of the Phi-3 technical report, specifically the paper titled "Phi-3 technical report: A highly capable language model locally on your phone."</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YI ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yi Zhang is an author of the paper titled "Phi-3 technical report: A highly capable language model locally on your phone." This technical report, known as the Phi-3 technical report, highlights the development and capabilities of a sophisticated language model that can be operated locally on mobile devices.</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="YUE ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yue Zhang is an author of the paper titled "Phi-3 technical report: A highly capable language model locally on your phone." This report, known as the Phi-3 technical report, highlights the development and capabilities of a sophisticated language model that can be operated directly on mobile devices.</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="YUNAN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yunan Zhang is an author of the paper titled "Phi-3 technical report: A highly capable language model locally on your phone." This report, known as the Phi-3 technical report, highlights the development and capabilities of a sophisticated language model designed to operate efficiently on mobile devices.</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="XIREN ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Xiren Zhou is an author of the paper titled "Phi-3 technical report: A highly capable language model locally on your phone." This technical report, known as the Phi-3 technical report, highlights the development and capabilities of a sophisticated language model that can be operated directly on mobile devices.</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="PHI-3 TECHNICAL REPORT">
      <data key="d0">DOCUMENT</data>
      <data key="d1">The "PHI-3 TECHNICAL REPORT" is a comprehensive document authored by multiple individuals, including Marah Abdin and Sam Ade Jacobs. Published in 2024, this technical report details a highly capable language model known as PHI-3, which is designed to run locally on a phone. The report provides in-depth insights into the model's capabilities, architecture, and potential applications, highlighting its significance in advancing mobile AI technology.</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Wang is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ISAAC COWHEY">
      <data key="d0">PERSON</data>
      <data key="d1">Isaac Cowhey is an author of the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="OREN ETZIONI">
      <data key="d0">PERSON</data>
      <data key="d1">Oren Etzioni is an author of the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="TUSHAR KHOT">
      <data key="d0">PERSON</data>
      <data key="d1">Tushar Khot is an author of the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ASHISH SABHARWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Ashish Sabharwal is an author of the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CARISSA SCHOENICK">
      <data key="d0">PERSON</data>
      <data key="d1">Carissa Schoenick is an author of the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="OYVIND TAFJORD">
      <data key="d0">PERSON</data>
      <data key="d1">Oyvind Tafjord is an author of the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="THINK YOU HAVE SOLVED QUESTION ANSWERING? TRY ARC, THE AI2 REASONING CHALLENGE">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the AI2 Reasoning Challenge (ARC) for question answering, published in 2018</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CODEPARROT">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">CodeParrot is the organization behind the Github-code clean dataset</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="GITHUB-CODE CLEAN DATASET">
      <data key="d0">DATASET</data>
      <data key="d1">A dataset provided by CodeParrot, accessed in 2022</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="NING DING">
      <data key="d0">PERSON</data>
      <data key="d1">Ning Ding is an author of the paper "Enhancing chat language models by scaling high-quality instructional conversations"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="YULIN CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Yulin Chen is an author of the paper "Enhancing chat language models by scaling high-quality instructional conversations"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="BOKAI XU">
      <data key="d0">PERSON</data>
      <data key="d1">Bokai Xu is an author of the paper "Enhancing chat language models by scaling high-quality instructional conversations"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ZHI ZHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhi Zheng is an author of the paper "Enhancing chat language models by scaling high-quality instructional conversations"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="SHENGDING HU">
      <data key="d0">PERSON</data>
      <data key="d1">Shengding Hu is an author of the paper "Enhancing chat language models by scaling high-quality instructional conversations"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ENHANCING CHAT LANGUAGE MODELS BY SCALING HIGH-QUALITY INSTRUCTIONAL CONVERSATIONS">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the enhancement of chat language models by scaling high-quality instructional conversations, published in 2023</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ZHAOYE FEI">
      <data key="d0">PERSON</data>
      <data key="d1">Zhaoye Fei is an author of the paper "Query of cc: Unearthing large scale domain-specific knowledge from public corpora"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="YUNFAN SHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Yunfan Shao is an author of the paper "Query of cc: Unearthing large scale domain-specific knowledge from public corpora"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="LINYANG LI">
      <data key="d0">PERSON</data>
      <data key="d1">Linyang Li is an author of the paper "Query of cc: Unearthing large scale domain-specific knowledge from public corpora"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ZHIYUAN ZENG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhiyuan Zeng is an author of the paper "Query of cc: Unearthing large scale domain-specific knowledge from public corpora"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HANG YAN">
      <data key="d0">PERSON</data>
      <data key="d1">Hang Yan is an author of the paper "Query of cc: Unearthing large scale domain-specific knowledge from public corpora"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="XIPENG QIU">
      <data key="d0">PERSON</data>
      <data key="d1">Xipeng Qiu is an author of the paper "Query of cc: Unearthing large scale domain-specific knowledge from public corpora"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="DAHUA LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Dahua Lin is an author of the paper "Query of cc: Unearthing large scale domain-specific knowledge from public corpora"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="QUERY OF CC: UNEARTHING LARGE SCALE DOMAIN-SPECIFIC KNOWLEDGE FROM PUBLIC CORPORA">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the extraction of domain-specific knowledge from public corpora, published in 2024</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ARNAV GUDIBANDE">
      <data key="d0">PERSON</data>
      <data key="d1">Arnav Gudibande is an author of the paper "The false promise of imitating proprietary llms"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ERIC WALLACE">
      <data key="d0">PERSON</data>
      <data key="d1">Eric Wallace is an author of the paper "The false promise of imitating proprietary llms"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CHARLIE SNELL">
      <data key="d0">PERSON</data>
      <data key="d1">Charlie Snell is an author of the paper "The false promise of imitating proprietary llms"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="XINYANG GENG">
      <data key="d0">PERSON</data>
      <data key="d1">Xinyang Geng is an author of the paper "The false promise of imitating proprietary llms"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HAO LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Hao Liu is an author of the paper "The false promise of imitating proprietary llms"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="THE FALSE PROMISE OF IMITATING PROPRIETARY LLMS">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the limitations of imitating proprietary large language models, published in 2023</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="AKUL ARORA">
      <data key="d0">PERSON</data>
      <data key="d1">Akul Arora is an author of the paper "Measuring mathematical problem solving with the math dataset"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ERIC TANG">
      <data key="d0">PERSON</data>
      <data key="d1">Eric Tang is an author of the paper "Measuring mathematical problem solving with the math dataset"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="MEASURING MATHEMATICAL PROBLEM SOLVING WITH THE MATH DATASET">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the measurement of mathematical problem solving using the math dataset, published in 2021</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HAMISH IVISON">
      <data key="d0">PERSON</data>
      <data key="d1">Hamish Ivison is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="VALENTINA PYATKIN">
      <data key="d0">PERSON</data>
      <data key="d1">Valentina Pyatkin is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="NATHAN LAMBERT">
      <data key="d0">PERSON</data>
      <data key="d1">Nathan Lambert is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="MATTHEW PETERS">
      <data key="d0">PERSON</data>
      <data key="d1">Matthew Peters is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="NOAH A. SMITH">
      <data key="d0">PERSON</data>
      <data key="d1">Noah A. Smith is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="IZ BELTAGY">
      <data key="d0">PERSON</data>
      <data key="d1">Iz Beltagy is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HANNANEH HAJISHIRZI">
      <data key="d0">PERSON</data>
      <data key="d1">Hannaneh Hajishirzi is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CAMELS IN A CHANGING CLIMATE: ENHANCING LM ADAPTATION WITH TULU 2">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the enhancement of language model adaptation with Tulu 2, published in 2023</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ALBERT Q. JIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Albert Q. Jiang is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ALEXANDRE SABLAYROLLES">
      <data key="d0">PERSON</data>
      <data key="d1">Alexandre Sablayrolles is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ARTHUR MENSCH">
      <data key="d0">PERSON</data>
      <data key="d1">Arthur Mensch is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CHRIS BAMFORD">
      <data key="d0">PERSON</data>
      <data key="d1">Chris Bamford is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="DEVENDRA SINGH CHAPLOT">
      <data key="d0">PERSON</data>
      <data key="d1">Devendra Singh Chaplot is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="DIEGO DE LAS CASAS">
      <data key="d0">PERSON</data>
      <data key="d1">Diego de las Casas is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="FLORIAN BRESSAND">
      <data key="d0">PERSON</data>
      <data key="d1">Florian Bressand is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="GIANNA LENGYEL">
      <data key="d0">PERSON</data>
      <data key="d1">Gianna Lengyel is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="GUILLAUME LAMPLE">
      <data key="d0">PERSON</data>
      <data key="d1">Guillaume Lample is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="LUCILE SAULNIER">
      <data key="d0">PERSON</data>
      <data key="d1">Lucile Saulnier is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="L&#201;LIO RENARD LAVAUD">
      <data key="d0">PERSON</data>
      <data key="d1">L&#233;lio Renard Lavaud is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="PIERRE STOCK">
      <data key="d0">PERSON</data>
      <data key="d1">Pierre Stock is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="TEVEN LE SAO">
      <data key="d0">PERSON</data>
      <data key="d1">Teven Le Sao is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="THOMAS WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Thomas Wang is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="TIMOTH&#201;E LACROIX">
      <data key="d0">PERSON</data>
      <data key="d1">Timoth&#233;e Lacroix is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="WILLIAM EL SAYED">
      <data key="d0">PERSON</data>
      <data key="d1">William El Sayed is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="MISTRAL 7B">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the Mistral 7b model, published in 2023</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HARRISON LEE">
      <data key="d0">PERSON</data>
      <data key="d1">Harrison Lee is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="SAMRAT PHATALE">
      <data key="d0">PERSON</data>
      <data key="d1">Samrat Phatale is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HASSAN MANSOOR">
      <data key="d0">PERSON</data>
      <data key="d1">Hassan Mansoor is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="THOMAS MESNARD">
      <data key="d0">PERSON</data>
      <data key="d1">Thomas Mesnard is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="JOHAN FERRET">
      <data key="d0">PERSON</data>
      <data key="d1">Johan Ferret is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="KELLIE LU">
      <data key="d0">PERSON</data>
      <data key="d1">Kellie Lu is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="COLTON BISHOP">
      <data key="d0">PERSON</data>
      <data key="d1">Colton Bishop is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ETHAN HALL">
      <data key="d0">PERSON</data>
      <data key="d1">Ethan Hall is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="VICTOR CARBUNE">
      <data key="d0">PERSON</data>
      <data key="d1">Victor Carbune is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ABHINAV RASTOGI">
      <data key="d0">PERSON</data>
      <data key="d1">Abhinav Rastogi is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="SUSHANT PRAKASH">
      <data key="d0">PERSON</data>
      <data key="d1">Sushant Prakash is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="RLAIF: SCALING REINFORCEMENT LEARNING FROM HUMAN FEEDBACK WITH AI FEEDBACK">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the scaling of reinforcement learning from human feedback with AI feedback, published in 2023</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="GUOHAO LI">
      <data key="d0">PERSON</data>
      <data key="d1">Guohao Li is an author of the paper "Camel: Communicative agents for 'mind' exploration of large language model society"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HASAN ABED AL KADER HAMMOUD">
      <data key="d0">PERSON</data>
      <data key="d1">Hasan Abed Al Kader Hammoud is an author of the paper "Camel: Communicative agents for 'mind' exploration of large language model society"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HANI ITANI">
      <data key="d0">PERSON</data>
      <data key="d1">Hani Itani is an author of the paper "Camel: Communicative agents for 'mind' exploration of large language model society"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="BERNARD GHANEM">
      <data key="d0">PERSON</data>
      <data key="d1">Bernard Ghanem is an author of the paper "Camel: Communicative agents for 'mind' exploration of large language model society"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CAMEL: COMMUNICATIVE AGENTS FOR 'MIND' EXPLORATION OF LARGE LANGUAGE MODEL SOCIETY">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing communicative agents for exploring the 'mind' of large language model society, published in 2023</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="XUECHEN LI">
      <data key="d0">PERSON</data>
      <data key="d1">Xuechen Li is an author of the paper "Alpaca" and also contributed to the paper "Alpacaeval: An automatic evaluator of instruction-following models."</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="TIANYI ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Tianyi Zhang is an author of the paper "Alpaca" and also contributed to the paper "Alpacaeval: An automatic evaluator of instruction-following models."</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="YANN DUBOIS">
      <data key="d0">PERSON</data>
      <data key="d1">Yann Dubois is an author of the paper "Alpaca" and also contributed to the paper "Alpacaeval: An automatic evaluator of instruction-following models."</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ROHAN TAORI">
      <data key="d0">PERSON</data>
      <data key="d1">Rohan Taori is an author of the paper "Alpaca" and also contributed to the paper "Alpacaeval: An automatic evaluator of instruction-following models." His work focuses on advancing the evaluation and development of instruction-following models within the field of Artificial Intelligence and Machine Learning.</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ISHAAN GULRAJANI">
      <data key="d0">PERSON</data>
      <data key="d1">Ishaan Gulrajani is an author of the paper "Alpaca" and also contributed to the paper "Alpacaeval: An automatic evaluator of instruction-following models."</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CARLOS GUESTRIN">
      <data key="d0">PERSON</data>
      <data key="d1">Carlos Guestrin is an author of the paper "Alpaca" and also contributed to the paper "Alpacaeval: An automatic evaluator of instruction-following models."</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="TATSUNORI B. HASHIMOTO">
      <data key="d0">PERSON</data>
      <data key="d1">Tatsunori B. Hashimoto is an author of the paper "Alpaca" and also contributed to the paper "Alpacaeval: An automatic evaluator of instruction-following models."</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ALPACA">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the Alpaca model, published in 2023</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="RII KHIZBULLIN">
      <data key="d0">PERSON</data>
      <data key="d1">Rii Khizbullin is an author of the paper "Camel: Communicative agents for 'mind' exploration of large language model society"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="CAMEL">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">Camel: Communicative agents for 'mind' exploration of large language model society is a paper published in 2023</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="YIXIN LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Yixin Liu is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ALEXANDER R. FABBRI">
      <data key="d0">PERSON</data>
      <data key="d1">Alexander R. Fabbri is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="JIAWEN CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Jiawen Chen is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="YILUN ZHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Yilun Zhao is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SIMENG HAN">
      <data key="d0">PERSON</data>
      <data key="d1">Simeng Han is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SHAFIQ JOTY">
      <data key="d0">PERSON</data>
      <data key="d1">Shafiq Joty is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="DRAGOMIR RADEV">
      <data key="d0">PERSON</data>
      <data key="d1">Dragomir Radev is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="CHIEN-SHENG WU">
      <data key="d0">PERSON</data>
      <data key="d1">Chien-Sheng Wu is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ARMAN COHAN">
      <data key="d0">PERSON</data>
      <data key="d1">Arman Cohan is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="BENCHMARKING GENERATION AND EVALUATION CAPABILITIES OF LARGE LANGUAGE MODELS FOR INSTRUCTION CONTROLLABLE SUMMARIZATION">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">A paper published in 2023 that benchmarks the generation and evaluation capabilities of large language models for instruction controllable summarization</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="LM-SYS">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Lm-sys is the organization behind the Mt-Bench project</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="DANIEL VAN STRIEN">
      <data key="d0">PERSON</data>
      <data key="d1">Daniel van Strien is an author of the paper "Cosmopedia: how to create large-scale synthetic data for pre-training"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="LOUBNA BEN ALLAL">
      <data key="d0">PERSON</data>
      <data key="d1">Loubna Ben Allal is an author of the paper "Cosmopedia: how to create large-scale synthetic data for pre-training"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ANTON LOZHKOV">
      <data key="d0">PERSON</data>
      <data key="d1">Anton Lozhkov is an author of the paper "Cosmopedia: how to create large-scale synthetic data for pre-training"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="COSMOPEDIA">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">Cosmopedia: how to create large-scale synthetic data for pre-training is a paper published in 2024</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="CLARISSE SIMOES">
      <data key="d0">PERSON</data>
      <data key="d1">Clarisse Simoes is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SAHAJ AGARWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Sahaj Agarwal is an author of the paper "Orca: Progressive learning from complex explanation traces of GPT-4"Sahaj Agarwal is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XUXI CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Xuxi Chen is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ANASTASIA RAZDAIBIEDINA">
      <data key="d0">PERSON</data>
      <data key="d1">Anastasia Razdaibiedina is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ERIK JONES">
      <data key="d0">PERSON</data>
      <data key="d1">Erik Jones is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="KRITI AGGARWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Kriti Aggarwal is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="HAMID PALANGI">
      <data key="d0">PERSON</data>
      <data key="d1">Hamid Palangi is an author of the paper "Orca: Progressive learning from complex explanation traces of GPT-4"Hamid Palangi is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ORCA 2">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">Orca 2: Teaching small language models how to reason is a paper published in 2023</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SUBHABRATA MUKHERJEE">
      <data key="d0">PERSON</data>
      <data key="d1">Subhabrata Mukherjee is an author of the paper "Orca: Progressive learning from complex explanation traces of GPT-4"Subhabrata Mukherjee is an author of the paper "Xtremedistil: Multi-stage distillation for massive multilingual models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XTREMEDISTIL">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">Xtremedistil: Multi-stage distillation for massive multilingual models is a paper published in 2020</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="GANESH JAWAHAR">
      <data key="d0">PERSON</data>
      <data key="d1">Ganesh Jawahar is an author of the paper "Orca: Progressive learning from complex explanation traces of GPT-4"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ORCA">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">Orca: Progressive learning from complex explanation traces of GPT-4 is a paper published in 2023</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SAMUEL J. PAECH">
      <data key="d0">PERSON</data>
      <data key="d1">Samuel J. Paech is an author of the paper "EQ-Bench: An emotional intelligence benchmark for large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="EQ-BENCH">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">EQ-Bench: An emotional intelligence benchmark for large language models is a paper published in 2024</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="BAOLIN PENG">
      <data key="d0">PERSON</data>
      <data key="d1">Baolin Peng is an author of the paper "Instruction tuning with GPT-4"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="CHUNYUAN LI">
      <data key="d0">PERSON</data>
      <data key="d1">Chunyuan Li is an author of the paper "Instruction tuning with GPT-4"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="PENGCHENG HE">
      <data key="d0">PERSON</data>
      <data key="d1">Pengcheng He is an author of the paper "Instruction tuning with GPT-4"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="MICHEL GALLEY">
      <data key="d0">PERSON</data>
      <data key="d1">Michel Galley is an author of the paper "Instruction tuning with GPT-4"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="INSTRUCTION TUNING WITH GPT-4">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">Instruction tuning with GPT-4 is a paper published in 2023</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="YIWEI QIN">
      <data key="d0">PERSON</data>
      <data key="d1">Yiwei Qin is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="KAIQIANG SONG">
      <data key="d0">PERSON</data>
      <data key="d1">Kaiqiang Song is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="YEBO WEN HU">
      <data key="d0">PERSON</data>
      <data key="d1">Yebo Wen Hu is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="WENLIN YAO">
      <data key="d0">PERSON</data>
      <data key="d1">Wenlin Yao is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SANGWOO CHO">
      <data key="d0">PERSON</data>
      <data key="d1">Sangwoo Cho is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="XIAOYANG WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Xiaoyang Wang is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="XUANSHENG WU">
      <data key="d0">PERSON</data>
      <data key="d1">Xuansheng Wu is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="TOOLLLM">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">ToolLLM: Facilitating large language models to master 16000+ real-world APIs is a paper published in 2023</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="MOHAMMED LATIF SIDDIQ">
      <data key="d0">PERSON</data>
      <data key="d1">Mohammed Latif Siddiq is an author known for his contributions to the field of Artificial Intelligence and Machine Learning. He has co-authored the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks," which focuses on the evaluation of generated regular expressions and their susceptibility to denial-of-service attacks. Additionally, he has contributed to the paper "The curse of recursion: Training on generated data makes models forget," which explores the challenges associated with training models on generated data and the resulting issues of model forgetfulness. Through these works, Mohammed Latif Siddiq has addressed critical aspects of AI and ML, particularly in the areas of security and model training.</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JIAHAO ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jiahao Zhang is an author of multiple research papers in the field of Artificial Intelligence and Machine Learning. Notably, he has contributed to the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks," which focuses on the evaluation of generated regular expressions and their susceptibility to denial-of-service attacks. Additionally, he has co-authored the paper "The curse of recursion: Training on generated data makes models forget," which explores the challenges associated with training models on generated data and the resulting issues of model forgetfulness.</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="LINDSAY RONEY">
      <data key="d0">PERSON</data>
      <data key="d1">Lindsay Roney is an author known for contributing to the field of Artificial Intelligence and Machine Learning through her research papers. She has co-authored the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks," which focuses on the evaluation of generated regular expressions and their susceptibility to denial-of-service attacks. Additionally, she has co-authored the paper "The curse of recursion: Training on generated data makes models forget," which explores the challenges associated with training models on generated data and the resulting impact on model performance and memory. Lindsay Roney's work addresses critical issues in AI and ML, particularly in the areas of model evaluation and training data quality.</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JOANNA C. S.">
      <data key="d0">PERSON</data>
      <data key="d1">Joanna C. S. is an author of the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="THE CURSE OF RECURSION">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">The curse of recursion: Training on generated data makes models forget is a paper published in 2024</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ILIA SHUMAILOV">
      <data key="d0">PERSON</data>
      <data key="d1">Ilia Shumailov is an author of the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ZAKHAR SHUMAYLOV">
      <data key="d0">PERSON</data>
      <data key="d1">Zakhar Shumaylov is an author of the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YIREN ZHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Yiren Zhao is an author of the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="NICOLAS PAPERNOT">
      <data key="d0">PERSON</data>
      <data key="d1">Nicolas Papernot is an author of the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ROSS ANDERSON">
      <data key="d0">PERSON</data>
      <data key="d1">Ross Anderson is an author of the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="CHING-AN CHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Ching-An Cheng is an author of the paper "Direct Nash optimization: Teaching language models to self-improve with general preferences"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="TENGYANG XIE">
      <data key="d0">PERSON</data>
      <data key="d1">Tengyang Xie is an author of the paper "Direct Nash optimization: Teaching language models to self-improve with general preferences"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="DIRECT NASH OPTIMIZATION">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">Direct Nash optimization: Teaching language models to self-improve with general preferences is a paper published in 2024</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="JOANNA C. S. SANTOS">
      <data key="d0">PERSON</data>
      <data key="d1">Joanna C. S. Santos is an author of the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="NATHANAEL SCH&#196;RLI">
      <data key="d0">PERSON</data>
      <data key="d1">Nathanael Sch&#228;rli is an author of the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="WEN WAI YIM">
      <data key="d0">PERSON</data>
      <data key="d1">Wen wai Yim is an author of the paper "Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YUJUAN FU">
      <data key="d0">PERSON</data>
      <data key="d1">Yujuan Fu is an author of the paper "Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ASMA BEN ABACHA">
      <data key="d0">PERSON</data>
      <data key="d1">Asma Ben Abacha is an author of the paper "Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="NEAL SNIDER">
      <data key="d0">PERSON</data>
      <data key="d1">Neal Snider is an author of the paper "Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="THOMAS LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Thomas Lin is an author of the paper "Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="MELIHA YETISGEN">
      <data key="d0">PERSON</data>
      <data key="d1">Meliha Yetisgen is an author of the paper "Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="AHMED HASSAN AWADALLAH">
      <data key="d0">PERSON</data>
      <data key="d1">Ahmed Hassan Awadallah is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="RYEN W WHITE">
      <data key="d0">PERSON</data>
      <data key="d1">Ryen W White is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="DOUG BURGER">
      <data key="d0">PERSON</data>
      <data key="d1">Doug Burger is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="CONGYING XIA">
      <data key="d0">PERSON</data>
      <data key="d1">Congying Xia is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="CHEN XING">
      <data key="d0">PERSON</data>
      <data key="d1">Chen Xing is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JIANGSHU DU">
      <data key="d0">PERSON</data>
      <data key="d1">Jiangshu Du is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="XINYI YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Xinyi Yang is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YIHAO FENG">
      <data key="d0">PERSON</data>
      <data key="d1">Yihao Feng is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="RAN XU">
      <data key="d0">PERSON</data>
      <data key="d1">Ran Xu is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="WENPENG YIN">
      <data key="d0">PERSON</data>
      <data key="d1">Wenpeng Yin is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="CAIMING XIONG">
      <data key="d0">PERSON</data>
      <data key="d1">Caiming Xiong is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="GUANGZHI XIONG">
      <data key="d0">PERSON</data>
      <data key="d1">Guangzhi Xiong is an author of the paper "Benchmarking retrieval-augmented generation for medicine"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="QIAO JIN">
      <data key="d0">PERSON</data>
      <data key="d1">Qiao Jin is an author of the paper "Benchmarking retrieval-augmented generation for medicine"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ZHIYONG LU">
      <data key="d0">PERSON</data>
      <data key="d1">Zhiyong Lu is an author of the paper "Benchmarking retrieval-augmented generation for medicine"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="AIDONG ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Aidong Zhang is an author of the paper "Benchmarking retrieval-augmented generation for medicine"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="QINGFENG SUN">
      <data key="d0">PERSON</data>
      <data key="d1">Qingfeng Sun is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="KAI ZHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Kai Zheng is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="XIUBO GENG">
      <data key="d0">PERSON</data>
      <data key="d1">Xiubo Geng is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="PU ZHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Pu Zhao is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JIAZHAN FENG">
      <data key="d0">PERSON</data>
      <data key="d1">Jiazhan Feng is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="CHONGYANG TAO">
      <data key="d0">PERSON</data>
      <data key="d1">Chongyang Tao is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="DAXIN JIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Daxin Jiang is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="LONGHUI YU">
      <data key="d0">PERSON</data>
      <data key="d1">Longhui Yu is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="WEISEN JIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Weisen Jiang is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="HAN SHI">
      <data key="d0">PERSON</data>
      <data key="d1">Han Shi is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JINCHENG YU">
      <data key="d0">PERSON</data>
      <data key="d1">Jincheng Yu is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ZHENGYING LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Zhengying Liu is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YU ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yu Zhang is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JAMES T KWOK">
      <data key="d0">PERSON</data>
      <data key="d1">James T Kwok is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ZHENGUO LI">
      <data key="d0">PERSON</data>
      <data key="d1">Zhenguo Li is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ADRIAN WELLER">
      <data key="d0">PERSON</data>
      <data key="d1">Adrian Weller is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="WEIYANG LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Weiyang Liu is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YIFAN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yifan Zhang is an author of the paper "Automathtext: Autonomous data selection with language models for mathematical texts"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YIFAN LUO">
      <data key="d0">PERSON</data>
      <data key="d1">Yifan Luo is an author of the paper "Automathtext: Autonomous data selection with language models for mathematical texts"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YANG YUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Yang Yuan is an author of the paper "Automathtext: Autonomous data selection with language models for mathematical texts"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ANDREW CHI-CHIH YAO">
      <data key="d0">PERSON</data>
      <data key="d1">Andrew Chi-Chih Yao is an author of the paper "Automathtext: Autonomous data selection with language models for mathematical texts"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="WANJUN ZHONG">
      <data key="d0">PERSON</data>
      <data key="d1">Wanjun Zhong is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="RUIXIANG CUI">
      <data key="d0">PERSON</data>
      <data key="d1">Ruixiang Cui is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YIDUO GUO">
      <data key="d0">PERSON</data>
      <data key="d1">Yiduo Guo is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YAOBO LIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yaobo Liang is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="SHUAI LU">
      <data key="d0">PERSON</data>
      <data key="d1">Shuai Lu is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YANLIN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yanlin Wang is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="NAN DUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Nan Duan is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JEFFREY ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Jeffrey Zhou is an author of the paper "Instruction-following evaluation for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="TIANJIAN LU">
      <data key="d0">PERSON</data>
      <data key="d1">Tianjian Lu is an author of the paper "Instruction-following evaluation for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="SIDDHARTHA BRAHMA">
      <data key="d0">PERSON</data>
      <data key="d1">Siddhartha Brahma is an author of the paper "Instruction-following evaluation for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="SUJOY BASU">
      <data key="d0">PERSON</data>
      <data key="d1">Sujoy Basu is an author of the paper "Instruction-following evaluation for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YI LUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Yi Luan is an author of the paper "Instruction-following evaluation for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="DEBATE PASSAGE GENERATOR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An agent that specializes in crafting passages that mimic the structure and content of debate transcripts</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="CONVERSATION PASSAGE GENERATOR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An agent that generates passages that depict dialogues</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="MEETING TRANSCRIPT GENERATOR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An agent designed to produce meeting transcripts</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="POEM GENERATOR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An agent that generates poems</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="SATIRICAL PASSAGE GENERATOR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An agent that creates texts infused with satirical wit</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="INSTRUCTIONAL PASSAGE GENERATOR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An agent that generates passages resembling instructional manuals</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="LONG TEXT GENERATOR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An agent that extends the original text by incorporating additional information, thereby increasing its length</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="IDENTITY AGENT">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A straightforward agent that replicates the input text verbatim</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="LITERAL COMPREHENSION QUESTION">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A question that asks for a specific detail(s) or fact(s) clearly stated in the text</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="NUMERICAL DISCRETE REASONING">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">Questions that require the reader to use numerical reasoning over many facts from the text</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="CRITICAL COMPREHENSION QUESTION">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">Construct two statements about the purpose or point of view that the reader must assess as true or false, with one being true and the other false</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="EVALUATIVE COMPREHENSION QUESTION">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An "EVALUATIVE COMPREHENSION QUESTION" is a type of question that requires an essay response to evaluate comprehension. It is an open-ended question that prompts an in-depth analysis of the text&#8217;s theme or the effectiveness of an argument.</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="VOCABULARY AND LANGUAGE USE">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">A fill-in-the-blank question that tests understanding of a particular word or phrase used in the text</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="RELATIONSHIP COMPREHENSION QUESTION">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">A matching question where respondents pair items based on a specific criterion</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="SEQUENCING EVENTS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">A series of events from the text arranged in the correct chronological order</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="STRENGTHEN">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Identify information that would make the argument&#8217;s conclusion more likely to be true</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="WEAKEN">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Find evidence or an argument that would make the conclusion less likely to be true</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="ASSUMPTION">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Determine what must be true for the argument to hold</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="FLAW">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Point out a mistake in the argument&#8217;s reasoning</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="INFERENCE">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Choose an option that logically follows from the information provided</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="PRINCIPLE">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Recognize the general rule or principle that underlies the argument</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="METHOD OF REASONING">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Describe how the argument is constructed logically</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="RESOLVE THE PARADOX">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Offer an explanation that reconciles seemingly contradictory information</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT SIMPLIFICATION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Making text easier to read and understand by using simpler words and sentence structures, often for children or language learners</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT EXPANSION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Adding more information or detail to make text more comprehensive or to meet a certain word count</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT TRANSLATION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Converting text from one language to another while attempting to preserve the original meaning as closely as possible</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT FORMATTING">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Altering the appearance of text to improve readability or for stylistic purposes</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="SENTIMENT MODIFICATION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Changing the tone of the text to alter its emotional impact, such as making a sentence sound more positive or negative</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT ANNOTATION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Adding notes, comments, or explanations to a text, often for the purpose of analysis or to provide additional context</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="KEYWORD REPLACEMENT">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Substituting specific words or phrases with synonyms or related terms</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT REMOVING">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Redacting or removing content from text</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT CAPITALIZATION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Adjusting the case of letters in text, such as converting to uppercase, lowercase, title case, or sentence case, starting every sentence with a particular letter, word</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT STYLING">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Applying styles like bold, italics, underline, etc., to emphasize certain parts of the text or for aesthetic purposes</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="CONTENT REWRITING">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Extensively modifying a text to produce a new version, which could involve changing the perspective, style, or target audience</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="DATA NORMALIZATION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Standardizing text to ensure consistency, such as converting dates and times to a standard format or unifying the spelling of words</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="PLAGIARISM REWORDING">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Altering text to avoid plagiarism, ensuring that the content is original</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT OBFUSCATION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Intentionally making text vague or harder to understand, sometimes for security purposes like masking personal data</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXTUAL ENTAILMENT">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Modifying a sentence or phrase to either entail or contradict another sentence, often used in natural language processing tasks</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="REWRITING WITH VOCABULARY LIMITATIONS">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Rewriting the entire text or a piece of it while using a limited vocabulary, such as all words starting with a specific letter</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="EVALUATOR ASSISTANT">
      <data key="d0">ROLE</data>
      <data key="d1">An unbiased evaluator that parses student responses and returns the alphabet ID of the answer selected by the student</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="SPECIFIC DETAIL(S) OR FACT(S)">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Questions that require the reader to identify specific details or facts clearly stated in the text</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT MODIFICATION FLOW">
      <data key="d0">PROCESS</data>
      <data key="d1">A flow that includes various methods for modifying text, such as paraphrasing, text simplification, and text expansion</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="INSTRUCTION TAXONOMY">
      <data key="d0">PROCESS</data>
      <data key="d1">A classification system used for generating seed instructions, including methods like paraphrasing and text simplification</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="EVALUATION DETAILS">
      <data key="d0">PROCESS</data>
      <data key="d1">Details about the types of tasks/benchmarks and the methods used to extract answers and generate metrics</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="OPEN-ENDED GENERATION SETTING">
      <data key="d0">EVALUATION METHOD</data>
      <data key="d1">A setting in which models are evaluated by generating open-ended responses</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="REGEX BASED EXTRACTION">
      <data key="d0">EVALUATION METHOD</data>
      <data key="d1">A method previously used for extracting options selected by models in multiple choice questions</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="STUDENT RESPONSE">
      <data key="d0">EVALUATION METHOD</data>
      <data key="d1">STUDENT RESPONSE refers to the answer or response provided by the student to a given question. This response is parsed to extract the selected answer, allowing for further analysis and evaluation.</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50,5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="ANSWER OPTIONS">
      <data key="d0">EVALUATION METHOD</data>
      <data key="d1">The set of possible answers provided to the student for a multiple choice question</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="PARSED STUDENT ANSWER">
      <data key="d0">EVALUATION METHOD</data>
      <data key="d1">The "PARSED STUDENT ANSWER" refers to the final answer extracted from the student's response. This answer is represented by the alphabet corresponding to the chosen option or the alphabet ID of the selected option.</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50,5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="EXACT MATCH/SPAN EXTRACTION PROBLEMS">
      <data key="d0">TASK TYPE</data>
      <data key="d1">Tasks involving math-based questions or problems where a ground-truth answer value is given, requiring the model to generate and match the answer with the provided ground-truth</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="MATHS GPT-4 EXTRACTION SYSTEM MESSAGE">
      <data key="d0">SYSTEM MESSAGE</data>
      <data key="d1">A specific system message used for evaluating student answers to math word problems, ensuring the final answer matches the problem setter's answer</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="GENERAL EXTRACTION SYSTEM MESSAGE">
      <data key="d0">SYSTEM MESSAGE</data>
      <data key="d1">A system message used for parsing student responses and matching them with the correct answer in exact match/span extraction problems</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="EQBENCH GPT-4 EXTRACTION SYSTEM MESSAGE">
      <data key="d0">SYSTEM MESSAGE</data>
      <data key="d1">A system message used for extracting emotion scores from evaluated model responses in EQBench tasks</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="OPTIONS">
      <data key="d0">TASK TYPE</data>
      <data key="d1">The set of possible answers provided for the student to choose from</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="FINAL ANSWER">
      <data key="d0">OUTPUT FORMAT</data>
      <data key="d1">The correct answer to the question, typically provided by the problem setter</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="ERROR ANALYSIS">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of comparing the student's answer with the correct answer to determine if they match</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="FINAL VERDICT">
      <data key="d0">OUTPUT FORMAT</data>
      <data key="d1">The result of the error analysis, indicating whether the student's answer is 'Correct' or 'Incorrect'</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="EMOTION SCORES">
      <data key="d0">OUTPUT FORMAT</data>
      <data key="d1">Scores assigned to different emotions based on the student's response in EQBench tasks</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="CRITIQUE">
      <data key="d0">PROCESS</data>
      <data key="d1">A step-by-step analysis of the student's response, used to revise emotion scores in EQBench tasks</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="ALEX">
      <data key="d0">PERSON</data>
      <data key="d1">Alex is a person who is already in a relationship and has been confessed to by Elliot, putting Alex in an awkward position.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="RESIGNED">
      <data key="d0">EMOTION</data>
      <data key="d1">Resigned is an emotion felt by Elliot, scored at 7, indicating a strong sense of acceptance of the situation.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="ANGRY">
      <data key="d0">EMOTION</data>
      <data key="d1">Angry is an emotion felt by Elliot, scored at 3, indicating a mild sense of frustration or self-directed anger.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="HOPEFUL">
      <data key="d0">EMOTION</data>
      <data key="d1">Hopeful is an emotion felt by Elliot, scored at 5, indicating a moderate sense of optimism that Alex might reciprocate his feelings.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="EMBARRASSED">
      <data key="d0">EMOTION</data>
      <data key="d1">Embarrassed is an emotion felt by Elliot, scored at 8, indicating a strong sense of discomfort for putting Alex in an awkward position.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="OPEN-ENDED GENERATION">
      <data key="d0">TASK</data>
      <data key="d1">Open-Ended Generation tasks involve prompting a model to generate an answer to an open-ended question without a ground-truth answer for comparison.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="QUALITY JUDGE">
      <data key="d0">PROCESS</data>
      <data key="d1">Quality Judge is a process where a judge evaluates the quality of a response provided by an AI assistant, assessing criteria like instruction adherence, content grounding, and overall quality.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="ELLIOT">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="REVISED SCORES">
      <data key="d0">DATA</data>
      <data key="d1">Revised scores are the numerical values assigned to each of Elliot's emotions: Resigned (7), Angry (3), Hopeful (5), and Embarrassed (8).</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="PROMPT TEMPLATE">
      <data key="d0">TOOL</data>
      <data key="d1">Prompt template is a predefined format used for evaluating hallucination detection and summarization quality in AI-generated responses.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="TEXT SUMMARIZATION">
      <data key="d0">TASK</data>
      <data key="d1">Text Summarization is the task of generating a concise and accurate summary of a given text, evaluated for quality and hallucination.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="HALLUCINATION DETECTION">
      <data key="d0">PROCESS</data>
      <data key="d1">Hallucination Detection is the process of identifying and evaluating hallucinated content in AI-generated summaries.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="SUMMARIZATION QUALITY">
      <data key="d0">PROCESS</data>
      <data key="d1">Summarization Quality is the process of evaluating the quality of AI-generated summaries based on criteria like instruction adherence, content grounding, and overall quality.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="EMOTIONS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <edge source="DARREN EDGE" target="HA TRINH">
      <data key="d4">16.0</data>
      <data key="d5">Darren Edge and Ha Trinh co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="DARREN EDGE" target="NEWMAN CHENG">
      <data key="d4">16.0</data>
      <data key="d5">Darren Edge and Newman Cheng co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="DARREN EDGE" target="JOSHUA BRADLEY">
      <data key="d4">16.0</data>
      <data key="d5">Darren Edge and Joshua Bradley co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="DARREN EDGE" target="ALEX CHAO">
      <data key="d4">16.0</data>
      <data key="d5">Darren Edge and Alex Chao co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="DARREN EDGE" target="APURVA MODY">
      <data key="d4">16.0</data>
      <data key="d5">Darren Edge and Apurva Mody co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="DARREN EDGE" target="STEVEN TRUITT">
      <data key="d4">16.0</data>
      <data key="d5">Darren Edge and Steven Truitt co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="DARREN EDGE" target="JONATHAN LARSON">
      <data key="d4">16.0</data>
      <data key="d5">Darren Edge and Jonathan Larson co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="HA TRINH" target="NEWMAN CHENG">
      <data key="d4">16.0</data>
      <data key="d5">Ha Trinh and Newman Cheng co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="HA TRINH" target="JOSHUA BRADLEY">
      <data key="d4">16.0</data>
      <data key="d5">Ha Trinh and Joshua Bradley co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="HA TRINH" target="ALEX CHAO">
      <data key="d4">16.0</data>
      <data key="d5">Ha Trinh and Alex Chao co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="HA TRINH" target="APURVA MODY">
      <data key="d4">16.0</data>
      <data key="d5">Ha Trinh and Apurva Mody co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="HA TRINH" target="STEVEN TRUITT">
      <data key="d4">16.0</data>
      <data key="d5">Ha Trinh and Steven Truitt co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="HA TRINH" target="JONATHAN LARSON">
      <data key="d4">16.0</data>
      <data key="d5">Ha Trinh and Jonathan Larson co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="NEWMAN CHENG" target="JOSHUA BRADLEY">
      <data key="d4">16.0</data>
      <data key="d5">Newman Cheng and Joshua Bradley co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="NEWMAN CHENG" target="ALEX CHAO">
      <data key="d4">16.0</data>
      <data key="d5">Newman Cheng and Alex Chao co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="NEWMAN CHENG" target="APURVA MODY">
      <data key="d4">16.0</data>
      <data key="d5">Newman Cheng and Apurva Mody co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="NEWMAN CHENG" target="STEVEN TRUITT">
      <data key="d4">16.0</data>
      <data key="d5">Newman Cheng and Steven Truitt co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="NEWMAN CHENG" target="JONATHAN LARSON">
      <data key="d4">16.0</data>
      <data key="d5">Newman Cheng and Jonathan Larson co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="JOSHUA BRADLEY" target="ALEX CHAO">
      <data key="d4">16.0</data>
      <data key="d5">Joshua Bradley and Alex Chao co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="JOSHUA BRADLEY" target="APURVA MODY">
      <data key="d4">16.0</data>
      <data key="d5">Joshua Bradley and Apurva Mody co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="JOSHUA BRADLEY" target="STEVEN TRUITT">
      <data key="d4">16.0</data>
      <data key="d5">Joshua Bradley and Steven Truitt co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="JOSHUA BRADLEY" target="JONATHAN LARSON">
      <data key="d4">16.0</data>
      <data key="d5">Joshua Bradley and Jonathan Larson co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="ALEX CHAO" target="APURVA MODY">
      <data key="d4">16.0</data>
      <data key="d5">Alex Chao and Apurva Mody co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="ALEX CHAO" target="STEVEN TRUITT">
      <data key="d4">16.0</data>
      <data key="d5">Alex Chao and Steven Truitt co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="ALEX CHAO" target="JONATHAN LARSON">
      <data key="d4">16.0</data>
      <data key="d5">Alex Chao and Jonathan Larson co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="APURVA MODY" target="STEVEN TRUITT">
      <data key="d4">16.0</data>
      <data key="d5">Apurva Mody and Steven Truitt co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="APURVA MODY" target="JONATHAN LARSON">
      <data key="d4">16.0</data>
      <data key="d5">Apurva Mody and Jonathan Larson co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="STEVEN TRUITT" target="JONATHAN LARSON">
      <data key="d4">16.0</data>
      <data key="d5">Steven Truitt and Jonathan Larson co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES">
      <data key="d4">14.0</data>
      <data key="d5">Microsoft Research and Microsoft Strategic Missions and Technologies are both part of Microsoft and collaborated on the paper</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="MICROSOFT OFFICE OF THE CTO">
      <data key="d4">14.0</data>
      <data key="d5">Microsoft Research and Microsoft Office of the CTO are both part of Microsoft and collaborated on the paper</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="ARINDAM MITRA">
      <data key="d4">8.0</data>
      <data key="d5">Arindam Mitra is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="LUCIANO DEL CORRO">
      <data key="d4">8.0</data>
      <data key="d5">Luciano Del Corro is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="GUOQING ZHENG">
      <data key="d4">8.0</data>
      <data key="d5">Guoqing Zheng is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="SHWETI MAHAJAN">
      <data key="d4">8.0</data>
      <data key="d5">Shweti Mahajan is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="DANY ROUHANA">
      <data key="d4">8.0</data>
      <data key="d5">Dany Rouhana is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="ANDRES CODAS">
      <data key="d4">8.0</data>
      <data key="d5">Andres Codas is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="YADONG LU">
      <data key="d4">8.0</data>
      <data key="d5">Yadong Lu is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="WEI-GE CHEN">
      <data key="d4">8.0</data>
      <data key="d5">Wei-ge Chen is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="OLGA VROUSGOS">
      <data key="d4">8.0</data>
      <data key="d5">Olga Vrousgos is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="CORBY ROSSET">
      <data key="d4">8.0</data>
      <data key="d5">Corby Rosset is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="FILLIPE SILVA">
      <data key="d4">8.0</data>
      <data key="d5">Fillipe Silva is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="HAMED KHANPOUR">
      <data key="d4">8.0</data>
      <data key="d5">Hamed Khanpour is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="YASH LARA">
      <data key="d4">8.0</data>
      <data key="d5">Yash Lara is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="AHMED AWADALLAH">
      <data key="d4">1.0</data>
      <data key="d5">Ahmed Awadallah is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES" target="MICROSOFT OFFICE OF THE CTO">
      <data key="d4">2.0</data>
      <data key="d5">Microsoft Strategic Missions and Technologies and Microsoft Office of the CTO are both part of Microsoft and collaborated on the paper</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="GRAPH RAG" target="GLOBAL SUMMARIZATION">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG is an approach based on global summarization of an LLM-derived knowledge graph</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GRAPH RAG" target="KNOWLEDGE GRAPH">
      <data key="d4">17.0</data>
      <data key="d5">Graph RAG is an approach that utilizes a Knowledge Graph to enhance its capabilities. Specifically, Graph RAG can create and reason over Knowledge Graphs, leveraging their structured information to improve understanding and decision-making processes. This integration allows Graph RAG to effectively manage and interpret complex data relationships, making it a powerful tool in the realm of Artificial Intelligence and Machine Learning.</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7,edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="COMMUNITY DESCRIPTIONS">
      <data key="d4">8.0</data>
      <data key="d5">Community Descriptions provide complete coverage of the underlying graph index</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GRAPH RAG" target="SOURCE TEXTS">
      <data key="d4">16.0</data>
      <data key="d5">GRAPH RAG is an approach that utilizes Source Texts for summarization. Source Texts are the original documents employed in the Graph RAG method to generate concise summaries. This technique leverages the foundational information contained within these Source Texts to produce accurate and coherent summaries, highlighting the importance of the original documents in the summarization process.</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="GRAPH RAG PIPELINE">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG Pipeline is the implementation of the Graph RAG approach</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GRAPH RAG" target="LLM PROMPTS">
      <data key="d4">8.0</data>
      <data key="d5">LLM Prompts are used to extract elements of a graph index from text chunks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GRAPH RAG" target="DATASET">
      <data key="d4">15.0</data>
      <data key="d5">Graph RAG is an entity that utilizes datasets to answer user queries. The dataset refers to the collection of data specifically used for evaluating Graph RAG's performance. By leveraging dataset summaries, Graph RAG can efficiently provide accurate responses to user inquiries.</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582,ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="CONTEXT WINDOW">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG uses a context window to generate answers</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="GRAPH RAG" target="PUBLIC FIGURES">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG identifies public figures in entertainment articles</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="GRAPH RAG" target="GRAPH COMMUNITIES">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG uses different levels of graph communities to answer user queries</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="GRAPH RAG" target="PUBLIC FIGURES IN CONTROVERSY">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG includes public figures involved in controversies</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="GRAPH RAG" target="TAYLOR SWIFT">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG mentions Taylor Swift</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="GRAPH RAG" target="TRAVIS KELCE">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG mentions Travis Kelce</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="GRAPH RAG" target="BRITNEY SPEARS">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG mentions Britney Spears</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="GRAPH RAG" target="JUSTIN TIMBERLAKE">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG mentions Justin Timberlake</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="GRAPH RAG" target="LLM">
      <data key="d4">24.0</data>
      <data key="d5">Graph RAG utilizes Large Language Models (LLMs) to process and generate text based on retrieved information. Additionally, the LLM assesses Graph RAG for its comprehensiveness, diversity, empowerment, and directness, ensuring that the information generated is robust and multifaceted. This symbiotic relationship highlights the importance of LLMs in enhancing the functionality and evaluative capabilities of Graph RAG.</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b,edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="ENTERTAINMENT ARTICLES">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG provides a comprehensive list of public figures mentioned in entertainment articles</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="GRAPH RAG" target="PODCAST DATASET">
      <data key="d4">8.0</data>
      <data key="d5">The Podcast dataset is used to evaluate the performance of the Graph RAG method</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="NEWS DATASET">
      <data key="d4">8.0</data>
      <data key="d5">The News dataset is used to evaluate the performance of the Graph RAG method</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="COMPREHENSIVENESS">
      <data key="d4">16.0</data>
      <data key="d5">Graph RAG achieves high comprehensiveness win rates, demonstrating its effectiveness and reliability in delivering comprehensive results. The high win rate in comprehensiveness underscores Graph RAG's capability to consistently perform at a superior level in this aspect.</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="DIVERSITY">
      <data key="d4">16.0</data>
      <data key="d5">Graph RAG is recognized for its exceptional performance in achieving high diversity win rates. The entity consistently demonstrates a high win rate in diversity, underscoring its effectiveness and influence in this area.</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="EMPOWERMENT">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG performs comparably on empowerment</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="COMMUNITY SUMMARIES">
      <data key="d4">23.0</data>
      <data key="d5">Graph RAG is a method that utilizes community summaries as a type of self-memory for generation-augmented retrieval. These community summaries are concise descriptions of root-level communities within the entity-based graph index employed by Graph RAG. The use of community summaries in Graph RAG enhances the retrieval process by providing structured and relevant information about the interconnected entities within the graph.</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8,edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="TS">
      <data key="d4">1.0</data>
      <data key="d5">TS represents global text summarization without a graph index in the Graph RAG method</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="NA&#207;VE RAG">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG is an advanced method that improves upon Na&#239;ve RAG by using a self-generated graph index</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="GENERATION-AUGMENTED RETRIEVAL (GAR)">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG incorporates concepts from generation-augmented retrieval (GAR)</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="MODULAR RAG">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG includes patterns from Modular RAG for iterative and dynamic cycles of interleaved retrieval and generation</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="SELF-MEMORY (SELFMEM)">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG uses the concept of self-memory for generation-augmented retrieval</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="ITERATIVE RETRIEVAL-GENERATION (ITER-RETGEN)">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG uses iterative retrieval-generation strategies</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="FEDERATED RETRIEVAL-GENERATION (FEB4RAG)">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG uses federated retrieval-generation strategies</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="MULTI-DOCUMENT SUMMARIZATION">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG can be used for multi-document summarization</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="MULTI-HOP QUESTION ANSWERING">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG can be used for multi-hop question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="HIERARCHICAL INDEX">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG uses a hierarchical index to organize text chunks</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="SCALABILITY">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG demonstrates scalability by reducing context token requirements</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="TUNING ELEMENT EXTRACTION PROMPTS">
      <data key="d4">7.0</data>
      <data key="d5">Tuning element extraction prompts can improve the retention of specific details in the Graph RAG index</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="RAPTOR">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's hierarchical index is similar to RAPTOR's method of clustering text chunk vectors</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="TREE OF CLARIFICATIONS">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's hierarchical approach is similar to generating a tree of clarifications</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="KAPING">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's use of a knowledge graph index is similar to KAPING</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="G-RETRIEVER">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's use of graph structures is similar to G-Retriever</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="GRAPH-TOOLFORMER">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's use of graph metrics is similar to Graph-ToolFormer</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="SURGE">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's narrative grounding is similar to SURGE</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="FABULA">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's use of narrative templates is similar to FABULA</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="NALLM">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's ability to create and reason over knowledge graphs is similar to NaLLM</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="GRAPHRAG">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's ability to create and reason over knowledge graphs is similar to GraphRAG</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="CAIRE-COVID">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's multi-document summarization is similar to CAiRE-COVID</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="ITRG">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's multi-hop question answering is similar to ITRG</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="IR-COT">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's multi-hop question answering is similar to IR-CoT</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="DSP">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's multi-hop question answering is similar to DSP</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="PODCAST INTERMEDIATE-LEVEL SUMMARIES">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG generates podcast intermediate-level summaries</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="NEWS LOW-LEVEL COMMUNITY SUMMARIES">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG generates news low-level community summaries</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="TABLE 3">
      <data key="d4">8.0</data>
      <data key="d5">Table 3 illustrates the scalability advantages of Graph RAG</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="ROOT-LEVEL COMMUNITY SUMMARIES">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG generates root-level community summaries</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="ITERATIVE QUESTION ANSWERING">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG is used for iterative question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="GRAPH INDEX">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG uses a graph index to partition data for global summarization</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="RAG">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG combines knowledge graph generation with Retrieval-Augmented Generation (RAG)</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="QFS">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG uses Query-Focused Summarization (QFS) to support human sensemaking</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="MAP-REDUCE">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG is compared to a graph-free approach using map-reduce for global summarization</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="SELF-CHECKGPT">
      <data key="d4">7.0</data>
      <data key="d5">SelfCheckGPT is suggested for comparing fabrication rates in the analysis of Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="ALONSO GUEVARA FERN&#193;NDEZ">
      <data key="d4">8.0</data>
      <data key="d5">Alonso Guevara Fern&#225;ndez contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="AMBER HOAK">
      <data key="d4">8.0</data>
      <data key="d5">Amber Hoak contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="ANDR&#201;S MORALES ESQUIVEL">
      <data key="d4">8.0</data>
      <data key="d5">Andr&#233;s Morales Esquivel contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="BEN CUTLER">
      <data key="d4">8.0</data>
      <data key="d5">Ben Cutler contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="BILLIE RINALDI">
      <data key="d4">8.0</data>
      <data key="d5">Billie Rinaldi contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="CHRIS SANCHEZ">
      <data key="d4">8.0</data>
      <data key="d5">Chris Sanchez contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="CHRIS TREVINO">
      <data key="d4">8.0</data>
      <data key="d5">Chris Trevino contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="CHRISTINE CAGGIANO">
      <data key="d4">8.0</data>
      <data key="d5">Christine Caggiano contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="DAVID TITTSWORTH">
      <data key="d4">8.0</data>
      <data key="d5">David Tittsworth contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="DAYENNE DE SOUZA">
      <data key="d4">8.0</data>
      <data key="d5">Dayenne de Souza contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="DOUGLAS ORBAKER">
      <data key="d4">8.0</data>
      <data key="d5">Douglas Orbaker contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="ED CLARK">
      <data key="d4">8.0</data>
      <data key="d5">Ed Clark contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="GABRIEL NIEVES-PONCE">
      <data key="d4">8.0</data>
      <data key="d5">Gabriel Nieves-Ponce contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="GAUDY BLANCO MENESES">
      <data key="d4">8.0</data>
      <data key="d5">Gaudy Blanco Meneses contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="KATE LYTVYNETS">
      <data key="d4">8.0</data>
      <data key="d5">Kate Lytvynets contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="KATY SMITH">
      <data key="d4">8.0</data>
      <data key="d5">Katy Smith contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="M&#211;NICA CARVAJAL">
      <data key="d4">8.0</data>
      <data key="d5">M&#243;nica Carvajal contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="NATHAN EVANS">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Evans contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="RICHARD ORTEGA">
      <data key="d4">8.0</data>
      <data key="d5">Richard Ortega contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="RODRIGO RACANICCI">
      <data key="d4">8.0</data>
      <data key="d5">Rodrigo Racanicci contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="SARAH SMITH">
      <data key="d4">8.0</data>
      <data key="d5">Sarah Smith contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="SHANE SOLOMON">
      <data key="d4">1.0</data>
      <data key="d5">Shane Solomon contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="SENSEMAKING QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">Sensemaking questions are used to evaluate the performance of Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="CORPORA">
      <data key="d4">7.0</data>
      <data key="d5">Corpora are collections of text data used in the evaluation of Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="USER QUERIES">
      <data key="d4">7.0</data>
      <data key="d5">User queries are used to retrieve information from the graph index in Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="COMMUNITY HIERARCHY">
      <data key="d4">7.0</data>
      <data key="d5">Community hierarchy refers to the hierarchical structure of communities in the graph index used in Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="MAP-REDUCE SUMMARIZATION">
      <data key="d4">7.0</data>
      <data key="d5">Map-Reduce summarization is a method compared to Graph RAG for global summarization</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="PYTHON">
      <data key="d4">8.0</data>
      <data key="d5">Python is the programming language used for the implementation of Graph RAG approaches</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="LOUVAIN">
      <data key="d4">9.0</data>
      <data key="d5">Louvain is a community detection algorithm used to partition graphs into modular communities</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="LEIDEN">
      <data key="d4">17.0</data>
      <data key="d5">COMMUNITY DETECTION and LEIDEN are closely related in the context of graph analysis. Leiden is a community detection algorithm specifically designed to partition graphs into modular communities. It is particularly effective for community detection in large-scale graphs, making it a valuable tool for analyzing complex networks and uncovering the underlying structure within them.</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7,e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="GRAPH COMMUNITIES">
      <data key="d4">8.0</data>
      <data key="d5">Community detection results in the formation of graph communities</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="FORTUNATO, 2010">
      <data key="d4">6.0</data>
      <data key="d5">Fortunato's 2010 survey is referenced in the context of community detection algorithms</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="JIN ET AL., 2021">
      <data key="d4">6.0</data>
      <data key="d5">Jin et al.'s 2021 survey is referenced in the context of community detection algorithms</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LEIDEN" target="TRAAG ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Traag et al. are referenced for their work on the Leiden algorithm in 2019</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="LEIDEN" target="MULTIHOP-RAG">
      <data key="d4">7.0</data>
      <data key="d5">Leiden is used to detect communities in the MultiHop-RAG dataset</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LEIDEN" target="TRAAG ET AL., 2019">
      <data key="d4">7.0</data>
      <data key="d5">Traag et al.'s 2019 work discusses the Leiden algorithm</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="MICROSOFT" target="KEVIN SCOTT">
      <data key="d4">8.0</data>
      <data key="d5">Kevin Scott is the CTO of Microsoft.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="LEWIS ET AL." target="MEMORY STRUCTURES">
      <data key="d4">14.0</data>
      <data key="d5">Lewis et al. contributed to the research on memory structures in agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="LEWIS ET AL." target="EXTERNAL MEMORY AND RAG">
      <data key="d4">8.0</data>
      <data key="d5">Lewis et al. are the authors of the External Memory and RAG methods</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="LASKAR ET AL." target="SUMMARIZATION TASKS">
      <data key="d4">8.0</data>
      <data key="d5">Laskar et al. are referenced for their work on summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="YAO ET AL." target="DENG ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Yao et al. and Deng et al. are co-referenced in the context of language models performing in complex environments such as web navigation.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="YAO ET AL." target="SCHICK ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Yao et al. and Schick et al. are co-referenced in the context of language models performing in complex environments such as tool-use.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="YAO ET AL." target="FAN ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Yao et al. and Fan et al. are co-referenced in the context of language models performing in complex environments such as open-ended games.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="YAO ET AL." target="CHAIN-OF-THOUGHT">
      <data key="d4">14.0</data>
      <data key="d5">Yao et al. contributed to the research on chain-of-thought planning and reasoning</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="YAO ET AL." target="CHAIN-OF-THOUGHT-BASED PLANNING AND REASONING METHODS">
      <data key="d4">8.0</data>
      <data key="d5">Yao et al. are the authors of the Chain-of-Thought-based planning and reasoning methods</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="GOODWIN ET AL." target="SUMMARIZATION TASKS">
      <data key="d4">8.0</data>
      <data key="d5">Goodwin et al. are referenced for their work on summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="LIU AND LAPATA" target="SUMMARIZATION TASKS">
      <data key="d4">8.0</data>
      <data key="d5">Liu and Lapata are referenced for their work on summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GPT" target="SUMMARIZATION TASKS">
      <data key="d4">9.0</data>
      <data key="d5">GPT models are referenced for their ability to perform summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GPT" target="ACHIAM ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Achiam et al. are referenced for their work on GPT in 2023</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GPT" target="BROWN ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Brown et al. are referenced for their work on GPT in 2020</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GPT" target="FOUNDATION MODELS (FMS)">
      <data key="d4">16.0</data>
      <data key="d5">GPT is an example of a Foundation Model used in agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="GPT" target="OPENAI">
      <data key="d4">16.0</data>
      <data key="d5">OpenAI is the organization that developed the GPT Foundation Model</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="LLAMA" target="SUMMARIZATION TASKS">
      <data key="d4">9.0</data>
      <data key="d5">Llama models are referenced for their ability to perform summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="LLAMA" target="TOUVRON ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Touvron et al. are referenced for their work on Llama in 2023</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GEMINI" target="SUMMARIZATION TASKS">
      <data key="d4">9.0</data>
      <data key="d5">Gemini models are referenced for their ability to perform summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GEMINI" target="ANIL ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Anil et al. are referenced for their work on Gemini in 2023</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GEMINI" target="R. ANIL">
      <data key="d4">8.0</data>
      <data key="d5">R. Anil is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="S. BORGEAUD">
      <data key="d4">8.0</data>
      <data key="d5">S. Borgeaud is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="Y. WU">
      <data key="d4">8.0</data>
      <data key="d5">Y. Wu is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="J.-B. ALAYRAC">
      <data key="d4">8.0</data>
      <data key="d5">J.-B. Alayrac is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="J. YU">
      <data key="d4">8.0</data>
      <data key="d5">J. Yu is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="R. SORICUT">
      <data key="d4">8.0</data>
      <data key="d5">R. Soricut is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="J. SCHALKWYK">
      <data key="d4">8.0</data>
      <data key="d5">J. Schalkwyk is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="A. M. DAI">
      <data key="d4">8.0</data>
      <data key="d5">A. M. Dai is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="A. HAUTH">
      <data key="d4">8.0</data>
      <data key="d5">A. Hauth is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="BROWN ET AL." target="CHOWDHERY ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Brown et al. and Chowdhery et al. are co-referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="BROWN ET AL." target="TOUVRON ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Brown et al. and Touvron et al. are co-referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="BROWN ET AL." target="OPENAI">
      <data key="d4">7.0</data>
      <data key="d5">Brown et al. and OpenAI are co-referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="BROWN ET AL." target="IN-CONTEXT LEARNING">
      <data key="d4">16.0</data>
      <data key="d5">Brown et al. contributed to the understanding of in-context learning in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TOUVRON ET AL." target="CHOWDHERY ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Chowdhery et al. and Touvron et al. are co-referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="TOUVRON ET AL." target="OPENAI">
      <data key="d4">7.0</data>
      <data key="d5">Touvron et al. and OpenAI are co-referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="KURATOV ET AL." target="LLM CONTEXT WINDOWS">
      <data key="d4">8.0</data>
      <data key="d5">Kuratov et al. are referenced for their work on LLM context windows in 2024</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="LIU ET AL." target="LLM CONTEXT WINDOWS">
      <data key="d4">8.0</data>
      <data key="d5">Liu et al. are referenced for their work on LLM context windows in 2023</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="LIU ET AL." target="SEARCH ALGORITHMS">
      <data key="d4">16.0</data>
      <data key="d5">Liu et al. explored combining search algorithms with language model agents</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="RAG" target="QUERY-FOCUSED SUMMARIZATION">
      <data key="d4">7.0</data>
      <data key="d5">RAG is mentioned as inadequate for query-focused summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="RAG" target="SEMANTIC SEARCH (SS)">
      <data key="d4">7.0</data>
      <data key="d5">RAG is used in the semantic search approach to retrieve and add text chunks</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="RAG" target="LANGCHAIN">
      <data key="d4">12.0</data>
      <data key="d5">RAG is a building block used in the LangChain framework</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="RAG" target="META AGENT SEARCH">
      <data key="d4">6.0</data>
      <data key="d5">RAG is used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="RAG" target="AGENTINSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct generates data covering the skill of RAG</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="RAG" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B's RAG skill is evaluated to generate informed, contextually precise responses</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="RAG" target="MIRAGE">
      <data key="d4">17.0</data>
      <data key="d5">RAG and MIRAGE are closely interconnected within the AI and ML landscape. The MIRAGE benchmark is utilized to evaluate the RAG skill of language models, providing a critical metric for assessing their capabilities. Conversely, RAG is employed to enhance model performance on MIRAGE datasets, indicating a symbiotic relationship where each entity supports and improves the other. This dynamic highlights the importance of both RAG and MIRAGE in advancing the effectiveness and accuracy of language models.</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="NEWMAN" target="GRAPH MODULARITY">
      <data key="d4">8.0</data>
      <data key="d5">Newman is referenced for their work on the modularity of graphs in 2006</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="LOUVAIN" target="BLONDEL ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Blondel et al. are referenced for their work on the Louvain algorithm in 2008</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="HOTPOTQA" target="ENTITY EXTRACTION">
      <data key="d4">8.0</data>
      <data key="d5">HotPotQA is a dataset used to evaluate entity extraction with varying chunk sizes</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="HOTPOTQA" target="YANG ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Yang et al. are recognized for their significant contribution to the field of Artificial Intelligence and Machine Learning through their work on the HotPotQA dataset in 2018. The HotPotQA dataset, developed by Yang et al., is a benchmark widely used in the empirical evaluation of various AI models, including the LATS framework. This dataset has been instrumental in advancing research in question answering systems by providing a robust platform for testing and comparing the performance of different models.</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7,93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="HOTPOTQA" target="RAG SYSTEMS">
      <data key="d4">7.0</data>
      <data key="d5">HotPotQA is used to evaluate RAG systems for open-domain question answering.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="HOTPOTQA" target="LANGUAGE AGENT TREE SEARCH (LATS)">
      <data key="d4">8.0</data>
      <data key="d5">LATS was evaluated on the HotPotQA benchmark to demonstrate its effectiveness in decision-making and reasoning.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="HOTPOTQA" target="LATS">
      <data key="d4">65.0</data>
      <data key="d5">LATS is evaluated on the HotPotQA benchmark to demonstrate its reasoning and acting capabilities. It achieves high performance in the decision-making setting of HotPotQA, and its performance and cost are assessed using metrics such as Exact Match (EM). The evaluation on the HotPotQA dataset also aims to demonstrate the effect of each component of LATS.</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HOTPOTQA" target="YANG ET AL., 2018">
      <data key="d4">8.0</data>
      <data key="d5">Yang et al., 2018 is the reference for the HotPotQA dataset</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="HOTPOTQA" target="GPT-3.5">
      <data key="d4">23.0</data>
      <data key="d5">GPT-3.5 is utilized to evaluate reasoning-based prompting results and solve question answering tasks in HotpotQA.</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf,b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="HOTPOTQA" target="YAO ET AL., 2023B">
      <data key="d4">7.0</data>
      <data key="d5">The setup for HotPotQA follows the method described in the paper by Yao et al. (2023b).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HOTPOTQA" target="SHINN ET AL., 2023">
      <data key="d4">7.0</data>
      <data key="d5">The setup for HotPotQA follows the method described in the paper by Shinn et al. (2023).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HOTPOTQA" target="HAO ET AL., 2023">
      <data key="d4">7.0</data>
      <data key="d5">The setup for HotPotQA follows the method described in the paper by Hao et al. (2023).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HOTPOTQA" target="WEI ET AL., 2022">
      <data key="d4">7.0</data>
      <data key="d5">The setup for HotPotQA follows the method described in the paper by Wei et al. (2022).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HOTPOTQA" target="WANG ET AL., 2022">
      <data key="d4">7.0</data>
      <data key="d5">The setup for HotPotQA follows the method described in the paper by Wang et al. (2022).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HOTPOTQA" target="YAO ET AL., 2023A">
      <data key="d4">1.0</data>
      <data key="d5">The setup for HotPotQA follows the method described in the paper by Yao et al. (2023a).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HOTPOTQA" target="COT">
      <data key="d4">6.0</data>
      <data key="d5">CoT slightly enhances performance on questions requiring reasoning in HotPotQA</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HOTPOTQA" target="REACT">
      <data key="d4">6.0</data>
      <data key="d5">ReAct is used as a prompting method in HotPotQA</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HOTPOTQA" target="TOT">
      <data key="d4">7.0</data>
      <data key="d5">ToT shows larger gains in performance on HotPotQA</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HOTPOTQA" target="RAP">
      <data key="d4">7.0</data>
      <data key="d5">RAP shows larger gains in performance on HotPotQA</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HOTPOTQA" target="REFLEXION">
      <data key="d4">7.0</data>
      <data key="d5">Reflexion is competitive with other methods on HotPotQA</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HOTPOTQA" target="TAB. 2">
      <data key="d4">8.0</data>
      <data key="d5">Tab. 2 shows the performance of internal reasoning and external retrieval strategies on HotPotQA</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HOTPOTQA" target="TAB. 3">
      <data key="d4">8.0</data>
      <data key="d5">Tab. 3 shows the performance of various methods on HotPotQA in different settings</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HOTPOTQA" target="TAB. 10">
      <data key="d4">7.0</data>
      <data key="d5">Tab. 10 shows the cost of different methods on HotPotQA</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="HOTPOTQA" target="CROWDWORKERS">
      <data key="d4">8.0</data>
      <data key="d5">Crowdworkers crafted the question-answer pairs in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="WIKIPEDIA">
      <data key="d4">8.0</data>
      <data key="d5">Wikipedia is the source of the documents used in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="FIG. 4">
      <data key="d4">7.0</data>
      <data key="d5">Figure 4 illustrates an example task of HotPotQA</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="TAB. 8">
      <data key="d4">7.0</data>
      <data key="d5">Table 8 shows the results for HotPotQA</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="TRAJECTORIES">
      <data key="d4">8.0</data>
      <data key="d5">Trajectories refer to the number of paths sampled in the HotPotQA experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="SUPPORTING FACTS">
      <data key="d4">8.0</data>
      <data key="d5">Supporting facts are provided by crowdworkers in the HotPotQA dataset to justify answers</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="ENTITY">
      <data key="d4">8.0</data>
      <data key="d5">Entity is a type of question in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="LOCATION">
      <data key="d4">8.0</data>
      <data key="d5">Location is a type of question in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="DATE">
      <data key="d4">8.0</data>
      <data key="d5">Date is a type of question in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="COMPARISON">
      <data key="d4">8.0</data>
      <data key="d5">Comparison is a type of question in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="SHARED PROPERTIES">
      <data key="d4">8.0</data>
      <data key="d5">Shared properties are compared between two entities in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="SUPPORTING DOCUMENTS">
      <data key="d4">8.0</data>
      <data key="d5">Supporting documents are used to answer questions in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="QUESTION-ANSWER PAIRS">
      <data key="d4">8.0</data>
      <data key="d5">Question-answer pairs are crafted by crowdworkers in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="WIKIPEDIA PARAGRAPHS">
      <data key="d4">8.0</data>
      <data key="d5">Wikipedia paragraphs are used in the HotPotQA benchmark setting</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="SUBSET OF 100 QUESTIONS">
      <data key="d4">8.0</data>
      <data key="d5">A randomly selected subset of 100 questions is used in the HotPotQA experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="MAXIMUM DEPTH LIMIT">
      <data key="d4">8.0</data>
      <data key="d5">Maximum depth limit is set to 6 in the HotPotQA experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="GPT-4-TURBO" target="ENTITY EXTRACTION">
      <data key="d4">9.0</data>
      <data key="d5">GPT-4-Turbo is used for entity extraction in the HotPotQA dataset</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GPT-4-TURBO" target="CONTEXT WINDOW SIZE">
      <data key="d4">8.0</data>
      <data key="d5">Context window size is tested on GPT-4-Turbo</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="MAP-REDUCE" target="QUERY-FOCUSED SUMMARIZATION">
      <data key="d4">8.0</data>
      <data key="d5">Map-Reduce is a technique used for query-focused summarization of an entire corpus</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="MAP-REDUCE" target="TEXT SUMMARIZATION (TS)">
      <data key="d4">8.0</data>
      <data key="d5">Map-Reduce is used in text summarization to shuffle and chunk source texts</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="PODCAST TRANSCRIPTS" target="SENSE-MAKING QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">Podcast transcripts are used to generate activity-centered sense-making questions</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="PODCAST TRANSCRIPTS" target="TECH JOURNALIST">
      <data key="d4">7.0</data>
      <data key="d5">A tech journalist uses podcast transcripts to gain insights and trends in the tech industry.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="PODCAST TRANSCRIPTS" target="KEVIN SCOTT">
      <data key="d4">8.0</data>
      <data key="d5">Kevin Scott is a participant in the podcast conversations included in the podcast transcripts dataset.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="PODCAST TRANSCRIPTS" target="TECH LEADERS">
      <data key="d4">8.0</data>
      <data key="d5">Tech leaders are participants in the podcast conversations included in the podcast transcripts dataset.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NEWS ARTICLES" target="SENSE-MAKING QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">News articles are used to generate activity-centered sense-making questions</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="NEWS ARTICLES" target="EDUCATOR">
      <data key="d4">7.0</data>
      <data key="d5">An educator uses news articles to teach about health and wellness.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NEWS ARTICLES" target="MULTIHOP-RAG">
      <data key="d4">8.0</data>
      <data key="d5">MultiHop-RAG includes news articles as part of its benchmark dataset.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NEWS ARTICLES" target="HEALTH EDUCATION">
      <data key="d4">8.0</data>
      <data key="d5">News articles are used by educators to integrate current topics into health education curricula.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NEWS ARTICLES" target="PREVENTIVE MEDICINE">
      <data key="d4">7.0</data>
      <data key="d5">News articles address the concept of preventive medicine, relevant to health education.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NEWS ARTICLES" target="WELLNESS">
      <data key="d4">7.0</data>
      <data key="d5">News articles address the concept of wellness, relevant to health education.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NEWS ARTICLES" target="PUBLIC HEALTH">
      <data key="d4">7.0</data>
      <data key="d5">News articles provide insights into public health priorities, relevant to health education.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NEWS ARTICLES" target="HEALTH LITERACY">
      <data key="d4">7.0</data>
      <data key="d5">News articles highlight the importance of health literacy, relevant to health education.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NAIVE RAG" target="GLOBAL SUMMARIZATION">
      <data key="d4">7.0</data>
      <data key="d5">Naive RAG is a baseline technique compared against global summarization approaches</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="NAIVE RAG" target="DIRECTNESS">
      <data key="d4">7.0</data>
      <data key="d5">Naive RAG produces the most direct responses</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GLOBAL MAP-REDUCE" target="GLOBAL SUMMARIZATION">
      <data key="d4">8.0</data>
      <data key="d5">Global Map-Reduce is a summarization technique compared against Naive RAG</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="GLOBAL SUMMARIZATION">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG Approach is a high-level data flow and pipeline for global summarization</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="TEXT CHUNKS">
      <data key="d4">8.0</data>
      <data key="d5">Text Chunks are segments of source documents used for processing in the Graph RAG approach</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="ELEMENT INSTANCES">
      <data key="d4">8.0</data>
      <data key="d5">Element Instances are graph nodes and edges extracted from text chunks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="ELEMENT INSTANCES" target="ELEMENT SUMMARIES">
      <data key="d4">8.0</data>
      <data key="d5">Element instances are further summarized into element summaries</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="NAMED ENTITIES" target="ENTITY EXTRACTION">
      <data key="d4">2.0</data>
      <data key="d5">Named Entities are broad classes of entities like people, places, and organizations extracted from text</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="NAMED ENTITIES" target="LLM">
      <data key="d4">8.0</data>
      <data key="d5">The LLM is used to extract named entities from text</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="SUMMARIZATION TASKS" target="LLM">
      <data key="d4">9.0</data>
      <data key="d5">LLMs are referenced for their ability to perform summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="GRAPH INDEXING">
      <data key="d4">8.0</data>
      <data key="d5">Entity extraction is used in the graph indexing process</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="ACTIVITY-CENTERED SENSE-MAKING QUESTIONS" target="EVALUATION">
      <data key="d4">8.0</data>
      <data key="d5">Activity-Centered Sense-Making Questions are generated from real-world datasets for evaluation</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="COMPREHENSIVENESS" target="EVALUATION">
      <data key="d4">8.0</data>
      <data key="d5">Comprehensiveness is a target quality for evaluating summarization approaches</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="COMPREHENSIVENESS" target="LLM EVALUATOR">
      <data key="d4">9.0</data>
      <data key="d5">LLM Evaluator assesses answers based on comprehensiveness</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="DIVERSITY" target="EVALUATION">
      <data key="d4">8.0</data>
      <data key="d5">Diversity is a target quality for evaluating summarization approaches</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="DIVERSITY" target="LLM EVALUATOR">
      <data key="d4">9.0</data>
      <data key="d5">LLM Evaluator assesses answers based on diversity</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="DIVERSITY" target="INSTRUCTION REFINEMENT FLOW">
      <data key="d4">8.0</data>
      <data key="d5">Instruction Refinement Flow aims to ensure diversity in the generated instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="EMPOWERMENT" target="EVALUATION">
      <data key="d4">8.0</data>
      <data key="d5">Empowerment is a target quality for evaluating summarization approaches</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="EMPOWERMENT" target="LLM EVALUATOR">
      <data key="d4">9.0</data>
      <data key="d5">LLM Evaluator assesses answers based on empowerment</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="EMPOWERMENT" target="SENSEMAKING">
      <data key="d4">8.0</data>
      <data key="d5">Empowerment is key to helping users reach an informed understanding during sensemaking activities</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="HIERARCHICAL LEVEL" target="EVALUATION">
      <data key="d4">7.0</data>
      <data key="d5">Hierarchical Level is a variable in the evaluation of community summaries</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="TOKEN COSTS" target="EVALUATION">
      <data key="d4">7.0</data>
      <data key="d5">Token Costs are a metric used to evaluate the efficiency of summarization approaches</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="LLM PROMPTS" target="FEW-SHOT EXAMPLES">
      <data key="d4">8.0</data>
      <data key="d5">Few-Shot Examples are used in LLM prompts for in-context learning</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="FEW-SHOT EXAMPLES" target="LLM">
      <data key="d4">7.0</data>
      <data key="d5">Few-shot examples are provided to the LLM to improve its performance in specialized domains</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LLM" target="COVARIATES">
      <data key="d4">7.0</data>
      <data key="d5">The LLM extracts covariates associated with detected entities</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LLM" target="LOGIT BIAS">
      <data key="d4">6.0</data>
      <data key="d5">Logit bias is used to guide the LLM in making yes/no decisions during entity extraction</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LLM" target="GLEANINGS">
      <data key="d4">7.0</data>
      <data key="d5">Gleanings involve multiple rounds of extraction by the LLM to detect additional entities</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LLM" target="CLAIMS">
      <data key="d4">7.0</data>
      <data key="d5">The LLM extracts claims linked to detected entities</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LLM" target="FIGURE 2">
      <data key="d4">6.0</data>
      <data key="d5">Figure 2 shows the impact of using larger chunk sizes without a drop in quality or the forced introduction of noise</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LLM" target="COMMUNITY SUMMARIES">
      <data key="d4">9.0</data>
      <data key="d5">The LLM is used to generate community summaries based on the hierarchical community structure.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="LLM" target="GLOBAL ANSWER">
      <data key="d4">9.0</data>
      <data key="d5">The LLM generates the global answer by processing community summaries and user queries.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="LLM" target="DATASET">
      <data key="d4">9.0</data>
      <data key="d5">The LLM generates questions based on the dataset</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="LLM" target="NA&#207;VE RAG">
      <data key="d4">9.0</data>
      <data key="d5">Na&#239;ve RAG is assessed by the LLM for its comprehensiveness, diversity, empowerment, and directnessLLM assesses Na&#239;ve RAG</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="LLM" target="AD-HOC LLM USE">
      <data key="d4">7.0</data>
      <data key="d5">Ad-hoc LLM use involves the spontaneous use of large language models</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LLM" target="RAG APPROACHES AND SYSTEMS">
      <data key="d4">8.0</data>
      <data key="d5">RAG approaches and systems involve using LLMs</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LLM" target="AGENT">
      <data key="d4">7.0</data>
      <data key="d5">Each agent is powered by an LLM and can optionally use tools such as search APIs, code interpreter, or a calculator.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="LLM" target="CONTENT TRANSFORMATION FLOW">
      <data key="d4">7.0</data>
      <data key="d5">An LLM is used in the Content Transformation Flow to hypothesize other APIs present in the library</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="EVALUATION" target="QUESTION GENERATION">
      <data key="d4">8.0</data>
      <data key="d5">The generated questions are evaluated for quality and effectiveness</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="EVALUATION" target="LATS">
      <data key="d4">8.0</data>
      <data key="d5">Evaluation is the third operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="EVALUATION" target="SCALAR VALUE">
      <data key="d4">8.0</data>
      <data key="d5">Scalar value is assigned to each new child node during the evaluation operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="EVALUATION" target="HEURISTIC">
      <data key="d4">8.0</data>
      <data key="d5">Heuristic is used to guide the search algorithm during the evaluation operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="EVALUATION" target="SELF-GENERATED LM SCORE">
      <data key="d4">1.0</data>
      <data key="d5">Self-generated LM score is a component of the value function used in the evaluation operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="EVALUATION" target="AGENT">
      <data key="d4">16.0</data>
      <data key="d5">The agent's progress in task completion is quantified during the evaluation process</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="EVALUATION" target="MATH">
      <data key="d4">1.0</data>
      <data key="d5">Math is another capability evaluated in language models</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="ELEMENT SUMMARIES" target="GRAPH ELEMENT">
      <data key="d4">8.0</data>
      <data key="d5">Element summaries are created for each graph element</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="GRAPH ELEMENT" target="ENTITY GRAPH">
      <data key="d4">8.0</data>
      <data key="d5">Graph elements are part of the entity graph</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="ENTITY GRAPH" target="KNOWLEDGE GRAPHS">
      <data key="d4">6.0</data>
      <data key="d5">Entity graphs are differentiated from typical knowledge graphs</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="GRAPH COMMUNITIES" target="COMMUNITY SUMMARIES">
      <data key="d4">8.0</data>
      <data key="d5">Community summaries are created for each graph community</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="GRAPH COMMUNITIES" target="HIERARCHICAL CLUSTERING">
      <data key="d4">7.0</data>
      <data key="d5">Hierarchical clustering reveals internal structure within graph communities</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="GRAPH COMMUNITIES" target="C0">
      <data key="d4">8.0</data>
      <data key="d5">C0 is the root-level community summaries in Graph RAG</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="GRAPH COMMUNITIES" target="C1">
      <data key="d4">8.0</data>
      <data key="d5">C1 is the high-level community summaries in Graph RAG</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="GRAPH COMMUNITIES" target="C2">
      <data key="d4">8.0</data>
      <data key="d5">C2 is the intermediate-level community summaries in Graph RAG</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="GRAPH COMMUNITIES" target="C3">
      <data key="d4">8.0</data>
      <data key="d5">C3 is the low-level community summaries in Graph RAG</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="LEAF-LEVEL COMMUNITIES">
      <data key="d4">8.0</data>
      <data key="d5">Community summaries are generated from the prioritized elements of leaf-level communities.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="HIGHER-LEVEL COMMUNITIES">
      <data key="d4">8.0</data>
      <data key="d5">Community summaries are generated from the ranked sub-communities of higher-level communities.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="GLOBAL ANSWER">
      <data key="d4">9.0</data>
      <data key="d5">Community summaries are used to generate the global answer to a user query.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="C0">
      <data key="d4">7.0</data>
      <data key="d5">C0 represents root-level community summaries</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="C1">
      <data key="d4">7.0</data>
      <data key="d5">C1 represents intermediate-level community summaries</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="C2">
      <data key="d4">7.0</data>
      <data key="d5">C2 represents intermediate-level community summaries</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="C3">
      <data key="d4">7.0</data>
      <data key="d5">C3 represents low-level community summaries</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="COMMUNITY ANSWERS">
      <data key="d4">7.0</data>
      <data key="d5">Community answers are generated from community summaries</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTIHOP-RAG" target="OPENORD">
      <data key="d4">6.0</data>
      <data key="d5">OpenORD is used for node layout in the visualization of the MultiHop-RAG dataset</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="MULTIHOP-RAG" target="FORCE ATLAS 2">
      <data key="d4">6.0</data>
      <data key="d5">Force Atlas 2 is used for node layout in the visualization of the MultiHop-RAG dataset</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="MULTIHOP-RAG" target="TANG AND YANG, 2024">
      <data key="d4">7.0</data>
      <data key="d5">Tang and Yang's 2024 work discusses the MultiHop-RAG dataset</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="MULTIHOP-RAG" target="TANG AND YANG">
      <data key="d4">1.0</data>
      <data key="d5">Tang and Yang (2024) conducted a study on the MultiHop-RAG benchmark dataset.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="OPENORD" target="MARTIN ET AL., 2011">
      <data key="d4">6.0</data>
      <data key="d5">Martin et al.'s 2011 work discusses the OpenORD tool</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="FORCE ATLAS 2" target="JACOMY ET AL., 2014">
      <data key="d4">1.0</data>
      <data key="d5">Jacomy et al.'s 2014 work discusses the Force Atlas 2 tool</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="HIERARCHICAL CLUSTERING" target="ROOT COMMUNITIES">
      <data key="d4">7.0</data>
      <data key="d5">Root communities are identified in the hierarchical clustering</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="HIERARCHICAL CLUSTERING" target="LEAF-LEVEL COMMUNITIES">
      <data key="d4">7.0</data>
      <data key="d5">Leaf-level communities are identified in the hierarchical clustering</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LEAF-LEVEL COMMUNITIES" target="TOKEN LIMIT">
      <data key="d4">1.0</data>
      <data key="d5">Leaf-level communities are prioritized and added to the LLM context window until the token limit is reached</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LEAF-LEVEL COMMUNITIES" target="ROOT-LEVEL COMMUNITIES">
      <data key="d4">8.0</data>
      <data key="d5">Leaf-level communities are subdivisions within root-level communities, focusing on more granular elements.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="SCIENCE" target="META AGENT SEARCH">
      <data key="d4">27.0</data>
      <data key="d5">Meta Agent Search is a tool utilized within the Science domain. It is designed to test agents and has demonstrated the capability to match state-of-the-art baselines in scientific research and applications.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="FIGURE 2" target="LATS">
      <data key="d4">7.0</data>
      <data key="d5">Figure 2 provides an overview of the six operations in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="ROOT-LEVEL COMMUNITIES" target="HIGHER-LEVEL COMMUNITIES">
      <data key="d4">8.0</data>
      <data key="d5">Higher-level communities are broader divisions within root-level communities, summarizing sub-communities.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="GLOBAL ANSWER" target="USER QUERY">
      <data key="d4">9.0</data>
      <data key="d5">The global answer is generated in response to a user query.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="MT-BENCH" target="RAG SYSTEMS">
      <data key="d4">7.0</data>
      <data key="d5">MT-Bench is used to evaluate RAG systems for open-domain question answering.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="MT-BENCH" target="GPT-4">
      <data key="d4">15.0</data>
      <data key="d5">MT-Bench utilizes GPT-4 as the evaluator for assessing chat assistants. Specifically, MT-Bench employs GPT-4 to judge each turn's response and provide a score, ensuring a comprehensive evaluation of the chat assistants' performance.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba,86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="MT-BENCH" target="OPEN-ENDED GENERATION">
      <data key="d4">7.0</data>
      <data key="d5">MT-Bench is a benchmark under the category of Open-Ended Generation tasks.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="DATA SENSEMAKING" target="KOESTEN ET AL.">
      <data key="d4">6.0</data>
      <data key="d5">Koesten et al. (2021) studied data sensemaking behaviors.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="DATA SENSEMAKING" target="XU AND LAPATA">
      <data key="d4">1.0</data>
      <data key="d5">Xu and Lapata (2021) studied methods for extracting latent summarization queries, relevant to data sensemaking.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="TECH LEADERS" target="PRIVACY LAWS">
      <data key="d4">7.0</data>
      <data key="d5">Tech leaders discuss the impact of privacy laws on technology development in the podcast transcripts.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="TECH LEADERS" target="POLICY AND REGULATION">
      <data key="d4">7.0</data>
      <data key="d5">Tech leaders discuss the role of policy and regulation in the tech industry in the podcast transcripts.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="TECH LEADERS" target="COLLABORATIONS">
      <data key="d4">7.0</data>
      <data key="d5">Tech leaders discuss collaborations between tech companies and governments in the podcast transcripts.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="INNOVATION" target="ETHICAL CONSIDERATIONS">
      <data key="d4">7.0</data>
      <data key="d5">Innovation and ethical considerations are topics discussed together by guests in the podcast transcripts.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="DATASET" target="TEXT SUMMARIZATION (TS)">
      <data key="d4">7.0</data>
      <data key="d5">Text Summarization applies map-reduce approach to source texts in the dataset</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="DATASET" target="SEMANTIC SEARCH (SS)">
      <data key="d4">7.0</data>
      <data key="d5">Semantic Search retrieves text chunks from the dataset</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="TEXT SUMMARIZATION (TS)" target="CONTEXT WINDOW">
      <data key="d4">7.0</data>
      <data key="d5">Text Summarization uses a context window to generate answers</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="SEMANTIC SEARCH (SS)" target="CONTEXT WINDOW">
      <data key="d4">7.0</data>
      <data key="d5">Semantic Search uses a context window to generate answers</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="CONTEXT WINDOW" target="PODCAST DATASET">
      <data key="d4">6.0</data>
      <data key="d5">Podcast dataset is indexed with a context window size of 600 tokens</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="CONTEXT WINDOW" target="NEWS DATASET">
      <data key="d4">6.0</data>
      <data key="d5">News dataset is indexed with a context window size of 600 tokens</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="CONTEXT WINDOW" target="TOKEN">
      <data key="d4">7.0</data>
      <data key="d5">Tokens are units of text used in the context window</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="DIRECTNESS" target="LLM EVALUATOR">
      <data key="d4">9.0</data>
      <data key="d5">LLM Evaluator assesses answers based on directness</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="LLM EVALUATOR" target="QUESTION">
      <data key="d4">9.0</data>
      <data key="d5">The LLM evaluator is provided with the question to assess the quality of answers</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="LLM EVALUATOR" target="ANSWER">
      <data key="d4">9.0</data>
      <data key="d5">The LLM evaluator assesses the quality of the answer</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="LLM EVALUATOR" target="ASSESSMENT">
      <data key="d4">9.0</data>
      <data key="d5">The LLM evaluator performs the assessment of answers</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="LLM EVALUATOR" target="STOCHASTICITY">
      <data key="d4">7.0</data>
      <data key="d5">Stochasticity is accounted for in the LLM evaluator's assessments</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="PUBLIC FIGURES" target="ENTERTAINMENT INDUSTRY">
      <data key="d4">8.0</data>
      <data key="d5">Public figures are repeatedly mentioned in various entertainment articles</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="PUBLIC FIGURES" target="ACTORS AND DIRECTORS">
      <data key="d4">8.0</data>
      <data key="d5">Actors and directors are public figures in the entertainment industry</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="PUBLIC FIGURES" target="MUSICIANS AND EXECUTIVES">
      <data key="d4">8.0</data>
      <data key="d5">Musicians and executives are public figures in the entertainment industry</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="PUBLIC FIGURES" target="ATHLETES AND COACHES">
      <data key="d4">8.0</data>
      <data key="d5">Athletes and coaches are public figures in the entertainment industry</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="PUBLIC FIGURES" target="INFLUENCERS AND ENTREPRENEURS">
      <data key="d4">1.0</data>
      <data key="d5">Influencers and entrepreneurs are public figures in the entertainment industry</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="ACTIVITY-CENTERED APPROACH" target="QUESTION GENERATION">
      <data key="d4">9.0</data>
      <data key="d5">The activity-centered approach is used to automate the generation of questions</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="RELATIONSHIP EXTRACTION" target="GRAPH INDEXING">
      <data key="d4">8.0</data>
      <data key="d5">Relationship extraction is used in the graph indexing process</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="TOKEN" target="LM">
      <data key="d4">16.0</data>
      <data key="d5">Tokens are the basic elements of natural language used in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="QUESTION" target="THOUGHT">
      <data key="d4">14.0</data>
      <data key="d5">The thought process involves reasoning about how to answer the question</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="QUESTION" target="ARTHUR&#8217;S MAGAZINE">
      <data key="d4">18.0</data>
      <data key="d5">Arthur&#8217;s Magazine is part of the question "Which magazine was started first Arthur&#8217;s Magazine or First for Women?"</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="QUESTION" target="FIRST FOR WOMEN">
      <data key="d4">18.0</data>
      <data key="d5">First for Women is part of the question "Which magazine was started first Arthur&#8217;s Magazine or First for Women?"</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="QUESTION" target="STUDENT RESPONSE">
      <data key="d4">17.0</data>
      <data key="d5">The student response is an answer provided in response to the given question.</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50,5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="QUESTION" target="OPTIONS">
      <data key="d4">8.0</data>
      <data key="d5">Options are the set of possible answers provided for the question</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="ANSWER" target="COT_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The cot_module generates the answer</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ANSWER" target="CRITIC_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The critic_module reviews the answer</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ANSWER" target="FINAL_CODE">
      <data key="d4">9.0</data>
      <data key="d5">Answer is generated from the final code</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="ANSWER" target="INTEGRATION MODULE">
      <data key="d4">9.0</data>
      <data key="d5">The Integration Module combines sub-solutions to form the final answer</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="STOCHASTICITY" target="MEAN SCORES">
      <data key="d4">7.0</data>
      <data key="d5">Mean scores are used to account for the stochasticity of LLMs</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="PUBLIC FIGURES IN CONTROVERSY" target="TAYLOR SWIFT">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift is frequently mentioned in the media due to her high-profile status and public interest in her career and personal life</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="PUBLIC FIGURES IN CONTROVERSY" target="TRAVIS KELCE">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce is frequently mentioned in the media due to his high-profile status and public interest in his career and personal life</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="PUBLIC FIGURES IN CONTROVERSY" target="BRITNEY SPEARS">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears is frequently mentioned in the media due to her high-profile status and public interest in her career and personal life</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="PUBLIC FIGURES IN CONTROVERSY" target="JUSTIN TIMBERLAKE">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake is frequently mentioned in the media due to his high-profile status and public interest in his career and personal life</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="PUBLIC FIGURES IN CONTROVERSY" target="ENTERTAINMENT ARTICLES">
      <data key="d4">9.0</data>
      <data key="d5">Entertainment articles frequently mention public figures involved in controversies</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="PUBLIC FIGURES IN CONTROVERSY" target="MEDIA COVERAGE">
      <data key="d4">9.0</data>
      <data key="d5">Public figures in controversy receive significant media coverage</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="PUBLIC FIGURES IN CONTROVERSY" target="NA&#207;VE RAG">
      <data key="d4">8.0</data>
      <data key="d5">Na&#239;ve RAG includes public figures involved in controversies</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="MUSICIANS" target="TAYLOR SWIFT">
      <data key="d4">10.0</data>
      <data key="d5">Taylor Swift is a musician</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="MUSICIANS" target="BRITNEY SPEARS">
      <data key="d4">10.0</data>
      <data key="d5">Britney Spears is a musician</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="MUSICIANS" target="JUSTIN TIMBERLAKE">
      <data key="d4">1.0</data>
      <data key="d5">Justin Timberlake is a musician</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="ATHLETES" target="TRAVIS KELCE">
      <data key="d4">10.0</data>
      <data key="d5">Travis Kelce is an athlete</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="ENTERTAINMENT ARTICLES">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift is frequently mentioned in entertainment articles</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="MEDIA COVERAGE">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift receives significant media coverage</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="PROFESSIONAL ACHIEVEMENTS">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift is highlighted for her professional achievements</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="PERSONAL LIVES">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift's personal life is of public interest</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="CULTURAL IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift has a significant cultural impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="ECONOMIC IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift has a significant economic impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="MEDIA REACTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift's activities generate media reactions</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="NA&#207;VE RAG">
      <data key="d4">8.0</data>
      <data key="d5">Na&#239;ve RAG mentions Taylor Swift</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="ENTERTAINMENT ARTICLES">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce is frequently mentioned in entertainment articles</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="MEDIA COVERAGE">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce receives significant media coverage</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="PROFESSIONAL ACHIEVEMENTS">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce is highlighted for his professional achievements</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="PERSONAL LIVES">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce's personal life is of public interest</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="CULTURAL IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce has a significant cultural impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="ECONOMIC IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce has a significant economic impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="MEDIA REACTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce's activities generate media reactions</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="NA&#207;VE RAG">
      <data key="d4">8.0</data>
      <data key="d5">Na&#239;ve RAG mentions Travis Kelce</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="ENTERTAINMENT ARTICLES">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears is frequently mentioned in entertainment articles</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="MEDIA COVERAGE">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears receives significant media coverage</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="PROFESSIONAL ACHIEVEMENTS">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears is highlighted for her professional achievements</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="PERSONAL LIVES">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears's personal life is of public interest</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="CULTURAL IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears has a significant cultural impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="ECONOMIC IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears has a significant economic impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="MEDIA REACTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears's activities generate media reactions</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="NA&#207;VE RAG">
      <data key="d4">8.0</data>
      <data key="d5">Na&#239;ve RAG mentions Britney Spears</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="ENTERTAINMENT ARTICLES">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake is frequently mentioned in entertainment articles</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="MEDIA COVERAGE">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake receives significant media coverage</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="PROFESSIONAL ACHIEVEMENTS">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake is highlighted for his professional achievements</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="PERSONAL LIVES">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake's personal life is of public interest</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="CULTURAL IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake has a significant cultural impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="ECONOMIC IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake has a significant economic impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="MEDIA REACTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake's activities generate media reactions</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="NA&#207;VE RAG">
      <data key="d4">8.0</data>
      <data key="d5">Na&#239;ve RAG mentions Justin Timberlake</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="NA&#207;VE RAG" target="ENTERTAINMENT ARTICLES">
      <data key="d4">8.0</data>
      <data key="d5">Na&#239;ve RAG provides a list of public figures mentioned in entertainment articles</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="NA&#207;VE RAG" target="GAO ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Gao et al., 2023 is a study on Na&#239;ve RAG</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="CONTEXT WINDOW SIZE" target="KURATOV ET AL., 2024">
      <data key="d4">6.0</data>
      <data key="d5">Kuratov et al. discuss the potential for information to be lost in the middle of longer contexts</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="CONTEXT WINDOW SIZE" target="LIU ET AL., 2023">
      <data key="d4">6.0</data>
      <data key="d5">Liu et al. discuss the potential for information to be lost in the middle of longer contexts</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="LIU ET AL., 2023" target="DYLAN">
      <data key="d4">8.0</data>
      <data key="d5">Liu et al., 2023 discusses DyLAN and its application in using FMs to score the response quality of nodes in each layer to prune the connections</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="GPT-4" target="HALLUCINATION JUDGE">
      <data key="d4">15.0</data>
      <data key="d5">GPT-4 is used in the Hallucination Judge process to evaluate generated summaries. This process leverages the advanced capabilities of GPT-4 to assess the accuracy and coherence of AI-generated content, ensuring that the summaries produced are reliable and free from errors. The integration of GPT-4 in this evaluative role highlights its importance in maintaining the quality and trustworthiness of AI-generated information.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GPT-4" target="LANGUAGE AGENT TREE SEARCH (LATS)">
      <data key="d4">9.0</data>
      <data key="d5">LATS uses GPT-4 in its experimental evaluation, achieving state-of-the-art pass@1 accuracy for programming on HumanEval.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="GPT-4" target="LATS">
      <data key="d4">18.0</data>
      <data key="d5">GPT-4, in conjunction with LATS, has demonstrated high performance across various benchmarks. Notably, this combination achieved a 92.7 Pass@1 rate on HumanEval, showcasing its effectiveness. Additionally, the integration of LATS with GPT-4 has been instrumental in excelling in other benchmarks such as HotPotQA, further highlighting the robust capabilities of this pairing in the AI and ML landscape.</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-4" target="REACT">
      <data key="d4">8.0</data>
      <data key="d5">ReAct is used with GPT-4 to enhance reasoning and acting capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-4" target="REFLEXION">
      <data key="d4">8.0</data>
      <data key="d5">Reflexion is used with GPT-4 to enhance reasoning capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-4" target="HUMANEVAL">
      <data key="d4">34.0</data>
      <data key="d5">GPT-4 is evaluated on the HumanEval benchmark to measure its Pass@1 accuracy. It is used to evaluate performance on the HumanEval dataset and has set the state of the art for HumanEval with LATS.</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-4" target="META AGENT SEARCH">
      <data key="d4">41.0</data>
      <data key="d5">Meta Agent Search leverages GPT-4 extensively in its operations. It transfers discovered agents from GPT-3.5 to GPT-4, utilizing GPT-4 as the language model for the meta agent. Additionally, GPT-4 serves as the meta agent itself and is employed to evaluate the performance of the discovered agents. This integration underscores the pivotal role of GPT-4 in enhancing the capabilities and evaluation processes within Meta Agent Search.</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="GPT-4" target="OPENAI">
      <data key="d4">1.0</data>
      <data key="d5">OpenAI is the organization behind the GPT-4 model</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="GPT-4" target="SYNTHETIC DATA">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4 is used to generate responses to prompts for creating synthetic data</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="GPT-4" target="AGENTINSTRUCT">
      <data key="d4">16.0</data>
      <data key="d5">AgentInstruct uses GPT-4 to generate high-quality data</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="GPT-4" target="ORCA-BENCH">
      <data key="d4">10.0</data>
      <data key="d5">GPT-4 is utilized as a baseline for scoring the performance of models on the ORCA-BENCH dataset. ORCA-BENCH scores are evaluated relative to GPT-4, which serves as the benchmark with a score of 10. This indicates that GPT-4 is a pivotal reference point in assessing the efficacy of other models within the ORCA-BENCH framework, highlighting its significance in the AI and ML community for performance evaluation.</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="ORCA-3">
      <data key="d4">15.0</data>
      <data key="d5">Orca-3 is evaluated against GPT-4 on various benchmarks. Orca-3's performance is compared to GPT-4 in various benchmarks.</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="AGIEVAL">
      <data key="d4">7.0</data>
      <data key="d5">AGIEval is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="MMLU">
      <data key="d4">7.0</data>
      <data key="d5">MMLU is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="ARC">
      <data key="d4">7.0</data>
      <data key="d5">ARC is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="BBH">
      <data key="d4">7.0</data>
      <data key="d5">BBH is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="GPQA">
      <data key="d4">7.0</data>
      <data key="d5">GPQA is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="DROP">
      <data key="d4">7.0</data>
      <data key="d5">DROP is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="GSM8K">
      <data key="d4">7.0</data>
      <data key="d5">GSM8K is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="FOFO">
      <data key="d4">15.0</data>
      <data key="d5">FOFO is a benchmark used to evaluate the performance of GPT-4. It utilizes GPT-4 to assess the format correctness of model responses, ensuring that the outputs generated by GPT-4 adhere to specified standards and criteria. This dual role highlights FOFO's importance in both testing and leveraging GPT-4's capabilities to maintain high-quality performance metrics.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="BENCHMARK RESULTS">
      <data key="d4">7.0</data>
      <data key="d5">Benchmark results section evaluates GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="METRIC-V2">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v2 is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="METRIC-V1">
      <data key="d4">1.0</data>
      <data key="d5">Metric-v1 is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="ORCA-3-7B">
      <data key="d4">17.0</data>
      <data key="d5">GPT-4 is utilized to evaluate the summarization and hallucination rates of ORCA-3-7B. Additionally, ORCA-3-7B's performance is benchmarked against GPT-4 across various metrics. This comparative analysis highlights the strengths and weaknesses of ORCA-3-7B in relation to the well-established capabilities of GPT-4, providing valuable insights into the effectiveness and reliability of both models in different AI and ML applications.</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="GPT-4" target="APPENDIX B">
      <data key="d4">6.0</data>
      <data key="d5">Appendix B contains the prompts used by GPT-4 for evaluating summarization abilities</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="GPT-4" target="CO-T">
      <data key="d4">1.0</data>
      <data key="d5">GPT-4 uses the CoT skill for medical question answering in the MIRAGE benchmark</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="GPT-4" target="MIRAGE">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4 is used in the evaluation of MIRAGE datasets</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="GPT-4" target="AZURE">
      <data key="d4">14.0</data>
      <data key="d5">Azure is recommended for reviewing transparency notes related to GPT-4</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="GPT-4" target="MULTIPLE CHOICE QUESTIONS">
      <data key="d4">9.0</data>
      <data key="d5">GPT-4 is used for extracting the option selected by the model from the model&#8217;s response in multiple choice questions</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="GPT-4" target="EVALUATOR ASSISTANT">
      <data key="d4">1.0</data>
      <data key="d5">GPT-4 assists the Evaluator Assistant in parsing student responses</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="GPT-4" target="SYSTEM MESSAGE">
      <data key="d4">9.0</data>
      <data key="d5">The system message is used to guide GPT-4 in extracting student responses</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="GPT-4" target="EXACT MATCH/SPAN EXTRACTION PROBLEMS">
      <data key="d4">9.0</data>
      <data key="d5">GPT-4 is used to extract exact answers and match them with ground-truth answers in exact match/span extraction problems</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="GPT-4" target="EQBENCH">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4 is used to extract and calibrate emotion scores in EQBench tasks</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="GPT-4" target="IFEVAL">
      <data key="d4">8.0</data>
      <data key="d5">IFEval uses GPT-4 to check if the model response follows verifiable instructions.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="GPT-4" target="ALPACAEVAL">
      <data key="d4">8.0</data>
      <data key="d5">AlpacaEval uses GPT-4-turbo to measure win-rates by comparing model outputs.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="GPT-4" target="INFOBENCH">
      <data key="d4">8.0</data>
      <data key="d5">InfoBench uses GPT-4 to determine if the model response follows decomposed instructions.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="GPT-4" target="QUALITY JUDGE">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4 is used in the Quality Judge process to evaluate the quality of AI-generated responses.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="GPT-4" target="HALLUCINATION DETECTION">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4 is used in the Hallucination Detection process to evaluate AI-generated summaries.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="GPT-4" target="SUMMARIZATION QUALITY">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4 is used in the Summarization Quality process to evaluate the quality of AI-generated summaries.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="HALLUCINATION JUDGE" target="PROMPT TEMPLATE">
      <data key="d4">8.0</data>
      <data key="d5">The Hallucination Judge process uses a specific prompt template to evaluate hallucination in AI-generated summaries.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="KOESTEN, L." target="GREGORY, K.">
      <data key="d4">24.0</data>
      <data key="d5">Koesten, L. and Gregory, K. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="KOESTEN, L." target="GROTH, P.">
      <data key="d4">16.0</data>
      <data key="d5">Koesten, L. and Groth, P. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="KOESTEN, L." target="SIMPERL, E.">
      <data key="d4">16.0</data>
      <data key="d5">Koesten, L. and Simperl, E. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GREGORY, K." target="GROTH, P.">
      <data key="d4">16.0</data>
      <data key="d5">Gregory, K. and Groth, P. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GREGORY, K." target="SIMPERL, E.">
      <data key="d4">16.0</data>
      <data key="d5">Gregory, K. and Simperl, E. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GROTH, P." target="SIMPERL, E.">
      <data key="d4">16.0</data>
      <data key="d5">Groth, P. and Simperl, E. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="KURATOV, Y." target="BULATOV, A.">
      <data key="d4">16.0</data>
      <data key="d5">Kuratov, Y. and Bulatov, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="KURATOV, Y." target="ANOKHIN, P.">
      <data key="d4">16.0</data>
      <data key="d5">Kuratov, Y. and Anokhin, P. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="KURATOV, Y." target="SOROKIN, D.">
      <data key="d4">16.0</data>
      <data key="d5">Kuratov, Y. and Sorokin, D. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="KURATOV, Y." target="SOROKIN, A.">
      <data key="d4">16.0</data>
      <data key="d5">Kuratov, Y. and Sorokin, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="KURATOV, Y." target="BURTSEV, M.">
      <data key="d4">16.0</data>
      <data key="d5">Kuratov, Y. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="BULATOV, A." target="ANOKHIN, P.">
      <data key="d4">16.0</data>
      <data key="d5">Bulatov, A. and Anokhin, P. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="BULATOV, A." target="SOROKIN, D.">
      <data key="d4">8.0</data>
      <data key="d5">Bulatov, A. and Sorokin, D. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="BULATOV, A." target="SOROKIN, A.">
      <data key="d4">8.0</data>
      <data key="d5">Bulatov, A. and Sorokin, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="BULATOV, A." target="BURTSEV, M.">
      <data key="d4">8.0</data>
      <data key="d5">Bulatov, A. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="ANOKHIN, P." target="SOROKIN, D.">
      <data key="d4">8.0</data>
      <data key="d5">Anokhin, P. and Sorokin, D. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="ANOKHIN, P." target="SOROKIN, A.">
      <data key="d4">8.0</data>
      <data key="d5">Anokhin, P. and Sorokin, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="ANOKHIN, P." target="BURTSEV, M.">
      <data key="d4">8.0</data>
      <data key="d5">Anokhin, P. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="SOROKIN, D." target="SOROKIN, A.">
      <data key="d4">8.0</data>
      <data key="d5">Sorokin, D. and Sorokin, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="SOROKIN, D." target="BURTSEV, M.">
      <data key="d4">8.0</data>
      <data key="d5">Sorokin, D. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="SOROKIN, A." target="BURTSEV, M.">
      <data key="d4">8.0</data>
      <data key="d5">Sorokin, A. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="LANGCHAIN" target="LLAMAINDEX">
      <data key="d4">7.0</data>
      <data key="d5">LangChain and LlamaIndex both support various graph databases and graph-based RAG applications</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LANGCHAIN" target="NEO4J">
      <data key="d4">7.0</data>
      <data key="d5">LangChain supports the Neo4J graph database</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LANGCHAIN" target="NEBULAGRAPH">
      <data key="d4">7.0</data>
      <data key="d5">LangChain supports the NebulaGraph graph database</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LANGCHAIN" target="GRAPH DATABASES">
      <data key="d4">7.0</data>
      <data key="d5">LangChain supports various graph databases</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LANGCHAIN" target="GRAPH-BASED RAG APPLICATIONS">
      <data key="d4">7.0</data>
      <data key="d5">LangChain supports graph-based RAG applications</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LANGCHAIN" target="SEARCH SPACE">
      <data key="d4">14.0</data>
      <data key="d5">LangChain is an open-source agent framework that can be used within the search space</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="LANGCHAIN" target="SEARCH ENGINE TOOLS">
      <data key="d4">1.0</data>
      <data key="d5">Search engine tools are components that can be used in open-source agent frameworks like LangChain in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="LASKAR, M. T. R." target="HOQUE, E.">
      <data key="d4">8.0</data>
      <data key="d5">Laskar, M. T. R. and Hoque, E. co-authored the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="LASKAR, M. T. R." target="HUANG, J.">
      <data key="d4">8.0</data>
      <data key="d5">Laskar, M. T. R. and Huang, J. co-authored the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="HOQUE, E." target="HUANG, J.">
      <data key="d4">1.0</data>
      <data key="d5">Hoque, E. and Huang, J. co-authored the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GENERATION-AUGMENTED RETRIEVAL (GAR)" target="MAO ET AL., 2020">
      <data key="d4">8.0</data>
      <data key="d5">Mao et al., 2020 is a study on generation-augmented retrieval</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="SELF-MEMORY (SELFMEM)" target="CHENG ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Cheng et al., 2024 is a study on self-memory</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="ITERATIVE RETRIEVAL-GENERATION (ITER-RETGEN)" target="SHAO ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Shao et al., 2023 is a study on iterative retrieval-generation</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="FEDERATED RETRIEVAL-GENERATION (FEB4RAG)" target="WANG ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Wang et al., 2024 is a study on federated retrieval-generation</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTI-DOCUMENT SUMMARIZATION" target="SU ET AL., 2020">
      <data key="d4">8.0</data>
      <data key="d5">Su et al., 2020 is a study on multi-document summarization</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTI-HOP QUESTION ANSWERING" target="MULTI-HOP QUESTION ANSWERING (ITRG)">
      <data key="d4">8.0</data>
      <data key="d5">ITRG is a system for multi-hop question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTI-HOP QUESTION ANSWERING" target="MULTI-HOP QUESTION ANSWERING (IR-COT)">
      <data key="d4">8.0</data>
      <data key="d5">IR-CoT is a system for multi-hop question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTI-HOP QUESTION ANSWERING" target="MULTI-HOP QUESTION ANSWERING (DSP)">
      <data key="d4">8.0</data>
      <data key="d5">DSP is a system for multi-hop question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTI-HOP QUESTION ANSWERING" target="FENG ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Feng et al., 2023 is a study on multi-hop question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTI-HOP QUESTION ANSWERING" target="TRIVEDI ET AL., 2022">
      <data key="d4">8.0</data>
      <data key="d5">Trivedi et al., 2022 is a study on multi-hop question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTI-HOP QUESTION ANSWERING" target="KHATTAB ET AL., 2022">
      <data key="d4">8.0</data>
      <data key="d5">Khattab et al., 2022 is a study on multi-hop question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="HIERARCHICAL INDEX" target="HIERARCHICAL INDEX OF TEXT CHUNKS">
      <data key="d4">8.0</data>
      <data key="d5">A hierarchical index of text chunks is a type of hierarchical index</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="HIERARCHICAL INDEX" target="SARTHI ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Sarthi et al., 2024 is a study on hierarchical index</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LLAMAINDEX" target="NEO4J">
      <data key="d4">7.0</data>
      <data key="d5">LlamaIndex supports the Neo4J graph database</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LLAMAINDEX" target="NEBULAGRAPH">
      <data key="d4">7.0</data>
      <data key="d5">LlamaIndex supports the NebulaGraph graph database</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LLAMAINDEX" target="GRAPH DATABASES">
      <data key="d4">7.0</data>
      <data key="d5">LlamaIndex supports various graph databases</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LLAMAINDEX" target="GRAPH-BASED RAG APPLICATIONS">
      <data key="d4">7.0</data>
      <data key="d5">LlamaIndex supports graph-based RAG applications</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="NEO4J" target="GRAPH-BASED RAG APPLICATIONS">
      <data key="d4">8.0</data>
      <data key="d5">Neo4J is a format used in graph-based RAG applications</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="NEO4J" target="NALLM">
      <data key="d4">8.0</data>
      <data key="d5">NaLLM uses Neo4J format for creating and reasoning over knowledge graphs</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="SENSEMAKING" target="SENSEMAKING ACTIVITY">
      <data key="d4">8.0</data>
      <data key="d5">Sensemaking activity involves sensemaking</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="TREE OF CLARIFICATIONS" target="KIM ET AL., 2023">
      <data key="d4">1.0</data>
      <data key="d5">Kim et al., 2023 is a study on tree of clarifications</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPHRAG" target="NEBULA-GRAPH">
      <data key="d4">8.0</data>
      <data key="d5">GraphRAG uses Nebula-Graph format for creating and reasoning over knowledge graphs</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="TRAJANOSKA ET AL., 2023" target="YAO ET AL., 2023">
      <data key="d4">7.0</data>
      <data key="d5">Both studies focus on using LLMs for knowledge graph creation and completion</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="BAN ET AL., 2023" target="ZHANG ET AL., 2024">
      <data key="d4">7.0</data>
      <data key="d5">Both studies focus on using LLMs for the extraction of causal graphs</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="BAEK ET AL., 2023" target="HE ET AL., 2024">
      <data key="d4">7.0</data>
      <data key="d5">Both studies focus on advanced RAG methods</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="ZHANG, 2023" target="KANG ET AL., 2023">
      <data key="d4">7.0</data>
      <data key="d5">Both studies focus on advanced RAG methods</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="RANADE AND JOSHI, 2023" target="WANG ET AL., 2023B">
      <data key="d4">7.0</data>
      <data key="d5">Both studies focus on advanced RAG methods</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="WANG ET AL., 2023B" target="SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT (COT-SC)">
      <data key="d4">14.0</data>
      <data key="d5">"Wang et al., 2023b is a publication that introduced the Self-Consistency with Chain-of-Thought (COT-SC) method. This method, referenced in the publication, aims to enhance the reliability and accuracy of AI and ML models by incorporating a self-consistency mechanism within the chain-of-thought framework."</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="WANG ET AL., 2023B" target="COT-SC">
      <data key="d4">14.0</data>
      <data key="d5">Wang et al., 2023b discusses the COT-SC method. The publication by Wang et al. in 2023 is related to COT-SC.</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="RAM ET AL., 2023" target="RAG APPROACHES AND SYSTEMS">
      <data key="d4">8.0</data>
      <data key="d5">Ram et al., 2023 is a study on RAG approaches and systems</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GAO ET AL., 2023" target="GSM-HARD">
      <data key="d4">8.0</data>
      <data key="d5">Gao et al., 2023 discusses the GSM-Hard dataset</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="WANG ET AL., 2024" target="META AGENT SEARCH">
      <data key="d4">16.0</data>
      <data key="d5">Wang et al., 2024 discusses the effectiveness of Meta Agent Search</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="AD-HOC LLM USE" target="QUOTES">
      <data key="d4">7.0</data>
      <data key="d5">Quotes are used in ad-hoc LLM use</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="AD-HOC LLM USE" target="CITATIONS">
      <data key="d4">7.0</data>
      <data key="d5">Citations are used in ad-hoc LLM use</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="AD-HOC LLM USE" target="EXAMPLES">
      <data key="d4">7.0</data>
      <data key="d5">Examples are used in ad-hoc LLM use</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="RAG APPROACHES AND SYSTEMS" target="PRE-RETRIEVAL STRATEGIES">
      <data key="d4">7.0</data>
      <data key="d5">Pre-retrieval strategies are part of RAG approaches and systems</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="RAG APPROACHES AND SYSTEMS" target="POST-RETRIEVAL STRATEGIES">
      <data key="d4">7.0</data>
      <data key="d5">Post-retrieval strategies are part of RAG approaches and systems</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="TEXT-RELATIONSHIP GRAPHS" target="GRAPH DATABASES">
      <data key="d4">7.0</data>
      <data key="d5">Text-relationship graphs are supported by graph databases</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH-BASED RAG APPLICATIONS" target="NEBULA-GRAPH">
      <data key="d4">8.0</data>
      <data key="d5">Nebula-Graph is a format used in graph-based RAG applications</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="ARXIV">
      <data key="d4">8.0</data>
      <data key="d5">arXiv is the repository where the GPT-4 technical report is published</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="J. ACHIAM">
      <data key="d4">8.0</data>
      <data key="d5">J. Achiam is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="S. ADLER">
      <data key="d4">8.0</data>
      <data key="d5">S. Adler is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="S. AGARWAL">
      <data key="d4">8.0</data>
      <data key="d5">S. Agarwal is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="L. AHMAD">
      <data key="d4">8.0</data>
      <data key="d5">L. Ahmad is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="I. AKKAYA">
      <data key="d4">8.0</data>
      <data key="d5">I. Akkaya is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="F. L. ALEMAN">
      <data key="d4">8.0</data>
      <data key="d5">F. L. Aleman is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="D. ALMEIDA">
      <data key="d4">8.0</data>
      <data key="d5">D. Almeida is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="J. ALTENSCHMIDT">
      <data key="d4">8.0</data>
      <data key="d5">J. Altenschmidt is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="S. ALTMAN">
      <data key="d4">8.0</data>
      <data key="d5">S. Altman is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="S. ANADKAT">
      <data key="d4">8.0</data>
      <data key="d5">S. Anadkat is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="KNOWLEDGE-AUGMENTED LANGUAGE MODEL PROMPTING" target="J. BAEK">
      <data key="d4">8.0</data>
      <data key="d5">J. Baek is an author of the paper on knowledge-augmented language model prompting</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="KNOWLEDGE-AUGMENTED LANGUAGE MODEL PROMPTING" target="A. F. AJI">
      <data key="d4">8.0</data>
      <data key="d5">A. F. Aji is an author of the paper on knowledge-augmented language model prompting</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="KNOWLEDGE-AUGMENTED LANGUAGE MODEL PROMPTING" target="A. SAFFARI">
      <data key="d4">8.0</data>
      <data key="d5">A. Saffari is an author of the paper on knowledge-augmented language model prompting</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="QUERY TOOLS" target="T. BAN">
      <data key="d4">8.0</data>
      <data key="d5">T. Ban is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="QUERY TOOLS" target="L. CHEN">
      <data key="d4">8.0</data>
      <data key="d5">L. Chen is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="QUERY TOOLS" target="X. WANG">
      <data key="d4">8.0</data>
      <data key="d5">X. Wang is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="QUERY TOOLS" target="H. CHEN">
      <data key="d4">8.0</data>
      <data key="d5">H. Chen is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="QUERY FOCUSED ABSTRACTIVE SUMMARIZATION" target="T. BAUMEL">
      <data key="d4">8.0</data>
      <data key="d5">T. Baumel is an author of the paper on query focused abstractive summarization</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="QUERY FOCUSED ABSTRACTIVE SUMMARIZATION" target="M. EYAL">
      <data key="d4">8.0</data>
      <data key="d5">M. Eyal is an author of the paper on query focused abstractive summarization</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="QUERY FOCUSED ABSTRACTIVE SUMMARIZATION" target="M. ELHADAD">
      <data key="d4">8.0</data>
      <data key="d5">M. Elhadad is an author of the paper on query focused abstractive summarization</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="FAST UNFOLDING OF COMMUNITIES" target="V. D. BLONDEL">
      <data key="d4">8.0</data>
      <data key="d5">V. D. Blondel is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="FAST UNFOLDING OF COMMUNITIES" target="J.-L. GUILLAUME">
      <data key="d4">8.0</data>
      <data key="d5">J.-L. Guillaume is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="FAST UNFOLDING OF COMMUNITIES" target="R. LAMBIOTTE">
      <data key="d4">8.0</data>
      <data key="d5">R. Lambiotte is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="FAST UNFOLDING OF COMMUNITIES" target="E. LEFEBVRE">
      <data key="d4">1.0</data>
      <data key="d5">E. Lefebvre is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="PYTHON" target="MBPP">
      <data key="d4">8.0</data>
      <data key="d5">Python is the programming language used in the MBPP dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="PYTHON" target="BOYER &amp; MOORE (1983)">
      <data key="d4">16.0</data>
      <data key="d5">Boyer &amp; Moore (1983) discusses Turing Completeness</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="PYTHON" target="LADHA (2024)">
      <data key="d4">16.0</data>
      <data key="d5">Ladha (2024) discusses Turing Completeness</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="ARXIV" target="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)">
      <data key="d4">16.0</data>
      <data key="d5">The paper on Automated Design of Agentic Systems is published on arXiv</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="CHEN, W." target="DUAN, N.">
      <data key="d4">16.0</data>
      <data key="d5">Duan, N. and Chen, W. co-authored the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"</data>
      <data key="d6">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
    </edge>
    <edge source="SU, D." target="XU, Y.">
      <data key="d4">16.0</data>
      <data key="d5">Su, D. and Xu, Y. co-authored the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management"</data>
      <data key="d6">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
    </edge>
    <edge source="SU, D." target="YU, T.">
      <data key="d4">16.0</data>
      <data key="d5">Su, D. and Yu, T. co-authored the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management"</data>
      <data key="d6">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
    </edge>
    <edge source="SU, D." target="SIDDIQUE, F. B.">
      <data key="d4">16.0</data>
      <data key="d5">Su, D. and Siddique, F. B. co-authored the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management"</data>
      <data key="d6">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
    </edge>
    <edge source="SU, D." target="BAREZI, E. J.">
      <data key="d4">16.0</data>
      <data key="d5">Su, D. and Barezi, E. J. co-authored the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management"</data>
      <data key="d6">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="GPT-3.5">
      <data key="d4">9.0</data>
      <data key="d5">LATS uses GPT-3.5 in its experimental evaluation, demonstrating gradient-free performance for web navigation on WebShop.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="MONTE CARLO TREE SEARCH (MCTS)">
      <data key="d4">9.0</data>
      <data key="d5">LATS integrates Monte Carlo Tree Search (MCTS) to enhance decision-making capabilities of language models.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="REACT">
      <data key="d4">15.0</data>
      <data key="d5">Language Agent Tree Search (LATS) expands upon the ReAct technique to improve reasoning and decision-making. By addressing the shortcomings of ReAct, LATS enhances the capabilities of AI and ML systems, providing a more robust framework for complex problem-solving and decision-making processes.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8,9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="HUMANEVAL">
      <data key="d4">8.0</data>
      <data key="d5">LATS was evaluated on the HumanEval benchmark, achieving state-of-the-art pass@1 accuracy for programming with GPT-4.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="WEBSHOP">
      <data key="d4">8.0</data>
      <data key="d5">LATS was evaluated on the WebShop benchmark, demonstrating gradient-free performance for web navigation with GPT-3.5.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="GITHUB">
      <data key="d4">8.0</data>
      <data key="d5">The code for LATS is available on GitHub.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="PROCEEDINGS OF THE 41ST INTERNATIONAL CONFERENCE ON MACHINE LEARNING">
      <data key="d4">8.0</data>
      <data key="d5">The paper on LATS was presented at the Proceedings of the 41st International Conference on Machine Learning.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="CHAIN-OF-THOUGHT (COT) PROMPTING">
      <data key="d4">7.0</data>
      <data key="d5">Language Agent Tree Search (LATS) addresses the shortcomings of Chain-of-thought (CoT) prompting.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="TREE-OF-THOUGHT (TOT) PROMPTING">
      <data key="d4">7.0</data>
      <data key="d5">Language Agent Tree Search (LATS) addresses the shortcomings of Tree-of-thought (ToT) prompting.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="ANDY ZHOU" target="UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN">
      <data key="d4">8.0</data>
      <data key="d5">Andy Zhou is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="ANDY ZHOU" target="LAPIS LABS">
      <data key="d4">8.0</data>
      <data key="d5">Andy Zhou is affiliated with Lapis Labs.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="KAI YAN" target="UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN">
      <data key="d4">8.0</data>
      <data key="d5">Kai Yan is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="MICHAL SHLAPENTOKH-ROTHMAN" target="UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN">
      <data key="d4">8.0</data>
      <data key="d5">Michal Shlapentokh-Rothman is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="HAOHAN WANG" target="UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN">
      <data key="d4">8.0</data>
      <data key="d5">Haohan Wang is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="YU-XIONG WANG" target="UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN">
      <data key="d4">8.0</data>
      <data key="d5">Yu-Xiong Wang is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="GPT-3.5" target="LATS">
      <data key="d4">17.0</data>
      <data key="d5">GPT-3.5, in conjunction with LATS, has demonstrated high performance across various benchmarks. Specifically, the combination of GPT-3.5 and LATS has excelled in tasks such as HotPotQA, HumanEval, and WebShop. This synergy highlights the effectiveness of LATS in enhancing the capabilities of GPT-3.5, making it a powerful tool in achieving superior results in diverse evaluation scenarios.</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="REACT">
      <data key="d4">8.0</data>
      <data key="d5">ReAct is used with GPT-3.5 to enhance reasoning and acting capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="REFLEXION">
      <data key="d4">8.0</data>
      <data key="d5">Reflexion is used with GPT-3.5 to enhance reasoning capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="TOT">
      <data key="d4">8.0</data>
      <data key="d5">ToT is used with GPT-3.5 to enhance decision-making capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="RAP">
      <data key="d4">8.0</data>
      <data key="d5">RAP is used with GPT-3.5 to enhance decision-making capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="COT">
      <data key="d4">8.0</data>
      <data key="d5">CoT is used with GPT-3.5 to enhance reasoning capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="COT-SC">
      <data key="d4">8.0</data>
      <data key="d5">CoT-SC is used with GPT-3.5 to enhance reasoning capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="HUMANEVAL">
      <data key="d4">25.0</data>
      <data key="d5">GPT-3.5 is evaluated on the HumanEval benchmark to measure its Pass@1 accuracy. This evaluation involves using GPT-3.5 to assess its performance on the HumanEval dataset, which is designed to test the model's ability to generate correct solutions to programming problems. The Pass@1 metric specifically measures the accuracy of the model's first attempt at solving each problem, providing a clear indicator of its effectiveness in code generation tasks.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="MBPP">
      <data key="d4">8.0</data>
      <data key="d5">GPT-3.5 is used to evaluate the performance of various prompting methods on MBPP</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="GPT-3.5" target="WEBSHOP">
      <data key="d4">8.0</data>
      <data key="d5">GPT-3.5 is used in WebShop for acting-based prompting methods</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="GPT-3.5" target="GAME OF 24">
      <data key="d4">8.0</data>
      <data key="d5">GPT-3.5 is used to perform experiments in the Game of 24</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="GPT-3.5" target="META AGENT SEARCH">
      <data key="d4">41.0</data>
      <data key="d5">Meta Agent Search utilizes GPT-3.5 to evaluate the performance of discovered agents and baselines. The discovered agents in Meta Agent Search are assessed using GPT-3.5, and the results are subsequently transferred to GPT-4 for further analysis. This process ensures a comprehensive evaluation and seamless transition between different versions of the GPT models.</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="GPT-3.5" target="OPENAI">
      <data key="d4">8.0</data>
      <data key="d5">OpenAI is the organization behind the GPT-3.5 model</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="GPT-3.5" target="NEURAL ARCHITECTURE SEARCH">
      <data key="d4">14.0</data>
      <data key="d5">Neural Architecture Search shows insights into Neural Networks, and GPT-3.5 was used to evaluate agentic systems</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="GPT-3.5" target="ORCA-3">
      <data key="d4">16.0</data>
      <data key="d5">Orca-3 outperforms GPT-3.5 on multiple benchmarks</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="LATS">
      <data key="d4">9.0</data>
      <data key="d5">LATS is based on Monte Carlo Tree Search to enhance language model performance</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="SILVER ET AL., 2017">
      <data key="d4">8.0</data>
      <data key="d5">Silver et al., 2017 is the reference for the success of Monte Carlo Tree Search in model-based reinforcement learning</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="TREE-OF-THOUGHT (TOT) PROMPTING">
      <data key="d4">8.0</data>
      <data key="d5">Tree-of-thought (ToT) prompting uses search algorithms like those in Monte Carlo Tree Search (MCTS) to explore multiple reasoning paths.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="UPPER CONFIDENCE BOUNDS APPLIED TO TREES (UCT)">
      <data key="d4">9.0</data>
      <data key="d5">Upper Confidence bounds applied to Trees (UCT) is a metric used in Monte Carlo Tree Search (MCTS) to select the best child state for expansion.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="ATARI">
      <data key="d4">8.0</data>
      <data key="d5">Monte Carlo Tree Search (MCTS) has been successfully applied in the Atari decision-making environment.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="GO">
      <data key="d4">8.0</data>
      <data key="d5">Monte Carlo Tree Search (MCTS) has been successfully applied in the Go decision-making environment.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="SILVER ET AL. (2016)">
      <data key="d4">8.0</data>
      <data key="d5">Silver et al. demonstrated the success of Monte Carlo Tree Search (MCTS) in the game of Go.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="YE ET AL. (2021)">
      <data key="d4">1.0</data>
      <data key="d5">Ye et al. demonstrated the success of Monte Carlo Tree Search (MCTS) in the Atari environment.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REACT" target="LATS">
      <data key="d4">47.0</data>
      <data key="d5">LATS incorporates designs from ReAct for reasoning, acting, and planning. In the HotPotQA experiments, LATS is compared with ReAct and consistently outperforms the ReAct method. LATS surpasses ReAct in performance by expanding more nodes with principled search. ReAct serves as a framework within LATS, where the action space consists of permissible actions and reasoning traces.</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8,99d90aededb61e04241516ed9ec656cc,c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="REACT" target="YAO ET AL., 2023B">
      <data key="d4">23.0</data>
      <data key="d5">The paper by Yao et al. (2023b) discusses the ReAct method and its application in language models. It also evaluates the performance of the ReAct method on WebShop, providing a comprehensive reference for this innovative approach.</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="REACT" target="COT">
      <data key="d4">14.0</data>
      <data key="d5">ReAct and CoT are both prompting techniques used in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="REACT" target="YA0 ET AL.">
      <data key="d4">18.0</data>
      <data key="d5">Yao et al. contributed to the development of the ReAct technique</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="REACT" target="CHAIN-OF-THOUGHT (COT) PROMPTING">
      <data key="d4">8.0</data>
      <data key="d5">ReAct builds on the principles of Chain-of-thought (CoT) prompting by adding interactions with an external environment to the reasoning process.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REACT" target="TREE-OF-THOUGHT (TOT) PROMPTING">
      <data key="d4">7.0</data>
      <data key="d5">Both Tree-of-thought (ToT) prompting and ReAct extend the basic principles of Chain-of-thought (CoT) prompting to improve reasoning and decision-making.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REACT" target="LANGUAGE MODEL (LM)">
      <data key="d4">8.0</data>
      <data key="d5">ReAct uses a language model (LM) to generate actions and reasoning traces, enhanced by interactions with an external environment.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REACT" target="OBSERVATION O">
      <data key="d4">8.0</data>
      <data key="d5">Observation o is used in the ReAct process to improve reasoning and acting.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REACT" target="ACTION A">
      <data key="d4">8.0</data>
      <data key="d5">Action a is generated in the ReAct process based on observations from the environment.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REACT" target="RAP (HAO ET AL., 2023)">
      <data key="d4">7.0</data>
      <data key="d5">RAP is a reasoning-based method similar to ReAct.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REACT" target="WEI ET AL., 2022">
      <data key="d4">9.0</data>
      <data key="d5">Wei et al., 2022 introduced the ReAct prompting method</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="REACT" target="WEBSHOP">
      <data key="d4">8.0</data>
      <data key="d5">ReAct is a prompting method used in WebShop with GPT-3.5</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="REACT" target="PERFORMANCE">
      <data key="d4">7.0</data>
      <data key="d5">ReAct is evaluated for performance in the study</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="REACT" target="TOKEN CONSUMPTION">
      <data key="d4">7.0</data>
      <data key="d5">ReAct is evaluated for token consumption in the study</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="REACT" target="FIG. 4">
      <data key="d4">7.0</data>
      <data key="d5">Figure 4 illustrates how ReAct works on an example task of HotPotQA</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HUMANEVAL" target="LATS">
      <data key="d4">26.0</data>
      <data key="d5">LATS is evaluated using the HumanEval dataset and has set the state of the art on HumanEval with GPT-4.</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="HUMANEVAL" target="CHEN ET AL., 2021">
      <data key="d4">17.0</data>
      <data key="d5">Chen et al., 2021 introduced the HumanEval dataset, which serves as a reference for evaluating human-like performance in AI and machine learning models. This dataset is pivotal in assessing the capabilities of AI systems in generating human-like responses and solutions, thereby contributing significantly to the advancement of AI research and development.</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="HUMANEVAL" target="TAB. 4">
      <data key="d4">8.0</data>
      <data key="d5">Tab. 4 shows the performance of various methods on programming tasks using the HumanEval dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HUMANEVAL" target="APPENDIX SEC. D">
      <data key="d4">7.0</data>
      <data key="d5">Appendix Sec. D provides additional details on the evaluation of LATS and other methods on programming tasks using the HumanEval dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HUMANEVAL" target="CHEN ET AL., 2023A">
      <data key="d4">8.0</data>
      <data key="d5">Chen et al., 2023a discusses the use of an LM to generate a synthetic test suite for evaluating programming tasks using the HumanEval dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HUMANEVAL" target="REFLEXION">
      <data key="d4">7.0</data>
      <data key="d5">Reflexion is compared with LATS in the HumanEval experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HUMANEVAL" target="FIG. 3">
      <data key="d4">7.0</data>
      <data key="d5">Figure 3 shows the results of the HumanEval experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HUMANEVAL" target="SAMPLING SIZE">
      <data key="d4">8.0</data>
      <data key="d5">Sampling size is a parameter used in the HumanEval dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HUMANEVAL" target="TRAJECTORIES">
      <data key="d4">8.0</data>
      <data key="d5">Trajectories refer to the number of paths sampled in the HumanEval experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HUMANEVAL" target="PROGRAMMING">
      <data key="d4">8.0</data>
      <data key="d5">Programming tasks are evaluated using the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="HUMANEVAL" target="FUNCTION SIGNATURE">
      <data key="d4">8.0</data>
      <data key="d5">Function signatures are part of the programming problems in the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="HUMANEVAL" target="DOCSTRING">
      <data key="d4">8.0</data>
      <data key="d5">Docstrings are part of the programming problems in the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="HUMANEVAL" target="REFERENCE IMPLEMENTATION">
      <data key="d4">8.0</data>
      <data key="d5">Reference implementations are provided for programming problems in the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="HUMANEVAL" target="UNIT TESTS">
      <data key="d4">8.0</data>
      <data key="d5">Unit tests are provided to check the correctness of function implementations in the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="HUMANEVAL" target="NATURAL LANGUAGE DESCRIPTION">
      <data key="d4">8.0</data>
      <data key="d5">Natural language descriptions explain programming tasks in the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="HUMANEVAL" target="ALGORITHMS">
      <data key="d4">8.0</data>
      <data key="d5">Algorithms are evaluated through programming tasks in the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="HUMANEVAL" target="BASIC MATHEMATICS">
      <data key="d4">8.0</data>
      <data key="d5">Basic mathematics is evaluated through programming tasks in the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="LATS">
      <data key="d4">25.0</data>
      <data key="d5">LATS is evaluated on the WebShop benchmark to demonstrate its reasoning and acting capabilities. It is used in WebShop to improve score and success rate, surpassing RL-based training methods. LATS has significantly improved performance on the WebShop dataset, showcasing its effectiveness in enhancing outcomes within this specific benchmark.</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="WEBSHOP" target="YAO ET AL., 2022">
      <data key="d4">14.0</data>
      <data key="d5">Yao et al., 2022 is a reference related to WebShop and serves as the primary citation for the WebShop dataset.</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="WEBSHOP" target="IL">
      <data key="d4">7.0</data>
      <data key="d5">IL is used to train models for tasks in WebShop</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="WEBSHOP" target="RL">
      <data key="d4">7.0</data>
      <data key="d5">RL is used to train models for tasks in WebShop</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="WEBSHOP" target="FINE-TUNING">
      <data key="d4">15.0</data>
      <data key="d5">Fine-tuning is a method mentioned in the context of improving performance in WebShop. It is used to enhance model performance specifically for WebShop, indicating its importance in optimizing the platform's functionality.</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="WEBSHOP" target="EXPERT">
      <data key="d4">7.0</data>
      <data key="d5">WEBSHOP utilizes expert performance as a benchmark for comparison. In this context, "Expert" refers to human performance metrics that are employed as a standard within the WebShop platform. This benchmarking process helps in evaluating and improving the performance of various systems and algorithms used in WebShop by comparing them against established human performance metrics.</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="WEBSHOP" target="REFLEXION">
      <data key="d4">7.0</data>
      <data key="d5">Reflexion is a prompting method used in WebShop, similar to ReAct</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="IMPROVEMENT LEARNING (IL)">
      <data key="d4">7.0</data>
      <data key="d5">IL is a method used in WebShop for training agents, mentioned in comparison with other methods</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="REINFORCEMENT LEARNING (RL)">
      <data key="d4">7.0</data>
      <data key="d5">RL is a method used in WebShop for training agents, mentioned in comparison with other methods</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="PROMPTING">
      <data key="d4">8.0</data>
      <data key="d5">Prompting methods are used to guide the behavior of models like GPT-3.5 in WebShop</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="SEARCH AND CLICK COMMANDS">
      <data key="d4">7.0</data>
      <data key="d5">Search and click commands are part of the preconstructed action space used in WebShop</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="BROWSER FEEDBACK">
      <data key="d4">7.0</data>
      <data key="d5">Browser feedback is used in WebShop as part of the observation mechanism for agents</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="REFLECTIONS">
      <data key="d4">7.0</data>
      <data key="d5">Reflections are used in WebShop as part of the observation mechanism for agents</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="SUCCESS RATE (SR)">
      <data key="d4">8.0</data>
      <data key="d5">Success Rate (SR) is a metric used in WebShop to indicate the frequency with which the chosen product fulfills all given conditions</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="AVERAGE SCORE">
      <data key="d4">8.0</data>
      <data key="d5">Average Score is a metric used in WebShop to reflect the percentage of user-specified attributes met by the selected product</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="MBPP">
      <data key="d4">12.0</data>
      <data key="d5">MBPP is a benchmark used to evaluate program synthesis techniques, while WebShop evaluates agents on grounded language understanding and decision-making.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="AMAZON">
      <data key="d4">16.0</data>
      <data key="d5">Amazon is the source of over 1 million real-world products used in the WebShop environment.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="TASK SCORE">
      <data key="d4">16.0</data>
      <data key="d5">Task Score is one of the evaluation metrics used in the WebShop environment.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="SUCCESS RATE">
      <data key="d4">9.0</data>
      <data key="d5">Success Rate is one of the evaluation metrics used in the WebShop environment.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="QUERY SEARCHES">
      <data key="d4">8.0</data>
      <data key="d5">Query searches are actions in WebShop that allow agents to search for products.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="BUTTON CLICKS">
      <data key="d4">8.0</data>
      <data key="d5">Button clicks are actions in WebShop that allow agents to interact with the web interface.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="HTML MODE">
      <data key="d4">8.0</data>
      <data key="d5">HTML mode in WebShop provides pixel-level observations for agents.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="SIMPLE MODE">
      <data key="d4">8.0</data>
      <data key="d5">Simple mode in WebShop converts raw HTML into structured text observations.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="LEXICAL MATCHING">
      <data key="d4">8.0</data>
      <data key="d5">Lexical matching is used in WebShop to compare purchased products against specified attributes.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="SEMANTIC SIMILARITY">
      <data key="d4">1.0</data>
      <data key="d5">Semantic similarity is used in WebShop to compare purchased products against specified attributes based on meaning.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="GAME OF 24">
      <data key="d4">6.0</data>
      <data key="d5">Both WebShop and Game of 24 are used as environments for experiments involving language models</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="WEBSHOP" target="BRIGHT CITRUS DEODORANT">
      <data key="d4">8.0</data>
      <data key="d5">Webshop is a platform where users can search for and purchase Bright Citrus Deodorant</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="WEBSHOP" target="GINGER FRESH DEODORANT">
      <data key="d4">8.0</data>
      <data key="d5">Webshop is a platform where users can search for and purchase Ginger Fresh Deodorant</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="WEBSHOP" target="CEDAR &amp; PATCHOULI DEODORANT">
      <data key="d4">8.0</data>
      <data key="d5">Webshop is a platform where users can search for and purchase Cedar &amp; Patchouli Deodorant</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="WEBSHOP" target="ACTING PROMPT">
      <data key="d4">8.0</data>
      <data key="d5">The ACTING PROMPT guides actions in the WEBSHOP</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="WEBSHOP" target="SEARCH">
      <data key="d4">8.0</data>
      <data key="d5">SEARCH is an action performed in the WEBSHOP to find products</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="PROCEEDINGS OF THE 41ST INTERNATIONAL CONFERENCE ON MACHINE LEARNING" target="PMLR 235">
      <data key="d4">8.0</data>
      <data key="d5">The paper on LATS was published in volume 235 of the Proceedings of the 41st International Conference on Machine Learning.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="PROCEEDINGS OF THE 41ST INTERNATIONAL CONFERENCE ON MACHINE LEARNING" target="2024">
      <data key="d4">1.0</data>
      <data key="d5">The paper on LATS was presented in the year 2024.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="PROCEEDINGS OF THE 41ST INTERNATIONAL CONFERENCE ON MACHINE LEARNING" target="PMLR">
      <data key="d4">1.0</data>
      <data key="d5">PMLR published the Proceedings of the 41st International Conference on Machine Learning where the paper on LATS was presented.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="GITHUB" target="HTTPS://GITHUB.COM/LAPISROCKS/LANGUAGEAGENTTREESEARCH">
      <data key="d4">8.0</data>
      <data key="d5">The URL where the code for LATS is available on GitHub.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="GITHUB" target="META AGENT SEARCH">
      <data key="d4">1.0</data>
      <data key="d5">All code, prompts, and experiment results related to Meta Agent Search are available on GitHub</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="GITHUB" target="SHENGRAN HU">
      <data key="d4">15.0</data>
      <data key="d5">Shengran Hu has made the framework code available on GitHub and is associated with the GitHub repository containing all agents from the experiment.</data>
      <data key="d6">449db721e37968e073e3579b59e023b2,d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="WOOLDRIDGE" target="JENNINGS">
      <data key="d4">7.0</data>
      <data key="d5">Wooldridge and Jennings are co-referenced in the context of general autonomous agents capable of reasoning and decision-making in a variety of environments.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="CHOWDHERY ET AL." target="OPENAI">
      <data key="d4">7.0</data>
      <data key="d5">Chowdhery et al. and OpenAI are co-referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="OPENAI" target="GPT-4O-2024-05-13">
      <data key="d4">26.0</data>
      <data key="d5">GPT-4o-2024-05-13 is a model developed by OpenAI.</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="OPENAI" target="GPT-3.5-TURBO-0125">
      <data key="d4">11.0</data>
      <data key="d5">OpenAI, a leading organization in the field of artificial intelligence, developed the GPT-3.5-turbo-0125 model. This model, known as GPT-3.5-turbo-0125, represents a significant advancement in AI technology, showcasing OpenAI's commitment to pushing the boundaries of machine learning and natural language processing.</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="NALLAPATI ET AL." target="BOWMAN ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Nallapati et al. and Bowman et al. are co-referenced in the context of language models excelling in standard natural language processing tasks.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="COBBE ET AL." target="SAPAROV AND HE">
      <data key="d4">7.0</data>
      <data key="d5">Cobbe et al. and Saparov and He are co-referenced in the context of language models being adapted to tasks requiring advanced common-sense reasoning or quantitative skills.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="COBBE ET AL." target="GSM8K">
      <data key="d4">8.0</data>
      <data key="d5">Cobbe et al. are the authors of the GSM8K dataset</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="SCHICK ET AL." target="TOOL USE">
      <data key="d4">22.0</data>
      <data key="d5">Schick et al. are the authors of the Tool Use method and have made significant contributions to the research on tool use in agentic systems. Their work has been influential in understanding how tools can be utilized within these systems, providing valuable insights into the mechanisms and applications of tool use in artificial intelligence and machine learning contexts.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="GAO ET AL." target="SHINN ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Gao et al. and Shinn et al. are co-referenced in the context of prompting techniques that augment language models with feedback or observations from an external environment.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="GAO ET AL." target="GSM-HARD">
      <data key="d4">8.0</data>
      <data key="d5">Gao et al. are the authors of the GSM-Hard dataset</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="SHINN ET AL." target="SELF-REFLECTION">
      <data key="d4">14.0</data>
      <data key="d5">Shinn et al. contributed to the research on self-reflection in agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="SHINN ET AL." target="REFLECTION">
      <data key="d4">8.0</data>
      <data key="d5">Shinn et al. are the authors of the Reflection method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="SLOMAN" target="EVANS">
      <data key="d4">7.0</data>
      <data key="d5">Sloman and Evans are co-referenced in the context of the limitations of reflexive methods in language models compared to humans' deliberate and thoughtful decision-making characteristics.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="XIE ET AL." target="HAO ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Xie et al. and Hao et al. are co-referenced in the context of recent search-guided language model work that addresses the issue of planning and multiple reasoning paths.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="XIE ET AL." target="BEAM SEARCH">
      <data key="d4">16.0</data>
      <data key="d5">Xie et al. contributed to the understanding of beam search in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="HAO ET AL." target="RAP">
      <data key="d4">18.0</data>
      <data key="d5">Hao et al. contributed to the development of the RAP technique</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LATS" target="SEARCH ALGORITHMS">
      <data key="d4">17.0</data>
      <data key="d5">LATS leverages search algorithms to enhance the performance of language agents by constructing trajectories and incorporating external feedback. This approach allows LATS to adapt search algorithms effectively, ensuring that language agents can perform more efficiently and accurately.</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="NODES">
      <data key="d4">14.0</data>
      <data key="d5">LATS, an entity involved in the evaluation of the Game of 24, utilizes nodes to store and retrieve external feedback. These nodes refer to the sampled points in the search space, playing a crucial role in the assessment process of LATS.</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="PROMPTS">
      <data key="d4">7.0</data>
      <data key="d5">Prompts in LATS store and retrieve external feedback</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="EXTERNAL FEEDBACK">
      <data key="d4">16.0</data>
      <data key="d5">LATS incorporates external feedback to improve performance</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="INTERNAL REASONING PERFORMANCE">
      <data key="d4">7.0</data>
      <data key="d5">LATS aims to surpass internal reasoning performance</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="PRETRAINED LMS">
      <data key="d4">8.0</data>
      <data key="d5">LATS repurposes pretrained language models for value functions and self-reflections</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="LM-POWERED VALUE FUNCTIONS">
      <data key="d4">8.0</data>
      <data key="d5">LATS uses LM-powered value functions to guide exploration</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="SELF-REFLECTIONS">
      <data key="d4">8.0</data>
      <data key="d5">LATS uses self-reflections to improve exploration</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="IN-CONTEXT LEARNING">
      <data key="d4">8.0</data>
      <data key="d5">LATS leverages in-context learning abilities of modern language models</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="ENVIRONMENTAL CONDITIONS">
      <data key="d4">8.0</data>
      <data key="d5">LATS adapts planning to environmental conditions</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="REASONING">
      <data key="d4">8.0</data>
      <data key="d5">LATS incorporates reasoning to enhance language model performance</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="ACTING">
      <data key="d4">8.0</data>
      <data key="d5">LATS incorporates acting to enhance language model performance</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="PLANNING">
      <data key="d4">16.0</data>
      <data key="d5">LATS incorporates planning to enhance language model performance. Planning in LATS involves organizing information, planning future actions, or injecting internal knowledge.</data>
      <data key="d6">c234cb83764b899335af0950677ad024,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="AUTONOMOUS REASONING">
      <data key="d4">8.0</data>
      <data key="d5">LATS enhances autonomous reasoning in language models</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="DECISION-MAKING">
      <data key="d4">8.0</data>
      <data key="d5">LATS enhances decision-making in language models</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="INTERACTIVE QUESTION-ANSWERING (QA)">
      <data key="d4">8.0</data>
      <data key="d5">LATS is evaluated on interactive question-answering tasks</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="WEB NAVIGATION">
      <data key="d4">8.0</data>
      <data key="d5">LATS is evaluated on web navigation tasks</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="MATH">
      <data key="d4">8.0</data>
      <data key="d5">LATS is evaluated on math tasks</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="PROGRAMMING">
      <data key="d4">8.0</data>
      <data key="d5">LATS is evaluated on programming tasks</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="COT">
      <data key="d4">23.0</data>
      <data key="d5">LATS incorporates designs from CoT for reasoning, acting, and planning, specifically using CoT as the base prompting design for the Game of 24.</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LATS" target="SELF-REFINE">
      <data key="d4">16.0</data>
      <data key="d5">LATS incorporates designs from Self-Refine for reasoning, acting, and planning</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LATS" target="REFLEXION">
      <data key="d4">16.0</data>
      <data key="d5">LATS incorporates designs from Reflexion for reasoning, acting, and planning</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LATS" target="ADAPLANNER">
      <data key="d4">16.0</data>
      <data key="d5">LATS incorporates designs from AdaPlanner for reasoning, acting, and planning</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LATS" target="MCTS">
      <data key="d4">45.0</data>
      <data key="d5">LATS adopts a variant of MCTS to frame decision-making as a tree search, incorporating improvements to MCTS for better performance and efficiency. By leveraging MCTS, LATS ensures a principled search approach, utilizing the search algorithm to enhance its decision-making capabilities.</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf,594449768ae2dea9b2efbe677075096b,c234cb83764b899335af0950677ad024,faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="LM AGENT">
      <data key="d4">8.0</data>
      <data key="d5">LM Agent is initialized with a language model and used within LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="P&#920;">
      <data key="d4">9.0</data>
      <data key="d5">P&#952; is repurposed as an agent, state evaluator, and feedback generator in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="SELECTION">
      <data key="d4">8.0</data>
      <data key="d5">Selection is the first operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="EXPANSION">
      <data key="d4">8.0</data>
      <data key="d5">Expansion is the second operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="SIMULATION">
      <data key="d4">8.0</data>
      <data key="d5">Simulation is an operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="BACKPROPAGATION">
      <data key="d4">8.0</data>
      <data key="d5">Backpropagation is an operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="REFLECTION">
      <data key="d4">8.0</data>
      <data key="d5">Reflection is an operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="TASK">
      <data key="d4">8.0</data>
      <data key="d5">A task in LATS is successfully completed when the series of operations result in a solution or a computational limit is reached</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="TRAJECTORY">
      <data key="d4">16.0</data>
      <data key="d5">In the context of LATS, a trajectory refers to a sequence of actions and observations sampled from P&#952;. It represents the path taken through the state space in the LATS algorithm. This concept is crucial for understanding how the LATS algorithm navigates and processes information within its defined parameters.</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8,c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="FEEDBACK">
      <data key="d4">8.0</data>
      <data key="d5">Feedback in LATS is the response from the environment to the agent's actions</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="REASONING TASKS">
      <data key="d4">7.0</data>
      <data key="d5">Reasoning tasks are a type of task in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="DECISION-MAKING TASKS">
      <data key="d4">7.0</data>
      <data key="d5">Decision-making tasks are a type of task in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="SAMPLING">
      <data key="d4">8.0</data>
      <data key="d5">Sampling in LATS involves selecting a diverse set of candidates at each step</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="SEARCH ALGORITHM">
      <data key="d4">8.0</data>
      <data key="d5">A search algorithm in LATS controls the problem-solving process with planning</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="PSEUDOCODE">
      <data key="d4">7.0</data>
      <data key="d5">The full pseudocode of LATS can be found in the Appendix</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="SEC. A">
      <data key="d4">7.0</data>
      <data key="d5">Section A in the Appendix contains the full pseudocode of LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="EVANS, 2010">
      <data key="d4">6.0</data>
      <data key="d5">Evans, 2010 is a reference cited in the context of sampling diverse candidates for complex decision-making tasks in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="HAO ET AL., 2023">
      <data key="d4">6.0</data>
      <data key="d5">Hao et al., 2023 is a reference cited in the context of standard MCTS and RAP in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="RAP">
      <data key="d4">29.0</data>
      <data key="d5">LATS and RAP are two entities compared in the context of performance and efficiency. LATS outperforms RAP in terms of overall performance and efficiency, particularly excelling in internal reasoning tasks. RAP, on the other hand, is a prompting method that has been compared with LATS in specific tasks such as HotPotQA and the Game of 24. Unlike LATS, RAP relies on internal dynamics models.</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,c234cb83764b899335af0950677ad024,faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="MODERN LMS">
      <data key="d4">1.0</data>
      <data key="d5">Modern LMs provide useful language representations that facilitate planning in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="ACTION SPACE">
      <data key="d4">16.0</data>
      <data key="d5">LATS, an algorithm in the field of Artificial Intelligence and Machine Learning, incorporates an action space (A) which refers to the set of all possible actions an agent can take. This action space is crucial for defining the range of potential actions within the LATS algorithm, enabling the agent to make decisions based on the available options.</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8,c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="VALUE FUNCTION">
      <data key="d4">34.0</data>
      <data key="d5">LATS, an advanced algorithm, utilizes a novel value function that integrates both a self-generated LM score and a self-consistency score. This value function, referred to as pV, is crucial for evaluating states within the LATS framework. By incorporating self-consistency as an additional heuristic, the value function enhances the algorithm's ability to make more accurate assessments and decisions.</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf,48e423e2baf2ed485872756f5b4d87d8,594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="LATS" target="CHEN ET AL., 2021">
      <data key="d4">7.0</data>
      <data key="d5">LATS is evaluated on programming tasks as referenced in the paper by Chen et al. (2021).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="LATS" target="AUSTIN ET AL., 2022">
      <data key="d4">7.0</data>
      <data key="d5">LATS is evaluated on programming tasks as referenced in the paper by Austin et al. (2022).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="LATS" target="GAME OF 24">
      <data key="d4">24.0</data>
      <data key="d5">LATS is a method designed to enhance performance in the Game of 24. It is rigorously evaluated on the Game of 24 benchmark to demonstrate its reasoning and acting capabilities. Through these evaluations, LATS is tested to assess its reasoning ability, showcasing its effectiveness in this specific context.</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,b8dd0300033963bb4a3e1bad37f8e7b9,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="LATS" target="MBPP">
      <data key="d4">9.0</data>
      <data key="d5">LATS achieves the highest performance on MBPP</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="LATS" target="TOT">
      <data key="d4">15.0</data>
      <data key="d5">LATS outperforms ToT in terms of performance and efficiency. ToT is a prompting method compared with LATS in HotPotQA.</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="DFS">
      <data key="d4">7.0</data>
      <data key="d5">DFS is a search algorithm variant compared with MCTS in LATS</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="LATS" target="SELF-REFLECTION">
      <data key="d4">8.0</data>
      <data key="d5">Self-reflection is a technique used in LATS to provide additional semantic signals for the agent</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="LATS" target="MCTS (MONTE CARLO TREE SEARCH)">
      <data key="d4">9.0</data>
      <data key="d5">MCTS is the search algorithm used in LATS</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="LATS" target="A*">
      <data key="d4">7.0</data>
      <data key="d5">A* is a search algorithm variant compared with MCTS in LATS</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="LATS" target="TOKEN CONSUMPTION">
      <data key="d4">15.0</data>
      <data key="d5">LATS, an entity in the AI and ML landscape, is noted for its efficiency in token consumption, requiring fewer tokens compared to other methods. Token consumption serves as a critical metric in the ablation study of LATS, highlighting its optimized performance in resource utilization.</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="TRAJECTORIES">
      <data key="d4">10.0</data>
      <data key="d5">LATS constructs trajectories using search algorithms for enhanced decision-making. These trajectories refer to the sampled paths in the search space, which are utilized in the evaluation of LATS on tasks such as the Game of 24 and HotPotQA.</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="REVERSION PROPERTY">
      <data key="d4">7.0</data>
      <data key="d5">LATS assumes the ability to revert to earlier states in decision-making environments</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="SYSTEM-2 LM APPROACHES">
      <data key="d4">8.0</data>
      <data key="d5">LATS is an example of a System-2 LM approach that involves reasoning and planning</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="PERFORMANCE">
      <data key="d4">9.0</data>
      <data key="d5">LATS achieves better performance compared to other methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="SAMPLE COMPLEXITY">
      <data key="d4">8.0</data>
      <data key="d5">LATS has the same sample complexity as other tree-based search methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="NODES EXPANDED">
      <data key="d4">8.0</data>
      <data key="d5">LATS expands fewer nodes compared to other methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="COMPUTATIONAL COST">
      <data key="d4">7.0</data>
      <data key="d5">LATS has a higher computational cost compared to simpler prompting methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="INFERENCE-TIME COMPUTE COSTS">
      <data key="d4">7.0</data>
      <data key="d5">LATS is expected to have reduced inference-time compute costs over time</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="CONCLUSION">
      <data key="d4">8.0</data>
      <data key="d5">The conclusion section summarizes the contributions and findings related to LATS</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="LIMITATIONS AND FUTURE DIRECTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The limitations and future directions section discusses the constraints and potential future work for LATS</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="EXPLORATION WEIGHT">
      <data key="d4">8.0</data>
      <data key="d5">Exploration weight is a parameter in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="DEPTH">
      <data key="d4">8.0</data>
      <data key="d5">Depth is a parameter in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="LM VALUE FUNCTION">
      <data key="d4">8.0</data>
      <data key="d5">The LM value function is used in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="YAO ET AL., 2023B">
      <data key="d4">7.0</data>
      <data key="d5">Yao et al., 2023b is referenced for setting the maximum depth in LATS experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="WIKIPEDIA WEB API">
      <data key="d4">8.0</data>
      <data key="d5">The Wikipedia web API is used for interactive information retrieval in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="LANGUAGE MODELS">
      <data key="d4">8.0</data>
      <data key="d5">Language models are used in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="HYPERPARAMETERS">
      <data key="d4">8.0</data>
      <data key="d5">Hyperparameters are used in the value function for the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="FIG. 3">
      <data key="d4">7.0</data>
      <data key="d5">Figure 3 shows the performance of LATS over time</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="FIG. 4">
      <data key="d4">7.0</data>
      <data key="d5">Figure 4 illustrates how LATS works on an example task of HotPotQA</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="ALGORITHM 1">
      <data key="d4">1.0</data>
      <data key="d5">Algorithm 1 describes the LATS process</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="SELECTION FORMULA">
      <data key="d4">8.0</data>
      <data key="d5">Selection formula is used in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="STATE SPACE">
      <data key="d4">8.0</data>
      <data key="d5">State space refers to the complexity of the environment in which the LATS algorithm operates</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="ROLL-OUTS">
      <data key="d4">8.0</data>
      <data key="d5">Roll-outs refer to the number of iterations (K) in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="CONTEXT">
      <data key="d4">8.0</data>
      <data key="d5">Context (c) is part of the state in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="VISIT COUNTER">
      <data key="d4">8.0</data>
      <data key="d5">Visit counter (N) is used in the LATS algorithm to track the number of visits to each state</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="OBSERVATION SPACE">
      <data key="d4">8.0</data>
      <data key="d5">Observation space (O) is the set of possible observations in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="INITIAL STATE">
      <data key="d4">8.0</data>
      <data key="d5">Initial state (s) is the starting point in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="ACTION GENERATOR">
      <data key="d4">8.0</data>
      <data key="d5">Action generator (p&#952;) is used to sample actions in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="REFLECTION GENERATOR">
      <data key="d4">8.0</data>
      <data key="d5">Reflection generator (pref) is used to generate reflections in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="ENVIRONMENT">
      <data key="d4">8.0</data>
      <data key="d5">Environment provides the context and rewards for the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="REWARD">
      <data key="d4">8.0</data>
      <data key="d5">Reward (r) is the feedback from the environment in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="TERMINAL STATE">
      <data key="d4">8.0</data>
      <data key="d5">Terminal state (st) is a state where the process ends in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="SUCCESS">
      <data key="d4">8.0</data>
      <data key="d5">Success is a condition checked in the reflection phase of the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="EVALUATION OPERATION">
      <data key="d4">8.0</data>
      <data key="d5">Evaluation operation is used to score states in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="ITERATIONS">
      <data key="d4">8.0</data>
      <data key="d5">Iterations refer to the number of times the LATS algorithm is run</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="VALUE FUNCTION HYPERPARAMETERS">
      <data key="d4">8.0</data>
      <data key="d5">Value function hyperparameters include &#955;=0.5 for the LM score and self-consistency score in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="SELF-CONSISTENCY">
      <data key="d4">7.0</data>
      <data key="d5">Self-consistency is a method to improve Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="LEAST-TO-MOST PROMPTING">
      <data key="d4">7.0</data>
      <data key="d5">Least-to-most prompting is a method to improve Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="TREE-OF-THOUGHT (TOT) PROMPTING">
      <data key="d4">16.0</data>
      <data key="d5">Tree-of-Thought (ToT) prompting is a method designed to enhance Chain-of-Thought (CoT) prompting. ToT prompting extends CoT prompting by exploring multiple reasoning paths over thoughts, thereby improving the overall reasoning process.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="REASONING VIA PLANNING (RAP)">
      <data key="d4">7.0</data>
      <data key="d5">Reasoning via Planning is a method to improve Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="COBBE ET AL., 2021">
      <data key="d4">16.0</data>
      <data key="d5">Cobbe et al., 2021 is the reference for reasoning in language models involving decomposing complex inputs into sequential steps</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="WEI ET AL., 2022">
      <data key="d4">16.0</data>
      <data key="d5">Wei et al., 2022 is the reference for Chain-of-Thought prompting and its variants</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="KOJIMA ET AL., 2022">
      <data key="d4">16.0</data>
      <data key="d5">Kojima et al., 2022 is the reference for a variant of Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="WANG ET AL., 2022">
      <data key="d4">16.0</data>
      <data key="d5">Wang et al., 2022 is the reference for self-consistency and Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="GUO ET AL., 2018">
      <data key="d4">16.0</data>
      <data key="d5">Guo et al., 2018 is the reference for the issue of error propagation in Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="CHEN ET AL., 2023B">
      <data key="d4">16.0</data>
      <data key="d5">Chen et al., 2023b is the reference for the issue of error propagation in Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="BESTA ET AL., 2023">
      <data key="d4">16.0</data>
      <data key="d5">Besta et al., 2023 is the reference for search algorithms in Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="YAO ET AL., 2023A">
      <data key="d4">8.0</data>
      <data key="d5">Yao et al., 2023a is the reference for Tree-of-Thought prompting and search algorithms in Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="HAO ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Hao et al., 2023 is the reference for Reasoning via Planning using Monte Carlo Tree Search</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="LANGUAGE MODEL (LM)">
      <data key="d4">8.0</data>
      <data key="d5">Chain-of-thought (CoT) prompting uses a language model (LM) to generate intermediate thoughts that act as stepping stones between the input and the output.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="RAP (HAO ET AL., 2023)">
      <data key="d4">7.0</data>
      <data key="d5">RAP is a reasoning-based method similar to Chain-of-thought (CoT) prompting.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="TREE-OF-THOUGHT (TOT) PROMPTING" target="YAO ET AL., 2023A">
      <data key="d4">8.0</data>
      <data key="d5">Yao et al., 2023a is the reference for Tree-of-Thought prompting and search algorithms in Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="TREE-OF-THOUGHT (TOT) PROMPTING" target="LANGUAGE MODEL (LM)">
      <data key="d4">8.0</data>
      <data key="d5">Tree-of-thought (ToT) prompting uses a language model (LM) to generate thoughts and explore multiple reasoning paths.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="TREE-OF-THOUGHT (TOT) PROMPTING" target="DEPTH-FIRST SEARCH (DFS)">
      <data key="d4">8.0</data>
      <data key="d5">Depth-first search (DFS) is used in Tree-of-thought (ToT) prompting to explore the tree of reasoning paths.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="TREE-OF-THOUGHT (TOT) PROMPTING" target="BREADTH-FIRST SEARCH (BFS)">
      <data key="d4">8.0</data>
      <data key="d5">Breadth-first search (BFS) is used in Tree-of-thought (ToT) prompting to explore the tree of reasoning paths.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="TREE-OF-THOUGHT (TOT) PROMPTING" target="RAP (HAO ET AL., 2023)">
      <data key="d4">7.0</data>
      <data key="d5">RAP is a reasoning-based method similar to Tree-of-thought (ToT) prompting.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REASONING VIA PLANNING (RAP)" target="HAO ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Hao et al., 2023 is the reference for Reasoning via Planning using Monte Carlo Tree Search</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="SELF-REFINE" target="REFLEXION">
      <data key="d4">23.0</data>
      <data key="d5">SELF-REFINE and REFLEXION are methods that utilize self-improvement techniques to enhance language model performance. Both approaches focus on improving reasoning and decision-making capabilities, thereby advancing the overall effectiveness of language models. By leveraging self-improvement, these methods aim to refine and optimize the models' abilities to process and generate language, making them more adept at handling complex tasks and providing more accurate and reliable outputs.</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="SELF-REFINE" target="MADAAN ET AL., 2023">
      <data key="d4">16.0</data>
      <data key="d5">Madaan et al., 2023 is the reference for the self-refine method</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="SELF-REFINE" target="ADAPLANNER">
      <data key="d4">14.0</data>
      <data key="d5">AdaPlanner and Self-Refine both aim to enhance reasoning and decision-making</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="SELF-REFINE" target="BEAM SEARCH">
      <data key="d4">16.0</data>
      <data key="d5">Beam search uses self-improvement to enhance reasoning and decision-making in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="SELF-REFINE" target="META AGENT SEARCH">
      <data key="d4">41.0</data>
      <data key="d5">Meta Agent Search is a sophisticated method that leverages the Self-Refine technique to perform iterations of refinement on proposals. It uses Self-Refine as one of the state-of-the-art hand-designed agents and compares its discovered agents against the Self-Refine baseline. Notably, Meta Agent Search outperforms the Self-Refine method, demonstrating its advanced capabilities in the field. Self-Refine, therefore, plays a crucial role within Meta Agent Search, serving both as a benchmark and a component of its iterative refinement process.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="SELF-REFINE" target="MADAAN ET AL., 2024">
      <data key="d4">28.0</data>
      <data key="d5">Madaan et al., 2024 is a publication that introduces and discusses the Self-Refine method. This method, detailed in the 2024 publication by Madaan et al., is a significant contribution to the field and is frequently referenced in relation to Self-Refine.</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="SELF-REFINE" target="SHINN ET AL., 2023">
      <data key="d4">14.0</data>
      <data key="d5">SELF-REFINE is a method that was significantly contributed to by the publication "Shinn et al., 2023." The work of Shinn et al. in 2023 is a key reference for understanding the development and application of the Self-Refine method. This collaboration highlights the influential role of Shinn et al. in advancing the methodologies within the AI and ML landscape.</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="SELF-REFINE" target="MADAAN ET AL.">
      <data key="d4">14.0</data>
      <data key="d5">Madaan et al. are the authors of the Self-Refine agent and the Self-Refine method.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="SELF-REFINE" target="ARC">
      <data key="d4">7.0</data>
      <data key="d5">Self-Refine is used as a baseline for experiments on ARC</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="SELF-REFINE" target="SHENGRAN HU">
      <data key="d4">7.0</data>
      <data key="d5">Shengran Hu is associated with the implementation of Self-Refine</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="REFLEXION" target="SHINN ET AL., 2023">
      <data key="d4">24.0</data>
      <data key="d5">Shinn et al., 2023 introduced the Reflexion prompting method, which is discussed in their paper and its application in language models. The reference by Shinn et al., 2023 is crucial for understanding the performance and implementation of the Reflexion method.</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="REFLEXION" target="ADAPLANNER">
      <data key="d4">14.0</data>
      <data key="d5">AdaPlanner and Reflexion both aim to enhance reasoning and decision-making</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="REFLEXION" target="LANGUAGE MODEL (LM)">
      <data key="d4">1.0</data>
      <data key="d5">Reflexion uses a language model (LM) for decision-making tasks, similar to ReAct.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="SILVER ET AL., 2017" target="HEURISTIC">
      <data key="d4">2.0</data>
      <data key="d5">Heuristic is a concept referenced in Silver et al., 2017</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="YAO ET AL., 2022" target="IL">
      <data key="d4">8.0</data>
      <data key="d5">Yao et al., 2022 discusses the IL method and its performance on WebShop</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="YAO ET AL., 2022" target="RL">
      <data key="d4">8.0</data>
      <data key="d5">Yao et al., 2022 discusses the RL method and its performance on WebShop</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="CHEN ET AL., 2021" target="META AGENT SEARCH">
      <data key="d4">16.0</data>
      <data key="d5">Chen et al., 2021 discusses safety concerns when executing untrusted model-generated code in Meta Agent Search</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="COBBE ET AL., 2021" target="GSM8K">
      <data key="d4">8.0</data>
      <data key="d5">Cobbe et al., 2021 discusses the GSM8K dataset</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="WEI ET AL., 2022" target="COT">
      <data key="d4">30.0</data>
      <data key="d5">Wei et al. (2022) introduced the Chain-of-Thought (CoT) prompting method, which is a significant advancement in the application of language models. The paper by Wei et al. (2022) discusses the CoT method in detail, highlighting its innovative approach and practical applications in enhancing the performance of language models.</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf,99d90aededb61e04241516ed9ec656cc,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="WEI ET AL., 2022" target="CHAIN-OF-THOUGHT (COT)">
      <data key="d4">14.0</data>
      <data key="d5">Chain-of-Thought (COT) was introduced by Wei et al., 2022. This publication is a key reference for the Chain-of-Thought (COT) method, which has been influential in the field of Artificial Intelligence and Machine Learning.</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="WEI ET AL., 2022" target="CHAIN-OF-THOUGHT">
      <data key="d4">14.0</data>
      <data key="d5">The publication by Wei et al. in 2022 is related to the Chain-of-Thought method. Wei et al., 2022 discusses the Chain-of-Thought method, which is a significant contribution to the field of Artificial Intelligence and Machine Learning. This method involves a structured approach to problem-solving and reasoning, enhancing the capabilities of AI systems to perform complex tasks by breaking them down into a series of intermediate steps. The work by Wei et al. has been influential in advancing the understanding and application of this method within the AI and ML community.</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="WANG ET AL., 2022" target="SELF-CONSISTENCY SCORE">
      <data key="d4">14.0</data>
      <data key="d5">Self-consistency score is a metric referenced in Wang et al., 2022</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="WANG ET AL., 2022" target="COT-SC">
      <data key="d4">1.0</data>
      <data key="d5">The paper by Wang et al. (2022) discusses the CoT-SC method and its application in language models.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="CHEN ET AL., 2023B" target="AGENTVERSE">
      <data key="d4">8.0</data>
      <data key="d5">Chen et al., 2023b discusses AgentVerse and its application in optimizing role definition in the prompt</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="YAO ET AL., 2023A" target="TOT">
      <data key="d4">30.0</data>
      <data key="d5">Yao et al. (2023a) introduced the ToT (Tree of Thoughts) search method and discussed its application in language models. The paper provides a comprehensive analysis of how the ToT method can be utilized to enhance the performance and capabilities of language models, making it a significant contribution to the field of artificial intelligence and machine learning.</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf,99d90aededb61e04241516ed9ec656cc,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HAO ET AL., 2023" target="RAP">
      <data key="d4">36.0</data>
      <data key="d5">Hao et al., 2023 introduced the RAP search method, which is a method referenced in their paper. The paper by Hao et al. (2023) discusses the RAP method and its application in language models, highlighting its performance and relevance in the field.</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="AHN ET AL., 2022" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Ahn et al., 2022 is the reference for using language models as high-level controllers in robotics</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="HUANG ET AL., 2022" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Huang et al., 2022 is the reference for using language models as high-level controllers in robotics</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="DRIESS ET AL., 2023" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Driess et al., 2023 is the reference for using language models as high-level controllers in robotics</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="BAKER ET AL., 2022" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Baker et al., 2022 is the reference for adapting language model agents to complex multimodal games</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="WANG ET AL., 2023" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Wang et al., 2023 is the reference for adapting language model agents to complex multimodal games</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="GUSS ET AL., 2019" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Guss et al., 2019 is the reference for the game Minecraft used in adapting language model agents</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="FAN ET AL., 2022" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Fan et al., 2022 is the reference for the game Minecraft used in adapting language model agents</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LIU ET AL., 2018" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Liu et al., 2018 is the reference for using language models in text-based environments</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="SHRIDHAR ET AL., 2020" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Shridhar et al., 2020 is the reference for using language models in text-based environments</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LIU ET AL., 2024" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Liu et al., 2024 is the reference for using language models in text-based environments</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LIU ET AL., 2024" target="EOH">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Liu et al. in 2024 is related to EoH</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="MADAAN ET AL., 2023" target="SELF-REFLECTION">
      <data key="d4">14.0</data>
      <data key="d5">Self-reflection is a process referenced in Madaan et al., 2023</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="SHINN ET AL., 2023" target="SELF-REFLECTION">
      <data key="d4">14.0</data>
      <data key="d5">Self-reflection is a process referenced in Shinn et al., 2023</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="SHINN ET AL., 2023" target="META AGENT">
      <data key="d4">6.0</data>
      <data key="d5">The meta agent's self-reflection process is influenced by the work of Shinn et al. (2023).</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="SEARCH ALGORITHMS" target="PLANNING">
      <data key="d4">16.0</data>
      <data key="d5">Planning refers to the use of a search algorithm to determine the best course of action in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="SEARCH ALGORITHMS" target="RAP">
      <data key="d4">16.0</data>
      <data key="d5">RAP incorporates planning and search algorithms to enhance reasoning and decision-making in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="SEARCH ALGORITHMS" target="TOT">
      <data key="d4">16.0</data>
      <data key="d5">ToT incorporates planning and search algorithms to enhance reasoning and decision-making in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="SEARCH ALGORITHMS" target="ZHUANG ET AL., 2023">
      <data key="d4">1.0</data>
      <data key="d5">Zhuang et al., 2023 is a reference related to search algorithms like A* and DFS</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="SEARCH ALGORITHMS" target="TRAJECTORY CONSTRUCTION">
      <data key="d4">9.0</data>
      <data key="d5">Trajectory construction in LATS is done using search algorithms</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PROMPTS" target="LM">
      <data key="d4">16.0</data>
      <data key="d5">Prompts are provided along with the input to improve reasoning in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="IN-CONTEXT LEARNING" target="LM">
      <data key="d4">16.0</data>
      <data key="d5">In-context learning leverages the abilities of language models to learn from the context</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="IN-CONTEXT LEARNING" target="MEMORY">
      <data key="d4">16.0</data>
      <data key="d5">In-context learning integrates stored trajectories and reflections as additional context</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="REASONING" target="LANGUAGE AGENT TREE SEARCH">
      <data key="d4">9.0</data>
      <data key="d5">LATS unifies reasoning, acting, and planning for enhanced LM problem-solving</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="REASONING" target="LANGUAGE MODELS">
      <data key="d4">8.0</data>
      <data key="d5">Language models use reasoning to solve problems</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="REASONING" target="DECISION-MAKING">
      <data key="d4">8.0</data>
      <data key="d5">Reasoning is a key component of decision-making</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="REASONING" target="EXPERIMENTS">
      <data key="d4">8.0</data>
      <data key="d5">Experiments were conducted on the Reasoning domain</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="REASONING" target="AGENTINSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct generates data covering the skill of reasoning</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="ACTING" target="LANGUAGE AGENT TREE SEARCH">
      <data key="d4">9.0</data>
      <data key="d5">LATS unifies reasoning, acting, and planning for enhanced LM problem-solving</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="ACTING" target="LANGUAGE MODELS">
      <data key="d4">8.0</data>
      <data key="d5">Language models execute actions based on decisions</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="ACTING" target="DECISION-MAKING">
      <data key="d4">8.0</data>
      <data key="d5">Acting is a key component of decision-making</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PLANNING" target="LANGUAGE AGENT TREE SEARCH">
      <data key="d4">9.0</data>
      <data key="d5">LATS unifies reasoning, acting, and planning for enhanced LM problem-solving</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PLANNING" target="LANGUAGE MODELS">
      <data key="d4">8.0</data>
      <data key="d5">Language models create plans to achieve goals</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PLANNING" target="DECISION-MAKING">
      <data key="d4">8.0</data>
      <data key="d5">Planning is a key component of decision-making</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="DECISION-MAKING" target="LM">
      <data key="d4">16.0</data>
      <data key="d5">Decision-making refers to the process of making choices based on reasoning and planning in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="DECISION-MAKING" target="LANGUAGE AGENT TREE SEARCH">
      <data key="d4">9.0</data>
      <data key="d5">LATS enhances decision-making capabilities of language models</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="DECISION-MAKING" target="LANGUAGE MODELS">
      <data key="d4">8.0</data>
      <data key="d5">Language models are used for decision-making tasks</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="DECISION-MAKING" target="INTERACTION">
      <data key="d4">8.0</data>
      <data key="d5">Interaction enhances decision-making in LATS</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="DECISION-MAKING" target="REFLECTION">
      <data key="d4">8.0</data>
      <data key="d5">Reflection enhances decision-making in LATS</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="MATH" target="MGSM">
      <data key="d4">8.0</data>
      <data key="d5">MGSM is a benchmark for math tasks</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="MATH" target="GSM8K">
      <data key="d4">8.0</data>
      <data key="d5">GSM8K is a benchmark for math tasks</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="MATH" target="GSM-HARD">
      <data key="d4">8.0</data>
      <data key="d5">GSM-Hard is a benchmark for math tasks</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="MATH" target="META AGENT SEARCH">
      <data key="d4">22.0</data>
      <data key="d5">Meta Agent Search is effective in the Math domain and tests agents within this field.</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="MATH" target="F1 SCORE">
      <data key="d4">7.0</data>
      <data key="d5">F1 scores are used to evaluate agents in the Math domain</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="MATH" target="ACCURACY">
      <data key="d4">1.0</data>
      <data key="d5">Accuracy rates are used to evaluate agents in the Math domain</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="MATH" target="AGENTINSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct generates data covering the skill of math</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="PROGRAMMING" target="MBPP">
      <data key="d4">8.0</data>
      <data key="d5">Programming tasks are evaluated using the MBPP dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="COT" target="WEI ET AL.">
      <data key="d4">18.0</data>
      <data key="d5">Wei et al. contributed to the development of Chain-of-thought (CoT) prompting</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="COT" target="LM AGENT">
      <data key="d4">7.0</data>
      <data key="d5">CoT is used as the base prompting framework for LM Agent in environments without feedback</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="COT" target="FM">
      <data key="d4">7.0</data>
      <data key="d5">FM is prompted to think step by step in the COT method</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="COT" target="MIRAGE">
      <data key="d4">8.0</data>
      <data key="d5">CoT shows the performance of models on MIRAGE datasets when answering directly without using RAG</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="HUANG ET AL." target="EXTERNAL TOOLS">
      <data key="d4">16.0</data>
      <data key="d5">Huang et al. suggested the use of external tools to enhance the reasoning and practical abilities of language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="EXTERNAL TOOLS" target="SHICK ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Schick et al. contributed to the understanding of using external tools to enhance language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="EXTERNAL TOOLS" target="SHEN ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Shen et al. contributed to the understanding of using external tools to enhance language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="EXTERNAL TOOLS" target="SURIS ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Suris et al. contributed to the understanding of using external tools to enhance language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="EXTERNAL TOOLS" target="REASONING TASKS">
      <data key="d4">7.0</data>
      <data key="d5">External tools are tools or APIs used by an agent in reasoning tasks</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="MCTS">
      <data key="d4">18.0</data>
      <data key="d5">Tree-based search and MCTS are both search algorithms used to explore multiple branches of outcomes</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="REINFORCEMENT LEARNING">
      <data key="d4">16.0</data>
      <data key="d5">Reinforcement learning and tree-based search are both used for their good exploration-exploitation trade-off in planning algorithms</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="VODOPIVEC ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Vodopivec et al. contributed to the understanding of tree-based search in reinforcement learning</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="SWIECHOWSKI ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Swiechowski et al. contributed to the understanding of tree-based search in planning algorithms</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="LAVALLE">
      <data key="d4">16.0</data>
      <data key="d5">LaValle contributed to the understanding of tree-based search in planning algorithms</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="HAFNER ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Hafner et al. contributed to the understanding of tree-based search in reinforcement learning</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="DU ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Du et al. contributed to the understanding of tree-based search in reinforcement learning</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="WU ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Wu et al. contributed to the understanding of tree-based search in reinforcement learning</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="MCTS" target="LM TASKS">
      <data key="d4">7.0</data>
      <data key="d5">LM tasks do not have the limitation of requiring an environment model, unlike MCTS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="MCTS" target="EXPLORATION WEIGHT">
      <data key="d4">8.0</data>
      <data key="d5">Exploration weight is a parameter used in MCTS to balance exploration and exploitation</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="MCTS" target="PARENT NODE">
      <data key="d4">8.0</data>
      <data key="d5">Parent node is a node in the tree structure from which child nodes are derived in MCTS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="MCTS" target="EPISODE">
      <data key="d4">7.0</data>
      <data key="d5">An episode refers to a complete sequence of actions and observations in MCTS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="MCTS" target="RETURN">
      <data key="d4">8.0</data>
      <data key="d5">Return is the reward or feedback used for updating the value function in MCTS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="MCTS" target="VALUE FUNCTION">
      <data key="d4">8.0</data>
      <data key="d5">Value function is used to estimate the expected return of a state in MCTS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="MCTS" target="LM-BASED HEURISTIC">
      <data key="d4">7.0</data>
      <data key="d5">MCTS incorporates the LM-based heuristic used in ToT to improve performance</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LM" target="SELF-REFLECTION">
      <data key="d4">16.0</data>
      <data key="d5">Self-reflection uses language model-generated feedback to improve reasoning and decision-making</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="EXTERNAL MEMORY">
      <data key="d4">16.0</data>
      <data key="d5">External memory is used to store past text context for future updates of the solution in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="LM TASKS">
      <data key="d4">16.0</data>
      <data key="d5">LM tasks refer to the various tasks that language models can perform</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="LM INTERNAL REASONING">
      <data key="d4">16.0</data>
      <data key="d5">LM internal reasoning refers to the reasoning capabilities of language models without external inputs</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="PROMPT IO">
      <data key="d4">16.0</data>
      <data key="d5">Prompt IO refers to the process where an input prompt is transformed into an output by a language model</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="INPUT X">
      <data key="d4">16.0</data>
      <data key="d5">Input X refers to the initial input provided to a language model for reasoning or decision-making</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="OUTPUT Y">
      <data key="d4">16.0</data>
      <data key="d5">Output Y refers to the final output generated by a language model based on the given input</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="P&#920;(X)">
      <data key="d4">16.0</data>
      <data key="d5">P&#952;(X) refers to the pre-trained language model parameterized by &#952; used to generate outputs based on given inputs</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="AUTOREGRESSIVE DECODING">
      <data key="d4">2.0</data>
      <data key="d5">Autoregressive decoding is a method where the language model generates text sequentially</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="YA0 ET AL." target="TOT">
      <data key="d4">18.0</data>
      <data key="d5">Yao et al. contributed to the development of the ToT technique</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="SELF-REFLECTION" target="REFLECTION">
      <data key="d4">18.0</data>
      <data key="d5">Reflection leverages self-reflection to refine decision-making</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="SELF-REFLECTION" target="MEMORY">
      <data key="d4">16.0</data>
      <data key="d5">Failed trajectories and reflections are stored in memory</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="SELF-REFLECTION" target="AI PYTHON ASSISTANT">
      <data key="d4">9.0</data>
      <data key="d5">The AI Python assistant performs self-reflection to identify errors and improve code</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="SELF-REFLECTION" target="FUNCTION IMPLEMENTATION">
      <data key="d4">8.0</data>
      <data key="d5">Self-reflection is used to review and improve function implementations</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="SELF-REFLECTION" target="REFLECTION PROMPT">
      <data key="d4">8.0</data>
      <data key="d5">The REFLECTION PROMPT guides the AI Python assistant in performing SELF-REFLECTION</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="SELF-REFLECTION" target="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)">
      <data key="d4">14.0</data>
      <data key="d5">Self-Reflection is a technique used as a building block in agentic systems within the research area of Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="SELF-REFLECTION" target="MADAAN ET AL.">
      <data key="d4">14.0</data>
      <data key="d5">Madaan et al. contributed to the research on self-reflection in agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="SELF-REFLECTION" target="META AGENT">
      <data key="d4">9.0</data>
      <data key="d5">The meta agent performs self-reflection to review and improve its proposed architecture and implementation.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="SELF-REFLECTION" target="FM MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The FM Module is used in the self-reflection process to improve task-solving</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="SELF-REFLECTION" target="COT_INITIAL_INSTRUCTION">
      <data key="d4">7.0</data>
      <data key="d5">Self-reflection uses the COT_INITIAL_INSTRUCTION for initial reasoning</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="SELF-REFLECTION" target="COT_REFLECT_INSTRUCTION">
      <data key="d4">7.0</data>
      <data key="d5">Self-reflection uses the COT_REFLECT_INSTRUCTION for reflecting on previous attempts and feedback</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="SELF-REFLECTION" target="CRITIC_INSTRUCTION">
      <data key="d4">7.0</data>
      <data key="d5">Self-reflection uses the CRITIC_INSTRUCTION for providing feedback and correcting the answer</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="EXTERNAL MEMORY" target="META AGENT SEARCH">
      <data key="d4">6.0</data>
      <data key="d5">External Memory is used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="REINFORCEMENT LEARNING" target="SEARCH ALGORITHM">
      <data key="d4">7.0</data>
      <data key="d5">Reinforcement Learning is a method used in search algorithms to explore the search space in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="WEI ET AL." target="CHAIN-OF-THOUGHT">
      <data key="d4">28.0</data>
      <data key="d5">Wei et al. are the authors of the Chain-of-Thought agent and method. They have significantly contributed to the research on chain-of-thought planning and reasoning, which is a notable advancement in the field of Artificial Intelligence and Machine Learning. Their work focuses on enhancing the cognitive processes of AI systems, enabling more sophisticated and human-like reasoning capabilities.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b,c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="DU ET AL." target="LLM DEBATE">
      <data key="d4">14.0</data>
      <data key="d5">Du et al. are the authors of the LLM Debate agent and the LLM Debate method.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="WU ET AL." target="ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES AND ENABLING THEM TO COLLABORATE">
      <data key="d4">8.0</data>
      <data key="d5">Wu et al. are the authors of the method for assigning FM modules in the agentic system with different roles and enabling them to collaborate</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="LM TASKS" target="RESET">
      <data key="d4">8.0</data>
      <data key="d5">Reset refers to the ability to revert to a previous state or step in LM tasks</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="RAP" target="LANGUAGE MODEL (LM)">
      <data key="d4">7.0</data>
      <data key="d5">RAP relies on the internal representations of the language model (LM) for reasoning and planning.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="RAP" target="PERFORMANCE">
      <data key="d4">7.0</data>
      <data key="d5">RAP is evaluated for performance in the study</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="RAP" target="TOKEN CONSUMPTION">
      <data key="d4">7.0</data>
      <data key="d5">RAP is evaluated for token consumption in the study</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="TOT" target="LM-BASED HEURISTIC">
      <data key="d4">8.0</data>
      <data key="d5">ToT uses the LM-based heuristic to prune branches with low values</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="INPUT X" target="PROMPT">
      <data key="d4">8.0</data>
      <data key="d5">A prompt is provided along with input x to improve reasoning in language models.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="INPUT X" target="OUTPUT Y">
      <data key="d4">9.0</data>
      <data key="d5">Input x is transformed into output y by the language model.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="INPUT X" target="THOUGHT Z">
      <data key="d4">8.0</data>
      <data key="d5">Thought z is created as an intermediate step between input x and output y in Chain-of-thought (CoT) prompting.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="OUTPUT Y" target="THOUGHT Z">
      <data key="d4">8.0</data>
      <data key="d5">Thought z is created as an intermediate step between input x and output y in Chain-of-thought (CoT) prompting.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="UPPER CONFIDENCE BOUNDS APPLIED TO TREES (UCT)" target="KOCSIS AND SZEPESV&#193;RI (2006)">
      <data key="d4">9.0</data>
      <data key="d5">Kocsis and Szepesv&#225;ri developed the Upper Confidence bounds applied to Trees (UCT) metric used in MCTS.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="PROMPT" target="THOUGHT">
      <data key="d4">8.0</data>
      <data key="d5">Prompt guides the Thought process in HotPotQA</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="PROMPT" target="META AGENT">
      <data key="d4">9.0</data>
      <data key="d5">The meta agent uses the prompt to format its output, including sections for thought process, agent name, and code implementation.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="PROMPT" target="FM MODULE">
      <data key="d4">9.0</data>
      <data key="d5">The FM Module generates a prompt by concatenating input Info objects</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="PROMPT" target="SYSTEM PROMPT">
      <data key="d4">8.0</data>
      <data key="d5">The prompt generated by the FM Module includes a system prompt</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="PROMPT" target="USER PROMPT">
      <data key="d4">8.0</data>
      <data key="d5">The prompt generated by the FM Module includes a user prompt</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="LM AGENT" target="ENVIRONMENT">
      <data key="d4">8.0</data>
      <data key="d5">The environment provides observations to the LM Agent and receives actions from it</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LM AGENT" target="P&#920;">
      <data key="d4">9.0</data>
      <data key="d5">P&#952; is the language model used to initialize the LM Agent</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LM AGENT" target="POLICY">
      <data key="d4">8.0</data>
      <data key="d5">Policy is a strategy followed by the LM Agent to decide actions based on observations and past actions</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="ENVIRONMENT" target="OBSERVATION">
      <data key="d4">8.0</data>
      <data key="d5">Observation is the feedback received from the environment after an action is taken by the agent</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="P&#920;" target="VALUE FUNCTION">
      <data key="d4">16.0</data>
      <data key="d5">P&#952; is repurposed into a value function by reasoning about a given state</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="UCT ALGORITHM" target="SELECTION">
      <data key="d4">8.0</data>
      <data key="d5">The UCT algorithm is used in the selection operation of LATS to balance exploration and exploitation</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="EXPANSION" target="LONG-TERM MEMORY STRUCTURE">
      <data key="d4">7.0</data>
      <data key="d5">The expanded tree in LATS is stored in a long-term memory structure</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="EXPANSION" target="AGENTINSTRUCT FLOW">
      <data key="d4">7.0</data>
      <data key="d5">The AgentInstruct Flow includes expansion as one of the text modification tasks.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SIMULATION" target="TERMINAL STATE">
      <data key="d4">16.0</data>
      <data key="d5">Simulation expands the selected node until a terminal state is reached</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="BACKPROPAGATION" target="TRAJECTORY">
      <data key="d4">18.0</data>
      <data key="d5">Backpropagation updates the values of the tree based on the outcome of a trajectory</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="BACKPROPAGATION" target="REWARD">
      <data key="d4">16.0</data>
      <data key="d5">Reward is used in backpropagation to update the values of the tree</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="BACKPROPAGATION" target="UCT FORMULA">
      <data key="d4">16.0</data>
      <data key="d5">The UCT formula uses updated values from backpropagation to guide node selection</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="REFLECTION" target="LANGUAGE AGENT TREE SEARCH">
      <data key="d4">8.0</data>
      <data key="d5">LATS incorporates reflection to enhance decision-making</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="REFLECTION" target="DAIRY FREE AND APPLE VARIETY PACK OF CHIPS">
      <data key="d4">6.0</data>
      <data key="d5">The reflection action is used to review the decision to buy the dairy-free and apple variety pack of chips</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="REFLECTION" target="ACTION">
      <data key="d4">7.0</data>
      <data key="d5">Reflection is a type of action taken by the user</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="REFLECTION" target="META AGENT SEARCH">
      <data key="d4">6.0</data>
      <data key="d5">Reflection is used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="TASK" target="COMPUTATIONAL LIMIT">
      <data key="d4">8.0</data>
      <data key="d5">A task in LATS is completed when a solution is found or a computational limit is reached</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="TRAJECTORY" target="TERMINAL STATE">
      <data key="d4">16.0</data>
      <data key="d5">A trajectory is the sequence of states from the root to the terminal state</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="TRAJECTORY" target="PREVIOUS TRIAL">
      <data key="d4">1.0</data>
      <data key="d5">Trajectory refers to the sequence of actions in the previous trial</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="FEEDBACK" target="META AGENT SEARCH">
      <data key="d4">8.0</data>
      <data key="d5">Feedback is provided by critics and experts to refine answers in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="FEEDBACK" target="CRITIC_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The critic_module provides feedback</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="FEEDBACK" target="THOUGHTS">
      <data key="d4">14.0</data>
      <data key="d5">Thoughts are evaluated to generate feedback</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FEEDBACK" target="CORRECT EXAMPLES">
      <data key="d4">14.0</data>
      <data key="d5">Feedback includes correct examples where the code produced the correct output</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FEEDBACK" target="WRONG EXAMPLES">
      <data key="d4">14.0</data>
      <data key="d5">Feedback includes wrong examples where the code produced the incorrect output</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FEEDBACK" target="CORRECT_COUNT">
      <data key="d4">1.0</data>
      <data key="d5">Feedback includes the correct_count of examples</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FEEDBACK" target="CODE">
      <data key="d4">9.0</data>
      <data key="d5">Feedback is provided based on the performance of the code</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="DECISION-MAKING TASKS" target="COMMANDS">
      <data key="d4">7.0</data>
      <data key="d5">Commands are specific actions taken by an agent in decision-making tasks</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="SAMPLING" target="STOCHASTIC NATURE">
      <data key="d4">8.0</data>
      <data key="d5">Sampling in LATS involves selecting a diverse set of candidates to mitigate the stochastic nature of LM text generation</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="SEARCH ALGORITHM" target="ADAS">
      <data key="d4">16.0</data>
      <data key="d5">The search algorithm is a key component of ADAS that defines how the search space is explored</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH ALGORITHM" target="SUTTON">
      <data key="d4">10.0</data>
      <data key="d5">Sutton is referenced in the context of the exploration-exploitation trade-off in search algorithms</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH ALGORITHM" target="BARTO">
      <data key="d4">6.0</data>
      <data key="d5">Barto is referenced in the context of the exploration-exploitation trade-off in search algorithms</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH ALGORITHM" target="EXPLORATION-EXPLOITATION TRADE-OFF">
      <data key="d4">7.0</data>
      <data key="d5">The exploration-exploitation trade-off is a concept in search algorithms that balances discovering high-performance agentic systems and avoiding local optima</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="VALUE FUNCTION" target="VOLD(S)">
      <data key="d4">7.0</data>
      <data key="d5">Vold(s) is the old value function before it is updated with the new return in MCTS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="VALUE FUNCTION" target="N(S)">
      <data key="d4">7.0</data>
      <data key="d5">N(s) is the number of times a state has been visited, used in the value function update in MCTS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="VALUE FUNCTION" target="ENVIRONMENTAL FEEDBACK">
      <data key="d4">14.0</data>
      <data key="d5">Environmental feedback is used to improve value assignment</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="VALUE FUNCTION" target="SELF-CONSISTENCY SCORE">
      <data key="d4">8.0</data>
      <data key="d5">Value Function uses Self-Consistency Score to improve performance in the Game of 24</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="ACTION SPACE" target="PERMISSIBLE ACTIONS">
      <data key="d4">7.0</data>
      <data key="d5">Permissible actions are the actions that an agent is allowed to take in the action space of LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="ACTION SPACE" target="REASONING TRACES">
      <data key="d4">7.0</data>
      <data key="d5">Reasoning traces are the language-based thoughts used in the action space of LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="OBSERVATION" target="THOUGHT">
      <data key="d4">8.0</data>
      <data key="d5">Observation leads to the next Thought in HotPotQA</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="OBSERVATION" target="ACTION">
      <data key="d4">31.0</data>
      <data key="d5">In the context of HotPotQA, "OBSERVATION" and "ACTION" are closely interrelated entities. Actions lead to observations that provide feedback or information, creating a dynamic loop where each action taken by the user results in an observation. Specifically, in HotPotQA, an action leads to an observation, which is a type of action taken by the user. This interplay between actions and observations is crucial for understanding user interactions and the flow of information within the system.</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f,5d356b8ff719763a38cecff22c4e17b7,b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="OBSERVATION" target="TRAJECTORIES">
      <data key="d4">16.0</data>
      <data key="d5">Trajectories include observations that provide feedback or information</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="OBSERVATION" target="SEARCH">
      <data key="d4">8.0</data>
      <data key="d5">OBSERVATION is the result received after performing a SEARCH</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="OBSERVATION" target="THINK">
      <data key="d4">7.0</data>
      <data key="d5">THINK is the process of reasoning about the OBSERVATION before taking further action</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="HEURISTIC" target="CAMPBELL ET AL., 2002">
      <data key="d4">14.0</data>
      <data key="d5">Heuristic is a concept referenced in Campbell et al., 2002</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="AGENT" target="META AGENT SEARCH">
      <data key="d4">9.0</data>
      <data key="d5">Meta Agent Search discovers agents that outperform state-of-the-art hand-designed baselines</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="AGENT" target="SUGGESTER AGENT">
      <data key="d4">7.0</data>
      <data key="d5">Suggester agents are a type of agent that propose various approaches to increase the intricacy of instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENT" target="EDITOR AGENT">
      <data key="d4">7.0</data>
      <data key="d5">Editor agents are a type of agent that modify instructions based on suggestions from Suggester agents.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENT" target="SEARCH API">
      <data key="d4">7.0</data>
      <data key="d5">Agents can use search APIs as tools to perform search operations as part of their tasks.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENT" target="CODE INTERPRETER">
      <data key="d4">7.0</data>
      <data key="d5">Agents can use code interpreters as tools to interpret and execute code as part of their tasks.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENT" target="CALCULATOR">
      <data key="d4">7.0</data>
      <data key="d5">Agents can use calculators as tools to perform calculations as part of their tasks.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENT" target="SEED INSTRUCTION GENERATION FLOW">
      <data key="d4">8.0</data>
      <data key="d5">The Seed Instruction Generation Flow uses multiple agents to generate questions from a given text.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="AGENT" target="CONTENT TRANSFORMATION AGENT">
      <data key="d4">7.0</data>
      <data key="d5">The Content Transformation Agent determines which subset of agents to engage in the Seed Instruction Generation Flow.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="REWARD" target="SUCCESS RATE">
      <data key="d4">8.0</data>
      <data key="d5">Reward and Success Rate are metrics used to evaluate performance in WebShop</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="COT-SC" target="PERFORMANCE">
      <data key="d4">7.0</data>
      <data key="d5">CoT-SC is evaluated for performance in the study</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="COT-SC" target="TOKEN CONSUMPTION">
      <data key="d4">7.0</data>
      <data key="d5">CoT-SC is evaluated for token consumption in the study</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="COT-SC" target="META AGENT SEARCH">
      <data key="d4">27.0</data>
      <data key="d5">COT-SC is a method used in Meta Agent Search. Meta Agent Search compares its discovered agents against the COT-SC baseline and consistently outperforms the COT-SC method.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="COT-SC" target="WANG ET AL.">
      <data key="d4">14.0</data>
      <data key="d5">Wang et al. are the authors of the COT-SC agent and the COT-SC method.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="COT-SC" target="FM">
      <data key="d4">7.0</data>
      <data key="d5">FM is used to sample answers and perform an ensemble in the COT-SC method</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="AUSTIN ET AL., 2022" target="MBPP">
      <data key="d4">9.0</data>
      <data key="d5">Austin et al., 2022 introduced the MBPP dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="MBPP" target="TAB. 4">
      <data key="d4">8.0</data>
      <data key="d5">Tab. 4 shows the performance of various methods on programming tasks using the MBPP dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="MBPP" target="TAB. 5">
      <data key="d4">8.0</data>
      <data key="d5">Tab. 5 shows the GPT-3.5 Pass@1 accuracy on MBPP for various prompting methods</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="MBPP" target="APPENDIX SEC. D">
      <data key="d4">7.0</data>
      <data key="d5">Appendix Sec. D provides additional details on the evaluation of LATS and other methods on programming tasks using the MBPP dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="MBPP" target="CHEN ET AL., 2023A">
      <data key="d4">8.0</data>
      <data key="d5">Chen et al., 2023a discusses the use of an LM to generate a synthetic test suite for evaluating programming tasks using the MBPP dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="MBPP" target="NATURAL LANGUAGE DESCRIPTION">
      <data key="d4">8.0</data>
      <data key="d5">Natural language descriptions explain programming tasks in the MBPP dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="MBPP" target="CROWDSOURCING">
      <data key="d4">8.0</data>
      <data key="d5">Crowdsourcing was used to create the MBPP dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="FINE-TUNING" target="FURUTA ET AL., 2024">
      <data key="d4">7.0</data>
      <data key="d5">Furuta et al., 2024 discusses the fine-tuning method and its performance on WebShop. This reference is significant in the context of fine-tuning methods, providing insights into their application and effectiveness within the WebShop environment.</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="FINE-TUNING" target="AGENTINSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct generates synthetic datasets for fine-tuning AI models</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="DFS" target="ZHUANG ET AL., 2023">
      <data key="d4">6.0</data>
      <data key="d5">Zhuang et al., 2023 is a reference that mentions DFS</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="ZHUANG ET AL., 2023" target="A*">
      <data key="d4">6.0</data>
      <data key="d5">Zhuang et al., 2023 is a reference that mentions A*</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="TOKEN CONSUMPTION" target="TAB. 9">
      <data key="d4">7.0</data>
      <data key="d5">Tab. 9 shows token consumption comparisons of different methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="TRAJECTORIES" target="THOUGHT">
      <data key="d4">16.0</data>
      <data key="d5">Trajectories include thoughts that reason about the current situation</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="TRAJECTORIES" target="ACTION">
      <data key="d4">16.0</data>
      <data key="d5">Trajectories include actions that can be performed to progress in the task</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="TRAJECTORIES" target="REFLECTION PROMPT">
      <data key="d4">18.0</data>
      <data key="d5">The Reflection Prompt involves analyzing the trajectories of a solution to a question-answering task</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="PROMPTING TECHNIQUES">
      <data key="d4">8.0</data>
      <data key="d5">LATS addresses key limitations of prior prompting techniques</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="TRAJECTORY CONSTRUCTION">
      <data key="d4">9.0</data>
      <data key="d5">LATS constructs trajectories with search algorithms to improve decision-making</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="INTERACTION">
      <data key="d4">8.0</data>
      <data key="d5">LATS incorporates interaction to enhance decision-making</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="AUTONOMOUS DECISION-MAKING">
      <data key="d4">9.0</data>
      <data key="d5">LATS enables autonomous decision-making in language models</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="COMPUTATIONAL BUDGET">
      <data key="d4">8.0</data>
      <data key="d5">LATS manages computational budget efficiently compared to other methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="GROUND-TRUTH FEEDBACK">
      <data key="d4">8.0</data>
      <data key="d5">LATS uses ground-truth feedback to improve performance</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="SYSTEM-2 LM APPROACHES">
      <data key="d4">8.0</data>
      <data key="d5">LATS is an example of a System-2 LM approach</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="SYSTEM-1 LM APPROACHES">
      <data key="d4">7.0</data>
      <data key="d5">LATS has a higher computational cost compared to System-1 LM approaches</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="SYSTEM-2 LM APPROACHES" target="PROMPTING TECHNIQUES">
      <data key="d4">7.0</data>
      <data key="d5">Advanced prompting techniques like LATS are examples of System-2 LM approaches</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="SYSTEM-2 LM APPROACHES" target="COMPUTATIONAL COST">
      <data key="d4">1.0</data>
      <data key="d5">System-2 LM approaches have higher computational cost compared to System-1 approaches</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PERFORMANCE" target="TAB. 8">
      <data key="d4">7.0</data>
      <data key="d5">Tab. 8 shows performance comparisons of different methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PERFORMANCE" target="GROUND-TRUTH FEEDBACK">
      <data key="d4">8.0</data>
      <data key="d5">Ground-truth feedback improves the performance of language models</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PERFORMANCE" target="EVALUATION FUNCTION">
      <data key="d4">7.0</data>
      <data key="d5">Performance is a metric used in the evaluation function to assess an agent's effectiveness</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SAMPLE COMPLEXITY" target="TAB. 9">
      <data key="d4">7.0</data>
      <data key="d5">Tab. 9 shows sample complexity comparisons of different methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="COMPUTATIONAL COST" target="COMPUTATIONAL BUDGET">
      <data key="d4">8.0</data>
      <data key="d5">Managing computational budget is crucial for controlling computational cost</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="COMPUTATIONAL COST" target="SYSTEM-1 LM APPROACHES">
      <data key="d4">7.0</data>
      <data key="d5">System-1 LM approaches have lower computational cost compared to System-2 approaches</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE MODELS" target="AUTONOMOUS DECISION-MAKING">
      <data key="d4">8.0</data>
      <data key="d5">Language models can make autonomous decisions</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PROMPTING TECHNIQUES" target="SYSTEM-1 LM APPROACHES">
      <data key="d4">7.0</data>
      <data key="d5">Simpler prompting techniques like ReAct and Reflexion are examples of System-1 LM approaches</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PROMPTING TECHNIQUES" target="META AGENT SEARCH">
      <data key="d4">6.0</data>
      <data key="d5">Prompting Techniques are used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="PROMPTING TECHNIQUES" target="CHEN ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Chen et al. are the authors of the Prompting Techniques method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="PROMPTING TECHNIQUES" target="SCHULHOFF ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Schulhoff et al. are the authors of the Prompting Techniques method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="CHELSEA FINN" target="RAFAEL RAFAILOV">
      <data key="d4">8.0</data>
      <data key="d5">Rafael Rafailov and Chelsea Finn co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="CHELSEA FINN" target="ARCHIT SHARMA">
      <data key="d4">8.0</data>
      <data key="d5">Archit Sharma and Chelsea Finn co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="CHELSEA FINN" target="ERIC MITCHELL">
      <data key="d4">8.0</data>
      <data key="d5">Eric Mitchell and Chelsea Finn co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="CHELSEA FINN" target="CHRISTOPHER D MANNING">
      <data key="d4">8.0</data>
      <data key="d5">Christopher D Manning and Chelsea Finn co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="CHELSEA FINN" target="STEFANO ERMON">
      <data key="d4">8.0</data>
      <data key="d5">Stefano Ermon and Chelsea Finn co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="QUOC LE" target="XUEZHI WANG">
      <data key="d4">8.0</data>
      <data key="d5">Xuezhi Wang and Quoc Le co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="QUOC LE" target="JASON WEI">
      <data key="d4">8.0</data>
      <data key="d5">Jason Wei and Quoc Le co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="QUOC LE" target="DALE SCHUURMANS">
      <data key="d4">8.0</data>
      <data key="d5">Dale Schuurmans and Quoc Le co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="QUOC LE" target="ED CHI">
      <data key="d4">16.0</data>
      <data key="d5">Quoc Le and Ed Chi co-authored the paper titled "Least-to-most prompting enables complex reasoning in large language models." This collaboration highlights their joint contribution to advancing the understanding of complex reasoning capabilities in large language models.</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="QUOC LE" target="DENNY ZHOU">
      <data key="d4">8.0</data>
      <data key="d5">Quoc Le and Denny Zhou co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="QUOC LE" target="OLIVIER BOUSQUET">
      <data key="d4">8.0</data>
      <data key="d5">Olivier Bousquet and Quoc Le co-authored the paper "Least-to-most prompting enables complex reasoning in large language models"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="JEFF CLUNE" target="SHENGRAN HU">
      <data key="d4">16.0</data>
      <data key="d5">Shengran Hu and Jeff Clune co-authored the study on Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="JEFF CLUNE" target="CONG LU">
      <data key="d4">16.0</data>
      <data key="d5">Cong Lu and Jeff Clune co-authored the study on Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="JEFF CLUNE" target="JENNY ZHANG">
      <data key="d4">16.0</data>
      <data key="d5">Jenny Zhang and Jeff Clune co-authored the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JEFF CLUNE" target="JOEL LEHMAN">
      <data key="d4">16.0</data>
      <data key="d5">Joel Lehman and Jeff Clune co-authored the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JEFF CLUNE" target="KENNETH STANLEY">
      <data key="d4">16.0</data>
      <data key="d5">Kenneth Stanley and Jeff Clune co-authored the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="CHRISTOPHER D MANNING" target="RAFAEL RAFAILOV">
      <data key="d4">8.0</data>
      <data key="d5">Rafael Rafailov and Christopher D Manning co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="CHRISTOPHER D MANNING" target="ARCHIT SHARMA">
      <data key="d4">8.0</data>
      <data key="d5">Archit Sharma and Christopher D Manning co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="CHRISTOPHER D MANNING" target="ERIC MITCHELL">
      <data key="d4">8.0</data>
      <data key="d5">Eric Mitchell and Christopher D Manning co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="CHRISTOPHER D MANNING" target="STEFANO ERMON">
      <data key="d4">8.0</data>
      <data key="d5">Christopher D Manning and Stefano Ermon co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="AMANDA ASKELL" target="ANTHROPIC">
      <data key="d4">12.0</data>
      <data key="d5">Amanda Askell is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ILYA SUTSKEVER" target="DAVID SILVER">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Ilya Sutskever co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="AAKANKSHA CHOWDHERY" target="MIRAC SUZGUN">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Aakanksha Chowdhery co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="AAKANKSHA CHOWDHERY" target="NATHAN SCALES">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Aakanksha Chowdhery co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="AAKANKSHA CHOWDHERY" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Aakanksha Chowdhery co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="AAKANKSHA CHOWDHERY" target="SEBASTIAN GEHRMANN">
      <data key="d4">8.0</data>
      <data key="d5">Sebastian Gehrmann and Aakanksha Chowdhery co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="AAKANKSHA CHOWDHERY" target="YI TAY">
      <data key="d4">8.0</data>
      <data key="d5">Yi Tay and Aakanksha Chowdhery co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="HYUNG WON CHUNG" target="MIRAC SUZGUN">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Hyung Won Chung co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="HYUNG WON CHUNG" target="NATHAN SCALES">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Hyung Won Chung co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="HYUNG WON CHUNG" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Hyung Won Chung co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="HYUNG WON CHUNG" target="SEBASTIAN GEHRMANN">
      <data key="d4">8.0</data>
      <data key="d5">Sebastian Gehrmann and Hyung Won Chung co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="HYUNG WON CHUNG" target="YI TAY">
      <data key="d4">8.0</data>
      <data key="d5">Yi Tay and Hyung Won Chung co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="MIRAC SUZGUN">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Sebastian Gehrmann co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="NATHAN SCALES">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Sebastian Gehrmann co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Sebastian Gehrmann co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="YI TAY">
      <data key="d4">8.0</data>
      <data key="d5">Sebastian Gehrmann and Yi Tay co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="QUOC V LE">
      <data key="d4">8.0</data>
      <data key="d5">Sebastian Gehrmann and Quoc V Le co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="ED H CHI">
      <data key="d4">8.0</data>
      <data key="d5">Sebastian Gehrmann and Ed H Chi co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="DENNY ZHOU">
      <data key="d4">8.0</data>
      <data key="d5">Sebastian Gehrmann and Denny Zhou co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="JASON WEI">
      <data key="d4">8.0</data>
      <data key="d5">Sebastian Gehrmann and Jason Wei co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YI TAY" target="MIRAC SUZGUN">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Yi Tay co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YI TAY" target="NATHAN SCALES">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Yi Tay co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YI TAY" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Yi Tay co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YI TAY" target="QUOC V LE">
      <data key="d4">8.0</data>
      <data key="d5">Yi Tay and Quoc V Le co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YI TAY" target="ED H CHI">
      <data key="d4">8.0</data>
      <data key="d5">Yi Tay and Ed H Chi co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="DENNY ZHOU" target="XUEZHI WANG">
      <data key="d4">8.0</data>
      <data key="d5">Xuezhi Wang and Denny Zhou co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="DENNY ZHOU" target="JASON WEI">
      <data key="d4">8.0</data>
      <data key="d5">Jason Wei and Denny Zhou co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="DENNY ZHOU" target="DALE SCHUURMANS">
      <data key="d4">8.0</data>
      <data key="d5">Dale Schuurmans and Denny Zhou co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="DENNY ZHOU" target="ED CHI">
      <data key="d4">8.0</data>
      <data key="d5">Ed Chi and Denny Zhou co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="DENNY ZHOU" target="MIRAC SUZGUN">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Denny Zhou co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="DENNY ZHOU" target="NATHAN SCALES">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Denny Zhou co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="DENNY ZHOU" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Denny Zhou co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="XUEZHI WANG" target="JASON WEI">
      <data key="d4">8.0</data>
      <data key="d5">Xuezhi Wang and Jason Wei co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="XUEZHI WANG" target="DALE SCHUURMANS">
      <data key="d4">8.0</data>
      <data key="d5">Xuezhi Wang and Dale Schuurmans co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="XUEZHI WANG" target="ED CHI">
      <data key="d4">8.0</data>
      <data key="d5">Xuezhi Wang and Ed Chi co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="JASON WEI" target="DALE SCHUURMANS">
      <data key="d4">8.0</data>
      <data key="d5">Jason Wei and Dale Schuurmans co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="JASON WEI" target="ED CHI">
      <data key="d4">8.0</data>
      <data key="d5">Jason Wei and Ed Chi co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="JASON WEI" target="MIRAC SUZGUN">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Jason Wei co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="JASON WEI" target="NATHAN SCALES">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Jason Wei co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="JASON WEI" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Jason Wei co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="DALE SCHUURMANS" target="ED CHI">
      <data key="d4">8.0</data>
      <data key="d5">Dale Schuurmans and Ed Chi co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="PIETER ABBEEL" target="PHILIPP WU">
      <data key="d4">8.0</data>
      <data key="d5">Philipp Wu and Pieter Abbeel co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="PIETER ABBEEL" target="ALEJANDRO ESCONTRELA">
      <data key="d4">8.0</data>
      <data key="d5">Alejandro Escontrela and Pieter Abbeel co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="PIETER ABBEEL" target="YOSHUA BENGIO">
      <data key="d4">8.0</data>
      <data key="d5">Yoshua Bengio and Pieter Abbeel co-authored the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="LINXI FAN" target="GUANZHI WANG">
      <data key="d4">8.0</data>
      <data key="d5">Guanzhi Wang and Linxi Fan co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="GUANZHI WANG" target="YUQI XIE">
      <data key="d4">8.0</data>
      <data key="d5">Guanzhi Wang and Yuqi Xie co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="GUANZHI WANG" target="YUNFAN JIANG">
      <data key="d4">8.0</data>
      <data key="d5">Guanzhi Wang and Yunfan Jiang co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="GUANZHI WANG" target="AJAY MANDLEKAR">
      <data key="d4">8.0</data>
      <data key="d5">Guanzhi Wang and Ajay Mandlekar co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="GUANZHI WANG" target="CHAOWEI XIAO">
      <data key="d4">8.0</data>
      <data key="d5">Guanzhi Wang and Chaowei Xiao co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="GUANZHI WANG" target="YUKE ZHU">
      <data key="d4">8.0</data>
      <data key="d5">Guanzhi Wang and Yuke Zhu co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="GUANZHI WANG" target="ANIMA ANANDKUMAR">
      <data key="d4">8.0</data>
      <data key="d5">Guanzhi Wang and Anima Anandkumar co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="PENGFEI LIU" target="YIXIN LIU">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Pengfei Liu co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="DANIJAR HAFNER" target="PHILIPP WU">
      <data key="d4">8.0</data>
      <data key="d5">Philipp Wu and Danijar Hafner co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="DANIJAR HAFNER" target="ALEJANDRO ESCONTRELA">
      <data key="d4">8.0</data>
      <data key="d5">Alejandro Escontrela and Danijar Hafner co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="PERCY LIANG" target="XUECHEN LI">
      <data key="d4">8.0</data>
      <data key="d5">Xuechen Li and Percy Liang co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="PERCY LIANG" target="TIANYI ZHANG">
      <data key="d4">8.0</data>
      <data key="d5">Tianyi Zhang and Percy Liang co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="PERCY LIANG" target="YANN DUBOIS">
      <data key="d4">8.0</data>
      <data key="d5">Yann Dubois and Percy Liang co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="PERCY LIANG" target="ROHAN TAORI">
      <data key="d4">8.0</data>
      <data key="d5">Rohan Taori and Percy Liang co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="PERCY LIANG" target="ISHAAN GULRAJANI">
      <data key="d4">8.0</data>
      <data key="d5">Ishaan Gulrajani and Percy Liang co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="PERCY LIANG" target="CARLOS GUESTRIN">
      <data key="d4">8.0</data>
      <data key="d5">Carlos Guestrin and Percy Liang co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="PERCY LIANG" target="TATSUNORI B. HASHIMOTO">
      <data key="d4">8.0</data>
      <data key="d5">Percy Liang and Tatsunori B. Hashimoto co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="PETER CLARK" target="ISAAC COWHEY">
      <data key="d4">8.0</data>
      <data key="d5">Peter Clark and Isaac Cowhey co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="PETER CLARK" target="OREN ETZIONI">
      <data key="d4">8.0</data>
      <data key="d5">Peter Clark and Oren Etzioni co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="PETER CLARK" target="TUSHAR KHOT">
      <data key="d4">8.0</data>
      <data key="d5">Peter Clark and Tushar Khot co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="PETER CLARK" target="ASHISH SABHARWAL">
      <data key="d4">8.0</data>
      <data key="d5">Peter Clark and Ashish Sabharwal co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="PETER CLARK" target="CARISSA SCHOENICK">
      <data key="d4">8.0</data>
      <data key="d5">Peter Clark and Carissa Schoenick co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="PETER CLARK" target="OYVIND TAFJORD">
      <data key="d4">8.0</data>
      <data key="d5">Peter Clark and Oyvind Tafjord co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="DAVID SILVER" target="AJA HUANG">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and Aja Huang co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"David Silver and Aja Huang co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="CHRIS J. MADDISON">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and Chris J. Maddison co-authored the paper "Mastering the game of Go with deep neural networks and tree search"David Silver and Chris J. Maddison co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="ARTHUR GUEZ">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and Arthur Guez co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"David Silver and Arthur Guez co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="L. SIFRE">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and L. Sifre co-authored the paper "Mastering the game of Go with deep neural networks and tree search"David Silver and L. Sifre co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="GEORGE VAN DEN DRIESSCHE">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and George van den Driessche co-authored the paper "Mastering the game of Go with deep neural networks and tree search"David Silver and George van den Driessche co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="JULIAN SCHRITTWIESER">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and Julian Schrittwieser co-authored the paper "Mastering the game of Go with deep neural networks and tree search"David Silver and Julian Schrittwieser co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="IOANNIS ANTONOGLOU">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and Ioannis Antonoglou co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"David Silver and Ioannis Antonoglou co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="VEDAVYAS PANNEERSHELVAM">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and Vedavyas Panneershelvam co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"David Silver and Vedavyas Panneershelvam co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="MARC LANCTOT">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and Marc Lanctot co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"David Silver and Marc Lanctot co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="SANDER DIELEMAN">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Sander Dieleman co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="DOMINIK GREWE">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Dominik Grewe co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="JOHN NHAM">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and John Nham co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="NAL KALCHBRENNER">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Nal Kalchbrenner co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="TIMOTHY P. LILLICRAP">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Timothy P. Lillicrap co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="MADELEINE LEACH">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Madeleine Leach co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="KORAY KAVUKCUOGLU">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Koray Kavukcuoglu co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="THORE GRAEPEL">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Thore Graepel co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="DEMIS HASSABIS">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Demis Hassabis co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="YUCHEN ZHUANG" target="XIANG CHEN">
      <data key="d4">8.0</data>
      <data key="d5">Yuchen Zhuang and Xiang Chen co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="YUCHEN ZHUANG" target="TONG YU">
      <data key="d4">8.0</data>
      <data key="d5">Yuchen Zhuang and Tong Yu co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="YUCHEN ZHUANG" target="SAAYAN MITRA">
      <data key="d4">8.0</data>
      <data key="d5">Yuchen Zhuang and Saayan Mitra co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="YUCHEN ZHUANG" target="VICTOR BURSZTYN">
      <data key="d4">8.0</data>
      <data key="d5">Yuchen Zhuang and Victor Bursztyn co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="YUCHEN ZHUANG" target="RYAN A. ROSSI">
      <data key="d4">8.0</data>
      <data key="d5">Yuchen Zhuang and Ryan A. Rossi co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="YUCHEN ZHUANG" target="SOMDEB SARKHEL">
      <data key="d4">8.0</data>
      <data key="d5">Yuchen Zhuang and Somdeb Sarkhel co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="YUCHEN ZHUANG" target="CHAO ZHANG">
      <data key="d4">8.0</data>
      <data key="d5">Yuchen Zhuang and Chao Zhang co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="CHAO ZHANG" target="XIANG CHEN">
      <data key="d4">8.0</data>
      <data key="d5">Xiang Chen and Chao Zhang co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="CHAO ZHANG" target="TONG YU">
      <data key="d4">8.0</data>
      <data key="d5">Tong Yu and Chao Zhang co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="CHAO ZHANG" target="SAAYAN MITRA">
      <data key="d4">8.0</data>
      <data key="d5">Saayan Mitra and Chao Zhang co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="CHAO ZHANG" target="VICTOR BURSZTYN">
      <data key="d4">8.0</data>
      <data key="d5">Victor Bursztyn and Chao Zhang co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="CHAO ZHANG" target="RYAN A. ROSSI">
      <data key="d4">8.0</data>
      <data key="d5">Ryan A. Rossi and Chao Zhang co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="CHAO ZHANG" target="SOMDEB SARKHEL">
      <data key="d4">1.0</data>
      <data key="d5">Somdeb Sarkhel and Chao Zhang co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="TOM VODOPIVEC" target="SPYRIDON SAMOTHRAKIS">
      <data key="d4">8.0</data>
      <data key="d5">Tom Vodopivec and Spyridon Samothrakis co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="TOM VODOPIVEC" target="BRANKO STER">
      <data key="d4">8.0</data>
      <data key="d5">Tom Vodopivec and Branko Ster co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="SPYRIDON SAMOTHRAKIS" target="BRANKO STER">
      <data key="d4">8.0</data>
      <data key="d5">Spyridon Samothrakis and Branko Ster co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="ED CHI" target="OLIVIER BOUSQUET">
      <data key="d4">8.0</data>
      <data key="d5">Olivier Bousquet and Ed Chi co-authored the paper "Least-to-most prompting enables complex reasoning in large language models"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="MICHAEL WOOLDRIDGE" target="NICHOLAS R JENNINGS">
      <data key="d4">8.0</data>
      <data key="d5">Michael Wooldridge and Nicholas R Jennings co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="PHILIPP WU" target="ALEJANDRO ESCONTRELA">
      <data key="d4">8.0</data>
      <data key="d5">Philipp Wu and Alejandro Escontrela co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="PHILIPP WU" target="KEN GOLDBERG">
      <data key="d4">8.0</data>
      <data key="d5">Philipp Wu and Ken Goldberg co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="ALEJANDRO ESCONTRELA" target="KEN GOLDBERG">
      <data key="d4">8.0</data>
      <data key="d5">Alejandro Escontrela and Ken Goldberg co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="YOSHUA BENGIO" target="GEOFFREY HINTON">
      <data key="d4">16.0</data>
      <data key="d5">Yoshua Bengio and Geoffrey Hinton co-authored the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="YOSHUA BENGIO" target="ANDREW YAO">
      <data key="d4">16.0</data>
      <data key="d5">Yoshua Bengio and Andrew Yao co-authored the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="YOSHUA BENGIO" target="DAWN SONG">
      <data key="d4">16.0</data>
      <data key="d5">Yoshua Bengio and Dawn Song co-authored the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="NATHAN SCALES" target="MIRAC SUZGUN">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Nathan Scales co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="NATHAN SCALES" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Nathanael Sch&#228;rli co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="NATHAN SCALES" target="QUOC V LE">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Quoc V Le co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="NATHAN SCALES" target="ED H CHI">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Ed H Chi co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="XIANG CHEN" target="TONG YU">
      <data key="d4">8.0</data>
      <data key="d5">Xiang Chen and Tong Yu co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="XIANG CHEN" target="SAAYAN MITRA">
      <data key="d4">8.0</data>
      <data key="d5">Xiang Chen and Saayan Mitra co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="XIANG CHEN" target="VICTOR BURSZTYN">
      <data key="d4">8.0</data>
      <data key="d5">Xiang Chen and Victor Bursztyn co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="XIANG CHEN" target="RYAN A. ROSSI">
      <data key="d4">8.0</data>
      <data key="d5">Xiang Chen and Ryan A. Rossi co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="XIANG CHEN" target="SOMDEB SARKHEL">
      <data key="d4">8.0</data>
      <data key="d5">Xiang Chen and Somdeb Sarkhel co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="TONG YU" target="SAAYAN MITRA">
      <data key="d4">8.0</data>
      <data key="d5">Tong Yu and Saayan Mitra co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="TONG YU" target="VICTOR BURSZTYN">
      <data key="d4">8.0</data>
      <data key="d5">Tong Yu and Victor Bursztyn co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="TONG YU" target="RYAN A. ROSSI">
      <data key="d4">8.0</data>
      <data key="d5">Tong Yu and Ryan A. Rossi co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="TONG YU" target="SOMDEB SARKHEL">
      <data key="d4">8.0</data>
      <data key="d5">Tong Yu and Somdeb Sarkhel co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="SAAYAN MITRA" target="VICTOR BURSZTYN">
      <data key="d4">8.0</data>
      <data key="d5">Saayan Mitra and Victor Bursztyn co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="SAAYAN MITRA" target="RYAN A. ROSSI">
      <data key="d4">8.0</data>
      <data key="d5">Saayan Mitra and Ryan A. Rossi co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="SAAYAN MITRA" target="SOMDEB SARKHEL">
      <data key="d4">8.0</data>
      <data key="d5">Saayan Mitra and Somdeb Sarkhel co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="VICTOR BURSZTYN" target="RYAN A. ROSSI">
      <data key="d4">8.0</data>
      <data key="d5">Victor Bursztyn and Ryan A. Rossi co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="VICTOR BURSZTYN" target="SOMDEB SARKHEL">
      <data key="d4">8.0</data>
      <data key="d5">Victor Bursztyn and Somdeb Sarkhel co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="RYAN A. ROSSI" target="SOMDEB SARKHEL">
      <data key="d4">8.0</data>
      <data key="d5">Ryan A. Rossi and Somdeb Sarkhel co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="WIKIPEDIA WEB API" target="SEARCH [ENTITY]">
      <data key="d4">8.0</data>
      <data key="d5">Search [entity] is an action type in the Wikipedia web API used in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="WIKIPEDIA WEB API" target="LOOKUP [STRING]">
      <data key="d4">1.0</data>
      <data key="d5">Lookup [string] is an action type in the Wikipedia web API used in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="ITERATIONS" target="INSTRUCTION REFINEMENT FLOW">
      <data key="d4">8.0</data>
      <data key="d5">Instruction Refinement Flow involves iterations to enhance the complexity and quality of instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="SEARCH" target="LOOKUP">
      <data key="d4">7.0</data>
      <data key="d5">Search and Lookup are actions used in HotPotQA</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="SEARCH" target="DAIRY FREE AND APPLE VARIETY PACK OF CHIPS">
      <data key="d4">8.0</data>
      <data key="d5">The search action is used to look for the dairy-free and apple variety pack of chips</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="SEARCH" target="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON">
      <data key="d4">8.0</data>
      <data key="d5">The search action is used to look for gluten-free vegetarian smoked peppered bacon</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="SEARCH" target="REFINE SEARCH">
      <data key="d4">7.0</data>
      <data key="d5">Refine Search is an action taken to improve the results of a Search</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="SEARCH" target="VEGETARIAN BACON">
      <data key="d4">7.0</data>
      <data key="d5">Vegetarian Bacon is the product being searched for</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="LOOKUP" target="INTERACTIVE INFORMATION RETRIEVAL">
      <data key="d4">16.0</data>
      <data key="d5">Lookup is one of the commands used in interactive information retrieval to return the next sentence containing a specified string.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="FINISH" target="INTERACTIVE INFORMATION RETRIEVAL">
      <data key="d4">16.0</data>
      <data key="d5">Finish is one of the commands used in interactive information retrieval to complete the current task.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="FINISH" target="THOUGHT">
      <data key="d4">8.0</data>
      <data key="d5">Thought can lead to Finish in HotPotQA</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="UNIT TESTS" target="MINIMUM SUBARRAY SUM">
      <data key="d4">9.0</data>
      <data key="d5">Unit tests are used to validate the correctness of the Minimum Subarray Sum function</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="BAUER MEDIA GROUP" target="FIRST FOR WOMEN">
      <data key="d4">8.0</data>
      <data key="d5">First for Women is published by Bauer Media Group</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="ARTHUR'S MAGAZINE" target="GODEY'S LADY'S BOOK">
      <data key="d4">8.0</data>
      <data key="d5">Arthur's Magazine was merged into Godey's Lady's Book in May 1846</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="ARTHUR'S MAGAZINE" target="FIRST FOR WOMEN">
      <data key="d4">1.0</data>
      <data key="d5">Arthur's Magazine and First for Women are compared in a question answering task</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="QUERY" target="RESULTS">
      <data key="d4">8.0</data>
      <data key="d5">Query leads to Results in WebShop</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="RESULTS" target="ITEM">
      <data key="d4">8.0</data>
      <data key="d5">Results lead to Item in WebShop</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="ITEM" target="ITEM-DETAIL">
      <data key="d4">15.0</data>
      <data key="d5">Item leads to Item-Detail in WebShopItem-Detail can lead back to Item in WebShop</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="ITEM" target="EPISODE END">
      <data key="d4">8.0</data>
      <data key="d5">Item can lead to Episode End in WebShop</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="THOUGHT" target="ACTION">
      <data key="d4">24.0</data>
      <data key="d5">THOUGHT and ACTION are intrinsically linked, with the thought process determining the actions to be taken. In the context of HotPotQA, thought leads directly to action, illustrating the critical role of cognitive processes in guiding subsequent behaviors.</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f,b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="ACTION" target="THINK">
      <data key="d4">7.0</data>
      <data key="d5">Think is a type of action taken by the user</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="ACTION" target="CLICK">
      <data key="d4">7.0</data>
      <data key="d5">Click is a type of action taken by the user</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="COLORADO OROGENY" target="HIGH PLAINS">
      <data key="d4">1.0</data>
      <data key="d5">Colorado Orogeny extends into the High Plains</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="VALUE FUNCTION PROMPT" target="WEB SHOP">
      <data key="d4">16.0</data>
      <data key="d5">The Value Function Prompt involves analyzing a purchasing trajectory on a web shop</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ARTHUR&#8217;S MAGAZINE" target="GODEY&#8217;S LADY&#8217;S BOOK">
      <data key="d4">8.0</data>
      <data key="d5">Arthur&#8217;s Magazine was merged into Godey&#8217;s Lady&#8217;s Book in May 1846</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="ARTHUR&#8217;S MAGAZINE" target="TIMOTHY SHAY ARTHUR">
      <data key="d4">9.0</data>
      <data key="d5">Timothy Shay Arthur was the editor of Arthur&#8217;s Magazine</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="ARTHUR&#8217;S MAGAZINE" target="EDGAR A. POE">
      <data key="d4">7.0</data>
      <data key="d5">Edgar A. Poe was a contributor to Arthur&#8217;s Magazine</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="ARTHUR&#8217;S MAGAZINE" target="J.H. INGRAHAM">
      <data key="d4">7.0</data>
      <data key="d5">J.H. Ingraham was a contributor to Arthur&#8217;s Magazine</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="ARTHUR&#8217;S MAGAZINE" target="SARAH JOSEPHA HALE">
      <data key="d4">7.0</data>
      <data key="d5">Sarah Josepha Hale was a contributor to Arthur&#8217;s Magazine</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="ARTHUR&#8217;S MAGAZINE" target="THOMAS G. SPEAR">
      <data key="d4">1.0</data>
      <data key="d5">Thomas G. Spear was a contributor to Arthur&#8217;s Magazine</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="REFLECTION PROMPT" target="WEB SHOP">
      <data key="d4">9.0</data>
      <data key="d5">The Reflection Prompt involves diagnosing a failure in a purchasing trial on a web shop</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="PROGRAMMING PROMPTS" target="HUMANEVAL FUNCTION IMPLEMENTATION EXAMPLE">
      <data key="d4">14.0</data>
      <data key="d5">Programming Prompts include examples like the HumanEval function implementation</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="HUMANEVAL FUNCTION IMPLEMENTATION EXAMPLE" target="MINIMUM SUBARRAY SUM">
      <data key="d4">16.0</data>
      <data key="d5">The HumanEval function implementation example includes a sample implementation of the Minimum Subarray Sum function</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="AI PYTHON ASSISTANT" target="FUNCTION IMPLEMENTATION">
      <data key="d4">9.0</data>
      <data key="d5">The AI Python assistant helps in writing and defining function implementations</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="AI PYTHON ASSISTANT" target="UNIT TEST">
      <data key="d4">9.0</data>
      <data key="d5">The AI Python assistant runs unit tests to validate the function implementations</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="AI PYTHON ASSISTANT" target="BASE ACTING/REASONING PROMPT">
      <data key="d4">8.0</data>
      <data key="d5">The BASE ACTING/REASONING PROMPT guides the AI Python assistant in implementing a function</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="FUNCTION IMPLEMENTATION" target="UNIT TEST">
      <data key="d4">8.0</data>
      <data key="d5">Unit tests are used to validate the correctness of function implementations</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="FUNCTION IMPLEMENTATION" target="IMPROVED IMPLEMENTATION">
      <data key="d4">8.0</data>
      <data key="d5">IMPROVED IMPLEMENTATION is the revised version of a FUNCTION IMPLEMENTATION</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="UNIT TEST" target="TEST CASE GENERATION PROMPT">
      <data key="d4">8.0</data>
      <data key="d5">The TEST CASE GENERATION PROMPT guides the AI coding assistant in writing UNIT TESTS</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="BRIGHT CITRUS DEODORANT" target="EARTH MAMA">
      <data key="d4">26.0</data>
      <data key="d5">Bright Citrus Deodorant is a product by the brand Earth Mama.</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad,785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="BRIGHT CITRUS DEODORANT" target="PRICE">
      <data key="d4">7.0</data>
      <data key="d5">PRICE is an attribute of the BRIGHT CITRUS DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="BRIGHT CITRUS DEODORANT" target="SIZE">
      <data key="d4">7.0</data>
      <data key="d5">SIZE is an attribute of the BRIGHT CITRUS DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="BRIGHT CITRUS DEODORANT" target="SCENT">
      <data key="d4">7.0</data>
      <data key="d5">SCENT is an attribute of the BRIGHT CITRUS DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="EARTH MAMA" target="GINGER FRESH DEODORANT">
      <data key="d4">17.0</data>
      <data key="d5">Ginger Fresh Deodorant is a product by the brand Earth Mama.</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad,785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="EARTH MAMA" target="CALMING LAVENDER DEODORANT">
      <data key="d4">9.0</data>
      <data key="d5">Calming Lavender Deodorant is a product made by the brand Earth Mama</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="EARTH MAMA" target="SIMPLY NON-SCENTS DEODORANT">
      <data key="d4">9.0</data>
      <data key="d5">Simply Non-Scents Deodorant is a product made by the brand Earth Mama</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="EARTH MAMA" target="TRAVEL SET (4-PACK)">
      <data key="d4">9.0</data>
      <data key="d5">Travel Set (4-Pack) is a product made by the brand Earth Mama</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="EARTH MAMA" target="3 OUNCE (PACK OF 1)">
      <data key="d4">9.0</data>
      <data key="d5">3 Ounce (Pack of 1) is a product made by the brand Earth Mama</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="EARTH MAMA" target="3-OUNCE (2-PACK)">
      <data key="d4">9.0</data>
      <data key="d5">3-Ounce (2-Pack) is a product made by the brand Earth Mama</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="GINGER FRESH DEODORANT" target="PRICE">
      <data key="d4">7.0</data>
      <data key="d5">PRICE is an attribute of the GINGER FRESH DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="GINGER FRESH DEODORANT" target="SIZE">
      <data key="d4">7.0</data>
      <data key="d5">SIZE is an attribute of the GINGER FRESH DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="GINGER FRESH DEODORANT" target="SCENT">
      <data key="d4">7.0</data>
      <data key="d5">SCENT is an attribute of the GINGER FRESH DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="BARREL AND OAK" target="CEDAR &amp; PATCHOULI DEODORANT">
      <data key="d4">1.0</data>
      <data key="d5">Cedar &amp; Patchouli Deodorant is a product by the brand Barrel and Oak</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="CEDAR &amp; PATCHOULI DEODORANT" target="PRICE">
      <data key="d4">7.0</data>
      <data key="d5">PRICE is an attribute of the CEDAR &amp; PATCHOULI DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="CEDAR &amp; PATCHOULI DEODORANT" target="SIZE">
      <data key="d4">7.0</data>
      <data key="d5">SIZE is an attribute of the CEDAR &amp; PATCHOULI DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="CEDAR &amp; PATCHOULI DEODORANT" target="SCENT">
      <data key="d4">1.0</data>
      <data key="d5">SCENT is an attribute of the CEDAR &amp; PATCHOULI DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="MIN SUM" target="CURRENT SUM">
      <data key="d4">7.0</data>
      <data key="d5">MIN SUM is updated based on the value of CURRENT SUM during iteration</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="CURRENT SUM" target="NUMS">
      <data key="d4">7.0</data>
      <data key="d5">CURRENT SUM is calculated by iterating over the elements in NUMS</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="THINK" target="CLICK">
      <data key="d4">7.0</data>
      <data key="d5">CLICK is the action taken after the THINK process</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="THINK" target="DAIRY FREE AND APPLE VARIETY PACK OF CHIPS">
      <data key="d4">7.0</data>
      <data key="d5">The think action is used to reflect on whether the dairy-free and apple variety pack of chips meets the criteria</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="THINK" target="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON">
      <data key="d4">7.0</data>
      <data key="d5">The think action is used to reflect on whether the gluten-free vegetarian smoked peppered bacon meets the criteria</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="CLICK" target="DAIRY FREE AND APPLE VARIETY PACK OF CHIPS">
      <data key="d4">8.0</data>
      <data key="d5">The click action is used to select the dairy-free and apple variety pack of chips</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="CLICK" target="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON">
      <data key="d4">8.0</data>
      <data key="d5">The click action is used to select the gluten-free vegetarian smoked peppered bacon</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="DAIRY FREE AND APPLE VARIETY PACK OF CHIPS">
      <data key="d4">27.0</data>
      <data key="d5">Enjoy Life Foods offers a dairy-free and apple variety pack of chips. This product is specifically made by Enjoy Life Foods, catering to those seeking dairy-free snack options.</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd,6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="ENJOY LIFE FOODS SOFT BAKED OVALS">
      <data key="d4">9.0</data>
      <data key="d5">Enjoy Life Foods Soft Baked Ovals are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="ENJOY LIFE SOFT BAKED CHEWY BARS">
      <data key="d4">9.0</data>
      <data key="d5">Enjoy Life Soft Baked Chewy Bars are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="ENJOY LIFE LENTIL CHIPS VARIETY PACK">
      <data key="d4">9.0</data>
      <data key="d5">Enjoy Life Lentil Chips Variety Pack is a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="DAIRY FREE CHIPS">
      <data key="d4">18.0</data>
      <data key="d5">Dairy-Free Chips are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="NUT FREE BARS">
      <data key="d4">9.0</data>
      <data key="d5">Nut-Free Bars are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="SOY FREE BARS">
      <data key="d4">9.0</data>
      <data key="d5">Soy-Free Bars are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="GLUTEN FREE BARS">
      <data key="d4">9.0</data>
      <data key="d5">Gluten-Free Bars are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="VEGAN BARS">
      <data key="d4">9.0</data>
      <data key="d5">Vegan Bars are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="NON GMO BARS">
      <data key="d4">9.0</data>
      <data key="d5">Non-GMO Bars are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="SOY FREE CHIPS">
      <data key="d4">9.0</data>
      <data key="d5">Soy-Free Chips are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="NUT FREE CHIPS">
      <data key="d4">9.0</data>
      <data key="d5">Nut-Free Chips are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="NON GMO CHIPS">
      <data key="d4">9.0</data>
      <data key="d5">Non-GMO Chips are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="VEGAN CHIPS">
      <data key="d4">9.0</data>
      <data key="d5">Vegan Chips are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="GLUTEN FREE CHIPS">
      <data key="d4">1.0</data>
      <data key="d5">Gluten-Free Chips are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="SOFT BAKED OVALS">
      <data key="d4">8.0</data>
      <data key="d5">Soft Baked Ovals are a product offered by Enjoy Life Foods</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="SOFT BAKED CHEWY BARS">
      <data key="d4">8.0</data>
      <data key="d5">Soft Baked Chewy Bars are a product offered by Enjoy Life Foods</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="LENTIL CHIPS">
      <data key="d4">8.0</data>
      <data key="d5">Lentil Chips are a product offered by Enjoy Life Foods</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="ENJOY LIFE FOODS SOFT BAKED OVALS" target="SOFT BAKED OVALS">
      <data key="d4">8.0</data>
      <data key="d5">Enjoy Life Foods Soft Baked Ovals is a specific product that includes breakfast bars</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="ENJOY LIFE SOFT BAKED CHEWY BARS" target="SOFT BAKED CHEWY BARS">
      <data key="d4">8.0</data>
      <data key="d5">Enjoy Life Soft Baked Chewy Bars is a specific product that includes chewy bars</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="LENTIL CHIPS" target="VARIETY PACK">
      <data key="d4">7.0</data>
      <data key="d5">The variety pack is an option for the Enjoy Life Lentil Chips</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="LENTIL CHIPS" target="0.8 OUNCE (PACK OF 24)">
      <data key="d4">7.0</data>
      <data key="d5">The 0.8 ounce (pack of 24) is a size option for the Enjoy Life Lentil Chips</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="LENTIL CHIPS" target="BUY NOW">
      <data key="d4">6.0</data>
      <data key="d5">The Buy Now action is used to purchase the Enjoy Life Lentil Chips</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="LENTIL CHIPS" target="ENJOY LIFE LENTIL CHIPS">
      <data key="d4">8.0</data>
      <data key="d5">Enjoy Life Lentil Chips is a specific product that includes lentil chips</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON" target="SMOKED BACON SEA SALT">
      <data key="d4">5.0</data>
      <data key="d5">Both products are gluten-free and have a smoked bacon flavor</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON" target="SPICY HOT PEPPER SEA SALT">
      <data key="d4">5.0</data>
      <data key="d5">Both products are gluten-free and contain pepper flavors</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON" target="LOUISVILLE VEGAN JERKY">
      <data key="d4">1.0</data>
      <data key="d5">Both products are gluten-free and vegetarian</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON" target="PREVIOUS TRIAL INSTRUCTION">
      <data key="d4">7.0</data>
      <data key="d5">The previous trial instruction was to search for gluten-free vegetarian smoked peppered bacon</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="SMOKED BACON SEA SALT" target="SMOKED BACON CHIPOTLE">
      <data key="d4">7.0</data>
      <data key="d5">Smoked Bacon Chipotle is a flavor option in the Smoked Bacon Sea Salt 3-Pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="SMOKED BACON SEA SALT" target="SMOKED BACON AND ONION">
      <data key="d4">7.0</data>
      <data key="d5">Smoked Bacon and Onion is a flavor option in the Smoked Bacon Sea Salt 3-Pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="SPICY HOT PEPPER SEA SALT" target="GHOST PEPPER">
      <data key="d4">7.0</data>
      <data key="d5">Ghost Pepper is a flavor option in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="SPICY HOT PEPPER SEA SALT" target="JALAPENO">
      <data key="d4">7.0</data>
      <data key="d5">Jalapeno is a flavor option in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="SPICY HOT PEPPER SEA SALT" target="HABANERO">
      <data key="d4">7.0</data>
      <data key="d5">Habanero is a flavor option in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="LOUISVILLE VEGAN JERKY" target="BLACK PEPPER">
      <data key="d4">15.0</data>
      <data key="d5">Louisville Vegan Jerky offers a variety of flavors in their 5-Flavor Variety Pack, one of which is Black Pepper. This flavor is a notable option within the assortment, providing consumers with a distinct and savory taste experience as part of the Louisville Vegan Jerky lineup.</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="LOUISVILLE VEGAN JERKY" target="BUFFALO DILL">
      <data key="d4">15.0</data>
      <data key="d5">Louisville Vegan Jerky offers a 5-Flavor Variety Pack that includes Buffalo Dill as one of its flavor options. Buffalo Dill is a notable flavor within this assortment, contributing to the diverse and appealing range of tastes provided by Louisville Vegan Jerky.</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="LOUISVILLE VEGAN JERKY" target="PEPPERONI">
      <data key="d4">15.0</data>
      <data key="d5">Louisville Vegan Jerky offers a 5-Flavor Variety Pack that includes Pepperoni as one of its flavor options. This variety pack provides consumers with a selection of different flavors, among which Pepperoni stands out as a notable choice.</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="LOUISVILLE VEGAN JERKY" target="MAPLE BACON">
      <data key="d4">15.0</data>
      <data key="d5">Louisville Vegan Jerky offers a variety pack that includes the Maple Bacon flavor. This flavor is one of the options available in their 5-Flavor Variety Pack, providing a diverse selection for consumers seeking plant-based jerky alternatives.</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="LOUISVILLE VEGAN JERKY" target="CAROLINA BBQ">
      <data key="d4">9.0</data>
      <data key="d5">Louisville Vegan Jerky offers a 5-Flavor Variety Pack that includes the Carolina BBQ flavor. This flavor is one of the options available in their variety pack, providing a diverse selection for consumers seeking plant-based jerky alternatives.</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="LOUISVILLE VEGAN JERKY" target="SOY PROTEIN">
      <data key="d4">9.0</data>
      <data key="d5">Soy Protein is the main ingredient in Louisville Vegan Jerky</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="GHOST PEPPER" target="SPICY HOT PEPPER SEA SALT 3-PACK">
      <data key="d4">9.0</data>
      <data key="d5">Ghost Pepper is one of the ingredients in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="JALAPENO" target="SPICY HOT PEPPER SEA SALT 3-PACK">
      <data key="d4">9.0</data>
      <data key="d5">Jalapeno is one of the ingredients in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="HABANERO" target="SPICY HOT PEPPER SEA SALT 3-PACK">
      <data key="d4">9.0</data>
      <data key="d5">Habanero is one of the ingredients in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="REFINE SEARCH" target="GLUTEN-FREE AND 4 OUNCE PACK OF 2">
      <data key="d4">1.0</data>
      <data key="d5">Refine Search is done to meet the constraints of Gluten-Free and 4 Ounce Pack of 2</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="STATUS" target="FAIL">
      <data key="d4">8.0</data>
      <data key="d5">Fail is a status indicating an unsuccessful attempt</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="SHENGRAN HU" target="CONG LU">
      <data key="d4">16.0</data>
      <data key="d5">Shengran Hu and Cong Lu co-authored the study on Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="SHENGRAN HU" target="SHENGRAN HU'S GITHUB">
      <data key="d4">14.0</data>
      <data key="d5">Shengran Hu's GitHub contains the code related to the research conducted by Shengran Hu</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="SHENGRAN HU" target="META AGENT SEARCH">
      <data key="d4">9.0</data>
      <data key="d5">Shengran Hu is the author of the paper on Automated Design of Agentic Systems and the creator of the Meta Agent Search algorithm</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="SHENGRAN HU" target="ADAS">
      <data key="d4">1.0</data>
      <data key="d5">Shengran Hu is associated with the ADAS repository</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="UNIVERSITY OF BRITISH COLUMBIA" target="VECTOR INSTITUTE">
      <data key="d4">14.0</data>
      <data key="d5">Researchers from the University of British Columbia and the Vector Institute collaborated on the study of Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="UNIVERSITY OF BRITISH COLUMBIA" target="CANADA CIFAR AI CHAIR">
      <data key="d4">12.0</data>
      <data key="d5">Jeff Clune, who holds a Canada CIFAR AI Chair, is affiliated with the University of British Columbia</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="VECTOR INSTITUTE" target="CANADA CIFAR AI CHAIR">
      <data key="d4">12.0</data>
      <data key="d5">Jeff Clune, who holds a Canada CIFAR AI Chair, is affiliated with the Vector Institute</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="VECTOR INSTITUTE" target="ADAS">
      <data key="d4">16.0</data>
      <data key="d5">Vector Institute supported the work on ADAS</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)" target="META AGENT SEARCH">
      <data key="d4">18.0</data>
      <data key="d5">Meta Agent Search is an algorithm used within the research area of Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)" target="FOUNDATION MODELS (FMS)">
      <data key="d4">16.0</data>
      <data key="d5">Foundation Models are used as modules within agentic systems in the research area of Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)" target="CHAIN-OF-THOUGHT">
      <data key="d4">14.0</data>
      <data key="d5">Chain-of-Thought is a technique used as a building block in agentic systems within the research area of Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)" target="TOOLFORMER">
      <data key="d4">14.0</data>
      <data key="d5">Toolformer is a technique used as a building block in agentic systems within the research area of Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)" target="CLUNE (2019)">
      <data key="d4">14.0</data>
      <data key="d5">Clune (2019) contributed to the research on the history of machine learning and the replacement of hand-designed solutions with learned solutions</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="META AGENT SEARCH" target="META AGENT">
      <data key="d4">18.0</data>
      <data key="d5">Meta Agent is an agent that is programmed and iteratively improved by the Meta Agent Search algorithm</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="META AGENT SEARCH" target="AGENT ARCHIVE">
      <data key="d4">16.0</data>
      <data key="d5">Agent Archive is a repository used by the Meta Agent Search algorithm to store discovered agents</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="META AGENT SEARCH" target="MULTI-STEP PEER REVIEW AGENT">
      <data key="d4">14.0</data>
      <data key="d5">Multi-Step Peer Review Agent is an example of an agent discovered by the Meta Agent Search algorithm</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="META AGENT SEARCH" target="VERIFIED MULTIMODAL AGENT">
      <data key="d4">14.0</data>
      <data key="d5">Verified Multimodal Agent is an example of an agent discovered by the Meta Agent Search algorithm</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="META AGENT SEARCH" target="DIVIDE AND CONQUER AGENT">
      <data key="d4">14.0</data>
      <data key="d5">Divide and Conquer Agent is an example of an agent discovered by the Meta Agent Search algorithm</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ADAS">
      <data key="d4">61.0</data>
      <data key="d5">ADAS is the research area that proposes Meta Agent Search, an algorithm and method used within the ADAS process to discover superior agents. Meta Agent Search is a proposed approach to ADAS, demonstrating the method of defining and searching for agents within this field.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,4884e8429ca1e567dadf5e22b4b68274,7de66b94cf868b37b1df51dc545c415f,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="FM">
      <data key="d4">9.0</data>
      <data key="d5">Meta Agent Search uses FMs as meta agents to iteratively program new agents</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="FUNSEARCH">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search uses a similar practice to FunSearch by programming a "forward" function to define a new agentic system</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ARC">
      <data key="d4">24.0</data>
      <data key="d5">Meta Agent Search is a system that evaluates the performance of discovered agents on the ARC dataset. It is specifically assessed on the ARC logic puzzle task, which involves complex problem-solving and pattern recognition challenges. The ARC (Abstraction and Reasoning Corpus) dataset is designed to test the generalization capabilities of AI systems, making it a suitable benchmark for evaluating the effectiveness of Meta Agent Search in discovering and optimizing intelligent agents.</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="DROP">
      <data key="d4">15.0</data>
      <data key="d5">Meta Agent Search is a system that enhances performance on reading comprehension tasks, specifically utilizing the DROP dataset. It employs the DROP benchmark as a means to evaluate and measure its effectiveness in reading comprehension.</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="MGSM">
      <data key="d4">31.0</data>
      <data key="d5">Meta Agent Search is a system designed to evaluate and enhance the performance of discovered agents, particularly in the context of mathematical tasks. It leverages the MGSM dataset as a benchmark to assess and improve the math capabilities of these agents. By utilizing the MGSM dataset, Meta Agent Search ensures that the agents are rigorously tested and optimized for mathematical proficiency, thereby contributing to advancements in AI and machine learning performance in this specific domain.</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="GSM8K">
      <data key="d4">31.0</data>
      <data key="d5">Meta Agent Search is a system designed to evaluate and enhance the performance of discovered agents on the GSM8K dataset. It specifically focuses on improving accuracy and overall performance on math tasks within this dataset. By leveraging advanced techniques, Meta Agent Search aims to optimize the capabilities of agents, ensuring they perform more effectively on the GSM8K dataset, which is known for its challenging mathematical problems.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="GSM-HARD">
      <data key="d4">31.0</data>
      <data key="d5">Meta Agent Search is a sophisticated evaluation framework that assesses the performance of discovered agents on the GSM-Hard dataset. It is designed to enhance accuracy and overall performance, particularly on mathematical tasks within the GSM-Hard dataset. By focusing on these specific areas, Meta Agent Search contributes significantly to the advancement of agent-based evaluations and improvements in the field of AI and Machine Learning.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="CHAIN-OF-THOUGHT">
      <data key="d4">34.0</data>
      <data key="d5">Meta Agent Search is an advanced method that leverages the Chain-of-Thought technique to generate, refine, and ensemble answers. It compares its discovered agents against the Chain-of-Thought baseline and consistently outperforms this method. The integration of Chain-of-Thought within Meta Agent Search enhances its ability to produce superior results by systematically breaking down complex problems into manageable steps.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="LLM DEBATE">
      <data key="d4">34.0</data>
      <data key="d5">META AGENT SEARCH is a sophisticated method that leverages the LLM DEBATE technique to enhance its agent discovery and refinement processes. By utilizing multiple critics through the LLM Debate method, Meta Agent Search is able to compare its discovered agents against the LLM Debate baseline, ultimately outperforming this method. This integration of LLM Debate not only aids in refining the agents but also establishes Meta Agent Search as a superior approach in the AI and ML landscape.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="QUALITY-DIVERSITY">
      <data key="d4">41.0</data>
      <data key="d5">Meta Agent Search is a sophisticated method that leverages the Quality-Diversity technique to explore and discover new agents based on an archive of previous discoveries. It compares its discovered agents against the Quality-Diversity baseline and consistently outperforms this method. Quality-Diversity is utilized as one of the state-of-the-art hand-designed agents within Meta Agent Search, highlighting its integral role in the exploration and evaluation process.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="LU">
      <data key="d4">6.0</data>
      <data key="d5">Lu is an author mentioned in relation to open-endedness algorithms that leverage human notions of interestingness, similar to Meta Agent Search</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ZHANG">
      <data key="d4">6.0</data>
      <data key="d5">Zhang is an author mentioned in relation to open-endedness algorithms that leverage human notions of interestingness, similar to Meta Agent Search</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="MADAAN">
      <data key="d4">6.0</data>
      <data key="d5">Madaan is an author mentioned in relation to self-reflection iterations in the meta agent</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="SHINN">
      <data key="d4">6.0</data>
      <data key="d5">Shinn is an author mentioned in relation to self-reflection iterations in the meta agent</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ARC CHALLENGE">
      <data key="d4">9.0</data>
      <data key="d5">Meta Agent Search is used to discover novel agentic systems that outperform existing state-of-the-art agents in the ARC challenge</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="CHAIN-OF-THOUGHT (COT)">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search uses Chain-of-Thought (COT) as one of the state-of-the-art hand-designed agents</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT (COT-SC)">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search uses Self-Consistency with Chain-of-Thought (COT-SC) as one of the state-of-the-art hand-designed agents</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="LLM-DEBATE">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search uses LLM-Debate as one of the state-of-the-art hand-designed agents</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ABSTRACTION AND REASONING CORPUS (ARC)">
      <data key="d4">9.0</data>
      <data key="d5">Meta Agent Search is applied to the Abstraction and Reasoning Corpus (ARC) dataset</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="FALDOR ET AL., 2024">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search builds on prior works on open-endedness and AI-GAs by Faldor et al., 2024</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="LEHMAN &amp; STANLEY, 2011">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search builds on prior works on open-endedness and AI-GAs by Lehman &amp; Stanley, 2011</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="WANG ET AL., 2019">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search builds on prior works on open-endedness and AI-GAs by Wang et al., 2019</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="WANG ET AL., 2020">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search builds on prior works on open-endedness and AI-GAs by Wang et al., 2020</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ZHANG ET AL., 2024A">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search builds on prior works on open-endedness and AI-GAs by Zhang et al., 2024a</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="APPENDIX C">
      <data key="d4">6.0</data>
      <data key="d5">Appendix C provides detailed implementation of the best agent discovered by Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="APPENDIX E">
      <data key="d4">12.0</data>
      <data key="d5">Appendix E contains more details about the baselines used in Meta Agent Search.</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="FIGURE 3">
      <data key="d4">6.0</data>
      <data key="d5">Figure 3 shows the results of Meta Agent Search on the ARC challenge</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="PUBLIC TRAINING SET (EASY)">
      <data key="d4">6.0</data>
      <data key="d5">Meta Agent Search uses the Public Training Set (Easy) for training agents</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="VALIDATION SET">
      <data key="d4">6.0</data>
      <data key="d5">Meta Agent Search uses a validation set of 20 questions for validating agents</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="TEST SET">
      <data key="d4">1.0</data>
      <data key="d5">Meta Agent Search uses a test set of 60 questions for testing agents</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="DYNAMIC MEMORY">
      <data key="d4">7.0</data>
      <data key="d5">Dynamic memory is introduced in Meta Agent Search for doing more refinements</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="MULTIPLE CRITICS">
      <data key="d4">7.0</data>
      <data key="d5">Multiple critics are introduced in Meta Agent Search for enhanced refinement</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="META-AGENT">
      <data key="d4">8.0</data>
      <data key="d5">The meta-agent in Meta Agent Search uses GPT-4</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="CRITIC">
      <data key="d4">7.0</data>
      <data key="d5">Critics provide feedback for refining answers in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="EFFICIENCY EXPERT">
      <data key="d4">7.0</data>
      <data key="d5">Efficiency experts evaluate the efficiency of answers in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="READABILITY EXPERT">
      <data key="d4">7.0</data>
      <data key="d5">Readability experts evaluate the readability of answers in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="SIMPLICITY EXPERT">
      <data key="d4">7.0</data>
      <data key="d5">Simplicity experts evaluate the simplicity of answers in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ENSEMBLE">
      <data key="d4">8.0</data>
      <data key="d5">Ensembling is used to combine the best answers in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="HUMAN-LIKE FEEDBACK">
      <data key="d4">7.0</data>
      <data key="d5">Human-like feedback is simulated to refine answers in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ITERATION 3">
      <data key="d4">7.0</data>
      <data key="d5">Iteration 3 in Meta Agent Search uses multiple COTs to generate possible answers, refine them, and ensemble the best answers</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ITERATION 5">
      <data key="d4">7.0</data>
      <data key="d5">Iteration 5 in Meta Agent Search incorporates diverse feedback</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ITERATION 11">
      <data key="d4">7.0</data>
      <data key="d5">Iteration 11 in Meta Agent Search evaluates for various specific traits via experts</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ITERATION 12">
      <data key="d4">1.0</data>
      <data key="d5">Iteration 12 in Meta Agent Search simulates human-like feedback</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="MMLU">
      <data key="d4">8.0</data>
      <data key="d5">Meta Agent Search uses the MMLU benchmark to evaluate multi-task problem solving</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="GPQA">
      <data key="d4">8.0</data>
      <data key="d5">Meta Agent Search uses the GPQA benchmark to evaluate the capability of solving hard science questions</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="STEP-BACK ABSTRACTION">
      <data key="d4">13.0</data>
      <data key="d5">Meta Agent Search is a framework that evaluates its discovered agents by comparing them against the Step-back Abstraction baseline. Step-back Abstraction, in turn, is a method utilized within Meta Agent Search, serving as a foundational technique for the framework's comparative analysis. This relationship highlights the integral role of Step-back Abstraction in the functioning and assessment processes of Meta Agent Search.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ROLE ASSIGNMENT">
      <data key="d4">13.0</data>
      <data key="d5">Meta Agent Search is a framework that compares its discovered agents against the Role Assignment baseline. Role Assignment, in this context, is a method utilized within Meta Agent Search to evaluate and benchmark the performance of the agents it identifies. This comparative approach helps in assessing the effectiveness and efficiency of the agents discovered by Meta Agent Search.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="READING COMPREHENSION">
      <data key="d4">29.0</data>
      <data key="d5">Meta Agent Search is a highly effective tool in the Reading Comprehension domain. It not only tests agents within this domain but also outperforms state-of-the-art baselines, demonstrating its superior performance and reliability in enhancing reading comprehension capabilities.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="MULTI-TASK PROBLEM SOLVING">
      <data key="d4">8.0</data>
      <data key="d5">Meta Agent Search tests agents in the Multi-task Problem Solving domain</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="F1 SCORE">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search reports F1 scores for agents in the Reading Comprehension and Math domains</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ACCURACY">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search reports accuracy rates for agents in the Reading Comprehension and Math domains</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="BOOTSTRAP CONFIDENCE INTERVAL">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search reports the 95% bootstrap confidence interval for performance comparison</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="EXPERIMENT SETTINGS">
      <data key="d4">6.0</data>
      <data key="d5">Meta Agent Search details about datasets and experiment settings can be found in Appendix D</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="BASELINES">
      <data key="d4">8.0</data>
      <data key="d5">Meta Agent Search compares discovered agents against state-of-the-art hand-designed baselines</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="APPENDIX D">
      <data key="d4">6.0</data>
      <data key="d5">Appendix D contains more details about datasets and experiment settings used in Meta Agent Search</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="FMS">
      <data key="d4">24.0</data>
      <data key="d5">Meta Agent Search is a tool designed to discover agents that can effectively leverage the knowledge embedded in Foundation Models (FMs). By utilizing Foundation Models in its process, Meta Agent Search enhances the capability of agents to access and utilize vast amounts of pre-existing knowledge, thereby improving their performance and efficiency in various applications.</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="MULTI-TASK">
      <data key="d4">19.0</data>
      <data key="d5">Meta Agent Search is a cutting-edge tool utilized in the Multi-task domain. It has demonstrated superior performance, consistently outperforming state-of-the-art baselines in Multi-task applications.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="CLAUDE-HAIKU">
      <data key="d4">16.0</data>
      <data key="d5">Meta Agent Search uses Claude-Haiku to evaluate the performance of discovered agents</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="CLAUDE-SONNET">
      <data key="d4">16.0</data>
      <data key="d5">Meta Agent Search uses Claude-Sonnet to evaluate the performance of discovered agents</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="STRUCTURED FEEDBACK AND ENSEMBLE AGENT">
      <data key="d4">16.0</data>
      <data key="d5">Structured Feedback and Ensemble Agent is one of the top agents discovered by Meta Agent Search</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="HIERARCHICAL COMMITTEE REINFORCEMENT AGENT">
      <data key="d4">16.0</data>
      <data key="d5">Hierarchical Committee Reinforcement Agent is one of the top agents discovered by Meta Agent Search</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="DYNAMIC MEMORY AND REFINEMENT AGENT">
      <data key="d4">16.0</data>
      <data key="d5">Dynamic Memory and Refinement Agent is one of the top agents discovered by Meta Agent Search</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="SVAMP">
      <data key="d4">8.0</data>
      <data key="d5">Meta Agent Search evaluates the performance of discovered agents on the SVAMP dataset</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ASDIV">
      <data key="d4">8.0</data>
      <data key="d5">Meta Agent Search evaluates the performance of discovered agents on the ASDiv dataset</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="DYNAMIC ROLE-PLAYING ARCHITECTURE">
      <data key="d4">7.0</data>
      <data key="d5">Dynamic Role-Playing Architecture is a top agent discovered by Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="META AGENT SEARCH" target="STRUCTURED MULTIMODAL FEEDBACK LOOP">
      <data key="d4">7.0</data>
      <data key="d5">Structured Multimodal Feedback Loop is a top agent discovered by Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="META AGENT SEARCH" target="INTERACTIVE MULTIMODAL FEEDBACK LOOP">
      <data key="d4">7.0</data>
      <data key="d5">Interactive Multimodal Feedback Loop is a top agent discovered by Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="META AGENT SEARCH" target="TOOL USE">
      <data key="d4">6.0</data>
      <data key="d5">Tool Use is used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="META AGENT SEARCH" target="FM MODULES">
      <data key="d4">6.0</data>
      <data key="d5">FM Modules are used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="META AGENT SEARCH" target="AI-GENERATING ALGORITHMS">
      <data key="d4">6.0</data>
      <data key="d5">AI-Generating Algorithms are related to Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="META AGENT SEARCH" target="AUTOML">
      <data key="d4">1.0</data>
      <data key="d5">AutoML is related to Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ROKON ET AL., 2020">
      <data key="d4">16.0</data>
      <data key="d5">Rokon et al., 2020 discusses safety concerns when executing untrusted model-generated code in Meta Agent Search</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="META AGENT SEARCH" target="YEE ET AL., 2010">
      <data key="d4">16.0</data>
      <data key="d5">Yee et al., 2010 discusses the use of sandbox environments to safely run untrusted model-generated code in Meta Agent Search</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="META AGENT SEARCH" target="EXPERIMENT">
      <data key="d4">8.0</data>
      <data key="d5">The experiment used Meta Agent Search to discover the best agent on ARC</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="META AGENT SEARCH" target="GPT-4O-MINI">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4o-Mini is used in Meta Agent Search to improve results and reduce costs</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ADAS ALGORITHMS">
      <data key="d4">1.0</data>
      <data key="d5">ADAS algorithms are used in the Meta Agent Search process</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="META AGENT SEARCH" target="EVALUATION FUNCTION">
      <data key="d4">7.0</data>
      <data key="d5">The evaluation function is used in the Meta Agent Search process</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="CLAUDE">
      <data key="d4">16.0</data>
      <data key="d5">Claude is an example of a Foundation Model used in agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="WANG ET AL.">
      <data key="d4">14.0</data>
      <data key="d5">Wang et al. contributed to the research on Foundation Models</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="ROCKT&#196;SCHEL">
      <data key="d4">14.0</data>
      <data key="d5">Rockt&#228;schel contributed to the research on compound agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="ZAHARIA ET AL.">
      <data key="d4">14.0</data>
      <data key="d5">Zaharia et al. contributed to the research on compound agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="ADAS">
      <data key="d4">16.0</data>
      <data key="d5">Foundation Models (FMs) are used as modules in the control flow of agentic systems in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="CHASE">
      <data key="d4">10.0</data>
      <data key="d5">Chase is referenced in the context of defining agentic systems involving Foundation Models</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="NG">
      <data key="d4">10.0</data>
      <data key="d5">Ng is referenced in the context of defining agentic systems involving Foundation Models</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="AGENTIC SYSTEMS">
      <data key="d4">8.0</data>
      <data key="d5">Agentic systems involve Foundation Models as modules to solve tasks</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="FRAMEWORK">
      <data key="d4">7.0</data>
      <data key="d5">The framework includes querying Foundation Models to assist the meta agent in generating and improving agent architectures.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="INFO OBJECT">
      <data key="d4">6.0</data>
      <data key="d5">Foundation Models' responses are encapsulated in Info objects within the framework.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="CLAUDE" target="ANTHROPIC">
      <data key="d4">16.0</data>
      <data key="d5">Anthropic is the organization that developed the Claude Foundation Model</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT" target="HU &amp; CLUNE">
      <data key="d4">14.0</data>
      <data key="d5">Hu &amp; Clune contributed to the research on chain-of-thought planning and reasoning</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="META AGENT" target="AGENTIC SYSTEMS">
      <data key="d4">8.0</data>
      <data key="d5">A meta agent creates other agents within agentic systems</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="META AGENT" target="ADAS">
      <data key="d4">8.0</data>
      <data key="d5">Meta Agent is a concept where a meta agent programs better agents in code, enhancing the ADAS algorithm's ability to discover new agentic systems</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="META AGENT" target="FRAMEWORK">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent uses the framework to implement basic functions and format prompts.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="IMPLEMENTATION MISTAKES">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent identifies and corrects implementation mistakes during the self-reflection process.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="IMPROVEMENT">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent suggests and implements improvements to increase the performance or effectiveness of the proposed architecture.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="RUNTIME ERROR">
      <data key="d4">1.0</data>
      <data key="d5">The meta agent debugs and corrects the code when a runtime error is encountered during execution.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="APPENDIX B">
      <data key="d4">7.0</data>
      <data key="d5">The meta agent uses the framework code provided in Appendix B.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="APPENDICES C AND D">
      <data key="d4">6.0</data>
      <data key="d5">The meta agent references information available in Appendices C and D.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="OUTPUT INSTRUCTION AND EXAMPLE">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent follows the output instructions and examples provided in this section.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="MADAAN ET AL., 2024">
      <data key="d4">6.0</data>
      <data key="d5">The meta agent's self-reflection process is influenced by the work of Madaan et al. (2024).</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="ARCHIVE">
      <data key="d4">7.0</data>
      <data key="d5">The meta agent compares its proposed architecture against existing methods in the archive during self-reflection.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="WRONG IMPLEMENTATION EXAMPLES">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent uses the "WRONG Implementation examples" section to identify and correct mistakes during self-reflection.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="INFO OBJECT">
      <data key="d4">1.0</data>
      <data key="d5">The meta agent uses Info objects to encapsulate and combine different types of information within the framework.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="ARC CHALLENGE">
      <data key="d4">9.0</data>
      <data key="d5">The meta agent is designed to perform well on the ARC challenge</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="META AGENT" target="GPT-4O-2024-05-13">
      <data key="d4">17.0</data>
      <data key="d5">META AGENT utilizes GPT-4O-2024-05-13 to identify and select optimal agents for various benchmarks. This integration allows META AGENT to leverage the advanced capabilities of GPT-4O-2024-05-13, ensuring high performance and efficiency in achieving benchmark standards.</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="META AGENT" target="GPT-3.5-TURBO-0125">
      <data key="d4">7.0</data>
      <data key="d5">Discovered agents and baselines are evaluated using GPT-3.5-turbo-0125</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="META AGENT" target="ARC">
      <data key="d4">9.0</data>
      <data key="d5">The meta agent is designed to perform well on the ARC challenge</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="META AGENT" target="AUTOMATED DESIGN OF AGENTIC SYSTEMS">
      <data key="d4">7.0</data>
      <data key="d5">Meta agent is discussed in the Automated Design of Agentic Systems document</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="META AGENT" target="DROP">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent aims to find an optimal agent performing well on the DROP benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="META AGENT" target="GPQA">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent aims to find an optimal agent performing well on the GPQA benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="META AGENT" target="MGSM">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent aims to find an optimal agent performing well on the MGSM benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="META AGENT" target="MMLU">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent aims to find an optimal agent performing well on the MMLU benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="MULTI-STEP PEER REVIEW AGENT" target="GPQA">
      <data key="d4">8.0</data>
      <data key="d5">Multi-Step Peer Review Agent was discovered in the GPQA domain</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="MULTI-STEP PEER REVIEW AGENT" target="REIN ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Multi-Step Peer Review Agent is discussed in the publication by Rein et al. in 2023</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="MULTI-STEP PEER REVIEW AGENT" target="PHYSICS CRITIC">
      <data key="d4">6.0</data>
      <data key="d5">Physics Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="MULTI-STEP PEER REVIEW AGENT" target="CHEMISTRY CRITIC">
      <data key="d4">6.0</data>
      <data key="d5">Chemistry Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="MULTI-STEP PEER REVIEW AGENT" target="BIOLOGY CRITIC">
      <data key="d4">6.0</data>
      <data key="d5">Biology Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="MULTI-STEP PEER REVIEW AGENT" target="GENERAL CRITIC">
      <data key="d4">6.0</data>
      <data key="d5">General Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="MULTI-STEP PEER REVIEW AGENT" target="FINAL DECISION">
      <data key="d4">6.0</data>
      <data key="d5">Final Decision is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="VERIFIED MULTIMODAL AGENT" target="MGSM">
      <data key="d4">8.0</data>
      <data key="d5">Verified Multimodal Agent was discovered in the MGSM domain</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="VERIFIED MULTIMODAL AGENT" target="SHI ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Verified Multimodal Agent is discussed in the publication by Shi et al. in 2023</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="VERIFIED MULTIMODAL AGENT" target="VISUAL REPRESENTATION MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Verified Multimodal Agent uses the Visual Representation Module to generate visual aids</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="VERIFIED MULTIMODAL AGENT" target="VERIFICATION MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Verified Multimodal Agent uses the Verification Module to verify visual aids</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="VERIFIED MULTIMODAL AGENT" target="CHAIN-OF-THOUGHT MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Verified Multimodal Agent uses the Chain-of-Thought Module to solve problems</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="DIVIDE AND CONQUER AGENT" target="GPQA">
      <data key="d4">8.0</data>
      <data key="d5">Divide and Conquer Agent was discovered in the GPQA domain</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="DIVIDE AND CONQUER AGENT" target="REIN ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Divide and Conquer Agent is discussed in the publication by Rein et al. in 2023</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="DIVIDE AND CONQUER AGENT" target="DECOMPOSITION MODULE">
      <data key="d4">6.0</data>
      <data key="d5">Decomposition Module is a role in the Divide and Conquer Agent</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="DIVIDE AND CONQUER AGENT" target="SPECIALIZED EXPERT">
      <data key="d4">1.0</data>
      <data key="d5">Specialized Expert is a role in the Divide and Conquer Agent</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="HOG" target="CONVOLUTIONAL NEURAL NETWORKS (CNNS)">
      <data key="d4">16.0</data>
      <data key="d5">HOG features were replaced by learned features from Convolutional Neural Networks</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="HOG" target="DALAL &amp; TRIGGS (2005)">
      <data key="d4">14.0</data>
      <data key="d5">Dalal &amp; Triggs (2005) contributed to the research on hand-designed features like HOG</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="CONVOLUTIONAL NEURAL NETWORKS (CNNS)" target="NEURAL ARCHITECTURE SEARCH">
      <data key="d4">16.0</data>
      <data key="d5">Neural Architecture Search is a method that led to the best-performing Convolutional Neural Networks</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="CONVOLUTIONAL NEURAL NETWORKS (CNNS)" target="KRIZHEVSKY ET AL. (2012)">
      <data key="d4">14.0</data>
      <data key="d5">Krizhevsky et al. (2012) contributed to the research on Convolutional Neural Networks</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="ELSKEN">
      <data key="d4">2.0</data>
      <data key="d5">Elsken contributed to the research on Neural Architecture Search</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="ELSKEN ET AL. (2019)">
      <data key="d4">8.0</data>
      <data key="d5">Elsken et al. (2019) discusses Neural Architecture Search</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="SHEN ET AL. (2023)">
      <data key="d4">8.0</data>
      <data key="d5">Shen et al. (2023) discusses Neural Architecture Search</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="ADAS">
      <data key="d4">7.0</data>
      <data key="d5">Neural Architecture Search is a research area related to ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="AI-GENERATING ALGORITHMS">
      <data key="d4">7.0</data>
      <data key="d5">Neural Architecture Search is a technique under the first pillar of AI-Generating Algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="ELSKEN ET AL., 2019">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Elsken et al. in 2019 is related to Neural Architecture Search</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="HU ET AL., 2021">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Hu et al. in 2021 is related to Neural Architecture Search</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="LU ET AL., 2019">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Lu et al. in 2019 is related to Neural Architecture Search</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="AUTOML" target="AI-GENERATING ALGORITHMS (AI-GAS)">
      <data key="d4">16.0</data>
      <data key="d5">AutoML and AI-Generating Algorithms are methods that demonstrate the superiority of learned AI systems over hand-designed ones</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="AUTOML" target="HUTTER ET AL. (2019)">
      <data key="d4">22.0</data>
      <data key="d5">Hutter et al. (2019) contributed to the research on AutoML methods, discussing various approaches and advancements in the field of Automated Machine Learning (AutoML). Their work provides significant insights into the methodologies and applications of AutoML, highlighting its importance in streamlining and optimizing the machine learning model development process.</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="AUTOML" target="ADAS">
      <data key="d4">15.0</data>
      <data key="d5">ADAS aims to invent novel building blocks and design powerful agentic systems, which aligns with the goals of AutoML. AutoML is a research area related to ADAS, indicating a close relationship between the two entities in their pursuit of advancing automated machine learning and agentic system design.</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274,7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="AUTOML" target="HUTTER ET AL., 2019">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Hutter et al. in 2019 is related to AutoML</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="AI-GENERATING ALGORITHMS (AI-GAS)" target="CLUNE (2019)">
      <data key="d4">8.0</data>
      <data key="d5">Clune (2019) discusses AI-Generating Algorithms</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="ANTHROPIC" target="CLAUDE-HAIKU">
      <data key="d4">8.0</data>
      <data key="d5">Anthropic is the organization behind the Claude-Haiku model</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="ANTHROPIC" target="CLAUDE-SONNET">
      <data key="d4">8.0</data>
      <data key="d5">Anthropic is the organization behind the Claude-Sonnet model</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="ANTHROPIC" target="YUNTAO BAI">
      <data key="d4">12.0</data>
      <data key="d5">Yuntao Bai is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="SAURAV KADAVATH">
      <data key="d4">12.0</data>
      <data key="d5">Saurav Kadavath is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="SANDIPAN KUNDU">
      <data key="d4">12.0</data>
      <data key="d5">Sandipan Kundu is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="JACKSON KERNION">
      <data key="d4">12.0</data>
      <data key="d5">Jackson Kernion is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="ANDY JONES">
      <data key="d4">12.0</data>
      <data key="d5">Andy Jones is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="ANNA CHEN">
      <data key="d4">12.0</data>
      <data key="d5">Anna Chen is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="ANNA GOLDIE">
      <data key="d4">12.0</data>
      <data key="d5">Anna Goldie is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="AZALIA MIRHOSEINI">
      <data key="d4">12.0</data>
      <data key="d5">Azalia Mirhoseini is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="CAMERON MCKINNON">
      <data key="d4">12.0</data>
      <data key="d5">Cameron McKinnon is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="WANG ET AL." target="DEVELOPING NEW SKILLS FOR EMBODIED AGENTS IN CODE">
      <data key="d4">8.0</data>
      <data key="d5">Wang et al. are the authors of the method for developing new skills for embodied agents in code</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="HU &amp; CLUNE" target="CHAIN-OF-THOUGHT-BASED PLANNING AND REASONING METHODS">
      <data key="d4">8.0</data>
      <data key="d5">Hu &amp; Clune are the authors of the Chain-of-Thought-based planning and reasoning methods</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="ZHANG ET AL." target="MEMORY STRUCTURES">
      <data key="d4">14.0</data>
      <data key="d5">Zhang et al. contributed to the research on memory structures in agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="ZHANG ET AL." target="EXTERNAL MEMORY AND RAG">
      <data key="d4">8.0</data>
      <data key="d5">Zhang et al. are the authors of the External Memory and RAG methods</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="QU ET AL." target="TOOL USE">
      <data key="d4">22.0</data>
      <data key="d5">Qu et al. are the authors of the Tool Use method and have significantly contributed to the research on tool use in agentic systems. Their work focuses on the development and application of methodologies that enable agents to effectively utilize tools, enhancing the capabilities and functionalities of these systems within the field of Artificial Intelligence and Machine Learning.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="KRIZHEVSKY ET AL. (2012)" target="CNN">
      <data key="d4">8.0</data>
      <data key="d5">Krizhevsky et al. (2012) discusses Convolutional Neural Networks</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="ELSKEN" target="ADAS">
      <data key="d4">10.0</data>
      <data key="d5">Elsken is referenced in the context of Neural Architecture Search related to ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="TOOL USE" target="AGENTIC SYSTEMS">
      <data key="d4">7.0</data>
      <data key="d5">Tool use is a component of agentic systems that involves using external tools to accomplish tasks</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="TOOL USE" target="NAKANO ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Nakano et al. are the authors of the Tool Use method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="TOOL USE" target="AGENTINSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct generates data covering the skill of tool use</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="TOOL USE" target="READING COMPREHENSION">
      <data key="d4">6.0</data>
      <data key="d5">Both Reading Comprehension and Tool Use are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="TOOL USE" target="TEXT MODIFICATION">
      <data key="d4">6.0</data>
      <data key="d5">Both Text Modification and Tool Use are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="TOOL USE" target="OPEN DOMAIN QUESTION ANSWERING">
      <data key="d4">6.0</data>
      <data key="d5">Both Open Domain Question Answering and Tool Use are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="TOOL USE" target="WEB AGENT">
      <data key="d4">7.0</data>
      <data key="d5">Web Agent is a type of tool use skill implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="BUILDING BLOCKS" target="AGENTIC SYSTEMS">
      <data key="d4">8.0</data>
      <data key="d5">Building blocks are fundamental components used to construct agentic systems</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="LEARNED LOSS FUNCTIONS" target="LU ET AL. (2024A)">
      <data key="d4">8.0</data>
      <data key="d5">Lu et al. (2024a) discusses learned loss functions in LLM alignment</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="DPO" target="RAFAILOV ET AL. (2024)">
      <data key="d4">8.0</data>
      <data key="d5">Rafailov et al. (2024) discusses DPO</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="AI SCIENTIST" target="LU ET AL. (2024B)">
      <data key="d4">8.0</data>
      <data key="d5">Lu et al. (2024b) discusses the AI Scientist</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="OMNI-EPIC" target="FALDOR ET AL. (2024)">
      <data key="d4">8.0</data>
      <data key="d5">Faldor et al. (2024) discusses OMNI-EPIC</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="OMNI-EPIC" target="AI-GENERATING ALGORITHMS">
      <data key="d4">7.0</data>
      <data key="d5">OMNI-EPIC is a technique under the third pillar of AI-Generating Algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="OMNI-EPIC" target="FOUNDATION MODELS">
      <data key="d4">1.0</data>
      <data key="d5">OMNI-EPIC enables Foundation Models to create robotics learning environments</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="OMNI-EPIC" target="FALDOR ET AL., 2024">
      <data key="d4">14.0</data>
      <data key="d5">Faldor et al., 2024 discusses OMNI-EPIC and its application in enabling FMs to create robotics learning environments by programming in code. The publication by Faldor et al. in 2024 is related to OMNI-EPIC, highlighting its significance in the field of robotics and AI.</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="OMNI-EPIC" target="FMS">
      <data key="d4">8.0</data>
      <data key="d5">OMNI-EPIC enables FMs to create robotics learning environments by programming in code</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="ADAS" target="FERNANDO ET AL. (2024)">
      <data key="d4">16.0</data>
      <data key="d5">Fernando et al. (2024) discusses ADAS methods focusing on designing prompts</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="ADAS" target="YANG ET AL. (2024)">
      <data key="d4">16.0</data>
      <data key="d5">Yang et al. (2024) discusses ADAS methods focusing on designing prompts</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="ADAS" target="SEARCH SPACE">
      <data key="d4">16.0</data>
      <data key="d5">The search space is a key component of ADAS that defines which agentic systems can be represented</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="EVALUATION FUNCTION">
      <data key="d4">16.0</data>
      <data key="d5">The evaluation function is a key component of ADAS that defines how to evaluate a candidate agent</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="CLUNE">
      <data key="d4">10.0</data>
      <data key="d5">Clune is referenced in the context of research areas related to ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="HUTTER">
      <data key="d4">10.0</data>
      <data key="d5">Hutter is referenced in the context of research areas related to ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="FERNANDO">
      <data key="d4">10.0</data>
      <data key="d5">Fernando is referenced in the context of ADAS-related works</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="ZHUGE">
      <data key="d4">10.0</data>
      <data key="d5">Zhuge is referenced in the context of search spaces and reinforcement learning in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="LIU">
      <data key="d4">10.0</data>
      <data key="d5">Liu is referenced in the context of search spaces such as feed-forward networks in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="AI-GAS">
      <data key="d4">7.0</data>
      <data key="d5">AI-GAs is a research area related to ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="MATHEMATICS">
      <data key="d4">6.0</data>
      <data key="d5">Mathematics is a domain where agentic systems can be applied and evaluated in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="READING COMPREHENSION">
      <data key="d4">6.0</data>
      <data key="d5">Reading comprehension is a domain where agentic systems can be applied and evaluated in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="AI SAFETY">
      <data key="d4">7.0</data>
      <data key="d5">AI safety is a concept that involves ensuring the safe operation of AI systems, which can be enhanced by using readable program code in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="DEBUGGING">
      <data key="d4">7.0</data>
      <data key="d5">Debugging is a process that involves identifying and fixing issues in program code, which is made easier by using readable code in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="EXISTING HUMAN EFFORTS">
      <data key="d4">7.0</data>
      <data key="d5">Existing human efforts refer to the prior work and knowledge that can be built upon in ADAS, especially when using programming languages as the search space</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="MECHANISM">
      <data key="d4">8.0</data>
      <data key="d5">The sophisticated feedback mechanism is part of the ADAS process</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="ADAS" target="MEYERSON ET AL.">
      <data key="d4">6.0</data>
      <data key="d5">Meyerson et al. discussed the concept of crossover in evolution via LLMs, which is related to ADAS</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="ADAS" target="AI-GENERATING ALGORITHMS">
      <data key="d4">8.0</data>
      <data key="d5">ADAS aims to invent novel building blocks and design powerful agentic systems, which aligns with the goals of AI-Generating Algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="ADAS" target="AGENTIC SYSTEM">
      <data key="d4">18.0</data>
      <data key="d5">ADAS aims to design powerful agentic systems</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="CANADA CIFAR AI CHAIRS PROGRAM">
      <data key="d4">16.0</data>
      <data key="d5">Canada CIFAR AI Chairs program supported the work on ADAS</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="SCHMIDT FUTURES">
      <data key="d4">16.0</data>
      <data key="d5">Schmidt Futures provided grants for the work on ADAS</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="OPEN PHILANTHROPY">
      <data key="d4">16.0</data>
      <data key="d5">Open Philanthropy provided grants for the work on ADAS</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="NSERC DISCOVERY GRANT">
      <data key="d4">16.0</data>
      <data key="d5">NSERC Discovery Grant supported the work on ADAS</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="RAFAEL COSMAN">
      <data key="d4">16.0</data>
      <data key="d5">Rafael Cosman made a generous donation to support the work on ADAS</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="JENNY ZHANG">
      <data key="d4">14.0</data>
      <data key="d5">Jenny Zhang provided insightful discussions and feedback on the work</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="RACH PRADHAN">
      <data key="d4">14.0</data>
      <data key="d5">Rach Pradhan provided insightful discussions and feedback on the work</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="RUIYU GOU">
      <data key="d4">14.0</data>
      <data key="d5">Ruiyu Gou provided insightful discussions and feedback on the work</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="NICHOLAS IOANNIDIS">
      <data key="d4">14.0</data>
      <data key="d5">Nicholas Ioannidis provided insightful discussions and feedback on the work</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="EUNJEONG HWANG">
      <data key="d4">14.0</data>
      <data key="d5">Eunjeong Hwang provided insightful discussions and feedback on the work</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ARC LOGIC PUZZLE TASK" target="CHOLLET (2019)">
      <data key="d4">8.0</data>
      <data key="d5">Chollet (2019) discusses the ARC logic puzzle task</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="CHOLLET (2019)" target="ARC">
      <data key="d4">8.0</data>
      <data key="d5">Chollet (2019) discusses the ARC logic puzzle task</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="DROP" target="DUA ET AL. (2019)">
      <data key="d4">8.0</data>
      <data key="d5">Dua et al. (2019) discusses the DROP benchmark</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="DROP" target="READING COMPREHENSION">
      <data key="d4">16.0</data>
      <data key="d5">DROP is a benchmark for reading comprehension tasks. It serves as a reading comprehension benchmark, providing a standardized measure for evaluating the performance of models in understanding and processing text.</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba,86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="DROP" target="DUA">
      <data key="d4">6.0</data>
      <data key="d5">Dua is an author mentioned in relation to the DROP dataset</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="DROP" target="DUA ET AL.">
      <data key="d4">6.0</data>
      <data key="d5">Dua et al. are the authors of the DROP benchmark</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="DROP" target="ONE-SHOT STYLE QUESTIONS">
      <data key="d4">16.0</data>
      <data key="d5">DROP uses one-shot style questions for evaluationDROP uses one-shot style questions</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="DROP" target="AUTOMATED DESIGN OF AGENTIC SYSTEMS">
      <data key="d4">7.0</data>
      <data key="d5">DROP is discussed in the Automated Design of Agentic Systems document</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="DROP" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">DROP is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="DROP" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">DROP is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="DROP" target="ORCA-3">
      <data key="d4">7.0</data>
      <data key="d5">DROP is a benchmark used to evaluate the performance of Orca-3</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="DROP" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">DROP is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="DROP" target="GPT-3.5-TURBO">
      <data key="d4">7.0</data>
      <data key="d5">DROP is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="DROP" target="EXACT MATCH/SPAN EXTRACTION PROBLEMS">
      <data key="d4">8.0</data>
      <data key="d5">DROP is a dataset used for problems where a ground-truth answer value is given in exact match/span extraction problems</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="MGSM" target="SHI ET AL. (2023)">
      <data key="d4">8.0</data>
      <data key="d5">Shi et al. (2023) discusses the MGSM benchmark</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="MGSM" target="SHI">
      <data key="d4">6.0</data>
      <data key="d5">Shi is an author mentioned in relation to the MGSM dataset</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="MGSM" target="SHI ET AL.">
      <data key="d4">6.0</data>
      <data key="d5">Shi et al. are the authors of the MGSM benchmark</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="GSM8K" target="COBBE ET AL. (2021)">
      <data key="d4">16.0</data>
      <data key="d5">Cobbe et al. (2021) discusses the GSM8K benchmark</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="GSM8K" target="COBBE">
      <data key="d4">6.0</data>
      <data key="d5">Cobbe is an author mentioned in relation to the GSM8K dataset</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="GSM8K" target="ORCA-3">
      <data key="d4">36.0</data>
      <data key="d5">GSM8K is a benchmark used to evaluate the performance of Orca-3. Orca-3 demonstrated a significant 54% improvement on the GSM8K math benchmark, showcasing its enhanced capabilities in this domain.</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GSM8K" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">GSM8K is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GSM8K" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">GSM8K is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GSM8K" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">GSM8K is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GSM8K" target="GPT-3.5-TURBO">
      <data key="d4">13.0</data>
      <data key="d5">GSM8K is a benchmark used to evaluate the performance of GPT-3.5-turbo. The scores for GPT-3.5-turbo on the GSM8K benchmark are taken from a specific reference, indicating a standardized method of assessment. This benchmark plays a crucial role in understanding the capabilities and limitations of GPT-3.5-turbo within the context of its performance metrics.</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GSM8K" target="GRADE SCHOOL MATH">
      <data key="d4">8.0</data>
      <data key="d5">GSM8K consists of grade school math word problems</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="GSM8K" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B shows significant improvements on the GSM8K math benchmark</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="GSM8K" target="EXACT MATCH/SPAN EXTRACTION PROBLEMS">
      <data key="d4">8.0</data>
      <data key="d5">GSM8K is a dataset used for math-based questions in exact match/span extraction problems</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="GSM-HARD" target="GAO ET AL. (2023)">
      <data key="d4">16.0</data>
      <data key="d5">Gao et al. (2023) discusses the GSM-Hard benchmark</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="GSM-HARD" target="GAO">
      <data key="d4">1.0</data>
      <data key="d5">Gao is an author mentioned in relation to the GSM-Hard dataset</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="FMS" target="LU ET AL. (2024C)">
      <data key="d4">16.0</data>
      <data key="d5">Lu et al. (2024c) discusses open-endedness algorithms leveraging human notions of interestingness</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="FMS" target="ZHANG ET AL. (2024A)">
      <data key="d4">2.0</data>
      <data key="d5">Zhang et al. (2024a) discusses open-endedness algorithms leveraging human notions of interestingness</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="FMS" target="LANGUAGE-TO-REWARD">
      <data key="d4">8.0</data>
      <data key="d5">Language-to-reward enables FMs to write reward functions for reinforcement learning in robotics</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGENTIC SYSTEMS" target="CONTROL FLOW">
      <data key="d4">7.0</data>
      <data key="d5">Control flow is a component of agentic systems that dictates the sequence of operations or steps an agent follows</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="AGENTIC SYSTEMS" target="TEXT PROMPTS">
      <data key="d4">7.0</data>
      <data key="d5">Text prompts are components of agentic systems that can be mutated to create new agents</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="AGENTIC SYSTEMS" target="STRUCTURED FEEDBACK AND ENSEMBLE AGENT">
      <data key="d4">7.0</data>
      <data key="d5">The structured feedback and ensemble agent is a type of agentic system</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="ARC" target="CHOLLET">
      <data key="d4">6.0</data>
      <data key="d5">Chollet is an author mentioned in relation to the ARC logic puzzle task</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="ARC" target="BEST AGENT">
      <data key="d4">16.0</data>
      <data key="d5">The best agent was evaluated on the ARC dataset</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="ARC" target="CHAIN-OF-THOUGHT (COT)">
      <data key="d4">7.0</data>
      <data key="d5">Chain-of-Thought (COT) is used as a baseline for experiments on ARC</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="ARC" target="SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT (COT-SC)">
      <data key="d4">7.0</data>
      <data key="d5">Self-Consistency with Chain-of-Thought (COT-SC) is used as a baseline for experiments on ARC</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="ARC" target="LLM-DEBATE">
      <data key="d4">7.0</data>
      <data key="d5">LLM-Debate is used as a baseline for experiments on ARC</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="ARC" target="QUALITY-DIVERSITY">
      <data key="d4">7.0</data>
      <data key="d5">Quality-Diversity is used as a baseline for experiments on ARC</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="ARC" target="GPT-3.5-TURBO-0125">
      <data key="d4">7.0</data>
      <data key="d5">ARC dataset is evaluated using GPT-3.5-Turbo-0125</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="ARC" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">ARC is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ARC" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">ARC is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ARC" target="ORCA-3">
      <data key="d4">7.0</data>
      <data key="d5">ARC is a benchmark used to evaluate the performance of Orca-3</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ARC" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">ARC is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ARC" target="GPT-3.5-TURBO">
      <data key="d4">7.0</data>
      <data key="d5">ARC is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ARC" target="MMLU">
      <data key="d4">6.0</data>
      <data key="d5">Both MMLU and ARC are benchmarks that measure various capabilities of language models</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="ARC" target="ALLENAI">
      <data key="d4">8.0</data>
      <data key="d5">AllenAI developed the AI2 Reasoning Challenge (ARC) benchmark</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="READING COMPREHENSION" target="F1 SCORE">
      <data key="d4">7.0</data>
      <data key="d5">F1 scores are used to evaluate agents in the Reading Comprehension domain</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="READING COMPREHENSION" target="ACCURACY">
      <data key="d4">7.0</data>
      <data key="d5">Accuracy rates are used to evaluate agents in the Reading Comprehension domain</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="READING COMPREHENSION" target="TEXT MODIFICATION">
      <data key="d4">6.0</data>
      <data key="d5">Both Reading Comprehension and Text Modification are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="READING COMPREHENSION" target="OPEN DOMAIN QUESTION ANSWERING">
      <data key="d4">6.0</data>
      <data key="d5">Both Open Domain Question Answering and Reading Comprehension are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="READING COMPREHENSION" target="MULTIPLE CHOICE QUESTIONS">
      <data key="d4">6.0</data>
      <data key="d5">Both Multiple Choice Questions and Reading Comprehension are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="READING COMPREHENSION" target="READING COMPREHENSION TESTS">
      <data key="d4">7.0</data>
      <data key="d5">Reading comprehension tests assess the reader&#8217;s understanding of text passages.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="READING COMPREHENSION" target="AGENTINSTRUCT FLOW">
      <data key="d4">9.0</data>
      <data key="d5">AgentInstruct Flow includes a specific flow for reading comprehension to facilitate learning and understanding.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="READING COMPREHENSION" target="CONTENT TRANSFORMATION FLOW">
      <data key="d4">8.0</data>
      <data key="d5">Content Transformation Flow is used to generate materials that support reading comprehension.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="READING COMPREHENSION" target="LSAT">
      <data key="d4">9.0</data>
      <data key="d5">LSAT is known for its difficult reading comprehension sections</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="READING COMPREHENSION" target="LLMS">
      <data key="d4">8.0</data>
      <data key="d5">Reading comprehension is a crucial capability for LLMs</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="READING COMPREHENSION" target="SLMS">
      <data key="d4">8.0</data>
      <data key="d5">Reading comprehension is arguably even more important for Small Language Models (SLMs)</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="SEARCH SPACE" target="PROMPTBREEDER">
      <data key="d4">14.0</data>
      <data key="d5">PromptBreeder mutates only the text prompts of an agent within the search space</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH SPACE" target="FEED-FORWARD NETWORKS">
      <data key="d4">7.0</data>
      <data key="d5">Feed-forward networks are a type of search space explored in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH SPACE" target="GRAPH STRUCTURES">
      <data key="d4">7.0</data>
      <data key="d5">Graph structures are a type of search space explored in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH SPACE" target="PROGRAMMING LANGUAGES">
      <data key="d4">7.0</data>
      <data key="d5">Programming languages are used as a search space in ADAS to define and discover new agentic systems</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH SPACE" target="OPEN-SOURCE AGENT FRAMEWORKS">
      <data key="d4">7.0</data>
      <data key="d5">Open-source agent frameworks like LangChain are used to build upon existing building blocks in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH SPACE" target="EXISTING BUILDING BLOCKS">
      <data key="d4">7.0</data>
      <data key="d5">Existing building blocks are components like RAG and search engine tools that can be used in open-source agent frameworks in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="EVALUATION FUNCTION" target="ACCURACY RATE">
      <data key="d4">7.0</data>
      <data key="d5">Accuracy rate is a metric used in the evaluation function to assess an agent's performance on validation data</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="EVALUATION FUNCTION" target="COST">
      <data key="d4">7.0</data>
      <data key="d5">Cost is a metric used in the evaluation function to assess the expense of running an agent</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="EVALUATION FUNCTION" target="LATENCY">
      <data key="d4">7.0</data>
      <data key="d5">Latency is a metric used in the evaluation function to assess the response time of an agent</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="EVALUATION FUNCTION" target="SAFETY">
      <data key="d4">7.0</data>
      <data key="d5">Safety is a metric used in the evaluation function to assess the risk associated with an agent</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="EVALUATION FUNCTION" target="VALIDATION DATA">
      <data key="d4">7.0</data>
      <data key="d5">Validation data is used to assess the performance of an agent on unseen future data in the evaluation function</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="EVALUATION FUNCTION" target="UNSEEN FUTURE DATA">
      <data key="d4">7.0</data>
      <data key="d5">Unseen future data is data that an agent has not encountered before, used to evaluate its performance in the evaluation function</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="EVALUATION FUNCTION" target="EXPERIMENT COST">
      <data key="d4">1.0</data>
      <data key="d5">Optimizing the evaluation function can reduce the cost of experiments</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="PROMPTBREEDER" target="FERNANDO">
      <data key="d4">10.0</data>
      <data key="d5">Fernando is referenced in the context of PromptBreeder</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="PROMPTBREEDER" target="OPRO">
      <data key="d4">7.0</data>
      <data key="d5">Both OPRO and PromptBreeder adopt FMs to automate prompt engineering for agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="PROMPTBREEDER" target="SELF-DISCOVER">
      <data key="d4">7.0</data>
      <data key="d5">Both PromptBreeder and Self-Discover adopt FMs to automate prompt engineering for agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="PROMPTBREEDER" target="FERNANDO ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Fernando et al., 2024 discusses PromptBreeder and its application in automating prompt engineering for agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="COST" target="AGENTINSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">Cost is a limitation of AgentInstruct and synthetic data generation</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="FUNSEARCH" target="ROMERA-PAREDES">
      <data key="d4">6.0</data>
      <data key="d5">Romera-Paredes is an author mentioned in relation to the FunSearch algorithm</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="FUNSEARCH" target="FOUNDATION MODELS">
      <data key="d4">6.0</data>
      <data key="d5">FunSearch uses Foundation Models to write code for discovering better optimization algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="FUNSEARCH" target="ROMERA-PAREDES ET AL., 2024">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Romera-Paredes et al. in 2024 is related to FunSearch</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="LLM DEBATE" target="DU ET AL., 2023">
      <data key="d4">14.0</data>
      <data key="d5">Du et al., 2023 discusses the LLM Debate method, providing a comprehensive analysis of this approach. The publication by Du et al. in 2023 is specifically focused on the intricacies and applications of the LLM Debate, contributing valuable insights to the field of Artificial Intelligence and Machine Learning. This work highlights the significance of the LLM Debate method in advancing the understanding and development of AI and ML technologies.</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="QUALITY-DIVERSITY" target="LU ET AL., 2024C">
      <data key="d4">28.0</data>
      <data key="d5">Lu et al., 2024c is a publication that discusses and introduces the Quality-Diversity method. This method is a significant topic within the publication, making it a key reference for understanding Quality-Diversity.</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="QUALITY-DIVERSITY" target="LU ET AL.">
      <data key="d4">14.0</data>
      <data key="d5">Lu et al. are the authors of the Quality-Diversity agent and the Quality-Diversity method.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="ARC CHALLENGE" target="TRANSFORMATION RULE">
      <data key="d4">18.0</data>
      <data key="d5">The ARC CHALLENGE is a task that requires AI systems to learn a transformation rule from examples. Specifically, it involves understanding and applying the transformation rule to grid patterns. This challenge is designed to test the ability of AI to generalize and apply learned rules to new, unseen examples, thereby assessing the system's capacity for abstract reasoning and pattern recognition.</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7,4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="NUMBER COUNTING">
      <data key="d4">8.0</data>
      <data key="d5">Number counting is a capability required by AI systems in the ARC challenge</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="GEOMETRY">
      <data key="d4">8.0</data>
      <data key="d5">Geometry is a capability required by AI systems in the ARC challenge</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="TOPOLOGY">
      <data key="d4">8.0</data>
      <data key="d5">Topology is a capability required by AI systems in the ARC challenge</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="TOOL FUNCTIONS">
      <data key="d4">7.0</data>
      <data key="d5">Tool functions are provided in the framework to evaluate the generated transformation code in the ARC challenge</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="STOCHASTIC SAMPLING OF FMS">
      <data key="d4">7.0</data>
      <data key="d5">Stochastic sampling of FMs is used to reduce variance in the validation and test accuracy of agents in the ARC challenge</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="INPUT GRID">
      <data key="d4">9.0</data>
      <data key="d5">The ARC challenge involves input grids</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="OUTPUT GRID">
      <data key="d4">9.0</data>
      <data key="d5">The ARC challenge involves output grids</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="EXACT MATCH">
      <data key="d4">1.0</data>
      <data key="d5">The accuracy rate in the ARC challenge is calculated using the Exact Match metric</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="EXPERIMENT DETAILS">
      <data key="d4">7.0</data>
      <data key="d5">The experiment details section provides information about the ARC challenge</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="DEMONSTRATION EXAMPLES">
      <data key="d4">9.0</data>
      <data key="d5">The ARC challenge provides demonstration examples to illustrate the transformation rule</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="TEST EXAMPLE">
      <data key="d4">9.0</data>
      <data key="d5">The ARC challenge includes a test example where the output grid needs to be predicted</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="EXAMPLE 0">
      <data key="d4">8.0</data>
      <data key="d5">Example 0 is a specific task from the ARC challenge</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="LLM-DEBATE" target="DU ET AL., 2023">
      <data key="d4">14.0</data>
      <data key="d5">LLM-Debate, introduced by Du et al., 2023, is a method that has been referenced in the publication by Du et al., 2023. This work highlights the innovative approach and contributions made by the authors in the field of Artificial Intelligence and Machine Learning.</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="LLM-DEBATE" target="PHYSICS EXPERT">
      <data key="d4">6.0</data>
      <data key="d5">Physics Expert is a role assigned in the LLM-Debate method</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="LLM-DEBATE" target="CHEMISTRY EXPERT">
      <data key="d4">6.0</data>
      <data key="d5">Chemistry Expert is a role assigned in the LLM-Debate method</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="LLM-DEBATE" target="BIOLOGY EXPERT">
      <data key="d4">6.0</data>
      <data key="d5">Biology Expert is a role assigned in the LLM-Debate method</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="LLM-DEBATE" target="SCIENCE GENERALIST">
      <data key="d4">6.0</data>
      <data key="d5">Science Generalist is a role assigned in the LLM-Debate method</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="WANG ET AL., 2019" target="POET">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Wang et al. in 2019 is related to POET</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="WANG ET AL., 2020" target="POET">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Wang et al. in 2020 is related to POET</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="EFFICIENCY EXPERT" target="EXPERT_ADVISORS">
      <data key="d4">8.0</data>
      <data key="d5">Efficiency Expert is one of the expert advisors</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="READABILITY EXPERT" target="EXPERT_ADVISORS">
      <data key="d4">8.0</data>
      <data key="d5">Readability Expert is one of the expert advisors</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="SIMPLICITY EXPERT" target="EXPERT_ADVISORS">
      <data key="d4">8.0</data>
      <data key="d5">Simplicity Expert is one of the expert advisors</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="ITERATION 5" target="MECHANISM">
      <data key="d4">7.0</data>
      <data key="d5">The idea of incorporating diverse feedback emerged in Iteration 5</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="ITERATION 11" target="MECHANISM">
      <data key="d4">7.0</data>
      <data key="d5">The idea of evaluating for various specific traits emerged in Iteration 11</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="ITERATION 12" target="MECHANISM">
      <data key="d4">7.0</data>
      <data key="d5">The idea of simulating human-like feedback emerged in Iteration 12</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="MMLU" target="HENDRYCKS ET AL.">
      <data key="d4">6.0</data>
      <data key="d5">Hendrycks et al. are the authors of the MMLU benchmark</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="MMLU" target="STEM">
      <data key="d4">8.0</data>
      <data key="d5">STEM is one of the subject areas included in the MMLU benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="MMLU" target="SOCIAL SCIENCES">
      <data key="d4">8.0</data>
      <data key="d5">Social Sciences is one of the subject areas included in the MMLU benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="MMLU" target="HUMANITIES">
      <data key="d4">8.0</data>
      <data key="d5">Humanities is one of the subject areas included in the MMLU benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="MMLU" target="ORCA-3">
      <data key="d4">36.0</data>
      <data key="d5">MMLU is a benchmark used to evaluate the performance of Orca-3. Orca-3 demonstrated a 19% improvement on the MMLU benchmark, showcasing significant advancements, particularly in the mathematical benchmarks.</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MMLU" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">MMLU is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MMLU" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">MMLU is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MMLU" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">MMLU is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MMLU" target="GPT-3.5-TURBO">
      <data key="d4">7.0</data>
      <data key="d5">MMLU is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MMLU" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B shows significant improvements on the MMLU mathematical benchmarks</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="MMLU" target="MIRAGE">
      <data key="d4">8.0</data>
      <data key="d5">MMLU is one of the datasets included in the MIRAGE collection</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="GPQA" target="REIN ET AL.">
      <data key="d4">6.0</data>
      <data key="d5">Rein et al. are the authors of the GPQA benchmark</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="GPQA" target="VALIDATION_SET">
      <data key="d4">8.0</data>
      <data key="d5">GPQA uses a validation set for evaluation</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="GPQA" target="TEST_SET">
      <data key="d4">8.0</data>
      <data key="d5">GPQA uses a test set for evaluation</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="GPQA" target="ZERO-SHOT STYLE QUESTIONS">
      <data key="d4">8.0</data>
      <data key="d5">GPQA uses zero-shot style questions</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="GPQA" target="AUTOMATED DESIGN OF AGENTIC SYSTEMS">
      <data key="d4">7.0</data>
      <data key="d5">GPQA is discussed in the Automated Design of Agentic Systems document</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="GPQA" target="BIOLOGY">
      <data key="d4">8.0</data>
      <data key="d5">Biology is one of the domains included in the GPQA benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="GPQA" target="PHYSICS">
      <data key="d4">8.0</data>
      <data key="d5">Physics is one of the domains included in the GPQA benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="GPQA" target="CHEMISTRY">
      <data key="d4">8.0</data>
      <data key="d5">Chemistry is one of the domains included in the GPQA benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="GPQA" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">GPQA is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPQA" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">GPQA is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPQA" target="ORCA-3">
      <data key="d4">7.0</data>
      <data key="d5">GPQA is a benchmark used to evaluate the performance of Orca-3</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPQA" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">GPQA is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPQA" target="GPT-3.5-TURBO">
      <data key="d4">7.0</data>
      <data key="d5">GPQA is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPQA" target="DOMAIN EXPERTS">
      <data key="d4">8.0</data>
      <data key="d5">GPQA questions are created by domain experts pursuing PhDs in their respective fields</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="STEP-BACK ABSTRACTION" target="ZHENG ET AL.">
      <data key="d4">14.0</data>
      <data key="d5">Zheng et al. are the authors of the Step-back Abstraction agent and the Step-back Abstraction method.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="STEP-BACK ABSTRACTION" target="ZHENG ET AL., 2023">
      <data key="d4">20.0</data>
      <data key="d5">Step-back Abstraction is a method discussed in the publication by Zheng et al. in 2023. The publication by Zheng et al., 2023, is specifically related to the Step-back Abstraction method and serves as a key reference for understanding this approach.</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="STEP-BACK ABSTRACTION" target="REASONING AND PROBLEM-SOLVING DOMAINS">
      <data key="d4">7.0</data>
      <data key="d5">Step-back Abstraction is used as a baseline for experiments on Reasoning and Problem-Solving domains</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="ROLE ASSIGNMENT" target="XU ET AL.">
      <data key="d4">9.0</data>
      <data key="d5">Xu et al. are the authors of the Role Assignment agent and the Role Assignment method.</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="ROLE ASSIGNMENT" target="XU ET AL., 2023">
      <data key="d4">10.0</data>
      <data key="d5">Role Assignment is a method discussed in the publication by Xu et al. in 2023. The publication by Xu et al. in 2023 is specifically related to the Role Assignment method and serves as a key reference for understanding this approach.</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="ROLE ASSIGNMENT" target="REASONING AND PROBLEM-SOLVING DOMAINS">
      <data key="d4">7.0</data>
      <data key="d5">Role Assignment is used as a baseline for experiments on Reasoning and Problem-Solving domains</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="MECHANISM" target="EXPERTS">
      <data key="d4">6.0</data>
      <data key="d5">Experts evaluate various specific traits such as efficiency and simplicity</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="ACCURACY" target="AGENTINSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">Accuracy is a limitation of AgentInstruct and synthetic data generation</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="BASELINES" target="GPT-3.5-TURBO-0125">
      <data key="d4">1.0</data>
      <data key="d5">Baselines use the GPT-3.5-turbo-0125 model</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="BASELINES" target="AUTOMATED DESIGN OF AGENTIC SYSTEMS">
      <data key="d4">1.0</data>
      <data key="d5">Baselines are discussed in the Automated Design of Agentic Systems document</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="STRUCTURED FEEDBACK AND ENSEMBLE AGENT" target="BEST AGENT">
      <data key="d4">16.0</data>
      <data key="d5">The best agent uses structured feedback and ensemble methods</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="STRUCTURED FEEDBACK AND ENSEMBLE AGENT" target="FM_MODULE">
      <data key="d4">16.0</data>
      <data key="d5">The agent uses FM_Module to generate initial candidate solutions</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="STRUCTURED FEEDBACK AND ENSEMBLE AGENT" target="ENSEMBLE METHODS">
      <data key="d4">8.0</data>
      <data key="d5">The structured feedback and ensemble agent uses ensemble methods</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="SVAMP" target="PATEL ET AL., 2021">
      <data key="d4">8.0</data>
      <data key="d5">Patel et al., 2021 discusses the SVAMP dataset</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="SVAMP" target="PATEL ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Patel et al. are the authors of the SVAMP dataset</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="ASDIV" target="MIAO ET AL., 2020">
      <data key="d4">8.0</data>
      <data key="d5">Miao et al., 2020 discusses the ASDiv dataset</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="ASDIV" target="MIAO ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Miao et al. are the authors of the ASDiv dataset</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="AI-GENERATING ALGORITHMS" target="MAML">
      <data key="d4">7.0</data>
      <data key="d5">MAML is a technique under the second pillar of AI-Generating Algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="AI-GENERATING ALGORITHMS" target="META-RL">
      <data key="d4">7.0</data>
      <data key="d5">Meta-RL is a technique under the second pillar of AI-Generating Algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="AI-GENERATING ALGORITHMS" target="POET">
      <data key="d4">7.0</data>
      <data key="d5">POET is a technique under the third pillar of AI-Generating Algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="AI-GENERATING ALGORITHMS" target="CLUNE, 2019">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Clune in 2019 is related to AI-Generating Algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="VEMPRALA ET AL." target="DEVELOPING NEW SKILLS FOR EMBODIED AGENTS IN CODE">
      <data key="d4">8.0</data>
      <data key="d5">Vemprala et al. are the authors of the method for developing new skills for embodied agents in code</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="HONG ET AL." target="ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES AND ENABLING THEM TO COLLABORATE">
      <data key="d4">8.0</data>
      <data key="d5">Hong et al. are the authors of the method for assigning FM modules in the agentic system with different roles and enabling them to collaborate</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="QIAN ET AL." target="ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES AND ENABLING THEM TO COLLABORATE">
      <data key="d4">8.0</data>
      <data key="d5">Qian et al. are the authors of the method for assigning FM modules in the agentic system with different roles and enabling them to collaborate</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="RICHARDS" target="ENABLING THE AGENT TO INSTRUCT ITSELF FOR THE NEXT ACTION">
      <data key="d4">1.0</data>
      <data key="d5">Richards is the author of the method for enabling the agent to instruct itself for the next action</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="MAML" target="FINN ET AL., 2017">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Finn et al. in 2017 is related to MAML</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="META-RL" target="DUAN ET AL., 2017">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Duan et al. in 2017 is related to Meta-RL</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="META-RL" target="NORMAN &amp; CLUNE, 2023">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Norman &amp; Clune in 2023 is related to Meta-RL</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="META-RL" target="WANG ET AL., 2016">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Wang et al. in 2016 is related to Meta-RL</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="META-RL" target="ZINTGRAF ET AL., 2021A">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Zintgraf et al. in 2021 is related to Meta-RL</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="META-RL" target="ZINTGRAF ET AL., 2021B">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Zintgraf et al. in 2021 is related to Meta-RL</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="POET" target="DHARNA ET AL., 2020">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Dharna et al. in 2020 is related to POET</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="FOUNDATION MODELS" target="EOH">
      <data key="d4">6.0</data>
      <data key="d5">EoH uses Foundation Models to write code for discovering better optimization algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="FOUNDATION MODELS" target="DISCOPOP">
      <data key="d4">6.0</data>
      <data key="d5">DiscoPOP uses Foundation Models to program the loss function for preference learning</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="FOUNDATION MODELS" target="EUREKA">
      <data key="d4">6.0</data>
      <data key="d5">Eureka uses Foundation Models to write reward functions for reinforcement learning in robotics</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="FOUNDATION MODELS" target="LANGUAGE-TO-REWARD">
      <data key="d4">6.0</data>
      <data key="d5">Language-to-Reward uses Foundation Models to write reward functions for reinforcement learning in robotics</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="DISCOPOP" target="LU ET AL., 2024A">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Lu et al. in 2024 is related to DiscoPOP</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="DISCOPOP" target="RAFAILOV ET AL., 2024">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Rafailov et al. in 2024 is related to FM alignment training, which is part of DiscoPOP</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="EUREKA" target="MA ET AL., 2023">
      <data key="d4">14.0</data>
      <data key="d5">Ma et al., 2023 discusses Eureka and its application in enabling foundation models (FMs) to write reward functions for reinforcement learning in robotics. The publication by Ma et al. in 2023 is related to Eureka, highlighting its significance in advancing the capabilities of FMs within the robotics domain.</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="LANGUAGE-TO-REWARD" target="YU ET AL., 2023">
      <data key="d4">14.0</data>
      <data key="d5">The publication by Yu et al. in 2023 is centered on the concept of language-to-reward. This work delves into the application of language-to-reward in enabling foundation models (FMs) to write reward functions for reinforcement learning in robotics. By leveraging natural language, the study aims to simplify and enhance the process of defining reward functions, which are crucial for guiding the behavior of reinforcement learning agents in robotic systems.</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="CLUNE, 2019" target="AGI">
      <data key="d4">14.0</data>
      <data key="d5">Clune, 2019 discusses the pursuit of AGI</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="XU ET AL., 2023" target="AGENTVERSE">
      <data key="d4">7.0</data>
      <data key="d5">Xu et al., 2023 discusses the benefits of assigning personas or roles to agents, which is a concept used in AgentVerse</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="OPRO" target="SELF-DISCOVER">
      <data key="d4">7.0</data>
      <data key="d5">Both OPRO and Self-Discover adopt FMs to automate prompt engineering for agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="OPRO" target="YANG ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Yang et al., 2024 discusses OPRO and its application in automating prompt engineering for agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="SELF-DISCOVER" target="ZHOU ET AL., 2024A">
      <data key="d4">8.0</data>
      <data key="d5">Zhou et al., 2024a discusses Self-Discover and its application in automating prompt engineering for agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="EVOAGENT" target="AGENTVERSE">
      <data key="d4">7.0</data>
      <data key="d5">Both EvoAgent and AgentVerse optimize role definition in the prompt</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="EVOAGENT" target="YUAN ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Yuan et al., 2024 discusses EvoAgent and its application in optimizing role definition in the prompt</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="DYLAN" target="DSPY">
      <data key="d4">7.0</data>
      <data key="d5">Both DyLAN and DSPy involve learning more components than just prompts in agentic systems</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="DYLAN" target="GPT-SWARM">
      <data key="d4">7.0</data>
      <data key="d5">Both DyLAN and GPT-Swarm involve learning more components than just prompts in agentic systems</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="DSPY" target="GPT-SWARM">
      <data key="d4">7.0</data>
      <data key="d5">Both DSPy and GPT-Swarm involve learning more components than just prompts in agentic systems</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="DSPY" target="KHATTAB ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Khattab et al., 2024 discusses DSPy and its application in generating a set of possible nodes and optimizing across the Cartesian product of these nodes</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="GPT-SWARM" target="ZHUGE ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Zhuge et al., 2024 discusses GPT-Swarm and its application in representing an agentic system in a graph with a predefined set of nodes</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGENTOPTIMIZER" target="AGENT SYMBOLIC LEARNING">
      <data key="d4">7.0</data>
      <data key="d5">Both AgentOptimizer and Agent Symbolic Learning learn tools used in agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGENTOPTIMIZER" target="ZHANG ET AL., 2024B">
      <data key="d4">8.0</data>
      <data key="d5">Zhang et al., 2024b discusses AgentOptimizer and its application in learning the tools used in agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGENT SYMBOLIC LEARNING" target="ZHOU ET AL., 2024B">
      <data key="d4">8.0</data>
      <data key="d5">Zhou et al., 2024b discusses Agent Symbolic Learning and its application in learning prompts, tools, and control flow together in agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGI" target="AI-GA">
      <data key="d4">16.0</data>
      <data key="d5">AI-GA could potentially contribute to creating AGI faster than the current manual approach</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGI" target="BENGIO ET AL., 2024">
      <data key="d4">14.0</data>
      <data key="d5">Bengio et al., 2024 discusses the pursuit of AGI</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGI" target="BOSTROM, 2002">
      <data key="d4">14.0</data>
      <data key="d5">Bostrom, 2002 discusses the pursuit of AGI</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGI" target="ECOFFET ET AL., 2020">
      <data key="d4">14.0</data>
      <data key="d5">Ecoffet et al., 2020 discusses the pursuit of AGI</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGI" target="YUDKOWSKY ET AL., 2008">
      <data key="d4">2.0</data>
      <data key="d5">Yudkowsky et al., 2008 discusses the pursuit of AGI</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="COMPLEXITY" target="INSTRUCTION REFINEMENT FLOW">
      <data key="d4">8.0</data>
      <data key="d5">Instruction Refinement Flow aims to enhance the complexity of the generated instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="INSTRUCTION" target="TASKS">
      <data key="d4">7.0</data>
      <data key="d5">Instructions are the tasks or guidelines that agents follow to perform their roles in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="INSTRUCTION" target="OBJECTIVES">
      <data key="d4">7.0</data>
      <data key="d5">Instructions are tailored to achieve specific objectives in the Content Transformation Flow.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AS" target="HUMAN ORGANIZATION">
      <data key="d4">14.0</data>
      <data key="d5">AS sheds light on the origins of complexity emerging from human organization</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="AS" target="HUMAN SOCIETY">
      <data key="d4">14.0</data>
      <data key="d5">AS sheds light on the origins of complexity emerging from human society</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="HUMAN ORGANIZATION" target="AGENTIC SYSTEM">
      <data key="d4">16.0</data>
      <data key="d5">Agentic systems operate over natural language, which is used by humans in constructing our organization</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="HUMAN SOCIETY" target="AGENTIC SYSTEM">
      <data key="d4">16.0</data>
      <data key="d5">Agentic systems operate over natural language, which is used by humans in constructing our society</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="AGENTIC SYSTEM" target="HONG ET AL., 2023">
      <data key="d4">12.0</data>
      <data key="d5">Hong et al. (2023) incorporated the organizational structure for human companies in agents</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="AGENTIC SYSTEM" target="PARK ET AL., 2023">
      <data key="d4">12.0</data>
      <data key="d5">Park et al. (2023) simulated a human town with agents</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="JENNY ZHANG" target="JOEL LEHMAN">
      <data key="d4">16.0</data>
      <data key="d5">Jenny Zhang and Joel Lehman co-authored the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JENNY ZHANG" target="KENNETH STANLEY">
      <data key="d4">16.0</data>
      <data key="d5">Jenny Zhang and Kenneth Stanley co-authored the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JOEL LEHMAN" target="KENNETH STANLEY">
      <data key="d4">16.0</data>
      <data key="d5">Joel Lehman and Kenneth Stanley co-authored the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="CHATGPT" target="ORCA-2.5">
      <data key="d4">6.0</data>
      <data key="d5">Orca-2.5 and ChatGPT are both baseline models evaluated on the Orca-Bench dataset</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="CHATGPT" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">6.0</data>
      <data key="d5">Mistral-Instruct-7B and ChatGPT are both baseline models evaluated on the Orca-Bench dataset</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="CHATGPT" target="ORCA-3">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3 shows notable enhancement in capabilities during post-training compared to ChatGPT</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="CHATGPT" target="TABLE 2">
      <data key="d4">7.0</data>
      <data key="d5">Table 2 encapsulates the average scores of ChatGPT</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="CHATGPT" target="FIGURE 4">
      <data key="d4">7.0</data>
      <data key="d5">Figure 4 illustrates the performance comparison of ChatGPT</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="QIANG WANG" target="DAWEI YIN">
      <data key="d4">8.0</data>
      <data key="d5">Qiang Wang and Dawei Yin co-authored the paper "Tool learning with large language models: A survey"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="QIANG WANG" target="JUN XU">
      <data key="d4">8.0</data>
      <data key="d5">Qiang Wang and Jun Xu co-authored the paper "Tool learning with large language models: A survey"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="QIANG WANG" target="JI-RONG WEN">
      <data key="d4">8.0</data>
      <data key="d5">Qiang Wang and Ji-Rong Wen co-authored the paper "Tool learning with large language models: A survey"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAWEI YIN" target="JUN XU">
      <data key="d4">8.0</data>
      <data key="d5">Dawei Yin and Jun Xu co-authored the paper "Tool learning with large language models: A survey"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAWEI YIN" target="JI-RONG WEN">
      <data key="d4">8.0</data>
      <data key="d5">Dawei Yin and Ji-Rong Wen co-authored the paper "Tool learning with large language models: A survey"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="JUN XU" target="JI-RONG WEN">
      <data key="d4">8.0</data>
      <data key="d5">Jun Xu and Ji-Rong Wen co-authored the paper "Tool learning with large language models: A survey"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="RAFAEL RAFAILOV" target="ARCHIT SHARMA">
      <data key="d4">8.0</data>
      <data key="d5">Rafael Rafailov and Archit Sharma co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="RAFAEL RAFAILOV" target="ERIC MITCHELL">
      <data key="d4">8.0</data>
      <data key="d5">Rafael Rafailov and Eric Mitchell co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="RAFAEL RAFAILOV" target="STEFANO ERMON">
      <data key="d4">8.0</data>
      <data key="d5">Rafael Rafailov and Stefano Ermon co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ARCHIT SHARMA" target="ERIC MITCHELL">
      <data key="d4">8.0</data>
      <data key="d5">Archit Sharma and Eric Mitchell co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ARCHIT SHARMA" target="STEFANO ERMON">
      <data key="d4">8.0</data>
      <data key="d5">Archit Sharma and Stefano Ermon co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ERIC MITCHELL" target="STEFANO ERMON">
      <data key="d4">8.0</data>
      <data key="d5">Eric Mitchell and Stefano Ermon co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAVID REIN" target="BETTY LI HOU">
      <data key="d4">8.0</data>
      <data key="d5">David Rein and Betty Li Hou co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAVID REIN" target="ASA COOPER STICKLAND">
      <data key="d4">8.0</data>
      <data key="d5">David Rein and Asa Cooper Stickland co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAVID REIN" target="JACKSON PETTY">
      <data key="d4">8.0</data>
      <data key="d5">David Rein and Jackson Petty co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAVID REIN" target="RICHARD YUANZHE PANG">
      <data key="d4">8.0</data>
      <data key="d5">David Rein and Richard Yuanzhe Pang co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAVID REIN" target="JULIEN DIRANI">
      <data key="d4">8.0</data>
      <data key="d5">David Rein and Julien Dirani co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAVID REIN" target="JULIAN MICHAEL">
      <data key="d4">8.0</data>
      <data key="d5">David Rein and Julian Michael co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAVID REIN" target="SAMUEL R. BOWMAN">
      <data key="d4">8.0</data>
      <data key="d5">David Rein and Samuel R. Bowman co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="BETTY LI HOU" target="ASA COOPER STICKLAND">
      <data key="d4">8.0</data>
      <data key="d5">Betty Li Hou and Asa Cooper Stickland co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="BETTY LI HOU" target="JACKSON PETTY">
      <data key="d4">8.0</data>
      <data key="d5">Betty Li Hou and Jackson Petty co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="BETTY LI HOU" target="RICHARD YUANZHE PANG">
      <data key="d4">8.0</data>
      <data key="d5">Betty Li Hou and Richard Yuanzhe Pang co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="BETTY LI HOU" target="JULIEN DIRANI">
      <data key="d4">8.0</data>
      <data key="d5">Betty Li Hou and Julien Dirani co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="BETTY LI HOU" target="JULIAN MICHAEL">
      <data key="d4">8.0</data>
      <data key="d5">Betty Li Hou and Julian Michael co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="BETTY LI HOU" target="SAMUEL R. BOWMAN">
      <data key="d4">8.0</data>
      <data key="d5">Betty Li Hou and Samuel R. Bowman co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ASA COOPER STICKLAND" target="JACKSON PETTY">
      <data key="d4">8.0</data>
      <data key="d5">Asa Cooper Stickland and Jackson Petty co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ASA COOPER STICKLAND" target="RICHARD YUANZHE PANG">
      <data key="d4">8.0</data>
      <data key="d5">Asa Cooper Stickland and Richard Yuanzhe Pang co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ASA COOPER STICKLAND" target="JULIEN DIRANI">
      <data key="d4">8.0</data>
      <data key="d5">Asa Cooper Stickland and Julien Dirani co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ASA COOPER STICKLAND" target="JULIAN MICHAEL">
      <data key="d4">8.0</data>
      <data key="d5">Asa Cooper Stickland and Julian Michael co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ASA COOPER STICKLAND" target="SAMUEL R. BOWMAN">
      <data key="d4">8.0</data>
      <data key="d5">Asa Cooper Stickland and Samuel R. Bowman co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="JACKSON PETTY" target="RICHARD YUANZHE PANG">
      <data key="d4">8.0</data>
      <data key="d5">Jackson Petty and Richard Yuanzhe Pang co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="JACKSON PETTY" target="JULIEN DIRANI">
      <data key="d4">8.0</data>
      <data key="d5">Jackson Petty and Julien Dirani co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="JACKSON PETTY" target="JULIAN MICHAEL">
      <data key="d4">8.0</data>
      <data key="d5">Jackson Petty and Julian Michael co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="JACKSON PETTY" target="SAMUEL R. BOWMAN">
      <data key="d4">8.0</data>
      <data key="d5">Jackson Petty and Samuel R. Bowman co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="RICHARD YUANZHE PANG" target="JULIEN DIRANI">
      <data key="d4">8.0</data>
      <data key="d5">Richard Yuanzhe Pang and Julien Dirani co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="RICHARD YUANZHE PANG" target="JULIAN MICHAEL">
      <data key="d4">8.0</data>
      <data key="d5">Richard Yuanzhe Pang and Julian Michael co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="RICHARD YUANZHE PANG" target="SAMUEL R. BOWMAN">
      <data key="d4">8.0</data>
      <data key="d5">Richard Yuanzhe Pang and Samuel R. Bowman co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="MIRAC SUZGUN" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Nathanael Sch&#228;rli co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="MIRAC SUZGUN" target="QUOC V LE">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Quoc V Le co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="MIRAC SUZGUN" target="ED H CHI">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Ed H Chi co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="QUOC V LE" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Quoc V Le co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="QINGYUN WU" target="SHAOKUN ZHANG">
      <data key="d4">16.0</data>
      <data key="d5">Shaokun Zhang and Qingyun Wu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="QINGYUN WU" target="JIEYU ZHANG">
      <data key="d4">16.0</data>
      <data key="d5">Jieyu Zhang and Qingyun Wu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="QINGYUN WU" target="JIALE LIU">
      <data key="d4">16.0</data>
      <data key="d5">Jiale Liu and Qingyun Wu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="QINGYUN WU" target="LINXIN SONG">
      <data key="d4">16.0</data>
      <data key="d5">Linxin Song and Qingyun Wu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="QINGYUN WU" target="CHI WANG">
      <data key="d4">16.0</data>
      <data key="d5">Chi Wang and Qingyun Wu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="QINGYUN WU" target="RANJAY KRISHNA">
      <data key="d4">1.0</data>
      <data key="d5">Ranjay Krishna and Qingyun Wu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JIEYU ZHANG" target="SHAOKUN ZHANG">
      <data key="d4">16.0</data>
      <data key="d5">Shaokun Zhang and Jieyu Zhang co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JIEYU ZHANG" target="JIALE LIU">
      <data key="d4">16.0</data>
      <data key="d5">Jieyu Zhang and Jiale Liu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JIEYU ZHANG" target="LINXIN SONG">
      <data key="d4">16.0</data>
      <data key="d5">Jieyu Zhang and Linxin Song co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JIEYU ZHANG" target="CHI WANG">
      <data key="d4">16.0</data>
      <data key="d5">Jieyu Zhang and Chi Wang co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JIEYU ZHANG" target="RANJAY KRISHNA">
      <data key="d4">16.0</data>
      <data key="d5">Jieyu Zhang and Ranjay Krishna co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="SHAOKUN ZHANG" target="JIALE LIU">
      <data key="d4">16.0</data>
      <data key="d5">Shaokun Zhang and Jiale Liu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="SHAOKUN ZHANG" target="LINXIN SONG">
      <data key="d4">16.0</data>
      <data key="d5">Shaokun Zhang and Linxin Song co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="SHAOKUN ZHANG" target="CHI WANG">
      <data key="d4">16.0</data>
      <data key="d5">Shaokun Zhang and Chi Wang co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="SHAOKUN ZHANG" target="RANJAY KRISHNA">
      <data key="d4">16.0</data>
      <data key="d5">Shaokun Zhang and Ranjay Krishna co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="CHI WANG" target="JIALE LIU">
      <data key="d4">16.0</data>
      <data key="d5">Jiale Liu and Chi Wang co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="CHI WANG" target="LINXIN SONG">
      <data key="d4">16.0</data>
      <data key="d5">Linxin Song and Chi Wang co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="CHI WANG" target="RANJAY KRISHNA">
      <data key="d4">16.0</data>
      <data key="d5">Chi Wang and Ranjay Krishna co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JIALE LIU" target="LINXIN SONG">
      <data key="d4">16.0</data>
      <data key="d5">Jiale Liu and Linxin Song co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JIALE LIU" target="RANJAY KRISHNA">
      <data key="d4">16.0</data>
      <data key="d5">Jiale Liu and Ranjay Krishna co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="LINXIN SONG" target="RANJAY KRISHNA">
      <data key="d4">16.0</data>
      <data key="d5">Linxin Song and Ranjay Krishna co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="ED H CHI" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Ed H Chi co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YARIN GAL" target="ILIA SHUMAILOV">
      <data key="d4">8.0</data>
      <data key="d5">Ilia Shumailov and Yarin Gal co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YARIN GAL" target="ZAKHAR SHUMAYLOV">
      <data key="d4">8.0</data>
      <data key="d5">Zakhar Shumaylov and Yarin Gal co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YARIN GAL" target="YIREN ZHAO">
      <data key="d4">8.0</data>
      <data key="d5">Yiren Zhao and Yarin Gal co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YARIN GAL" target="NICOLAS PAPERNOT">
      <data key="d4">8.0</data>
      <data key="d5">Yarin Gal and Nicolas Papernot co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YARIN GAL" target="ROSS ANDERSON">
      <data key="d4">8.0</data>
      <data key="d5">Yarin Gal and Ross Anderson co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="CODE" target="THOUGHTS">
      <data key="d4">14.0</data>
      <data key="d5">Thoughts include the generated code</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="CODE" target="THINKING">
      <data key="d4">8.0</data>
      <data key="d5">Thinking is applied to the code during evaluation and feedback generation</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="CODE" target="CORRECT_EXAMPLES">
      <data key="d4">8.0</data>
      <data key="d5">Correct examples are instances where the code produced the correct output</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="CODE" target="WRONG_EXAMPLES">
      <data key="d4">8.0</data>
      <data key="d5">Wrong examples are instances where the code produced incorrect output</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="CODE" target="INITIAL_SOLUTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Initial solutions are the first set of code solutions generated</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FRAMEWORK" target="NAMEDTUPLE INFO OBJECT">
      <data key="d4">7.0</data>
      <data key="d5">The framework uses the namedtuple Info object to encapsulate and combine different types of information.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="APPENDIX B" target="BENCHMARKS">
      <data key="d4">7.0</data>
      <data key="d5">Appendix B specifies the types of tasks/benchmarks and the corresponding methods used to extract answers and generate metrics</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="FM MODULE" target="INFO">
      <data key="d4">9.0</data>
      <data key="d5">The FM Module constructs prompts by concatenating input Info objects</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="FORMAT_INST">
      <data key="d4">8.0</data>
      <data key="d5">The FM Module uses FORMAT_INST to format instructions for FM responses</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="ROLE_DESC">
      <data key="d4">7.0</data>
      <data key="d5">The FM Module uses ROLE_DESC to describe its role</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="GET_JSON_RESPONSE_FROM_GPT">
      <data key="d4">8.0</data>
      <data key="d5">The FM Module uses the function GET_JSON_RESPONSE_FROM_GPT to get JSON responses from a GPT model</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="AGENT SYSTEM">
      <data key="d4">7.0</data>
      <data key="d5">The Agent System can use the FM Module to process task information</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="META-AGENT SEARCH">
      <data key="d4">8.0</data>
      <data key="d5">The FM Module is part of the Meta-Agent Search system</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="FM RESPONSES">
      <data key="d4">8.0</data>
      <data key="d5">The FM Module generates FM responses based on the constructed prompts</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="RESULTS FROM TOOL FUNCTION CALLS">
      <data key="d4">7.0</data>
      <data key="d5">The FM Module may use results from tool function calls in its operations</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="TASK DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The FM Module uses task descriptions to facilitate communication between different modules</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="INFO" target="REFINEMENT_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">Info is used by the Refinement Module to provide structured feedback</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FM_MODULE" target="INITIAL INSTRUCTION">
      <data key="d4">14.0</data>
      <data key="d5">FM_Module uses the initial instruction to generate candidate solutions</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FM_MODULE" target="TASKINFO">
      <data key="d4">14.0</data>
      <data key="d5">FM_Module uses TaskInfo as input data to generate solutions</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FM_MODULE" target="INITIAL SOLUTIONS">
      <data key="d4">16.0</data>
      <data key="d5">FM_Module generates initial solutions based on the initial instruction and task information</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FM_MODULE" target="NUM_CANDIDATES">
      <data key="d4">12.0</data>
      <data key="d5">FM_Module generates a specified number of initial candidate solutions</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FM_MODULE" target="TEMPERATURE">
      <data key="d4">7.0</data>
      <data key="d5">FM_Module uses the temperature parameter to control the randomness of the generated solutions</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FM_MODULE" target="INITIAL CANDIDATE SOLUTIONS">
      <data key="d4">7.0</data>
      <data key="d5">FM_Module generates initial candidate solutions</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FM_MODULE" target="HUMAN_LIKE_FEEDBACK_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">Human-like Feedback Module is an instance of FM_Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FM_MODULE" target="EXPERT_ADVISORS">
      <data key="d4">8.0</data>
      <data key="d5">Expert advisors are instances of FM_Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FM_MODULE" target="REFINEMENT_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">Refinement Module is an instance of FM_Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FM_MODULE" target="FINAL_DECISION_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">Final Decision Module is an instance of FM_Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FM_MODULE" target="DECOMPOSITION MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Decomposition Module is an instance of FM_Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="FM_MODULE" target="SPECIALIZED EXPERT">
      <data key="d4">8.0</data>
      <data key="d5">Specialized Experts are instances of FM_Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="FM_MODULE" target="INTEGRATION MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Integration Module is an instance of FM_Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="FM_MODULE" target="VISUAL REPRESENTATION MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Visual Representation Module is an instance of FM_Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="FM_MODULE" target="VERIFICATION MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Verification Module is an instance of FM_Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="FM_MODULE" target="CHAIN-OF-THOUGHT MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Chain-of-Thought Module is an instance of FM_Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="META-AGENT SEARCH" target="CODE 1">
      <data key="d4">7.0</data>
      <data key="d5">Code 1 shows the simple framework used in Meta-Agent Search</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="META-AGENT SEARCH" target="CODE 2">
      <data key="d4">7.0</data>
      <data key="d5">Code 2 shows an example of implementing self-reflection using the framework</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="COT_INITIAL_INSTRUCTION" target="COT_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The cot_module uses the cot_initial_instruction to start the task-solving process</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="COT_REFLECT_INSTRUCTION" target="COT_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The cot_module uses the cot_reflect_instruction to refine the answer</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="CRITIC_INSTRUCTION" target="CRITIC_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The critic_module uses the critic_instruction to review and correct the answer</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="COT_MODULE" target="TASKINFO">
      <data key="d4">8.0</data>
      <data key="d5">The cot_module uses taskInfo as initial input</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="COT_MODULE" target="THINKING">
      <data key="d4">8.0</data>
      <data key="d5">The cot_module generates the thinking process</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="CRITIC_MODULE" target="TASKINFO">
      <data key="d4">8.0</data>
      <data key="d5">The critic_module uses taskInfo as input</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="CRITIC_MODULE" target="THINKING">
      <data key="d4">8.0</data>
      <data key="d5">The critic_module reviews the thinking process</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="TASKINFO" target="INITIAL_INSTRUCTION">
      <data key="d4">9.0</data>
      <data key="d5">TaskInfo provides the initial instruction for the code evaluation process</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="TASKINFO" target="DECOMPOSITION MODULE">
      <data key="d4">8.0</data>
      <data key="d5">TaskInfo is the input for the Decomposition Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="GPT-4O-2024-05-13" target="META_AGENT">
      <data key="d4">9.0</data>
      <data key="d5">Meta agent uses the GPT-4o-2024-05-13 model</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="GPT-3.5-TURBO-0125" target="DISCOVERED_AGENTS">
      <data key="d4">9.0</data>
      <data key="d5">Discovered agents use the GPT-3.5-turbo-0125 model</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="GPT-3.5-TURBO-0125" target="OPENAI API">
      <data key="d4">8.0</data>
      <data key="d5">GPT-3.5-Turbo-0125 is queried using the OpenAI API</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="INITIAL SOLUTIONS" target="THOUGHTS">
      <data key="d4">14.0</data>
      <data key="d5">Initial solutions include thoughts, which consist of thinking and code</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="EXPERIMENT" target="REPOSITORY">
      <data key="d4">7.0</data>
      <data key="d5">The experiment results are stored in a repository on GitHub</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="HUMAN_LIKE_FEEDBACK_MODULE" target="HUMAN_FEEDBACK_INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">Human-like Feedback Module uses the human feedback instruction to generate feedback</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="HUMAN_LIKE_FEEDBACK_MODULE" target="HUMAN_FEEDBACK">
      <data key="d4">9.0</data>
      <data key="d5">Human feedback is generated by the Human-like Feedback Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="EXPERT_ROLES" target="EXPERT_ADVISORS">
      <data key="d4">8.0</data>
      <data key="d5">Expert advisors are assigned specific expert roles</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="EXPERT_ADVISORS" target="EXPERT_INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">Expert advisors use the expert instruction to evaluate code</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="EXPERT_ADVISORS" target="EXPERT_FEEDBACK">
      <data key="d4">9.0</data>
      <data key="d5">Expert feedback is provided by expert advisors</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="REFINEMENT_MODULE" target="REFINEMENT_INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">Refinement Module uses the refinement instruction to refine code solutions</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="REFINEMENT_MODULE" target="REFINEMENT_THINKING">
      <data key="d4">8.0</data>
      <data key="d5">Refinement thinking is applied by the Refinement Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="REFINEMENT_MODULE" target="REFINED_CODE">
      <data key="d4">9.0</data>
      <data key="d5">Refined code is generated by the Refinement Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="REFINED_CODE" target="REFINED_SOLUTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Refined solutions include the refined code</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="REFINED_SOLUTIONS" target="SORTED_SOLUTIONS">
      <data key="d4">8.0</data>
      <data key="d5">Sorted solutions are derived from refined solutions</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="SORTED_SOLUTIONS" target="TOP_SOLUTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Top solutions are selected from sorted solutions</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FINAL_DECISION_INSTRUCTION" target="FINAL_DECISION_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">Final Decision Module uses the final decision instruction to make the final decision</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FINAL_DECISION_MODULE" target="FINAL_THOUGHTS">
      <data key="d4">8.0</data>
      <data key="d5">Final thoughts are generated by the Final Decision Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FINAL_THOUGHTS" target="FINAL_CODE">
      <data key="d4">9.0</data>
      <data key="d5">Final code is derived from final thoughts</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="DISCOVERED AGENTS" target="AUTOMATED DESIGN OF AGENTIC SYSTEMS">
      <data key="d4">7.0</data>
      <data key="d5">Discovered agents are discussed in the Automated Design of Agentic Systems document</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="BAHRAIN" target="INDIANS">
      <data key="d4">7.0</data>
      <data key="d5">Indians are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="BAHRAIN" target="BANGLADESHIS">
      <data key="d4">7.0</data>
      <data key="d5">Bangladeshis are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="BAHRAIN" target="PAKISTANIS">
      <data key="d4">7.0</data>
      <data key="d5">Pakistanis are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="BAHRAIN" target="FILIPINOS">
      <data key="d4">7.0</data>
      <data key="d5">Filipinos are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="BAHRAIN" target="INDONESIANS">
      <data key="d4">7.0</data>
      <data key="d5">Indonesians are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="PROBLEM-SOLVING" target="EXPERIMENTS">
      <data key="d4">8.0</data>
      <data key="d5">Experiments were conducted on the Problem-Solving domain</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="DECOMPOSITION MODULE" target="SUB_PROBLEMS">
      <data key="d4">9.0</data>
      <data key="d5">The Decomposition Module generates sub-problems from the main task</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="DECOMPOSITION MODULE" target="DECOMPOSITION INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">The Decomposition Module uses the decomposition instruction to break down the main task</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="SPECIALIZED EXPERT" target="SUB_SOLUTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Specialized Experts generate solutions to the sub-problems</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="SPECIALIZED EXPERT" target="SUB_PROBLEMS">
      <data key="d4">8.0</data>
      <data key="d5">Sub-problems are solved by Specialized Experts</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="SPECIALIZED EXPERT" target="SUB_PROBLEM INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">Specialized Experts use the sub-problem instruction to solve sub-problems</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="INTEGRATION MODULE" target="SUB_SOLUTIONS">
      <data key="d4">8.0</data>
      <data key="d5">Sub-solutions are integrated by the Integration Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="INTEGRATION MODULE" target="INTEGRATION INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">The Integration Module uses the integration instruction to combine sub-solutions</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="VISUAL REPRESENTATION MODULE" target="VISUAL INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">The Visual Representation Module uses the visual instruction to create visual aids</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="VERIFICATION MODULE" target="VERIFICATION INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">The Verification Module uses the verification instruction to verify visual aids</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT MODULE" target="COT INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">The Chain-of-Thought Module uses the CoT instruction to solve problems</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="GPT-4O-MINI" target="OPENAI API">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4o-Mini is queried using the OpenAI API</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="OPENAI API" target="EXPERIMENT COST">
      <data key="d4">9.0</data>
      <data key="d5">The cost of experiments is primarily due to querying the OpenAI API</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="GENERATIVE TEACHING">
      <data key="d4">41.0</data>
      <data key="d5">AgentInstruct is an agentic solution designed to facilitate Generative Teaching. It is highly effective in enhancing model performance across various datasets by creating synthetic data. This innovative approach leverages agentic methodologies to improve the generative teaching process, making it a valuable tool in the AI and ML landscape.</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="MISTRAL-7B">
      <data key="d4">24.0</data>
      <data key="d5">AgentInstruct created a synthetic dataset used to fine-tune the Mistral-7B model. Additionally, AgentInstruct was utilized to generate data for post-training Mistral-7B. This highlights the significant role of AgentInstruct in enhancing the performance and capabilities of the Mistral-7B model through both fine-tuning and post-training data generation.</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="MULTI-AGENT WORKFLOWS">
      <data key="d4">8.0</data>
      <data key="d5">AgentInstruct can utilize multi-agent workflows to generate high-quality synthetic data</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="SYNTHETIC-DATA-GENERATION-AS-A-SERVICE">
      <data key="d4">16.0</data>
      <data key="d5">AgentInstruct can enable the creation of Synthetic-Data-Generation-As-A-Service</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="CONTENT TRANSFORMATION AGENTS">
      <data key="d4">16.0</data>
      <data key="d5">AgentInstruct uses Content Transformation Agents to transform raw seeds</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="REFINEMENT AGENTS">
      <data key="d4">16.0</data>
      <data key="d5">AgentInstruct uses Refinement Agents to refine seed instructions</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="RAW SEEDS">
      <data key="d4">16.0</data>
      <data key="d5">AgentInstruct uses raw seeds as input to generate diverse data</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="CREATIVE WRITING">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct generates data covering the skill of creative writing</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="DATA FILTERING">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct applies data filtering to ensure the quality of generated data</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="VERIFICATION">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct applies verification to ensure the quality of generated data</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="DEMONSTRATION DATA">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct creates demonstration data to teach AI models specific skills</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="FEEDBACK DATA">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct creates feedback data to teach AI models specific skills</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="POST-TRAINING">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct generates synthetic datasets for post-training AI models</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="CONTINUAL LEARNING">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct enables continual learning of AI models</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="SELF-IMPROVEMENT">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct enables self-improvement of AI models</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="WEB DATA">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct uses web data as raw material for generating synthetic datasets</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="DOMAIN SPECIFIC DATA">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct uses domain specific data as raw material for generating synthetic datasets</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="TEXTBOOK CHAPTERS">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct uses textbook chapters as raw seeds</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="WEB ARTICLES">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct uses web articles as raw seeds</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="CODE SNIPPETS">
      <data key="d4">1.0</data>
      <data key="d5">AgentInstruct uses code snippets as raw seeds</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="CONTENT TRANSFORMATION FLOW">
      <data key="d4">9.0</data>
      <data key="d5">Content Transformation Flow is one of the three flows defined by AgentInstruct to automate the generation process.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="SEED INSTRUCTION GENERATION FLOW">
      <data key="d4">9.0</data>
      <data key="d5">Seed Instruction Generation Flow is one of the three flows defined by AgentInstruct to automate the generation process.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="INSTRUCTION REFINEMENT FLOW">
      <data key="d4">9.0</data>
      <data key="d5">Instruction Refinement Flow is one of the three flows defined by AgentInstruct to automate the generation process.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="ORCA-3">
      <data key="d4">9.0</data>
      <data key="d5">Orca-3 is trained using the AgentInstruct dataset</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="KNOWLEDGEPILE">
      <data key="d4">7.0</data>
      <data key="d5">KnowledgePile is one of the data sources used to create the AgentInstruct dataset</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="AUTOMATHTEXT">
      <data key="d4">7.0</data>
      <data key="d5">AutoMathText is one of the data sources used to create the AgentInstruct dataset</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="OPENSTAX">
      <data key="d4">7.0</data>
      <data key="d5">Openstax is one of the data sources used to create the AgentInstruct dataset</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="APACHE-2.0 LICENSED SOURCE CODE">
      <data key="d4">7.0</data>
      <data key="d5">Apache-2.0 licensed source code is one of the data sources used to create the AgentInstruct dataset</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="ORCA-BENCH">
      <data key="d4">8.0</data>
      <data key="d5">Orca-Bench dataset is created using data curated from AgentInstruct</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="ORCA-2.5">
      <data key="d4">8.0</data>
      <data key="d5">AgentInstruct data led to a performance augmentation of 33.94% over the Orca 2.5 baseline</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">8.0</data>
      <data key="d5">AgentInstruct data led to an enhancement of 14.92% over Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="MISTRAL">
      <data key="d4">16.0</data>
      <data key="d5">AgentInstruct is used to improve Mistral&#8217;s reading comprehension capabilities</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="MISTRAL-7B-INSTRUCT">
      <data key="d4">14.0</data>
      <data key="d5">AgentInstruct is used to enhance Mistral-7B-Instruct's proficiency in math</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="ORCA-3-7B">
      <data key="d4">15.0</data>
      <data key="d5">AgentInstruct is an approach utilized to reduce hallucinations in the Orca-3-7B model. Orca-3-7B, which is fine-tuned with AgentInstruct data, is based on the Mistral model family. This fine-tuning process aims to enhance the model's accuracy and reliability by leveraging the structured guidance provided by AgentInstruct, thereby improving its performance in generating coherent and factual responses.</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="LIMITATIONS">
      <data key="d4">8.0</data>
      <data key="d5">The Limitations section discusses the limitations of AgentInstruct and synthetic data generation</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="VALIDATION">
      <data key="d4">7.0</data>
      <data key="d5">Validation is a limitation of AgentInstruct and synthetic data generation</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="DEPENDENCY ON SEED DATA">
      <data key="d4">7.0</data>
      <data key="d5">Dependency on Seed Data is a limitation of AgentInstruct and synthetic data generation</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="EXTENSIBILITY">
      <data key="d4">7.0</data>
      <data key="d5">Extensibility is a limitation of AgentInstruct and synthetic data generation</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="BIAS">
      <data key="d4">7.0</data>
      <data key="d5">Bias is a limitation of AgentInstruct and synthetic data generation</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="SYNTHETIC DATA" target="LLMS">
      <data key="d4">9.0</data>
      <data key="d5">Synthetic data has significantly accelerated the development of Large Language Models (LLMs)</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="SYNTHETIC DATA" target="SLMS">
      <data key="d4">8.0</data>
      <data key="d5">Synthetic data has significantly accelerated the development of Small Language Models (SLMs)</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="SYNTHETIC DATA" target="RLHF">
      <data key="d4">7.0</data>
      <data key="d5">Synthetic data is used in the process of Reinforcement Learning from Human Feedback (RLHF)</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MISTRAL-7B" target="ORCA-3">
      <data key="d4">27.0</data>
      <data key="d5">Orca-3 is the fine-tuned version of the Mistral-7B model. It is the result of post-training Mistral-7B with data generated by AgentInstruct.</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="ORCA-3" target="AGIEVAL">
      <data key="d4">36.0</data>
      <data key="d5">AGIEval is a benchmark used to evaluate the performance of Orca-3. Orca-3, when assessed using the AGIEval benchmark, demonstrated a significant 40% improvement.</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="BBH">
      <data key="d4">28.0</data>
      <data key="d5">ORCA-3 demonstrated a significant performance enhancement, showing a 38% improvement on the BBH benchmark. BBH is a benchmark used to evaluate the performance of ORCA-3, highlighting the model's advancements and effectiveness in the AI and ML landscape.</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="ALPACAEVAL">
      <data key="d4">21.0</data>
      <data key="d5">ORCA-3 demonstrated a significant performance enhancement, showing a 45% improvement on the ALPACAEVAL benchmark. This notable achievement highlights ORCA-3's advanced capabilities and its potential impact within the AI and ML community.</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="ORCA-3" target="LLAMA-8B-INSTRUCT">
      <data key="d4">24.0</data>
      <data key="d5">ORCA-3 consistently outperformed LLAMA-8B-INSTRUCT on multiple benchmarks, establishing itself as a superior model in various performance evaluations.</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="ORCA-3" target="GPT-3.5-TURBO">
      <data key="d4">15.0</data>
      <data key="d5">ORCA-3 consistently outperformed GPT-3.5-TURBO when evaluated on various benchmarks.</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="MISTRAL-7B-INSTRUCT">
      <data key="d4">34.0</data>
      <data key="d5">ORCA-3 demonstrated significant improvements compared to MISTRAL-7B-INSTRUCT across various benchmarks. This indicates that ORCA-3 has shown relative advancements and superior performance in multiple evaluative measures when assessed against MISTRAL-7B-INSTRUCT.</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="ORCA-3" target="FOFO">
      <data key="d4">22.0</data>
      <data key="d5">ORCA-3 is an advanced AI model that has demonstrated significant improvements on the FOFO benchmark, which is used to evaluate its performance. The FOFO benchmark, also referred to as the FoFo format-following benchmark, serves as a critical metric for assessing the capabilities of ORCA-3. Through rigorous testing, ORCA-3 has shown notable advancements, highlighting its effectiveness and efficiency in meeting the criteria set by the FOFO benchmark.</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="MIRAGE-RAG">
      <data key="d4">1.0</data>
      <data key="d5">Orca-3 showed improvements on the MIRAGE-RAG benchmark</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="ORCA-3" target="ORCA-2.5-DATASET">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3 is trained using the Orca-2.5-dataset</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-3" target="MISTRAL-7B-V0.1">
      <data key="d4">9.0</data>
      <data key="d5">Orca-3 is finetuned on Mistral-7b-v0.1</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-3" target="NVIDIA A100">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3 is trained using 152 NVIDIA A100 GPUs</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-3" target="ADAMW OPTIMIZER">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3 is trained using the AdamW optimizer</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-3" target="ORCA-2.5">
      <data key="d4">17.0</data>
      <data key="d5">ORCA-3 demonstrates notable enhancements in capabilities during post-training compared to ORCA-2.5. Additionally, ORCA-3 shows significant improvements over ORCA-2.5 in various benchmarks, indicating a substantial advancement in performance and efficiency.</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3 shows notable enhancement in capabilities during post-training compared to Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">Orca-3 is evaluated against LLAMA3-8B-Instruct on various benchmarks</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="ORCA-3 CHECKPOINT EPOCH 1">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3 checkpoint epoch 1 is a specific checkpoint of the Orca-3 model</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="ORCA-3 CHECKPOINT EPOCH 2">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3 checkpoint epoch 2 is a specific checkpoint of the Orca-3 model</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="TABLE 2">
      <data key="d4">7.0</data>
      <data key="d5">Table 2 encapsulates the average scores of Orca-3</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="FIGURE 4">
      <data key="d4">7.0</data>
      <data key="d5">Figure 4 illustrates the performance comparison of Orca-3</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="BENCHMARK RESULTS">
      <data key="d4">7.0</data>
      <data key="d5">Benchmark results section evaluates Orca-3</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="METRIC-V2">
      <data key="d4">14.0</data>
      <data key="d5">ORCA-3 is an entity whose performance has been evaluated using the benchmark known as METRIC-V2. METRIC-V2 serves as a standard for assessing the capabilities and effectiveness of ORCA-3, providing a structured framework for performance evaluation.</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="METRIC-V1">
      <data key="d4">14.0</data>
      <data key="d5">ORCA-3 is an entity whose performance has been evaluated using the benchmark known as METRIC-V1. METRIC-V1 serves as a standard for assessing the capabilities and effectiveness of ORCA-3, providing a structured framework for performance evaluation.</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="LSAT">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3's performance on the LSAT reading comprehension sections matches that of GPT-4</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="ORCA-3" target="GEMINI PRO">
      <data key="d4">7.0</data>
      <data key="d5">Orca-3 surpasses Gemini Pro in format-following capabilities</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="AGIEVAL" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">AGIEval is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="AGIEVAL" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">AGIEval is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="AGIEVAL" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">AGIEval is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="AGIEVAL" target="GPT-3.5-TURBO">
      <data key="d4">7.0</data>
      <data key="d5">AGIEval is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="AGIEVAL" target="SAT">
      <data key="d4">7.0</data>
      <data key="d5">AGIEval evaluates models on tasks pertinent to human cognition, including standardized exams like SAT</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="AGIEVAL" target="LSAT">
      <data key="d4">7.0</data>
      <data key="d5">AGIEval evaluates models on tasks pertinent to human cognition, including standardized exams like LSAT</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="AGIEVAL" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B is evaluated using the AGIEval benchmark</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="BBH" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">BBH is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="BBH" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">BBH is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="BBH" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">BBH is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="BBH" target="GPT-3.5-TURBO">
      <data key="d4">7.0</data>
      <data key="d5">BBH is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="BBH" target="BIG-BENCH">
      <data key="d4">16.0</data>
      <data key="d5">BBH consists of tasks selected from the broader Big-Bench benchmark</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="ALPACAEVAL" target="ALPACA WEB DEMO">
      <data key="d4">7.0</data>
      <data key="d5">AlpacaEval consists of instructions representative of user interactions on the Alpaca web demo</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="ALPACAEVAL" target="OPEN-ENDED GENERATION">
      <data key="d4">7.0</data>
      <data key="d5">AlpacaEval is a benchmark under the category of Open-Ended Generation tasks.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="FOFO">
      <data key="d4">7.0</data>
      <data key="d5">FOFO is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="BENCHMARK RESULTS">
      <data key="d4">7.0</data>
      <data key="d5">Benchmark results section evaluates GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="METRIC-V2">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v2 is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="METRIC-V1">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v1 is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="PHI3">
      <data key="d4">2.0</data>
      <data key="d5">The accuracy scores for GPT-3.5-turbo on the GSM8K benchmark are reported in the Phi3 paper</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B's performance is compared to GPT-3.5-turbo in various benchmarks</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="MIRAGE">
      <data key="d4">8.0</data>
      <data key="d5">GPT-3.5-turbo is used in the evaluation of MIRAGE datasets</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="AZURE">
      <data key="d4">2.0</data>
      <data key="d5">Azure is recommended for reviewing transparency notes related to GPT-3.5-turbo</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="AGENTIC WORKFLOWS" target="MULTI-AGENT WORKFLOWS">
      <data key="d4">8.0</data>
      <data key="d5">Multi-agent workflows are a type of agentic workflow that involve multiple agents working together</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MISTRAL-7B-INSTRUCT" target="ORCA-3-7B">
      <data key="d4">17.0</data>
      <data key="d5">Orca-3-7B shows significant improvements over Mistral-7B-Instruct in various benchmarks. The performance of Orca-3-7B is compared to Mistral-7B-Instruct in these benchmarks, highlighting the advancements and enhancements made by Orca-3-7B.</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="FOFO" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">FOFO is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="FOFO" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">FOFO is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="FOFO" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">FOFO is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="FOFO" target="DOMAIN-SPECIFIC FORMATS">
      <data key="d4">8.0</data>
      <data key="d5">FoFo evaluates a model&#8217;s ability to follow complex, domain-specific formats</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="FOFO" target="AI-HUMAN COLLABORATION">
      <data key="d4">8.0</data>
      <data key="d5">FoFo benchmark tests format following on a diverse range of real-world formats and instructions created using AI-Human collaboration</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="FOFO" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B shows significant improvements on the FoFo format-following benchmark</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="FOFO" target="OPEN-ENDED GENERATION">
      <data key="d4">7.0</data>
      <data key="d5">FOFO is a benchmark under the category of Open-Ended Generation tasks.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION AGENTS" target="CONTENT TRANSFORMATION FLOW">
      <data key="d4">14.0</data>
      <data key="d5">Content Transformation Agents are used in the Content Transformation Flow</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="REFINEMENT AGENTS" target="REFINEMENT FLOW">
      <data key="d4">8.0</data>
      <data key="d5">Refinement Agents are used in the Refinement Flow</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="SEED INSTRUCTION CREATION FLOW">
      <data key="d4">14.0</data>
      <data key="d5">Content Transformation Flow is followed by Seed Instruction Creation Flow</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="AGENTIC FLOWS">
      <data key="d4">9.0</data>
      <data key="d5">Content Transformation Flow is a part of the agentic flows used to convert raw seeds into intermediate representations.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="SEED INSTRUCTION GENERATION FLOW">
      <data key="d4">8.0</data>
      <data key="d5">Seed Instruction Generation Flow takes transformed content from the Content Transformation Flow to generate diverse instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="INTERMEDIATE REPRESENTATION">
      <data key="d4">8.0</data>
      <data key="d5">Content Transformation Flow converts raw seeds into an intermediate representation to simplify the creation of instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="ARGUMENT PASSAGE GENERATOR">
      <data key="d4">8.0</data>
      <data key="d5">Argument Passage Generator is a tool within the Content Transformation Flow.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="LSAT LOGICAL REASONING TEST">
      <data key="d4">8.0</data>
      <data key="d5">The Content Transformation Flow aims to generate materials that support the creation of diverse question types required for comprehensive reading comprehension evaluation, including those found in the LSAT Logical Reasoning test.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="AGENTINSTRUCT FLOW">
      <data key="d4">6.0</data>
      <data key="d5">Both the AgentInstruct Flow and Content Transformation Flow involve the use of agents to perform specific tasks</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="API RETRIEVAL AGENT">
      <data key="d4">8.0</data>
      <data key="d5">The API Retrieval Agent is used in the Content Transformation Flow to expand the API list</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="LIBRARY RECONSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">Library Reconstruction is a scenario in the Content Transformation Flow</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SEED INSTRUCTION CREATION FLOW" target="REFINEMENT FLOW">
      <data key="d4">21.0</data>
      <data key="d5">The SEED INSTRUCTION CREATION FLOW and the REFINEMENT FLOW are integral processes involved in creating and refining tasks for the AI assistant to perform. The Seed Instruction Creation Flow is the initial phase, which is subsequently followed by the Refinement Flow. Together, these processes ensure that the tasks assigned to the AI assistant are well-defined and optimized for performance.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229,b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="SEED INSTRUCTION CREATION FLOW" target="AGENT-INSTRUCT FLOW">
      <data key="d4">8.0</data>
      <data key="d5">The Agent-Instruct flow uses the tasks created by the Seed Instruction Creation Flow to generate multi-turn conversations.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="AGENTIC FLOWS" target="SEED INSTRUCTION GENERATION FLOW">
      <data key="d4">9.0</data>
      <data key="d5">Seed Instruction Generation Flow is a part of the agentic flows used to generate diverse instructions from transformed content.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENTIC FLOWS" target="INSTRUCTION REFINEMENT FLOW">
      <data key="d4">9.0</data>
      <data key="d5">Instruction Refinement Flow is a part of the agentic flows used to iteratively enhance the complexity and quality of instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENTIC FLOWS" target="RAW ARTICLES">
      <data key="d4">8.0</data>
      <data key="d5">Raw articles are used as seeds in agentic flows to foster diversity and ensure broad coverage of generated problems.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENTIC FLOWS" target="SKILLS">
      <data key="d4">1.0</data>
      <data key="d5">Agentic flows are implemented for 17 different skills, each having multiple subcategories.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="INSTRUCTION REFINEMENT FLOW">
      <data key="d4">8.0</data>
      <data key="d5">Instruction Refinement Flow takes instructions from the Seed Instruction Generation Flow and enhances their complexity and quality.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="TAXONOMY">
      <data key="d4">8.0</data>
      <data key="d5">Seed Instruction Generation Flow uses a pre-defined, but extensible, taxonomy to introduce diversity in the generated instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="PASSAGE-QUESTION PAIRS">
      <data key="d4">8.0</data>
      <data key="d5">The Seed Instruction Generation Flow produces passage-question pairs as its output.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="LITERAL COMPREHENSION QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">The Seed Instruction Generation Flow includes literal comprehension questions as one of the types of reading comprehension questions.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="CRITICAL COMPREHENSION QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">The Seed Instruction Generation Flow includes critical comprehension questions as one of the types of reading comprehension questions.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="EVALUATIVE COMPREHENSION QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">The Seed Instruction Generation Flow includes evaluative comprehension questions as one of the types of reading comprehension questions.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="REASONING QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">The Seed Instruction Generation Flow includes reasoning questions as one of the types of reading comprehension questions.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="IDENTIFYING ASSUMPTIONS QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">The Seed Instruction Generation Flow includes identifying assumptions questions as one of the types of reading comprehension questions.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="IDENTIFYING INFORMATION THAT STRENGTHENS/WEAKENS AN ARGUMENT QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">The Seed Instruction Generation Flow includes questions that identify information that strengthens or weakens an argument.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="ORDERING EVENTS QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">The Seed Instruction Generation Flow includes ordering events questions as one of the types of reading comprehension questions.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="APPENDIX A">
      <data key="d4">7.0</data>
      <data key="d5">Appendix A lists the types of reading comprehension questions included in the Seed Instruction Generation Flow.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="INSTRUCTION TAXONOMY">
      <data key="d4">8.0</data>
      <data key="d5">The Instruction Taxonomy is used in the Seed Instruction Generation Flow</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="INSTRUCTION REFINEMENT FLOW" target="QUALITY">
      <data key="d4">8.0</data>
      <data key="d5">Instruction Refinement Flow aims to boost the quality of the generated instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="INSTRUCTION REFINEMENT FLOW" target="SUGGESTER AGENT">
      <data key="d4">8.0</data>
      <data key="d5">The Instruction Refinement Flow involves the Suggester Agent to provide suggestions for modifying passage-question pairs.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="INSTRUCTION REFINEMENT FLOW" target="EDITOR AGENT">
      <data key="d4">8.0</data>
      <data key="d5">The Instruction Refinement Flow involves the Editor Agent to implement modifications suggested by the Suggester Agent.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="INSTRUCTION REFINEMENT FLOW" target="HYPOTHETICAL STUDY">
      <data key="d4">7.0</data>
      <data key="d5">The Instruction Refinement Flow suggests introducing a hypothetical study to strengthen an argument.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="INSTRUCTION REFINEMENT FLOW" target="GENETIC PREDISPOSITION">
      <data key="d4">7.0</data>
      <data key="d5">The Instruction Refinement Flow suggests adding complexity by considering genetic predisposition to hyperuricemia.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="INSTRUCTION REFINEMENT FLOW" target="DISTRACTOR OPTION">
      <data key="d4">7.0</data>
      <data key="d5">The Instruction Refinement Flow suggests including a distractor option to test the test-taker's ability to discern relevant from irrelevant information.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="INSTRUCTION REFINEMENT FLOW" target="SUGGESTER-EDITOR PAIR">
      <data key="d4">8.0</data>
      <data key="d5">The Instruction Refinement Flow involves a Suggester-Editor pair that increases the complexity of generated instructions</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SUGGESTER AGENT" target="EDITOR AGENT">
      <data key="d4">8.0</data>
      <data key="d5">Suggester agents propose approaches to increase the intricacy of instructions, which are then modified by Editor agents.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="SUGGESTER AGENT" target="SUGGESTIONS">
      <data key="d4">8.0</data>
      <data key="d5">Suggester agents propose various suggestions to increase the intricacy of the initial instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="EDITOR AGENT" target="EDITING">
      <data key="d4">8.0</data>
      <data key="d5">Editor agents perform editing to modify instructions based on the suggestions from Suggester agents.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="TEXT MODIFICATION" target="OPEN DOMAIN QUESTION ANSWERING">
      <data key="d4">6.0</data>
      <data key="d5">Both Open Domain Question Answering and Text Modification are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="OPEN DOMAIN QUESTION ANSWERING" target="MULTIPLE-CHOICE QUESTIONS FLOWS">
      <data key="d4">14.0</data>
      <data key="d5">Both processes are used to generate math problems for evaluating AI models</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="BRAIN TEASER" target="ANALYTICAL REASONING">
      <data key="d4">6.0</data>
      <data key="d5">Both Brain Teaser and Analytical Reasoning are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="ANALYTICAL REASONING" target="FERMI PROBLEMS">
      <data key="d4">6.0</data>
      <data key="d5">Both Fermi Problems and Analytical Reasoning are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="MULTIPLE CHOICE QUESTIONS" target="EVALUATION DETAILS">
      <data key="d4">9.0</data>
      <data key="d5">Evaluation Details include the method used to extract answers and generate metrics for Multiple Choice Questions</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="MULTIPLE CHOICE QUESTIONS" target="OPEN-ENDED GENERATION SETTING">
      <data key="d4">8.0</data>
      <data key="d5">Multiple Choice Questions are evaluated in an open-ended generation setting</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="MULTIPLE CHOICE QUESTIONS" target="REGEX BASED EXTRACTION">
      <data key="d4">7.0</data>
      <data key="d5">Regex based extraction was previously used for extracting options selected by models in Multiple Choice Questions</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="MULTIPLE CHOICE QUESTIONS" target="SYSTEM MESSAGE">
      <data key="d4">8.0</data>
      <data key="d5">A system message is used to guide the GPT-4 model in extracting student responses for Multiple Choice Questions</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="DATA TO TEXT" target="TEXT EXTRACTION">
      <data key="d4">6.0</data>
      <data key="d5">Both Data to Text and Text Extraction are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="FERMI PROBLEMS" target="ENRICO FERMI">
      <data key="d4">18.0</data>
      <data key="d5">Fermi problems are named after physicist Enrico Fermi.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="CODING" target="TEXT EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">Both Coding and Text Extraction are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="TEXT EXTRACTION" target="TEXT CLASSIFICATION">
      <data key="d4">7.0</data>
      <data key="d5">Both text extraction and text classification are processes used in handling and analyzing text documents.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="TEXT EXTRACTION" target="RETRIEVAL AUGMENTED GENERATION">
      <data key="d4">8.0</data>
      <data key="d5">Retrieval Augmented Generation involves retrieving relevant documents, which is a form of text extraction.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="TABLE 1" target="SKILLS">
      <data key="d4">7.0</data>
      <data key="d5">Table 1 provides a full list of the 17 different skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="CASE STUDIES" target="SKILLS">
      <data key="d4">7.0</data>
      <data key="d5">Case studies explain how the workflows work for generating data for specific skills.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENTINSTRUCT FLOW" target="TEXT MODIFICATION TASKS">
      <data key="d4">8.0</data>
      <data key="d5">The AgentInstruct Flow involves performing various text modification tasks.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="AGENTINSTRUCT FLOW" target="PARAPHRASING AGENT">
      <data key="d4">1.0</data>
      <data key="d5">The AgentInstruct Flow includes the Paraphrasing Agent to create paraphrased versions of text.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="AGENTINSTRUCT FLOW" target="PARAPHRASING">
      <data key="d4">7.0</data>
      <data key="d5">The AgentInstruct Flow includes paraphrasing as one of the text modification tasks.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="AGENTINSTRUCT FLOW" target="SIMPLIFICATION">
      <data key="d4">7.0</data>
      <data key="d5">The AgentInstruct Flow includes simplification as one of the text modification tasks.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="AGENTINSTRUCT FLOW" target="REDACTING">
      <data key="d4">7.0</data>
      <data key="d5">The AgentInstruct Flow includes redacting as one of the text modification tasks.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="AGENTINSTRUCT FLOW" target="STYLING">
      <data key="d4">7.0</data>
      <data key="d5">The AgentInstruct Flow includes styling as one of the text modification tasks.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="AGENTINSTRUCT FLOW" target="CODE SWITCHING">
      <data key="d4">1.0</data>
      <data key="d5">The AgentInstruct Flow includes code switching as one of the text modification tasks.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="URIC ACID" target="HYPERURICEMIA">
      <data key="d4">18.0</data>
      <data key="d5">High levels of uric acid in the blood lead to the condition known as hyperuricemia. Hyperuricemia is a condition caused by high levels of uric acid in the blood.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="URIC ACID" target="HYPOURICEMIA">
      <data key="d4">10.0</data>
      <data key="d5">Hypouricemia is a condition characterized by low levels of uric acid in the blood. This condition, known as hypouricemia, arises when the concentration of uric acid in the bloodstream falls below normal levels. Uric acid, a waste product formed from the breakdown of purines, is typically excreted through the kidneys. When its levels are insufficient, it can indicate underlying health issues or metabolic disorders. Understanding the balance of uric acid is crucial for diagnosing and managing hypouricemia effectively.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="URIC ACID" target="CARDIOVASCULAR DISEASE">
      <data key="d4">17.0</data>
      <data key="d5">High levels of uric acid, a condition known as hyperuricemia, are associated with an increased risk of cardiovascular disease. This relationship highlights the importance of monitoring uric acid levels to potentially mitigate the risk of developing cardiovascular conditions.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="URIC ACID" target="KIDNEY DISEASES">
      <data key="d4">8.0</data>
      <data key="d5">Kidney diseases can be caused by an imbalance of uric acid in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="LEUKEMIA">
      <data key="d4">8.0</data>
      <data key="d5">Leukemia can cause an imbalance of uric acid in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="OBESITY">
      <data key="d4">8.0</data>
      <data key="d5">Obesity can cause an imbalance of uric acid in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="ANEMIA">
      <data key="d4">8.0</data>
      <data key="d5">Anemia can cause an imbalance of uric acid in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="ALCOHOL">
      <data key="d4">8.0</data>
      <data key="d5">Alcohol consumption is a lifestyle factor that can contribute to high levels of uric acid in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="PROCESSED FOODS">
      <data key="d4">8.0</data>
      <data key="d5">Processed foods are a lifestyle factor that can contribute to high levels of uric acid in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="RED MEAT">
      <data key="d4">15.0</data>
      <data key="d5">Red meat is a dietary source of purines, which can contribute to high levels of uric acid in the body. Uric acid, found in red meat, can influence its levels within the body, potentially leading to health implications.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="URIC ACID" target="SEAFOOD">
      <data key="d4">15.0</data>
      <data key="d5">Seafood is a dietary source of purines, which can contribute to high levels of uric acid in the body. Seafood contains uric acid, which can influence its levels in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="URIC ACID" target="LABORATORY BLOOD TESTS">
      <data key="d4">9.0</data>
      <data key="d5">Laboratory blood tests are used to diagnose conditions related to uric acid levels in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="URINE TESTS">
      <data key="d4">1.0</data>
      <data key="d5">Urine tests are used to diagnose conditions related to uric acid levels in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="ALCOHOL CONSUMPTION">
      <data key="d4">7.0</data>
      <data key="d5">Alcohol consumption can influence uric acid levels in the body.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="URIC ACID" target="PHYSICAL INACTIVITY">
      <data key="d4">7.0</data>
      <data key="d5">Physical inactivity can influence uric acid levels in the body.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="HYPERURICEMIA" target="CARDIOVASCULAR DISEASE">
      <data key="d4">8.0</data>
      <data key="d5">Hyperuricemia is associated with an increased risk of cardiovascular disease.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="HYPERURICEMIA" target="LABORATORY TESTS">
      <data key="d4">7.0</data>
      <data key="d5">Laboratory tests are required to diagnose hyperuricemia.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="HYPOURICEMIA" target="LABORATORY TESTS">
      <data key="d4">7.0</data>
      <data key="d5">Laboratory tests are required to diagnose hypouricemia.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="HYPOURICEMIA" target="KIDNEY ISSUES">
      <data key="d4">7.0</data>
      <data key="d5">Hypouricemia can indicate underlying kidney issues.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="HYPOURICEMIA" target="LIVER ISSUES">
      <data key="d4">7.0</data>
      <data key="d5">Hypouricemia can indicate underlying liver issues.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="PARAPHRASING AGENT" target="NATASCHA VAN DER ZWAN">
      <data key="d4">5.0</data>
      <data key="d5">The Paraphrasing Agent could be used to modify text related to Natascha van der Zwan's research on financialization</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="PARAPHRASING AGENT" target="SUGGESTER-EDITOR PAIR">
      <data key="d4">7.0</data>
      <data key="d5">The Suggester-Editor pair provides suggestions and edits to increase the complexity of instructions generated by the Paraphrasing Agent</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="PARAPHRASING AGENT" target="RANDOM SEED">
      <data key="d4">7.0</data>
      <data key="d5">The Paraphrasing Agent uses a random seed to create text modification tasks</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="NATASCHA VAN DER ZWAN" target="FINANCIALIZATION">
      <data key="d4">9.0</data>
      <data key="d5">Natascha van der Zwan identifies three distinct research streams that have approached financialization</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="FINANCIALIZATION" target="FINANCE">
      <data key="d4">8.0</data>
      <data key="d5">Financialization is a broad concept that encompasses the increasing social impact and interconnection of financial discourses, markets, actors, and institutions</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="AMERICAN ANTHROPOLOGICAL ASSOCIATION (AAA)" target="SEA 2017 ANNUAL MEETING">
      <data key="d4">8.0</data>
      <data key="d5">The American Anthropological Association hosts the SEA 2017 Annual Meeting</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SEA 2017 ANNUAL MEETING" target="UNIVERSITY OF IOWA">
      <data key="d4">8.0</data>
      <data key="d5">The SEA 2017 Annual Meeting was held at the University of Iowa</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SEA 2017 ANNUAL MEETING" target="APRIL 6-8, 2017">
      <data key="d4">9.0</data>
      <data key="d5">The SEA 2017 Annual Meeting took place on April 6-8, 2017</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SEA 2017 ANNUAL MEETING" target="DECEMBER 1, 2016">
      <data key="d4">9.0</data>
      <data key="d5">The abstract submission deadline for the SEA 2017 Annual Meeting was December 1, 2016</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SUGGESTER-EDITOR PAIR" target="SUGGESTION 1">
      <data key="d4">7.0</data>
      <data key="d5">Suggestion 1 is a suggestion provided by the Suggester-Editor pair</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SUGGESTER-EDITOR PAIR" target="SUGGESTION 2">
      <data key="d4">7.0</data>
      <data key="d5">Suggestion 2 is a suggestion provided by the Suggester-Editor pair</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SUGGESTER-EDITOR PAIR" target="SUGGESTION 3">
      <data key="d4">7.0</data>
      <data key="d5">Suggestion 3 is a suggestion provided by the Suggester-Editor pair</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SUGGESTER-EDITOR PAIR" target="MODIFIED INSTRUCTION 1">
      <data key="d4">7.0</data>
      <data key="d5">Modified Instruction 1 is an edit provided by the Suggester-Editor pair</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SUGGESTER-EDITOR PAIR" target="MODIFIED INSTRUCTION 2">
      <data key="d4">7.0</data>
      <data key="d5">Modified Instruction 2 is an edit provided by the Suggester-Editor pair</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SUGGESTER-EDITOR PAIR" target="MODIFIED INSTRUCTION 3">
      <data key="d4">7.0</data>
      <data key="d5">Modified Instruction 3 is an edit provided by the Suggester-Editor pair</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="VIEW ALL FOOD ITEMS" target="SEARCH FOOD ITEMS">
      <data key="d4">1.0</data>
      <data key="d5">Both "View All Food Items" and "Search Food Items" are APIs related to food item data</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="VIEW ALL FOOD ITEMS" target="FOOD ITEMS">
      <data key="d4">8.0</data>
      <data key="d5">The "View All Food Items" API provides a detailed list of food items</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SEARCH FOOD ITEMS" target="FOOD ITEMS">
      <data key="d4">1.0</data>
      <data key="d5">The "Search Food Items" API allows clients to search for food items</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SEARCH FOOD ITEMS" target="GET FOOD ITEM DETAILS">
      <data key="d4">7.0</data>
      <data key="d5">Both APIs are related to managing and retrieving information about food items.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="CREATE MEAL PLAN" target="TRACK USER MEAL">
      <data key="d4">8.0</data>
      <data key="d5">Both APIs are used in the process of managing a user's diet and meal tracking.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="CREATE MEAL PLAN" target="ASSISTANT">
      <data key="d4">9.0</data>
      <data key="d5">The assistant uses the Create Meal Plan API to generate a meal plan for the user.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="UPDATE FOOD ITEM" target="ADD NEW FOOD ITEM">
      <data key="d4">8.0</data>
      <data key="d5">Both APIs are used for managing the food database, either by adding new items or updating existing ones.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="UPDATE FOOD ITEM" target="DELETE FOOD ITEM">
      <data key="d4">8.0</data>
      <data key="d5">Both APIs are used for managing the food database, either by deleting or updating existing items.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="UPDATE FOOD ITEM" target="ASSISTANT">
      <data key="d4">9.0</data>
      <data key="d5">The assistant uses the Update Food Item API to update the nutritional information of an existing food item.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="UPDATE FOOD ITEM" target="CHANA MASALA">
      <data key="d4">9.0</data>
      <data key="d5">The user wants to update the nutritional information for Chana Masala using the Update Food Item API.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="TRACK USER MEAL" target="GET USER NUTRITIONAL STATS">
      <data key="d4">8.0</data>
      <data key="d5">Both APIs are used for tracking and analyzing a user's nutritional intake.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="TRACK USER MEAL" target="ASSISTANT">
      <data key="d4">9.0</data>
      <data key="d5">The assistant uses the Track User Meal API to help the user track their daily meals.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="GET DIETARY RECOMMENDATIONS" target="ASSISTANT">
      <data key="d4">9.0</data>
      <data key="d5">The assistant uses the Get Dietary Recommendations API to provide new food recommendations to the user.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="ADD NEW FOOD ITEM" target="ASSISTANT">
      <data key="d4">9.0</data>
      <data key="d5">The assistant uses the Add New Food Item API to add a new recipe to the database.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="ADD NEW FOOD ITEM" target="QUINOA SALAD">
      <data key="d4">9.0</data>
      <data key="d5">The user wants to add the Quinoa Salad recipe to the database using the Add New Food Item API.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="DELETE FOOD ITEM" target="ASSISTANT">
      <data key="d4">1.0</data>
      <data key="d5">The assistant uses the Delete Food Item API to remove a food item from the database.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="DELETE FOOD ITEM" target="BUTTER CHICKEN">
      <data key="d4">9.0</data>
      <data key="d5">The user wants to remove Butter Chicken from the database using the Delete Food Item API.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="GET USER NUTRITIONAL STATS" target="ASSISTANT">
      <data key="d4">9.0</data>
      <data key="d5">The assistant uses the Get User Nutritional Stats API to generate a nutritional summary for the user.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="USER" target="ASSISTANT">
      <data key="d4">17.0</data>
      <data key="d5">The user interacts with the assistant to achieve their dietary and nutritional goals. Both the user and the assistant are participants in the multi-turn interaction in Orca-Bench, a platform designed to facilitate complex, goal-oriented conversations. This interaction highlights the collaborative nature of their relationship, where the assistant provides guidance and support to help the user meet their specific health objectives.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="USER" target="QUINOA SALAD">
      <data key="d4">16.0</data>
      <data key="d5">The user requests the assistant to add the Quinoa Salad recipe to the database. The user wants to ensure that the Quinoa Salad recipe is included in the database for future reference and use.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="USER" target="CHANA MASALA">
      <data key="d4">16.0</data>
      <data key="d5">The user has requested the assistant to update the Chana Masala recipe in the database, specifically aiming to revise the calorie count for Chana Masala.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="USER" target="BUTTER CHICKEN">
      <data key="d4">9.0</data>
      <data key="d5">The user has requested the assistant to remove the Butter Chicken recipe from the database.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="USER" target="SYSTEM MESSAGE">
      <data key="d4">7.0</data>
      <data key="d5">System message is part of the multi-turn interaction involving the user in Orca-Bench</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ASSISTANT" target="VEGETARIAN MEAL PLAN">
      <data key="d4">9.0</data>
      <data key="d5">The assistant creates and provides an overview of the vegetarian meal plan</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ASSISTANT" target="SYSTEM MESSAGE">
      <data key="d4">7.0</data>
      <data key="d5">System message is part of the multi-turn interaction involving the assistant in Orca-Bench</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-2.5-DATASET" target="ORCA-1">
      <data key="d4">14.0</data>
      <data key="d5">Orca-2.5-dataset includes instructions sourced from Orca-1</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-2.5-DATASET" target="ORCA-2">
      <data key="d4">14.0</data>
      <data key="d5">Orca-2.5-dataset includes instructions sourced from Orca-2</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-2.5-DATASET" target="ORCA-MATH">
      <data key="d4">14.0</data>
      <data key="d5">Orca-2.5-dataset includes instructions sourced from Orca-Math</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-BENCH" target="OPEN DOMAIN QUESTION ANSWERING (ODQA)">
      <data key="d4">8.0</data>
      <data key="d5">Orca-Bench includes a test set for Open Domain Question Answering (ODQA)</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-BENCH" target="COMPLEX ODQA">
      <data key="d4">1.0</data>
      <data key="d5">Orca-Bench includes a test set for Complex ODQA</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ALMOND MILK" target="DAY 1">
      <data key="d4">8.0</data>
      <data key="d5">Almond milk is included in the breakfast for Day 1</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="CHICKPEA SALAD" target="DAY 1">
      <data key="d4">8.0</data>
      <data key="d5">Chickpea salad is included in the lunch for Day 1</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="WHOLE WHEAT BREAD" target="DAY 1">
      <data key="d4">8.0</data>
      <data key="d5">Whole wheat bread is included in the lunch for Day 1</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="MIXED VEGETABLE STIR FRY" target="DAY 1">
      <data key="d4">8.0</data>
      <data key="d5">Mixed vegetable stir fry is included in the dinner for Day 1</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="BROWN RICE" target="DAY 1">
      <data key="d4">8.0</data>
      <data key="d5">Brown rice is included in the dinner for Day 1</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-2.5" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">6.0</data>
      <data key="d5">Orca-2.5 and Mistral-Instruct-7B are both baseline models evaluated on the Orca-Bench dataset</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-2.5" target="TABLE 2">
      <data key="d4">7.0</data>
      <data key="d5">Table 2 encapsulates the average scores of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-2.5" target="FIGURE 4">
      <data key="d4">7.0</data>
      <data key="d5">Figure 4 illustrates the performance comparison of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-2.5" target="BENCHMARK RESULTS">
      <data key="d4">7.0</data>
      <data key="d5">Benchmark results section evaluates Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-2.5" target="METRIC-V2">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v2 is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-2.5" target="METRIC-V1">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v1 is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-2.5" target="MISTRAL">
      <data key="d4">8.0</data>
      <data key="d5">Mistral showed an 18% improvement over Orca-2.5 in reading comprehension capabilities</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="MISTRAL-INSTRUCT-7B" target="TABLE 2">
      <data key="d4">7.0</data>
      <data key="d5">Table 2 encapsulates the average scores of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MISTRAL-INSTRUCT-7B" target="FIGURE 4">
      <data key="d4">7.0</data>
      <data key="d5">Figure 4 illustrates the performance comparison of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MISTRAL-INSTRUCT-7B" target="BENCHMARK RESULTS">
      <data key="d4">7.0</data>
      <data key="d5">Benchmark results section evaluates Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MISTRAL-INSTRUCT-7B" target="METRIC-V2">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v2 is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MISTRAL-INSTRUCT-7B" target="METRIC-V1">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v1 is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="LLAMA3-8B-INSTRUCT" target="BENCHMARK RESULTS">
      <data key="d4">7.0</data>
      <data key="d5">Benchmark results section evaluates LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="LLAMA3-8B-INSTRUCT" target="METRIC-V2">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v2 is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="LLAMA3-8B-INSTRUCT" target="METRIC-V1">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v1 is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="LLAMA3-8B-INSTRUCT" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B's performance is compared to LLAMA3-8B-Instruct in various benchmarks</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="IFEVAL" target="INSTRUCTION-FOLLOWING">
      <data key="d4">8.0</data>
      <data key="d5">IFEval measures a model&#8217;s ability to follow natural language instructions</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="IFEVAL" target="OPEN-ENDED GENERATION">
      <data key="d4">7.0</data>
      <data key="d5">IFEval is a benchmark under the category of Open-Ended Generation tasks.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="INFOBENCH" target="DRFR">
      <data key="d4">8.0</data>
      <data key="d5">InFoBench uses the Decomposed Requirements Following Ratio (DRFR) metric</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="INFOBENCH" target="OPEN-ENDED GENERATION">
      <data key="d4">7.0</data>
      <data key="d5">InfoBench is a benchmark under the category of Open-Ended Generation tasks.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="EQBENCH" target="EQBENCH GPT-4 EXTRACTION SYSTEM MESSAGE">
      <data key="d4">1.0</data>
      <data key="d5">The EQBench GPT-4 Extraction System Message is used for extracting emotion scores from evaluated model responses in EQBench tasks</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="EQBENCH" target="EMOTION SCORES">
      <data key="d4">8.0</data>
      <data key="d5">EQBench tasks involve generating and evaluating emotion scores</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="EQBENCH" target="CRITIQUE">
      <data key="d4">1.0</data>
      <data key="d5">Critique is part of the process in EQBench tasks to revise emotion scores</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="TEACHER" target="STUDENT">
      <data key="d4">9.0</data>
      <data key="d5">Teacher (GPT-4) and student (model being evaluated) are roles in the multi-turn interaction in Orca-Bench</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="LSAT" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B's performance on the LSAT reading comprehension sections matches that of GPT-4</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="MISTRAL" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B is fine-tuned with AgentInstruct data based on the Mistral model family</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="GEMINI PRO" target="ORCA-3-7B">
      <data key="d4">7.0</data>
      <data key="d5">Orca-3-7B surpasses Gemini Pro in format-following capabilities</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="ORCA-3-7B" target="ORCA-2.5-7B">
      <data key="d4">17.0</data>
      <data key="d5">ORCA-3-7B shows significant improvements over ORCA-2.5-7B in various benchmarks. The performance of ORCA-3-7B is compared to ORCA-2.5-7B in these benchmarks, highlighting the advancements made in the newer model.</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="ORCA-3-7B" target="FOFO BENCHMARK">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B is evaluated on the FoFo benchmark</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="ORCA-3-7B" target="ACI-BENCH">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B is evaluated on the ACI-Bench for summarization abilities</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="ORCA-3-7B" target="INSTRUSUM">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B is evaluated on the InstruSum for summarization abilities</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="ORCA-3-7B" target="ORCA-SUM">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B is evaluated on the Orca-Sum for summarization abilities</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="ORCA-3-7B" target="MIRAGE">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B is used in the evaluation of MIRAGE datasets</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="ORCA-3-7B" target="AGENTINSTRUCT RAG FLOW">
      <data key="d4">9.0</data>
      <data key="d5">Orca-3-7B shows substantial improvement due to training with AgentInstruct RAG flow</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="ORCA-3-7B" target="DATA BIASES">
      <data key="d4">7.0</data>
      <data key="d5">Orca-3-7B retains many limitations, including data biases</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="ORCA-3-7B" target="LACK OF TRANSPARENCY">
      <data key="d4">7.0</data>
      <data key="d5">Orca-3-7B retains many limitations, including lack of transparency</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="ORCA-3-7B" target="CONTENT HARMS">
      <data key="d4">7.0</data>
      <data key="d5">Orca-3-7B retains many limitations, including content harms</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="ORCA-2.5-7B" target="MIRAGE">
      <data key="d4">8.0</data>
      <data key="d5">Orca-2.5-7B is used in the evaluation of MIRAGE datasets</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="ORCA-SUM" target="HUGGING FACE">
      <data key="d4">7.0</data>
      <data key="d5">Orca-Sum benchmark uses datasets collected from Hugging Face</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="MIRAGE" target="MMLU-MED">
      <data key="d4">7.0</data>
      <data key="d5">MMLU-Med is a dataset used in the MIRAGE benchmark</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="MIRAGE" target="MEDQA-US">
      <data key="d4">7.0</data>
      <data key="d5">MedQA-US is a dataset used in the MIRAGE benchmark</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="MIRAGE" target="MEDMCQA">
      <data key="d4">7.0</data>
      <data key="d5">MedMCQA is a dataset used in the MIRAGE benchmark</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="MIRAGE" target="PUBMEDQA">
      <data key="d4">16.0</data>
      <data key="d5">MIRAGE is a benchmark that includes various datasets designed to evaluate models' capabilities in Retrieval-Augmented Generation (RAG). One of the key datasets in the MIRAGE collection is PubMedQA. PubMedQA serves as an effective testbed for assessing models' ability to perform RAG tasks, making it a crucial component of the MIRAGE benchmark.</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="MIRAGE" target="BIOASQ">
      <data key="d4">9.0</data>
      <data key="d5">MIRAGE is a benchmark that includes the BioASQ dataset as part of its collection. BioASQ is one of the datasets utilized within the MIRAGE benchmark, contributing to the comprehensive evaluation framework provided by MIRAGE.</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="MIRAGE" target="MEDMEDQA">
      <data key="d4">8.0</data>
      <data key="d5">MedMedQA is one of the datasets included in the MIRAGE collection</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="MIRAGE" target="USMEDMCQA">
      <data key="d4">8.0</data>
      <data key="d5">USMedMCQA is one of the datasets included in the MIRAGE collection</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="MIRAGE" target="MISTRAL-7B-INSTRUCT-V0.1">
      <data key="d4">8.0</data>
      <data key="d5">Mistral-7B-Instruct-v0.1 is used in the evaluation of MIRAGE datasets</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="MIRAGE" target="MEDRAG">
      <data key="d4">8.0</data>
      <data key="d5">MedRAG is the retrieval mechanism used across all models on MIRAGE</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="MIRAGE" target="TABLE 8">
      <data key="d4">8.0</data>
      <data key="d5">Table 8 shows the evaluation results of RAG skill on MIRAGE datasets</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="AZURE" target="TRANSPARENCY NOTES">
      <data key="d4">8.0</data>
      <data key="d5">Azure provides transparency notes to explain the rationale behind specific outputs or decisions</data>
      <data key="d6">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </edge>
    <edge source="CONTENT HARMS" target="CONTENT MODERATION SERVICES">
      <data key="d4">9.0</data>
      <data key="d5">Content moderation services are recommended to prevent content harms caused by large language models</data>
      <data key="d6">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </edge>
    <edge source="PHILIPP WITTE" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Philipp Witte co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="HAIPING WU" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Haiping Wu co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="MICHAEL WYATT" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Michael Wyatt co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="BIN XIAO" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Bin Xiao co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="CAN XU" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Can Xu co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="JIAHANG XU" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Jiahang Xu co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="WEIJIAN XU" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Weijian Xu co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="SONALI YADAV" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Sonali Yadav co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="FAN YANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Fan Yang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="JIANWEI YANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Jianwei Yang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="ZIYI YANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Ziyi Yang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="YIFAN YANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Yifan Yang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="DONGHAN YU" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Donghan Yu co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="LU YUAN" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Lu Yuan co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="CHENGRUIDONG ZHANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Chengruidong Zhang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="CYRIL ZHANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Cyril Zhang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="JIANWEN ZHANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Jianwen Zhang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="LI LYNA ZHANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Li Lyna Zhang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="YI ZHANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Yi Zhang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="YUE ZHANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Yue Zhang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="YUNAN ZHANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Yunan Zhang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="XIREN ZHOU" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Xiren Zhou co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="ISAAC COWHEY" target="OREN ETZIONI">
      <data key="d4">8.0</data>
      <data key="d5">Isaac Cowhey and Oren Etzioni co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="ISAAC COWHEY" target="TUSHAR KHOT">
      <data key="d4">8.0</data>
      <data key="d5">Isaac Cowhey and Tushar Khot co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="ISAAC COWHEY" target="ASHISH SABHARWAL">
      <data key="d4">8.0</data>
      <data key="d5">Isaac Cowhey and Ashish Sabharwal co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="ISAAC COWHEY" target="CARISSA SCHOENICK">
      <data key="d4">8.0</data>
      <data key="d5">Isaac Cowhey and Carissa Schoenick co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="ISAAC COWHEY" target="OYVIND TAFJORD">
      <data key="d4">8.0</data>
      <data key="d5">Isaac Cowhey and Oyvind Tafjord co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="OREN ETZIONI" target="TUSHAR KHOT">
      <data key="d4">8.0</data>
      <data key="d5">Oren Etzioni and Tushar Khot co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="OREN ETZIONI" target="ASHISH SABHARWAL">
      <data key="d4">8.0</data>
      <data key="d5">Oren Etzioni and Ashish Sabharwal co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="BERNARD GHANEM" target="RII KHIZBULLIN">
      <data key="d4">8.0</data>
      <data key="d5">Rii Khizbullin and Bernard Ghanem co-authored the paper "Camel: Communicative agents for 'mind' exploration of large language model society"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="XUECHEN LI" target="TIANYI ZHANG">
      <data key="d4">8.0</data>
      <data key="d5">Xuechen Li and Tianyi Zhang co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="XUECHEN LI" target="YANN DUBOIS">
      <data key="d4">8.0</data>
      <data key="d5">Xuechen Li and Yann Dubois co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="XUECHEN LI" target="ROHAN TAORI">
      <data key="d4">8.0</data>
      <data key="d5">Xuechen Li and Rohan Taori co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="XUECHEN LI" target="ISHAAN GULRAJANI">
      <data key="d4">8.0</data>
      <data key="d5">Xuechen Li and Ishaan Gulrajani co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="XUECHEN LI" target="CARLOS GUESTRIN">
      <data key="d4">8.0</data>
      <data key="d5">Xuechen Li and Carlos Guestrin co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="XUECHEN LI" target="TATSUNORI B. HASHIMOTO">
      <data key="d4">8.0</data>
      <data key="d5">Xuechen Li and Tatsunori B. Hashimoto co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="TIANYI ZHANG" target="YANN DUBOIS">
      <data key="d4">8.0</data>
      <data key="d5">Tianyi Zhang and Yann Dubois co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="TIANYI ZHANG" target="ROHAN TAORI">
      <data key="d4">8.0</data>
      <data key="d5">Tianyi Zhang and Rohan Taori co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="TIANYI ZHANG" target="ISHAAN GULRAJANI">
      <data key="d4">8.0</data>
      <data key="d5">Tianyi Zhang and Ishaan Gulrajani co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="TIANYI ZHANG" target="CARLOS GUESTRIN">
      <data key="d4">8.0</data>
      <data key="d5">Tianyi Zhang and Carlos Guestrin co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="TIANYI ZHANG" target="TATSUNORI B. HASHIMOTO">
      <data key="d4">8.0</data>
      <data key="d5">Tianyi Zhang and Tatsunori B. Hashimoto co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YANN DUBOIS" target="ROHAN TAORI">
      <data key="d4">8.0</data>
      <data key="d5">Yann Dubois and Rohan Taori co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YANN DUBOIS" target="ISHAAN GULRAJANI">
      <data key="d4">8.0</data>
      <data key="d5">Yann Dubois and Ishaan Gulrajani co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YANN DUBOIS" target="CARLOS GUESTRIN">
      <data key="d4">8.0</data>
      <data key="d5">Yann Dubois and Carlos Guestrin co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YANN DUBOIS" target="TATSUNORI B. HASHIMOTO">
      <data key="d4">8.0</data>
      <data key="d5">Yann Dubois and Tatsunori B. Hashimoto co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ROHAN TAORI" target="ISHAAN GULRAJANI">
      <data key="d4">8.0</data>
      <data key="d5">Rohan Taori and Ishaan Gulrajani co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ROHAN TAORI" target="CARLOS GUESTRIN">
      <data key="d4">8.0</data>
      <data key="d5">Rohan Taori and Carlos Guestrin co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ROHAN TAORI" target="TATSUNORI B. HASHIMOTO">
      <data key="d4">8.0</data>
      <data key="d5">Rohan Taori and Tatsunori B. Hashimoto co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ISHAAN GULRAJANI" target="CARLOS GUESTRIN">
      <data key="d4">8.0</data>
      <data key="d5">Ishaan Gulrajani and Carlos Guestrin co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ISHAAN GULRAJANI" target="TATSUNORI B. HASHIMOTO">
      <data key="d4">8.0</data>
      <data key="d5">Ishaan Gulrajani and Tatsunori B. Hashimoto co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="CARLOS GUESTRIN" target="TATSUNORI B. HASHIMOTO">
      <data key="d4">8.0</data>
      <data key="d5">Carlos Guestrin and Tatsunori B. Hashimoto co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="ALEXANDER R. FABBRI">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Alexander R. Fabbri co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="JIAWEN CHEN">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Jiawen Chen co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="YILUN ZHAO">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Yilun Zhao co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="SIMENG HAN">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Simeng Han co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="SHAFIQ JOTY">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Shafiq Joty co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="DRAGOMIR RADEV">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Dragomir Radev co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="CHIEN-SHENG WU">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Chien-Sheng Wu co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="ARMAN COHAN">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Arman Cohan co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ALEXANDER R. FABBRI" target="JIAWEN CHEN">
      <data key="d4">8.0</data>
      <data key="d5">Alexander R. Fabbri and Jiawen Chen co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ALEXANDER R. FABBRI" target="YILUN ZHAO">
      <data key="d4">8.0</data>
      <data key="d5">Alexander R. Fabbri and Yilun Zhao co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ALEXANDER R. FABBRI" target="SIMENG HAN">
      <data key="d4">8.0</data>
      <data key="d5">Alexander R. Fabbri and Simeng Han co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ALEXANDER R. FABBRI" target="SHAFIQ JOTY">
      <data key="d4">8.0</data>
      <data key="d5">Alexander R. Fabbri and Shafiq Joty co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="MOHAMMED LATIF SIDDIQ" target="JIAHAO ZHANG">
      <data key="d4">8.0</data>
      <data key="d5">Mohammed Latif Siddiq and Jiahao Zhang co-authored the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="MOHAMMED LATIF SIDDIQ" target="LINDSAY RONEY">
      <data key="d4">8.0</data>
      <data key="d5">Mohammed Latif Siddiq and Lindsay Roney co-authored the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="MOHAMMED LATIF SIDDIQ" target="JOANNA C. S. SANTOS">
      <data key="d4">8.0</data>
      <data key="d5">Mohammed Latif Siddiq and Joanna C. S. Santos co-authored the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="JIAHAO ZHANG" target="LINDSAY RONEY">
      <data key="d4">8.0</data>
      <data key="d5">Jiahao Zhang and Lindsay Roney co-authored the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="JIAHAO ZHANG" target="JOANNA C. S. SANTOS">
      <data key="d4">8.0</data>
      <data key="d5">Jiahao Zhang and Joanna C. S. Santos co-authored the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="LINDSAY RONEY" target="JOANNA C. S. SANTOS">
      <data key="d4">8.0</data>
      <data key="d5">Lindsay Roney and Joanna C. S. Santos co-authored the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="ILIA SHUMAILOV" target="ZAKHAR SHUMAYLOV">
      <data key="d4">8.0</data>
      <data key="d5">Ilia Shumailov and Zakhar Shumaylov co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="ILIA SHUMAILOV" target="YIREN ZHAO">
      <data key="d4">8.0</data>
      <data key="d5">Ilia Shumailov and Yiren Zhao co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="ILIA SHUMAILOV" target="NICOLAS PAPERNOT">
      <data key="d4">8.0</data>
      <data key="d5">Ilia Shumailov and Nicolas Papernot co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="ILIA SHUMAILOV" target="ROSS ANDERSON">
      <data key="d4">8.0</data>
      <data key="d5">Ilia Shumailov and Ross Anderson co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="ZAKHAR SHUMAYLOV" target="YIREN ZHAO">
      <data key="d4">8.0</data>
      <data key="d5">Zakhar Shumaylov and Yiren Zhao co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="ZAKHAR SHUMAYLOV" target="NICOLAS PAPERNOT">
      <data key="d4">8.0</data>
      <data key="d5">Zakhar Shumaylov and Nicolas Papernot co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="ZAKHAR SHUMAYLOV" target="ROSS ANDERSON">
      <data key="d4">8.0</data>
      <data key="d5">Zakhar Shumaylov and Ross Anderson co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YIREN ZHAO" target="NICOLAS PAPERNOT">
      <data key="d4">8.0</data>
      <data key="d5">Yiren Zhao and Nicolas Papernot co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YIREN ZHAO" target="ROSS ANDERSON">
      <data key="d4">8.0</data>
      <data key="d5">Yiren Zhao and Ross Anderson co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="NICOLAS PAPERNOT" target="ROSS ANDERSON">
      <data key="d4">8.0</data>
      <data key="d5">Nicolas Papernot and Ross Anderson co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="TEXT MODIFICATION FLOW" target="INSTRUCTION TAXONOMY">
      <data key="d4">8.0</data>
      <data key="d5">The Text Modification Flow is part of the Instruction Taxonomy for generating seed instructions</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="STUDENT RESPONSE" target="PARSED STUDENT ANSWER">
      <data key="d4">18.0</data>
      <data key="d5">The parsed student answer is extracted from the student response. The student response is parsed to extract the final answer.</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50,5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="STUDENT RESPONSE" target="ANSWER OPTIONS">
      <data key="d4">8.0</data>
      <data key="d5">The student response includes selecting an option from the provided answer options</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="ANSWER OPTIONS" target="PARSED STUDENT ANSWER">
      <data key="d4">1.0</data>
      <data key="d5">The parsed student answer is represented by the alphabet ID of the selected option from the answer options</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="EXACT MATCH/SPAN EXTRACTION PROBLEMS" target="MATHS GPT-4 EXTRACTION SYSTEM MESSAGE">
      <data key="d4">7.0</data>
      <data key="d5">The Maths GPT-4 Extraction System Message is used for evaluating student answers to math word problems in exact match/span extraction problems</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="EXACT MATCH/SPAN EXTRACTION PROBLEMS" target="GENERAL EXTRACTION SYSTEM MESSAGE">
      <data key="d4">7.0</data>
      <data key="d5">The General Extraction System Message is used for parsing student responses and matching them with the correct answer in exact match/span extraction problems</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="FINAL ANSWER" target="ERROR ANALYSIS">
      <data key="d4">8.0</data>
      <data key="d5">Error analysis involves comparing the final answer with the student's answer</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="ERROR ANALYSIS" target="FINAL VERDICT">
      <data key="d4">8.0</data>
      <data key="d5">The final verdict is the result of the error analysis</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="EMOTION SCORES" target="CRITIQUE">
      <data key="d4">7.0</data>
      <data key="d5">Critique is used to revise the emotion scores</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="RESIGNED" target="ELLIOT">
      <data key="d4">8.0</data>
      <data key="d5">Elliot feels resigned after confessing his feelings to Alex, with a score of 7.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="ANGRY" target="ELLIOT">
      <data key="d4">7.0</data>
      <data key="d5">Elliot feels angry at himself for putting himself in this situation, with a score of 3.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="HOPEFUL" target="ELLIOT">
      <data key="d4">7.0</data>
      <data key="d5">Elliot feels hopeful that Alex might reciprocate his feelings, with a score of 5.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="EMBARRASSED" target="ELLIOT">
      <data key="d4">8.0</data>
      <data key="d5">Elliot feels embarrassed for putting Alex in an awkward position, with a score of 8.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="QUALITY JUDGE" target="PROMPT TEMPLATE">
      <data key="d4">8.0</data>
      <data key="d5">The Quality Judge process uses a specific prompt template to evaluate the quality of AI-generated summaries.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="REVISED SCORES" target="EMOTIONS">
      <data key="d4">8.0</data>
      <data key="d5">Revised scores provide numerical values for each of the emotions experienced by Elliot.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="TEXT SUMMARIZATION" target="HALLUCINATION DETECTION">
      <data key="d4">8.0</data>
      <data key="d5">Hallucination Detection is a part of the Text Summarization task, focusing on identifying hallucinated content.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="TEXT SUMMARIZATION" target="SUMMARIZATION QUALITY">
      <data key="d4">8.0</data>
      <data key="d5">Summarization Quality is a part of the Text Summarization task, focusing on evaluating the quality of the generated summaries.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
  </graph>
</graphml>