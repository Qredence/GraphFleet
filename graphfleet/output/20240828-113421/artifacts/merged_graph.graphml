<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d6" for="edge" attr.name="source_id" attr.type="string" />
  <key id="d5" for="edge" attr.name="description" attr.type="string" />
  <key id="d4" for="edge" attr.name="weight" attr.type="double" />
  <key id="d3" for="node" attr.name="entity_type" attr.type="string" />
  <key id="d2" for="node" attr.name="source_id" attr.type="string" />
  <key id="d1" for="node" attr.name="description" attr.type="string" />
  <key id="d0" for="node" attr.name="type" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="DARREN EDGE">
      <data key="d0">PERSON</data>
      <data key="d1">Darren Edge is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="HA TRINH">
      <data key="d0">PERSON</data>
      <data key="d1">Ha Trinh is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="NEWMAN CHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Newman Cheng is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="JOSHUA BRADLEY">
      <data key="d0">PERSON</data>
      <data key="d1">Joshua Bradley is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="ALEX CHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Alex Chao is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="APURVA MODY">
      <data key="d0">PERSON</data>
      <data key="d1">Apurva Mody is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="STEVEN TRUITT">
      <data key="d0">PERSON</data>
      <data key="d1">Steven Truitt is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="JONATHAN LARSON">
      <data key="d0">PERSON</data>
      <data key="d1">Jonathan Larson is an author of the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="MICROSOFT RESEARCH">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Microsoft Research is the organization where Darren Edge, Ha Trinh, and Jonathan Larson are affiliated
Microsoft Research is the organization where the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows" are affiliated</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Microsoft Strategic Missions and Technologies is the organization where Newman Cheng, Joshua Bradley, and Steven Truitt are affiliated</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="MICROSOFT OFFICE OF THE CTO">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Microsoft Office of the CTO is the organization where Alex Chao and Apurva Mody are affiliated</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
    </node>
    <node id="GRAPH RAG">
      <data key="d0">METHOD/APPROACH</data>
      <data key="d1">Graph RAG is a proposed approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed
Graph RAG is an approach based on global summarization of an LLM-derived knowledge graph
A method using graph communities to answer user queries
A method used to generate comprehensive and detailed lists of public figures from various entertainment sectors

Graph RAG is a method that uses a self-generated graph index to enable efficient and comprehensive summarization and iterative question answering
Graph RAG is an approach that uses the natural modularity of graphs to partition data for global summarization</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,26b2dad01a219bc034ac7d6a32d07582,64476a39d7d8b87b399e3bd3cead79c7,ac21ebe9a9d70d691c717f961d3f10c8,c8e8019de153e439d6a79dcf209b943b,edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="RETRIEVAL-AUGMENTED GENERATION (RAG)">
      <data key="d0">METHOD/APPROACH</data>
      <data key="d1">Retrieval-augmented generation (RAG) is a method to retrieve relevant information from an external knowledge source to enable large language models to answer questions over private and/or previously unseen document collections</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">METHOD/APPROACH</data>
    </node>
    <node id="QUERY-FOCUSED SUMMARIZATION (QFS)">
      <data key="d0">METHOD/APPROACH</data>
      <data key="d1">Query-focused summarization (QFS) is a task that generates natural language summaries based on specific user queries</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">METHOD/APPROACH</data>
    </node>
    <node id="LARGE LANGUAGE MODELS (LLMS)">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Large language models (LLMs) are advanced models used to automate human-like sensemaking in complex domains</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="COMMUNITY DETECTION">
      <data key="d0">PROCESS</data>
      <data key="d1">Community detection is a process used to partition a graph index into groups of elements that can be summarized in parallel

Community detection is the process of partitioning a graph into communities of nodes with stronger connections to one another than to other nodes</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7,e66ed885a08f92cc69f4895302c33047</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="LEIDEN">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Leiden is an algorithm used for community detection in the Graph RAG approach
Leiden is a community detection algorithm used to partition graphs into modular communities
Leiden is an algorithm used for community detection in large-scale graphs, known for its efficiency in recovering hierarchical community structures</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7,e66ed885a08f92cc69f4895302c33047</data>
      <data key="d3">ALGORITHM</data>
    </node>
    <node id="GLOBAL SENSEMAKING QUESTIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Global sensemaking questions are questions that require understanding connections among people, places, and events to anticipate their trajectories and act effectively</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="OPEN-SOURCE IMPLEMENTATION">
      <data key="d0">RESOURCE</data>
      <data key="d1">An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">RESOURCE</data>
    </node>
    <node id="MICROSOFT">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Microsoft is the company associated with the research and development of the Graph RAG approach
Microsoft is the company where Kevin Scott serves as CTO, and it is mentioned in the context of the podcast transcripts dataset.
Microsoft is the organization behind the study "The impact of large language models on scientific discovery: a preliminary study using gpt-4"Microsoft is the organization behind the podcast "Behind the Tech"
Microsoft is the organization behind the technical report "ChatGPT for robotics: Design principles and model abilities"</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,34d0bb2211fc795fe1096442e086a2b3,4930fce6da868f894757a9da465807ba,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">ORGANIZATION</data>
    </node>
    <node id="RANADE AND JOSHI">
      <data key="d0">PERSON</data>
      <data key="d1">Ranade and Joshi are researchers mentioned in the context of intelligence analysis using LLMs</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KLEIN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Klein et al. are researchers who defined sensemaking as a motivated, continuous effort to understand connections</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LEWIS ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Lewis et al. are researchers associated with the development of retrieval-augmented generation (RAG)
Lewis et al. are authors who have contributed to the research on memory structures in agentic systems
Lewis et al. are the authors of the External Memory and RAG methods
Lewis et al. are authors referenced in the context of RAG</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,0c932f7def033fa2b1bf210fbb771e7d,6bdf681c0bd9e401ac72344a6a0ae479,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DANG">
      <data key="d0">PERSON</data>
      <data key="d1">Dang is a researcher associated with query-focused summarization (QFS)</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BAUMEL ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Baumel et al. are researchers associated with query-focused abstractive summarization</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LASKAR ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Laskar et al. are researchers associated with query-focused abstractive summarization
Laskar et al. are authors referenced for their work on summarization tasks in 2022</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YAO ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Yao et al. are researchers associated with query-focused abstractive summarization
Yao et al. are referenced multiple times in the context of web navigation, prompting techniques, and search-guided language model work.
Yao et al. are authors who have contributed to the research on chain-of-thought planning and reasoning
Yao et al. are the authors of the Chain-of-Thought-based planning and reasoning methods</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,0c932f7def033fa2b1bf210fbb771e7d,93cb0d0456e0822b5fe30a3e627405f8,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GOODWIN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Goodwin et al. are researchers associated with the application of the transformer architecture to summarization tasks
</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIU AND LAPATA">
      <data key="d0">PERSON</data>
      <data key="d1">Liu and Lapata are researchers associated with the application of the transformer architecture to summarization tasks
Liu and Lapata are authors referenced for their work on summarization tasks in 2019</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GPT">
      <data key="d0">MODEL</data>
      <data key="d1">GPT is a large language model used for various summarization tasks
GPT is a series of language models referenced for their ability to perform summarization tasks
GPT is a Foundation Model developed by OpenAI, used for general-purpose agentic tasks</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="LLAMA">
      <data key="d0">MODEL</data>
      <data key="d1">Llama is a large language model used for various summarization tasks
Llama is a series of language models referenced for their ability to perform summarization tasks</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="GEMINI">
      <data key="d0">MODEL</data>
      <data key="d1">Gemini is a large language model used for various summarization tasks
Gemini is a series of language models referenced for their ability to perform summarization tasks
Gemini is a family of highly capable multimodal models</data>
      <data key="d2">0c932f7def033fa2b1bf210fbb771e7d,64476a39d7d8b87b399e3bd3cead79c7,ac21ebe9a9d70d691c717f961d3f10c8</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="ACHIAM ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Achiam et al. are authors referenced for their work on GPT in 2023</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="BROWN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Brown et al. are authors referenced for their work on GPT in 2020
Brown et al. are referenced in the context of the rise of language models with strong reasoning and general adaptability.
Brown et al. are authors who have contributed to the understanding of in-context learning in language models</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,93cb0d0456e0822b5fe30a3e627405f8,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="TOUVRON ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Touvron et al. are authors referenced for their work on Llama in 2023
Touvron et al. are referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="ANIL ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Anil et al. are authors referenced for their work on Gemini in 2023</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="KURATOV ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Kuratov et al. are authors referenced for their work on summarization and LLM context windows in 2024</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="LIU ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Liu et al. are authors referenced for their work on summarization and LLM context windows in 2023
Liu et al. are authors who have explored combining search algorithms with language model agents
Liu et al. are authors referenced in the context of balancing exploration and exploitation</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,6bdf681c0bd9e401ac72344a6a0ae479,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="RAG">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">RAG (Retrieval-Augmented Generation) is a technique mentioned as inadequate for query-focused summarization tasks
Retrieval-Augmented Generation, a method used to retrieve and add text chunks to the context window
RAG stands for Retrieval-Augmented Generation, a method used in query-focused summarization
RAG is a building block used in agent frameworks like LangChain
RAG (Retrieval-Augmented Generation) is a method used for improving the performance of models through retrieval and generation
RAG stands for Retrieval-Augmented Generation, a tool that can be used as a building block in ADAS
RAG (Retrieval-Augmented Generation) is one of the skills covered by the synthetic post-training dataset created by AgentInstruct
RAG (Retrieval Augmented Generation) is a skill that enhances the capacity of language models to generate informed, contextually precise responses
RAG (Retrieval-Augmented Generation) is a technique used to enhance model performance by incorporating retrieved documents into their responses</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,26b2dad01a219bc034ac7d6a32d07582,4884e8429ca1e567dadf5e22b4b68274,64476a39d7d8b87b399e3bd3cead79c7,6bdf681c0bd9e401ac72344a6a0ae479,8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2,ac21ebe9a9d70d691c717f961d3f10c8,b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="NEWMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Newman is referenced for their work on the modularity of graphs in 2006</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="LOUVAIN">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Louvain is a community detection algorithm used to partition graphs into modular communities</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="BLONDEL ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Blondel et al. are authors referenced for their work on the Louvain algorithm in 2008</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="TRAAG ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Traag et al. are authors referenced for their work on the Leiden algorithm in 2019</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="HOTPOTQA">
      <data key="d0">DATASET</data>
      <data key="d1">HotPotQA is a dataset used to evaluate entity extraction with varying chunk sizes
A benchmark dataset for open-domain question answering, targeting explicit fact retrieval.
HotPotQA is a benchmark used in the empirical evaluation of LATS to demonstrate its effectiveness in decision-making and reasoning.
HotPotQA is a dataset used to evaluate the performance of LATS, demonstrating its effectiveness in interactive question-answering tasks
A dataset used to evaluate reasoning-based prompting results
HotPotQA is a multi-hop question-answering benchmark that requires retrieval over two or more Wikipedia passages. It is used to evaluate reasoning and acting strategies in language models.
HotPotQA is a dataset used to evaluate the performance of internal reasoning and external retrieval strategies
HotPotQA is a task used to demonstrate the effect of each component of LATS, involving reasoning and question answering
HotPotQA is a dataset used to compare the cost and performance of different methods
HotpotQA is a dataset for diverse, explainable multi-hop question answering mentioned in the text
HotPotQA is a dataset used in experiments with the LATS algorithm
HotPotQA is a question-answering dataset that requires reasoning over multiple supporting documents to answer questions. It contains 113k Wikipedia-based question-answer pairs crafted by crowdworkers
HotPotQA is a dataset used to measure the performance of models in answering questions, with metrics such as Exact Match (EM) used for evaluation.
HotPotQA is a question answering task that involves interleaving Thought, Action, and Observation steps to solve questions</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,42de130f5b6144472a86a4c8260a87c7,48e423e2baf2ed485872756f5b4d87d8,4930fce6da868f894757a9da465807ba,594449768ae2dea9b2efbe677075096b,64476a39d7d8b87b399e3bd3cead79c7,8180bf20b7577f3eee40df5991e2886d,93cb0d0456e0822b5fe30a3e627405f8,99d90aededb61e04241516ed9ec656cc,b8dd0300033963bb4a3e1bad37f8e7b9,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="YANG ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Yang et al. are authors referenced for their work on the HotPotQA dataset in 2018
Yang et al. are referenced in the context of the HotPotQA benchmark used in the empirical evaluation of LATS.</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="GPT-4-TURBO">
      <data key="d0">MODEL</data>
      <data key="d1">GPT-4-Turbo is a version of OpenAI's language model used for entity extraction in the HotPotQA dataset
GPT-4-Turbo is a language model with a large context size of 128k tokens, used in the evaluation of context window sizes</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="MAP-REDUCE">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Map-Reduce is a technique used for query-focused summarization of an entire corpus
A method used in text summarization to shuffle and chunk source texts for summarization stages
Map-Reduce is a method used for global summarization of source texts</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,64476a39d7d8b87b399e3bd3cead79c7,ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="PODCAST TRANSCRIPTS">
      <data key="d0">DATASET</data>
      <data key="d1">Podcast transcripts are one of the real-world datasets used to generate activity-centered sense-making questions
A dataset comprising compiled transcripts of podcast conversations between Kevin Scott, Microsoft CTO, and other technology leaders, used for evaluation purposes.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba,64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="NEWS ARTICLES">
      <data key="d0">DATASET</data>
      <data key="d1">News articles are one of the real-world datasets used to generate activity-centered sense-making questions
A benchmark dataset comprising news articles published from September 2013 to December 2023 in various categories, used for evaluation purposes.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba,64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="NAIVE RAG">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Naive RAG is a baseline technique compared against global summarization approaches
Naive RAG is a baseline approach for summarizing and indexing data, which is outperformed by global approaches in comprehensiveness and diversity</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="GLOBAL MAP-REDUCE">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Global Map-Reduce is a summarization technique compared against Naive RAG</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="GRAPH RAG APPROACH">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Graph RAG Approach is a high-level data flow and pipeline for global summarization</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="TEXT CHUNKS">
      <data key="d0">DATA UNIT</data>
      <data key="d1">Text Chunks are segments of source documents used for processing in the Graph RAG approach</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="ELEMENT INSTANCES">
      <data key="d0">DATA UNIT</data>
      <data key="d1">Element Instances are graph nodes and edges extracted from text chunks
Element instances are the initial extracted descriptions of entities, relationships, and claims from source texts</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="NAMED ENTITIES">
      <data key="d0">DATA UNIT</data>
      <data key="d1">Named Entities are broad classes of entities like people, places, and organizations extracted from text
Named entities refer to specific categories like people, places, and organizations that are extracted from text</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,e66ed885a08f92cc69f4895302c33047</data>
      <data key="d3">DATA UNIT</data>
    </node>
    <node id="SUMMARIZATION TASKS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="LLM CONTEXT WINDOWS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="QUERY-FOCUSED SUMMARIZATION">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="GLOBAL SUMMARIZATION">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="GRAPH MODULARITY">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="ENTITY EXTRACTION">
      <data key="d0" />
      <data key="d1">
The process of extracting entities from text for graph indexing</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="SENSE-MAKING QUESTIONS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="KNOWLEDGE GRAPH">
      <data key="d0">DATA UNIT</data>
      <data key="d1">A Knowledge Graph is a structured representation of knowledge used in the Graph RAG approach
A knowledge graph is a structured representation of information, used in various advanced RAG systems for retrieval and reasoning</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="COMMUNITY DESCRIPTIONS">
      <data key="d0">DATA UNIT</data>
      <data key="d1">Community Descriptions are summaries of graph communities used in the Graph RAG approach</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="ACTIVITY-CENTERED SENSE-MAKING QUESTIONS">
      <data key="d0">DATA UNIT</data>
      <data key="d1">Activity-Centered Sense-Making Questions are generated from real-world datasets for evaluation</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="COMPREHENSIVENESS">
      <data key="d0">QUALITY</data>
      <data key="d1">Comprehensiveness is a target quality for evaluating summarization approaches
A metric evaluating how much detail an answer provides to cover all aspects and details of the question
Comprehensiveness is a metric used to evaluate the completeness of answers in summarization tasks
Comprehensiveness refers to the extent to which a system covers all relevant information, as indicated by Graph RAG's win rate in comprehensiveness</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,64476a39d7d8b87b399e3bd3cead79c7,edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="DIVERSITY">
      <data key="d0">QUALITY</data>
      <data key="d1">Diversity is a target quality for evaluating summarization approaches
A metric evaluating how varied and rich an answer is in providing different perspectives and insights on the question
Diversity is a metric used to evaluate the variety of answers in summarization tasks
Diversity refers to the variety of information or perspectives included in a summary, as indicated by Graph RAG's win rate in diversity
Diversity is one of the attributes that the agentic flows aim to ensure in the generated data and instructions.</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,64476a39d7d8b87b399e3bd3cead79c7,edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="EMPOWERMENT">
      <data key="d0">QUALITY</data>
      <data key="d1">Empowerment is a target quality for evaluating summarization approaches
A metric evaluating how well an answer helps the reader understand and make informed judgements about the topic
Empowerment is a metric used to evaluate the effectiveness of answers in summarization tasks
Empowerment refers to the ability of a system to help users reach an informed understanding, often by providing specific examples, quotes, and citations</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,64476a39d7d8b87b399e3bd3cead79c7,edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="SOURCE TEXTS">
      <data key="d0">DATA UNIT</data>
      <data key="d1">Source Texts are the original documents used for summarization in the Graph RAG approach
Source texts are the original texts used for summarization in the Graph RAG method</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="HIERARCHICAL LEVEL">
      <data key="d0">QUALITY</data>
      <data key="d1">Hierarchical Level is a variable in the evaluation of community summaries</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="TOKEN COSTS">
      <data key="d0">QUALITY</data>
      <data key="d1">Token Costs are a metric used to evaluate the efficiency of summarization approaches</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="GRAPH RAG PIPELINE">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Graph RAG Pipeline is the implementation of the Graph RAG approach</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="LLM PROMPTS">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">LLM Prompts are used to extract elements of a graph index from text chunks</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7</data>
    </node>
    <node id="FEW-SHOT EXAMPLES">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Few-Shot Examples are used in LLM prompts for in-context learning
Few-shot examples are specialized examples provided to the LLM to improve its performance in domains with specialized knowledge such as science, medicine, and law</data>
      <data key="d2">64476a39d7d8b87b399e3bd3cead79c7,e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="LLM">
      <data key="d0" />
      <data key="d1">
LLM (Large Language Model) is used for extracting descriptions of entities, relationships, and claims from source texts, and for creating summaries of these elements
A large language model (LLM) used to generate community summaries and global answers based on user queries.
Large Language Model used to generate questions and evaluate answers
A language model used to assess the comprehensiveness, diversity, empowerment, and directness of generated answers
LLM stands for Large Language Model, which is used in various RAG approaches to process and generate text based on retrieved information
A large language model (LLM) powers each agent and can optionally use tools such as search APIs, code interpreter, or a calculator.
A Large Language Model (LLM) is used to hypothesize other APIs present in the library during the Content Transformation Flow</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,427e98b00e49b6a8f8649054122dd45b,4930fce6da868f894757a9da465807ba,64476a39d7d8b87b399e3bd3cead79c7,c8e8019de153e439d6a79dcf209b943b,e66ed885a08f92cc69f4895302c33047,edab4014b8f55e5b25bd7f396314be1f,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="EVALUATION">
      <data key="d0" />
      <data key="d1">
The process of assessing the quality and effectiveness of generated questions and answers
Evaluation is the third operation in LATS where a scalar value is assigned to each new child node to quantify the agent&#8217;s progress in task completion
The process of assigning a scalar value to each new child node for selection and backpropagation
</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,26b2dad01a219bc034ac7d6a32d07582,64476a39d7d8b87b399e3bd3cead79c7,86f77e15d41cbd0cb33f635ccb2cb66b,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="COVARIATES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Covariates are additional variables or attributes associated with extracted node instances, such as claims linked to detected entities</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="LOGIT BIAS">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Logit bias is a technique used to force the LLM to make a yes/no decision during the entity extraction process</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="GLEANINGS">
      <data key="d0">PROCESS</data>
      <data key="d1">Gleanings refer to multiple rounds of extraction to encourage the LLM to detect any additional entities it may have missed in prior rounds</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="ELEMENT SUMMARIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Element summaries are single blocks of descriptive text for each graph element, created by further summarizing instance-level summaries</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="GRAPH ELEMENT">
      <data key="d0">CONCEPT</data>
      <data key="d1">Graph elements include entity nodes, relationship edges, and claim covariates in a graph structure</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="ENTITY GRAPH">
      <data key="d0">CONCEPT</data>
      <data key="d1">An entity graph is a graph structure where nodes represent entities and edges represent relationships between them</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="KNOWLEDGE GRAPHS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Knowledge graphs are graph structures that rely on concise and consistent knowledge triples (subject, predicate, object) for reasoning tasks</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="GRAPH COMMUNITIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Graph communities are groups of nodes in a graph that are more strongly connected to each other than to other nodes, detected using community detection algorithms
Different levels of graph communities used in the Graph RAG method to answer user queries</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="COMMUNITY SUMMARIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Community summaries are report-like summaries of each community in a graph, providing an understanding of the global structure and semantics of the dataset
Community summaries are generated descriptions of elements within a community, used to create a final answer in a multi-stage process based on hierarchical community structure.
Community summaries are summaries generated at different levels of a graph community hierarchy, used in the Graph RAG method
Community summaries are a type of self-memory used in Graph RAG for generation-augmented retrieval, facilitating future generation cycles
Community summaries are summaries of root-level communities in the entity-based graph index</data>
      <data key="d2">4930fce6da868f894757a9da465807ba,ac21ebe9a9d70d691c717f961d3f10c8,e66ed885a08f92cc69f4895302c33047,edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="MULTIHOP-RAG">
      <data key="d0">DATASET</data>
      <data key="d1">MultiHop-RAG is a dataset used in the context of graph indexing and community detection
A benchmark dataset used for evaluating open-domain question answering systems, including news articles in various categories.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba,e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="OPENORD">
      <data key="d0">TOOL</data>
      <data key="d1">OpenORD is a tool used for node layout in graph visualization</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="FORCE ATLAS 2">
      <data key="d0">TOOL</data>
      <data key="d1">Force Atlas 2 is a tool used for node layout in graph visualization</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="HIERARCHICAL CLUSTERING">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Hierarchical clustering is a technique used to reveal internal structure within root-level communities in a graph</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="ROOT COMMUNITIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Root communities are the top-level communities in a hierarchical clustering of a graph</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="LEAF-LEVEL COMMUNITIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Leaf-level communities are the bottom-level communities in a hierarchical clustering of a graph, prioritized for summarization
Leaf-level communities are the most granular divisions within a hierarchical community structure, where element summaries (nodes, edges, covariates) are prioritized and added to the LLM context window until the token limit is reached.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba,e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="TOKEN LIMIT">
      <data key="d0">CONCEPT</data>
      <data key="d1">Token limit refers to the maximum number of tokens that can be processed in a single context window by the LLM</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="BROWN ET AL., 2020">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A reference to the work by Brown et al. in 2020, which discusses examples provided to the LLM for in-context learning</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="SCIENCE">
      <data key="d0">DOMAIN</data>
      <data key="d1">A specialized domain that benefits from few-shot examples for LLM performance improvement
A domain tested by Meta Agent Search using the GPQA benchmark
Science is a domain where the knowledge in FMs is not sufficient to solve challenging questions
Science is a domain where models are evaluated on their ability to understand and interpret scientific information</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b,e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="MEDICINE">
      <data key="d0">DOMAIN</data>
      <data key="d1">A specialized domain that benefits from few-shot examples for LLM performance improvement</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="LAW">
      <data key="d0">DOMAIN</data>
      <data key="d1">A specialized domain that benefits from few-shot examples for LLM performance improvement</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="CLAIMS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Claims are statements linked to detected entities, including attributes like subject, object, type, description, source text span, and start and end dates</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="FIGURE 2">
      <data key="d0">VISUALIZATION</data>
      <data key="d1">A figure referenced in the text that shows the impact of using larger chunk sizes without a drop in quality or the forced introduction of noise
Figure 2 provides an overview of the six operations in LATS, depicting the sequence of operations performed</data>
      <data key="d2">c234cb83764b899335af0950677ad024,e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="FORTUNATO, 2010">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A survey by Fortunato in 2010, referenced in the context of community detection algorithms</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="JIN ET AL., 2021">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A survey by Jin et al. in 2021, referenced in the context of community detection algorithms</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="TRAAG ET AL., 2019">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A reference to the work by Traag et al. in 2019, which discusses the Leiden algorithm for community detection</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="TANG AND YANG, 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A reference to the work by Tang and Yang in 2024, which discusses the MultiHop-RAG dataset</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="MARTIN ET AL., 2011">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A reference to the work by Martin et al. in 2011, which discusses the OpenORD tool for node layout</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="JACOMY ET AL., 2014">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A reference to the work by Jacomy et al. in 2014, which discusses the Force Atlas 2 tool for node layout</data>
      <data key="d2">e66ed885a08f92cc69f4895302c33047</data>
    </node>
    <node id="ROOT-LEVEL COMMUNITIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Root-level communities refer to the primary divisions within a hierarchical community structure, revealing internal structure within these communities.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="HIGHER-LEVEL COMMUNITIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Higher-level communities are broader divisions within a hierarchical community structure, where sub-communities are ranked and summarized to fit within the context window if element summaries exceed the token limit.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="GLOBAL ANSWER">
      <data key="d0">CONCEPT</data>
      <data key="d1">The global answer is the final response generated from community summaries to answer a user query, ensuring a balance of summary detail and scope.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="USER QUERY">
      <data key="d0">CONCEPT</data>
      <data key="d1">A user query is a question posed by the user that the system aims to answer using community summaries and hierarchical community structure.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="TECH JOURNALIST">
      <data key="d0">USER</data>
      <data key="d1">A tech journalist is a potential user looking for insights and trends in the tech industry, particularly regarding the role of policy and regulation.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="EDUCATOR">
      <data key="d0">USER</data>
      <data key="d1">An educator is a potential user incorporating current affairs into curricula, particularly focusing on health and wellness.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="MT-BENCH">
      <data key="d0">DATASET</data>
      <data key="d1">A benchmark dataset for open-domain question answering, targeting explicit fact retrieval.
MT-Bench is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4
MT-Bench is a benchmark designed to assess the competence of chat assistants in multi-turn conversations using GPT-4 as the evaluator.
Mt-Bench is a project by Lm-sys published in 2023
MT-Bench is a benchmark that evaluates model responses to a first-turn and second-turn query, with GPT-4 providing scores from 1 to 10 for each turn.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba,3d1f6634f93f8a4c296dc8df7e59859e,4930fce6da868f894757a9da465807ba,86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="RAG SYSTEMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Retrieval-Augmented Generation (RAG) systems are used for generating answers to questions by retrieving relevant information from large datasets.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="DATA SENSEMAKING">
      <data key="d0">PROCESS</data>
      <data key="d1">Data sensemaking is the process through which people inspect, engage with, and contextualize data within the broader scope of real-world activities.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="KOESTEN ET AL.">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study by Koesten et al. (2021) on data sensemaking behaviors.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="XU AND LAPATA">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study by Xu and Lapata (2021) on methods for extracting latent summarization queries from source texts.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="KEVIN SCOTT">
      <data key="d0">PERSON</data>
      <data key="d1">Kevin Scott is the CTO of Microsoft and a participant in the podcast conversations included in the podcast transcripts dataset.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="TANG AND YANG">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study by Tang and Yang (2024) on the MultiHop-RAG benchmark dataset.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="TECH LEADERS">
      <data key="d0">PERSON</data>
      <data key="d1">Tech leaders are participants in the podcast conversations included in the podcast transcripts dataset, discussing various technology-related topics.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="PRIVACY LAWS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Privacy laws are regulations discussed by guests in the podcast transcripts dataset, focusing on their impact on technology development.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="INNOVATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Innovation is a topic discussed by guests in the podcast transcripts dataset, particularly in relation to ethical considerations.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="ETHICAL CONSIDERATIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Ethical considerations are discussed by guests in the podcast transcripts dataset, especially in the context of balancing innovation and ethics.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="POLICY AND REGULATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Policy and regulation are topics discussed in the podcast transcripts dataset, focusing on their role in the tech industry.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="COLLABORATIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Collaborations between tech companies and governments are discussed by guests in the podcast transcripts dataset.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="HEALTH EDUCATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Health education is a topic of interest for educators using news articles to teach about health and wellness.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="PREVENTIVE MEDICINE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Preventive medicine is a concept addressed in news articles, relevant to health education curricula.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="WELLNESS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Wellness is a concept addressed in news articles, relevant to health education curricula.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="PUBLIC HEALTH">
      <data key="d0">CONCEPT</data>
      <data key="d1">Public health priorities are insights gleaned from news coverage, relevant to health education.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="HEALTH LITERACY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Health literacy is an important aspect highlighted by educators using news articles to teach about health and wellness.</data>
      <data key="d2">4930fce6da868f894757a9da465807ba</data>
    </node>
    <node id="DATASET">
      <data key="d0">DATA/RESOURCE</data>
      <data key="d1">A collection of data used for analysis and evaluation in the study
Dataset refers to the collection of data used for evaluating Graph RAG</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="TEXT SUMMARIZATION (TS)">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">A method applying map-reduce approach directly to source texts for summarization</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="SEMANTIC SEARCH (SS)">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">A naive RAG approach where text chunks are retrieved and added to the context window until the token limit is reached</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="CONTEXT WINDOW">
      <data key="d0">CONCEPT/TECHNIQUE</data>
      <data key="d1">The size of the text window used for generating answers</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="PODCAST DATASET">
      <data key="d0">DATA/RESOURCE</data>
      <data key="d1">A dataset used in the study, indexed with a context window size of 600 tokens and 1 gleaning
The Podcast dataset consists of 8564 nodes and 20691 edges, used for evaluating summarization techniques</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="NEWS DATASET">
      <data key="d0">DATA/RESOURCE</data>
      <data key="d1">A dataset used in the study, indexed with a context window size of 600 tokens and 0 gleanings
The News dataset consists of 15754 nodes and 19520 edges, used for evaluating summarization techniques</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="DIRECTNESS">
      <data key="d0">METRIC</data>
      <data key="d1">A control metric evaluating how specifically and clearly an answer addresses the question
Directness is a metric used to evaluate the straightforwardness of answers in summarization tasks</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="LLM EVALUATOR">
      <data key="d0">MODEL/TOOL</data>
      <data key="d1">A Large Language Model used to assess the quality of answers based on specific metrics</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="PUBLIC FIGURES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Individuals repeatedly mentioned in various entertainment articles due to their significant contributions and influence</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="ACTIVITY-CENTERED APPROACH">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">An approach used to automate the generation of questions based on a short description of a dataset</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="QUESTION GENERATION">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of generating questions that require understanding of the entire corpus</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="C0">
      <data key="d0">CONCEPT</data>
      <data key="d1">Root-level community summaries used in Graph RAG to answer user queries
C0 is a condition representing root-level community summaries in the Graph RAG method</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="C1">
      <data key="d0">CONCEPT</data>
      <data key="d1">High-level community summaries used in Graph RAG to answer user queries
C1 is a condition representing intermediate-level community summaries in the Graph RAG method</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="C2">
      <data key="d0">CONCEPT</data>
      <data key="d1">Intermediate-level community summaries used in Graph RAG to answer user queries
C2 is a condition representing intermediate-level community summaries in the Graph RAG method</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="C3">
      <data key="d0">CONCEPT</data>
      <data key="d1">Low-level community summaries used in Graph RAG to answer user queries
C3 is a condition representing low-level community summaries in the Graph RAG method</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="RELATIONSHIP EXTRACTION">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of extracting relationships between entities from text for graph indexing</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="TOKEN">
      <data key="d0">CONCEPT</data>
      <data key="d1">A unit of text used in the context window for generating answers
Tokens are the basic elements of natural language, often words, used in language models</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="QUESTION">
      <data key="d0">CONCEPT</data>
      <data key="d1">A query provided to the LLM evaluator to assess the quality of answers
A query that needs to be answered, such as "Which magazine was started first Arthur&#8217;s Magazine or First for Women?"
The question provided to the student, which is used to evaluate their response
A problem or query presented to the student for which they need to provide an answer</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50,26b2dad01a219bc034ac7d6a32d07582,357f3442ba581c9d2bdf84d90509056f,5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="ANSWER">
      <data key="d0">CONCEPT</data>
      <data key="d1">A response generated by the LLM to address a given question
The answer generated by the cot_module
Answer is the output generated from the final code solution
The final solution to the original problem after integrating sub-solutions</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="ASSESSMENT">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of evaluating the quality of answers based on specific metrics</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="STOCHASTICITY">
      <data key="d0">CONCEPT</data>
      <data key="d1">The randomness in the behavior of LLMs during evaluation</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="MEAN SCORES">
      <data key="d0">CONCEPT</data>
      <data key="d1">The average scores used to account for the stochasticity of LLMs in evaluations</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="ENTERTAINMENT INDUSTRY">
      <data key="d0">CONCEPT</data>
      <data key="d1">A sector encompassing film, television, music, sports, and digital media</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="ACTORS AND DIRECTORS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Public figures in the entertainment industry known for their roles in film and television</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="MUSICIANS AND EXECUTIVES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Public figures in the entertainment industry known for their contributions to music and management</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="ATHLETES AND COACHES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Public figures in the entertainment industry known for their roles in sports</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="INFLUENCERS AND ENTREPRENEURS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Public figures in the entertainment industry known for their influence and business ventures</data>
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="GRAPH INDEXING">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">26b2dad01a219bc034ac7d6a32d07582</data>
    </node>
    <node id="DIRECTORS">
      <data key="d0">PROFESSION</data>
      <data key="d1">Individuals involved in directing films, television shows, or other media productions</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="PUBLIC FIGURES IN CONTROVERSY">
      <data key="d0">CATEGORY</data>
      <data key="d1">Individuals who are frequently mentioned in the media due to their involvement in controversies</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="MUSICIANS">
      <data key="d0">PROFESSION</data>
      <data key="d1">Individuals who create, perform, or produce music</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="EXECUTIVES">
      <data key="d0">PROFESSION</data>
      <data key="d1">Individuals in high-level management positions within organizations, particularly in the entertainment industry</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="ATHLETES">
      <data key="d0">PROFESSION</data>
      <data key="d1">Individuals who compete in sports at a professional level</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="COACHES">
      <data key="d0">PROFESSION</data>
      <data key="d1">Individuals who train and guide athletes or sports teams</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="INFLUENCERS">
      <data key="d0">PROFESSION</data>
      <data key="d1">Individuals who have the power to affect the purchasing decisions of others because of their authority, knowledge, position, or relationship with their audience</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="ENTREPRENEURS">
      <data key="d0">PROFESSION</data>
      <data key="d1">Individuals who start and run businesses, often within the entertainment and digital media sectors</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="TAYLOR SWIFT">
      <data key="d0">PERSON</data>
      <data key="d1">A musician and public figure frequently mentioned in entertainment articles for her professional achievements and personal life</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="TRAVIS KELCE">
      <data key="d0">PERSON</data>
      <data key="d1">An athlete and public figure frequently mentioned in entertainment articles for his professional achievements and personal life</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="BRITNEY SPEARS">
      <data key="d0">PERSON</data>
      <data key="d1">A musician and public figure frequently mentioned in entertainment articles for her professional achievements and personal life</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="JUSTIN TIMBERLAKE">
      <data key="d0">PERSON</data>
      <data key="d1">A musician and public figure frequently mentioned in entertainment articles for his professional achievements and personal life</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="NA&#207;VE RAG">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A method used to generate lists of public figures, focusing on a smaller number of individuals and their personal lives
Na&#239;ve RAG is a basic approach to retrieval-augmented generation that converts documents to text, splits them into chunks, and embeds these chunks into a vector space for context retrieval</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="ENTERTAINMENT ARTICLES">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Articles that cover various aspects of the entertainment industry, including news about public figures, trends, and cultural narratives</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="CULTURAL NARRATIVES">
      <data key="d0">CATEGORY</data>
      <data key="d1">Stories and themes that shape and reflect the values, beliefs, and experiences of a culture, often influenced by public figures in entertainment</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="FILM">
      <data key="d0">CATEGORY</data>
      <data key="d1">A sector of the entertainment industry focused on the production and distribution of movies</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="TELEVISION">
      <data key="d0">CATEGORY</data>
      <data key="d1">A sector of the entertainment industry focused on the production and distribution of TV shows</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="MUSIC">
      <data key="d0">CATEGORY</data>
      <data key="d1">A sector of the entertainment industry focused on the creation, performance, and distribution of music</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="DIGITAL MEDIA">
      <data key="d0">CATEGORY</data>
      <data key="d1">A sector of the entertainment industry that includes online content, social media, and other digital platforms</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="SPORTS">
      <data key="d0">CATEGORY</data>
      <data key="d1">A sector of the entertainment industry focused on athletic competitions and events</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="GAMING">
      <data key="d0">CATEGORY</data>
      <data key="d1">A sector of the entertainment industry focused on video games and interactive entertainment</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="SOCIAL DISCUSSIONS">
      <data key="d0">CATEGORY</data>
      <data key="d1">Public conversations and debates often influenced by the actions and statements of public figures</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="PUBLIC DISCOURSE">
      <data key="d0">CATEGORY</data>
      <data key="d1">The exchange of ideas and opinions in the public sphere, often shaped by media coverage and public figures</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="MEDIA COVERAGE">
      <data key="d0">CATEGORY</data>
      <data key="d1">The reporting and analysis of events, trends, and public figures by various media outlets</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="PROFESSIONAL ACHIEVEMENTS">
      <data key="d0">CATEGORY</data>
      <data key="d1">Notable accomplishments in one's career, often highlighted in media coverage of public figures</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="PERSONAL LIVES">
      <data key="d0">CATEGORY</data>
      <data key="d1">Aspects of public figures' private lives that are often covered by the media and of interest to the public</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="CULTURAL IMPACT">
      <data key="d0">CATEGORY</data>
      <data key="d1">The influence that public figures and their activities have on culture and society</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="ECONOMIC IMPACT">
      <data key="d0">CATEGORY</data>
      <data key="d1">The financial effects that public figures and their activities have on the economy</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="MEDIA REACTIONS">
      <data key="d0">CATEGORY</data>
      <data key="d1">The responses and opinions expressed by media outlets regarding public figures and their activities</data>
      <data key="d2">c8e8019de153e439d6a79dcf209b943b</data>
    </node>
    <node id="CONTEXT WINDOW SIZE">
      <data key="d0">PARAMETER</data>
      <data key="d1">Context window size refers to the number of tokens used in the context for language model tasks, tested at 8k, 16k, 32k, and 64k sizes</data>
      <data key="d2">ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="KURATOV ET AL., 2024">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study by Kuratov et al. in 2024, discussing the potential for information to be lost in the middle of longer contexts</data>
      <data key="d2">ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="LIU ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study by Liu et al. in 2023, discussing the potential for information to be lost in the middle of longer contexts
A paper discussing DyLAN and its application in using FMs to score the response quality of nodes in each layer to prune the connections</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="TS">
      <data key="d0">CONDITION</data>
      <data key="d1">TS is a condition representing global text summarization without a graph index in the Graph RAG method</data>
      <data key="d2">ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="GPT-4">
      <data key="d0">MODEL</data>
      <data key="d1">GPT-4 is a version of OpenAI's language model used to evaluate InfoBench
GPT-4 is a version of OpenAI's language model used in the experimental evaluation of LATS, achieving state-of-the-art pass@1 accuracy for programming on HumanEval.
GPT-4 is a version of OpenAI's language model used with LATS to achieve a 92.7 Pass@1 rate on HumanEval
GPT-4 is a more advanced version of OpenAI's language model, used to evaluate the performance of different prompting methods, including LATS and Reflexion.
GPT-4 is a version of OpenAI's language model used to set the state of the art for HumanEval when combined with LATS
GPT-4 is a language model developed by OpenAI, as described in the technical report
GPT-4 is a version of OpenAI's language model used for evaluating performance on tasks such as HumanEval.
GPT-4 is a version of OpenAI's language model used to evaluate the transferability of discovered agents from GPT-3.5
GPT-4 is a version of OpenAI's language model used by the meta agent in Meta Agent Search
GPT-4 (OpenAI, 2024) is a language model used by the meta agent in Meta Agent Search
GPT-4 is a version of OpenAI's language model used to evaluate the performance of agents discovered by Meta Agent Search
GPT-4 is a language model mentioned in the technical report by Ryan Greenblatt
GPT-4 is a powerful language model used to generate responses to prompts for creating synthetic data
GPT-4 is a powerful language model used by AgentInstruct to generate high-quality data
The model used as a baseline for scoring the performance of other models on the Orca-Bench dataset
GPT-4 is a language model used as a benchmark with a score of 10 in the evaluation of other models on the Orca-Bench dataset

GPT-4 is a model used as a benchmark for evaluating the performance of other models like Orca-3
GPT-4 is a language model used as an evaluator for summarization and hallucination detection
GPT-4 is a version of OpenAI's language model used in the evaluation of MIRAGE datasets
GPT-4 is used for extracting the option selected by the model from the model&#8217;s response in multiple choice questions
A version of OpenAI's language model used to extract exact answers and match them with ground-truth answers in exact match/span extraction problems
GPT-4 is a version of OpenAI's language model used in various benchmarks to evaluate model responses.</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a,0cf2e43f324fa4175b9b00b90e5e90ba,103d98395c393552cc954c89d4e59f50,1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,2d4672dfb7bd4283f0b5f23ab4f26653,5819b66e04fd77fa705574edc49395bb,6109537356a2ce2339f77c827aa3668e,6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,8ee9617c145e19fa95f1f9349bfbe69b,93cb0d0456e0822b5fe30a3e627405f8,99d90aededb61e04241516ed9ec656cc,ab04427ae0415a1c812a35cf8d3ee1a2,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bc26e68b0b2783ba912b9e5606d9eb0b,bd4eb9459bc29b4c2da4658914fd4635,ede7063998065122cf7a7152979c1909,f8e7ed806916bf15245bcb4d52570c26,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="HALLUCINATION JUDGE">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A process where a judge determines if there is any hallucination in a generated summary
Hallucination Judge is a process where a judge determines if there is any hallucination in a generated summary, using a specific prompt template.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="KOESTEN, L.">
      <data key="d0">PERSON</data>
      <data key="d1">Koesten, L. is an author of the paper "Talking datasets&#8211;understanding data sensemaking behaviours"
Koesten, L. is
Koesten, L. is an author of the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb,df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="GREGORY, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Gregory, K. is an author of the paper "Talking datasets&#8211;understanding data sensemaking behaviours"
Gregory, K. is an author of the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="GROTH, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Groth, P. is an author of the paper "Talking datasets&#8211;understanding data sensemaking behaviours"
Groth, P. is an author of the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="SIMPERL, E.">
      <data key="d0">PERSON</data>
      <data key="d1">Simperl, E. is an author of the paper "Talking datasets&#8211;understanding data sensemaking behaviours"
Simperl, E. is an author of the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="INTERNATIONAL JOURNAL OF HUMAN-COMPUTER STUDIES">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Talking datasets&#8211;understanding data sensemaking behaviours" was published
The journal where the paper "Talking datasets&#8211;understanding data sensemaking behaviours" was published</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="KURATOV, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Kuratov, Y. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"
Kuratov, Y. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="BULATOV, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Bulatov, A. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"
Bulatov, A. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="ANOKHIN, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Anokhin, P. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"
Anokhin, P. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="SOROKIN, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Sorokin, D. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"
Sorokin, D. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="SOROKIN, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Sorokin, A. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"
Sorokin, A. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="BURTSEV, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Burtsev, M. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"
Burtsev, M. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="LANGCHAIN">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">LangChain is the organization behind the LangChain graphs project
LangChain is a library that supports various graph databases and graph-based RAG applications
LangChain is the organization behind the LangChain graphs project
LangChain is an open-source agent framework that can be used to build upon existing building blocks like RAG and search engine tools
LangChain is an existing agent framework that can provide functions for ADAS
Langchain is a tool for building context-aware reasoning applications developed by LangChainAI</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,6109537356a2ce2339f77c827aa3668e,6bdf681c0bd9e401ac72344a6a0ae479,df50c95dff7da074cbb2f68e88686f88,edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="LASKAR, M. T. R.">
      <data key="d0">PERSON</data>
      <data key="d1">Laskar, M. T. R. is an author of the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"
Laskar, M. T. R. is an author of the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="HOQUE, E.">
      <data key="d0">PERSON</data>
      <data key="d1">Hoque, E. is an author of the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"
Hoque, E. is an author of the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="HUANG, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Huang, J. is an author of the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"
Huang, J. is an author of the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="ADVANCES IN ARTIFICIAL INTELLIGENCE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The conference where the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models" was presented
The conference where the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models" was presented</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </node>
    <node id="GENERATION-AUGMENTED RETRIEVAL (GAR)">
      <data key="d0">METHOD</data>
      <data key="d1">GAR is a method that combines retrieval and generation processes to enhance the performance of information retrieval systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="MODULAR RAG">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">Modular RAG includes patterns for iterative and dynamic cycles of interleaved retrieval and generation to improve upon Na&#239;ve RAG</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="SELF-MEMORY (SELFMEM)">
      <data key="d0">CONCEPT</data>
      <data key="d1">Self-memory is a concept used in generation-augmented retrieval to store and utilize past generated content for future retrieval and generation cycles</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="ITERATIVE RETRIEVAL-GENERATION (ITER-RETGEN)">
      <data key="d0">METHOD</data>
      <data key="d1">Iter-RetGen is a strategy for iterative retrieval and generation, used in advanced RAG systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="FEDERATED RETRIEVAL-GENERATION (FEB4RAG)">
      <data key="d0">METHOD</data>
      <data key="d1">FeB4RAG is a federated strategy for retrieval and generation, used in advanced RAG systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="MULTI-DOCUMENT SUMMARIZATION">
      <data key="d0">TASK</data>
      <data key="d1">Multi-document summarization is the process of creating a summary from multiple documents, often using advanced RAG systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="MULTI-HOP QUESTION ANSWERING">
      <data key="d0">TASK</data>
      <data key="d1">Multi-hop question answering involves answering questions that require reasoning over multiple pieces of information, often using advanced RAG systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="HIERARCHICAL INDEX">
      <data key="d0">TOOL/STRUCTURE</data>
      <data key="d1">A hierarchical index is a structure used to organize text chunks by clustering their vector embeddings, facilitating efficient retrieval and summarization</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="LLAMAINDEX">
      <data key="d0">SOFTWARE/LIBRARY</data>
      <data key="d1">LlamaIndex is a library that supports various graph databases and graph-based RAG applications
LlamaIndex is the organization behind the LlamaIndex Knowledge Graph Index project</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="NEO4J">
      <data key="d0">SOFTWARE/TOOL</data>
      <data key="d1">Neo4J is a graph database supported by LangChain and LlamaIndex for creating and reasoning over knowledge graphs
Neo4J is a format used for creating and reasoning over knowledge graphs
Neo4J is the organization behind the project "Project NaLLM"</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8,df50c95dff7da074cbb2f68e88686f88,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="NEBULAGRAPH">
      <data key="d0">SOFTWARE/TOOL</data>
      <data key="d1">NebulaGraph is a graph database supported by LangChain and LlamaIndex for creating and reasoning over knowledge graphs
NebulaGraph is the organization behind the project "Nebulagraph launches industry-first graph rag: Retrieval-augmented generation with llm based on knowledge graphs"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="SENSEMAKING">
      <data key="d0">ACTIVITY</data>
      <data key="d1">Sensemaking is the activity of understanding and making sense of information, often facilitated by iterative question answering and summarization methods like Graph RAG</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="SCALABILITY">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Scalability refers to the ability of a system to handle increasing amounts of work or data efficiently, as demonstrated by Graph RAG's reduced context token requirements</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="TUNING ELEMENT EXTRACTION PROMPTS">
      <data key="d0">METHOD</data>
      <data key="d1">Tuning element extraction prompts is a method to improve the retention of specific details in the Graph RAG index</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="RAPTOR">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">RAPTOR is a method that generates a hierarchical index of text chunks by clustering their vector embeddings</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="TREE OF CLARIFICATIONS">
      <data key="d0">TOOL/STRUCTURE</data>
      <data key="d1">A tree of clarifications is a structure used to answer multiple interpretations of ambiguous questions</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="KAPING">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">KAPING is an advanced RAG method where the index is a knowledge graph</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="G-RETRIEVER">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">G-Retriever is a method where subsets of the graph structure are the objects of enquiry</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="GRAPH-TOOLFORMER">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">Graph-ToolFormer is a method where derived graph metrics are the objects of enquiry</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="SURGE">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">SURGE is a method where narrative outputs are strongly grounded in the facts of retrieved subgraphs</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="FABULA">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">FABULA is a method where retrieved event-plot subgraphs are serialized using narrative templates</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="NALLM">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">NaLLM is a system that can create and reason over knowledge graphs in Neo4J format
NaLLM is a system that can create and reason over knowledge graphs in Neo4J format</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="GRAPHRAG">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">GraphRAG is a system that can create and reason over knowledge graphs in NebulaGraph format
GraphRAG is a system that can create and reason over knowledge graphs in Nebula-Graph format</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="CAIRE-COVID">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">CAiRE-COVID is a system that combines various concepts for multi-document summarization</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="ITRG">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">ITRG is a system for multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="IR-COT">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">IR-CoT is a system for multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="DSP">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">DSP is a system for multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="TRAJANOSKA ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on using LLMs for knowledge graph creation and completion</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="YAO ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on using LLMs for knowledge graph completion</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="BAN ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on using LLMs for the extraction of causal graphs from source texts</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="ZHANG ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on using LLMs for the extraction of causal graphs from source texts</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="BAEK ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on advanced RAG methods where the index is a knowledge graph</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="HE ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on advanced RAG methods where subsets of the graph structure are the objects of enquiry</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="ZHANG, 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on advanced RAG methods where derived graph metrics are the objects of enquiry</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="KANG ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on advanced RAG methods where narrative outputs are strongly grounded in the facts of retrieved subgraphs</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="RANADE AND JOSHI, 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on advanced RAG methods where retrieved event-plot subgraphs are serialized using narrative templates</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="WANG ET AL., 2023B">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on advanced RAG methods that support both creation and traversal of text-relationship graphs for multi-hop question answering
A reference to the authors who introduced the Self-Consistency with Chain-of-Thought (COT-SC) technique
A publication that discusses the COT-SC method
A publication by Wang et al. in 2023 related to COT-SC
Wang et al., 2023b is a publication referenced for the Self-Consistency with Chain-of-Thought (COT-SC) method</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="NEO4J, 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on systems that can create and reason over knowledge graphs in Neo4J format</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="NEBULAGRAPH, 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on systems that can create and reason over knowledge graphs in NebulaGraph format</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="RAM ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on RAG approaches and systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="GAO ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on Na&#239;ve RAG and advanced RAG systems
A publication that discusses the GSM-Hard dataset</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="CHENG ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on self-memory for generation-augmented retrieval</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="MAO ET AL., 2020">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on generation-augmented retrieval</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="SHAO ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on iterative retrieval-generation strategies</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="WANG ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on federated retrieval-generation strategies
A publication that discusses the effectiveness of Meta Agent Search in various domains</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="SU ET AL., 2020">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on multi-document summarization</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="FENG ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="TRIVEDI ET AL., 2022">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="KHATTAB ET AL., 2022">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="SARTHI ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on generating a hierarchical index of text chunks by clustering their vector embeddings</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="KIM ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A study on generating a tree of clarifications to answer multiple interpretations of ambiguous questions</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="PODCAST INTERMEDIATE-LEVEL SUMMARIES">
      <data key="d0">OUTPUT/RESULT</data>
      <data key="d1">Podcast intermediate-level summaries are summaries generated for podcasts at an intermediate level of detail</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="NEWS LOW-LEVEL COMMUNITY SUMMARIES">
      <data key="d0">OUTPUT/RESULT</data>
      <data key="d1">News low-level community summaries are summaries generated for news articles at a low level of detail</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="TABLE 3">
      <data key="d0">DATA/RESULT</data>
      <data key="d1">Table 3 illustrates the scalability advantages of Graph RAG compared to source text summarization</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="ROOT-LEVEL COMMUNITY SUMMARIES">
      <data key="d0">OUTPUT/RESULT</data>
      <data key="d1">Root-level community summaries are summaries generated at the root level of detail</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="ITERATIVE QUESTION ANSWERING">
      <data key="d0">TASK</data>
      <data key="d1">Iterative question answering is a process characterized by repeated cycles of asking and answering questions to facilitate sensemaking</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="AD-HOC LLM USE">
      <data key="d0">METHOD</data>
      <data key="d1">Ad-hoc LLM use refers to the spontaneous use of large language models to analyze reasoning and provide specific examples, quotes, and citations</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="RAG APPROACHES AND SYSTEMS">
      <data key="d0">CONCEPT</data>
      <data key="d1">RAG approaches and systems involve retrieving relevant information from external data sources and adding it to the context window of an LLM along with the original query</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="PRE-RETRIEVAL STRATEGIES">
      <data key="d0">METHOD</data>
      <data key="d1">Pre-retrieval strategies are techniques used before the retrieval process to enhance the performance of RAG systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="POST-RETRIEVAL STRATEGIES">
      <data key="d0">METHOD</data>
      <data key="d1">Post-retrieval strategies are techniques used after the retrieval process to enhance the performance of RAG systems</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="COMMUNITY ANSWERS">
      <data key="d0">OUTPUT/RESULT</data>
      <data key="d1">Community answers are generated from community summaries in a parallel generation process</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="MULTI-HOP QUESTION ANSWERING (ITRG)">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">ITRG is a system for multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="MULTI-HOP QUESTION ANSWERING (IR-COT)">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">IR-CoT is a system for multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="MULTI-HOP QUESTION ANSWERING (DSP)">
      <data key="d0">TOOL/METHOD</data>
      <data key="d1">DSP is a system for multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="HIERARCHICAL INDEX OF TEXT CHUNKS">
      <data key="d0">TOOL/STRUCTURE</data>
      <data key="d1">A hierarchical index of text chunks is a structure used to organize text chunks by clustering their vector embeddings</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="TEXT-RELATIONSHIP GRAPHS">
      <data key="d0">TOOL/STRUCTURE</data>
      <data key="d1">Text-relationship graphs are structures that support both creation and traversal for multi-hop question answering</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="GRAPH DATABASES">
      <data key="d0">TOOL/STRUCTURE</data>
      <data key="d1">Graph databases are databases that use graph structures for semantic queries, supported by LangChain and LlamaIndex</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="GRAPH-BASED RAG APPLICATIONS">
      <data key="d0">TOOL/STRUCTURE</data>
      <data key="d1">Graph-based RAG applications are systems that use graph structures for retrieval-augmented generation
Graph-based RAG applications are systems that create and reason over knowledge graphs in various formats</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8,edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="SENSEMAKING ACTIVITY">
      <data key="d0">ACTIVITY</data>
      <data key="d1">Sensemaking activity involves understanding and making sense of information, often facilitated by iterative question answering and summarization methods like Graph RAG</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="QUOTES">
      <data key="d0">DATA/RESULT</data>
      <data key="d1">Quotes are specific excerpts from texts used to support reasoning and understanding in LLM analyses</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="CITATIONS">
      <data key="d0">DATA/RESULT</data>
      <data key="d1">Citations are references to sources used to support reasoning and understanding in LLM analyses</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="EXAMPLES">
      <data key="d0">DATA/RESULT</data>
      <data key="d1">Examples are specific instances used to illustrate points and support reasoning in LLM analyses</data>
      <data key="d2">edab4014b8f55e5b25bd7f396314be1f</data>
    </node>
    <node id="INDEX">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Index is a publication mentioned in the context of libraries and graph-based RAG applications</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="NEBULA-GRAPH">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Nebula-Graph is another format used for creating and reasoning over knowledge graphs</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="SELF-CHECKGPT">
      <data key="d0">TOOL</data>
      <data key="d1">SelfCheckGPT is a tool used for comparing fabrication rates in analysis</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="GRAPH INDEX">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Graph index is a method used in Graph RAG for building a data index that supports global summarization</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="QFS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">QFS stands for Query-Focused Summarization, a method used to support human sensemaking over text corpora</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="ALONSO GUEVARA FERN&#193;NDEZ">
      <data key="d0">PERSON</data>
      <data key="d1">Alonso Guevara Fern&#225;ndez is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="AMBER HOAK">
      <data key="d0">PERSON</data>
      <data key="d1">Amber Hoak is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="ANDR&#201;S MORALES ESQUIVEL">
      <data key="d0">PERSON</data>
      <data key="d1">Andr&#233;s Morales Esquivel is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="BEN CUTLER">
      <data key="d0">PERSON</data>
      <data key="d1">Ben Cutler is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="BILLIE RINALDI">
      <data key="d0">PERSON</data>
      <data key="d1">Billie Rinaldi is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="CHRIS SANCHEZ">
      <data key="d0">PERSON</data>
      <data key="d1">Chris Sanchez is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="CHRIS TREVINO">
      <data key="d0">PERSON</data>
      <data key="d1">Chris Trevino is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="CHRISTINE CAGGIANO">
      <data key="d0">PERSON</data>
      <data key="d1">Christine Caggiano is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="DAVID TITTSWORTH">
      <data key="d0">PERSON</data>
      <data key="d1">David Tittsworth is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="DAYENNE DE SOUZA">
      <data key="d0">PERSON</data>
      <data key="d1">Dayenne de Souza is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="DOUGLAS ORBAKER">
      <data key="d0">PERSON</data>
      <data key="d1">Douglas Orbaker is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="ED CLARK">
      <data key="d0">PERSON</data>
      <data key="d1">Ed Clark is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="GABRIEL NIEVES-PONCE">
      <data key="d0">PERSON</data>
      <data key="d1">Gabriel Nieves-Ponce is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="GAUDY BLANCO MENESES">
      <data key="d0">PERSON</data>
      <data key="d1">Gaudy Blanco Meneses is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="KATE LYTVYNETS">
      <data key="d0">PERSON</data>
      <data key="d1">Kate Lytvynets is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="KATY SMITH">
      <data key="d0">PERSON</data>
      <data key="d1">Katy Smith is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="M&#211;NICA CARVAJAL">
      <data key="d0">PERSON</data>
      <data key="d1">M&#243;nica Carvajal is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="NATHAN EVANS">
      <data key="d0">PERSON</data>
      <data key="d1">Nathan Evans is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="RICHARD ORTEGA">
      <data key="d0">PERSON</data>
      <data key="d1">Richard Ortega is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="RODRIGO RACANICCI">
      <data key="d0">PERSON</data>
      <data key="d1">Rodrigo Racanicci is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="SARAH SMITH">
      <data key="d0">PERSON</data>
      <data key="d1">Sarah Smith is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="SHANE SOLOMON">
      <data key="d0">PERSON</data>
      <data key="d1">Shane Solomon is a contributor to the work on Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="GPT-4 TECHNICAL REPORT">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The GPT-4 technical report is a document detailing the capabilities and features of GPT-4
GPT-4 Technical Report is a document published by OpenAI in 2024
GPT-4 technical report is a paper published in 2023 by OpenAI</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,3d1f6634f93f8a4c296dc8df7e59859e,ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="KNOWLEDGE-AUGMENTED LANGUAGE MODEL PROMPTING">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">A method for zero-shot knowledge graph question answering</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="QUERY TOOLS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Tools used for querying data and discovering causal relationships</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="QUERY FOCUSED ABSTRACTIVE SUMMARIZATION">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">A method that incorporates query relevance, multi-document coverage, and summary length constraints into seq2seq models</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="FAST UNFOLDING OF COMMUNITIES">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">A method for detecting communities in large networks</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="SENSEMAKING QUESTIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Sensemaking questions are a class of questions used to evaluate the performance of Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="CORPORA">
      <data key="d0">DATA</data>
      <data key="d1">Corpora refer to collections of text data used in the evaluation of Graph RAG</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="USER QUERIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">User queries are questions or requests made by users to retrieve information from the graph index</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="COMMUNITY HIERARCHY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Community hierarchy refers to the hierarchical structure of communities in the graph index</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="MAP-REDUCE SUMMARIZATION">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Map-Reduce summarization is a method used for global summarization of source texts</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="PYTHON">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Python is the programming language used for the implementation of Graph RAG approaches
Python is a programming language used in datasets like MBPP for writing short functions.
Python is a Turing Complete programming language used in ADAS research</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba,ac21ebe9a9d70d691c717f961d3f10c8,fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="ARXIV">
      <data key="d0">PUBLICATION</data>
      <data key="d1">arXiv is a repository where the GPT-4 technical report and other related papers are published
The platform where the paper "summary length constraints into seq2seq models" was published
The repository where the paper "Fabula: Intelligence report generation using retrieval-augmented narrative construction" was publishedThe repository where the paper "Lost in the middle: How language models use long contexts" was publishedThe repository where the paper "Raptor: Recursive abstractive processing for tree-organized retrieval" was published
arXiv is the platform where the paper "Evaluating large language models trained on code" was published in 2021
arXiv is the platform where the paper "Mastering diverse domains through world models" was published
arXiv is the repository where the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm" was published
arXiv is a repository where the paper on Automated Design of Agentic Systems is published
The platform where the paper "Metagpt: Meta programming for multi-agent collaborative framework" was publishedThe platform where the paper "Discovering preference optimization algorithms with and for large language models" was publishedThe platform where the paper "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization" was published
arXiv is the platform where the paper "Learning to reinforcement learn" was publishedarXiv is the platform where the paper "Tool learning with large language models: A survey" was publishedarXiv is the platform where the paper "Voyager: An open-ended embodied agent with large language models" was publishedarXiv is the platform where the paper "The prompt report: A systematic survey of prompting techniques" was published
arXiv is the platform where the paper "Learning to reinforcement learn" was published
The platform where the paper "A survey on the memory mechanism of large language model based agents" was published</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e,68e5573b596d253a03047b1e41988598,7a48515e86161237c03c9a8373197126,aa79049289e6532592eec17b9e76adfb,ac21ebe9a9d70d691c717f961d3f10c8,c3d0436082aada237ee4bee645f16059,cc802d9b841fde55e9c0c2ba0ef7869d,df50c95dff7da074cbb2f68e88686f88</data>
    </node>
    <node id="J. ACHIAM">
      <data key="d0">PERSON</data>
      <data key="d1">J. Achiam is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="S. ADLER">
      <data key="d0">PERSON</data>
      <data key="d1">S. Adler is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="S. AGARWAL">
      <data key="d0">PERSON</data>
      <data key="d1">S. Agarwal is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="L. AHMAD">
      <data key="d0">PERSON</data>
      <data key="d1">L. Ahmad is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="I. AKKAYA">
      <data key="d0">PERSON</data>
      <data key="d1">I. Akkaya is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="F. L. ALEMAN">
      <data key="d0">PERSON</data>
      <data key="d1">F. L. Aleman is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="D. ALMEIDA">
      <data key="d0">PERSON</data>
      <data key="d1">D. Almeida is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="J. ALTENSCHMIDT">
      <data key="d0">PERSON</data>
      <data key="d1">J. Altenschmidt is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="S. ALTMAN">
      <data key="d0">PERSON</data>
      <data key="d1">S. Altman is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="S. ANADKAT">
      <data key="d0">PERSON</data>
      <data key="d1">S. Anadkat is an author of the GPT-4 technical report</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="R. ANIL">
      <data key="d0">PERSON</data>
      <data key="d1">R. Anil is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="S. BORGEAUD">
      <data key="d0">PERSON</data>
      <data key="d1">S. Borgeaud is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="Y. WU">
      <data key="d0">PERSON</data>
      <data key="d1">Y. Wu is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="J.-B. ALAYRAC">
      <data key="d0">PERSON</data>
      <data key="d1">J.-B. Alayrac is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="J. YU">
      <data key="d0">PERSON</data>
      <data key="d1">J. Yu is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="R. SORICUT">
      <data key="d0">PERSON</data>
      <data key="d1">R. Soricut is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="J. SCHALKWYK">
      <data key="d0">PERSON</data>
      <data key="d1">J. Schalkwyk is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="A. M. DAI">
      <data key="d0">PERSON</data>
      <data key="d1">A. M. Dai is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="A. HAUTH">
      <data key="d0">PERSON</data>
      <data key="d1">A. Hauth is an author of the Gemini paper</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="J. BAEK">
      <data key="d0">PERSON</data>
      <data key="d1">J. Baek is an author of the paper on knowledge-augmented language model prompting</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="A. F. AJI">
      <data key="d0">PERSON</data>
      <data key="d1">A. F. Aji is an author of the paper on knowledge-augmented language model prompting</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="A. SAFFARI">
      <data key="d0">PERSON</data>
      <data key="d1">A. Saffari is an author of the paper on knowledge-augmented language model prompting</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="T. BAN">
      <data key="d0">PERSON</data>
      <data key="d1">T. Ban is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="L. CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">L. Chen is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="X. WANG">
      <data key="d0">PERSON</data>
      <data key="d1">X. Wang is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="H. CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">H. Chen is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="T. BAUMEL">
      <data key="d0">PERSON</data>
      <data key="d1">T. Baumel is an author of the paper on query focused abstractive summarization</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="M. EYAL">
      <data key="d0">PERSON</data>
      <data key="d1">M. Eyal is an author of the paper on query focused abstractive summarization</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="M. ELHADAD">
      <data key="d0">PERSON</data>
      <data key="d1">M. Elhadad is an author of the paper on query focused abstractive summarization</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="V. D. BLONDEL">
      <data key="d0">PERSON</data>
      <data key="d1">V. D. Blondel is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="J.-L. GUILLAUME">
      <data key="d0">PERSON</data>
      <data key="d1">J.-L. Guillaume is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="R. LAMBIOTTE">
      <data key="d0">PERSON</data>
      <data key="d1">R. Lambiotte is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="E. LEFEBVRE">
      <data key="d0">PERSON</data>
      <data key="d1">E. Lefebvre is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d2">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </node>
    <node id="BLONDEL, V. D.">
      <data key="d0">PERSON</data>
      <data key="d1">Blondel, V. D. is an author of the paper "Fast unfolding of communities in large networks"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
    </node>
    <node id="GUILLAUME, J.-L.">
      <data key="d0">PERSON</data>
      <data key="d1">Guillaume, J.-L. is an author of the paper "Fast unfolding of communities in large networks"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
    </node>
    <node id="LAMBIOTTE, R.">
      <data key="d0">PERSON</data>
      <data key="d1">Lambiotte, R. is an author of the paper "Fast unfolding of communities in large networks"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
    </node>
    <node id="LEFEBVRE, E.">
      <data key="d0">PERSON</data>
      <data key="d1">Lefebvre, E. is an author of the paper "Fast unfolding of communities in large networks"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
    </node>
    <node id="JOURNAL OF STATISTICAL MECHANICS: THEORY AND EXPERIMENT">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Fast unfolding of communities in large networks" was published</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
    </node>
    <node id="BROWN, T.">
      <data key="d0">PERSON</data>
      <data key="d1">Brown, T. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MANN, B.">
      <data key="d0">PERSON</data>
      <data key="d1">Mann, B. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RYDER, N.">
      <data key="d0">PERSON</data>
      <data key="d1">Ryder, N. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SUBBIAH, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Subbiah, M. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KAPLAN, J. D.">
      <data key="d0">PERSON</data>
      <data key="d1">Kaplan, J. D. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DHARIWAL, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Dhariwal, P. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NEELAKANTAN, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Neelakantan, A. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHYAM, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Shyam, P. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SASTRY, G.">
      <data key="d0">PERSON</data>
      <data key="d1">Sastry, G. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ASKELL, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Askell, A. is an author of the paper "Language models are few-shot learners"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The conference where the paper "Language models are few-shot learners" was presented
The conference where the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks" was presented
The conference where the paper "Judging llm-as-a-judge with mt-bench and chatbot arena" was presented
The conference where the paper "Imagenet classification with deep convolutional neural networks" was presentedThe conference where the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks" was presentedThe conference where the paper "Thought Cloning: Learning to think while acting by imitating human thinking" was presented
The conference where the paper "Self-refine: Iterative refinement with self-feedback" was presented
The conference where the paper "Reflexion: Language agents with verbal reinforcement learning" was presentedThe conference where the paper "Direct preference optimization: Your language model is secretly a reward model" was presented
Advances in Neural Information Processing Systems is the journal where the paper "Chain-of-thought prompting elicits reasoning in large language models" was published</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e,aa79049289e6532592eec17b9e76adfb,d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="CHENG, X.">
      <data key="d0">PERSON</data>
      <data key="d1">Cheng, X. is an author of the paper "Lift yourself up: Retrieval-augmented text generation with self-memory"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LUO, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Luo, D. is an author of the paper "Lift yourself up: Retrieval-augmented text generation with self-memory"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHEN, X.">
      <data key="d0">PERSON</data>
      <data key="d1">Chen, X. is an author of the paper "Lift yourself up: Retrieval-augmented text generation with self-memory"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIU, L.">
      <data key="d0">PERSON</data>
      <data key="d1">Liu, L. is an author of the paper "Lift yourself up: Retrieval-augmented text generation with self-memory"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHAO, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhao, D. is an author of the paper "Lift yourself up: Retrieval-augmented text generation with self-memory"Zhao, D. is an author of the paper "Retrieval-generation synergy augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YAN, R.">
      <data key="d0">PERSON</data>
      <data key="d1">Yan, R. is an author of the paper "Lift yourself up: Retrieval-augmented text generation with self-memory"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DANG, H. T.">
      <data key="d0">PERSON</data>
      <data key="d1">Dang, H. T. is an author of the paper "Duc 2005: Evaluation of question-focused summarization systems"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PROCEEDINGS OF THE WORKSHOP ON TASK-FOCUSED SUMMARIZATION AND QUESTION ANSWERING">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The workshop where the paper "Duc 2005: Evaluation of question-focused summarization systems" was presented</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="ES, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Es, S. is an author of the paper "Ragas: Automated evaluation of retrieval augmented generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAMES, J.">
      <data key="d0">PERSON</data>
      <data key="d1">James, J. is an author of the paper "Ragas: Automated evaluation of retrieval augmented generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ESPINOSA-ANKE, L.">
      <data key="d0">PERSON</data>
      <data key="d1">Espinosa-Anke, L. is an author of the paper "Ragas: Automated evaluation of retrieval augmented generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SCHOCKAERT, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Schockaert, S. is an author of the paper "Ragas: Automated evaluation of retrieval augmented generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FENG, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Feng, Z. is an author of the paper "Retrieval-generation synergy augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FENG, X.">
      <data key="d0">PERSON</data>
      <data key="d1">Feng, X. is an author of the paper "Retrieval-generation synergy augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YANG, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Yang, M. is an author of the paper "Retrieval-generation synergy augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QIN, B.">
      <data key="d0">PERSON</data>
      <data key="d1">Qin, B. is an author of the paper "Retrieval-generation synergy augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FORTUNATO, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Fortunato, S. is an author of the paper "Community detection in graphs"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PHYSICS REPORTS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Community detection in graphs" was published</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="GAO, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Gao, Y. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIONG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Xiong, Y. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GAO, X.">
      <data key="d0">PERSON</data>
      <data key="d1">Gao, X. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIA, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Jia, K. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PAN, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Pan, J. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BI, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Bi, Y. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAI, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Dai, Y. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SUN, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Sun, J. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WANG, H.">
      <data key="d0">PERSON</data>
      <data key="d1">Wang, H. is an author of the paper "Retrieval-augmented generation for large language models: A survey"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GOODWIN, T. R.">
      <data key="d0">PERSON</data>
      <data key="d1">Goodwin, T. R. is an author of the paper "Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAVERY, M. E.">
      <data key="d0">PERSON</data>
      <data key="d1">Savery, M. E. is an author of the paper "Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DEMNER-FUSHMAN, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Demner-Fushman, D. is an author of the paper "Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PROCEEDINGS OF COLING. INTERNATIONAL CONFERENCE ON COMPUTATIONAL LINGUISTICS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The conference where the paper "Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization" was presented</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="HE, X.">
      <data key="d0">PERSON</data>
      <data key="d1">He, X. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TIAN, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Tian, Y. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SUN, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Sun, Y. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHAWLA, N. V.">
      <data key="d0">PERSON</data>
      <data key="d1">Chawla, N. V. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LAURENT, T.">
      <data key="d0">PERSON</data>
      <data key="d1">Laurent, T. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LECUN, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">LeCun, Y. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BRESSON, X.">
      <data key="d0">PERSON</data>
      <data key="d1">Bresson, X. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HOOI, B.">
      <data key="d0">PERSON</data>
      <data key="d1">Hooi, B. is an author of the paper "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JACOMY, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Jacomy, M. is an author of the paper "Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VENTURINI, T.">
      <data key="d0">PERSON</data>
      <data key="d1">Venturini, T. is an author of the paper "Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HEYMANN, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Heymann, S. is an author of the paper "Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BASTIAN, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Bastian, M. is an author of the paper "Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PLOS ONE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software" was published</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="JIN, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Jin, D. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YU, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Yu, Z. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIAO, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Jiao, P. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PAN, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Pan, S. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HE, D.">
      <data key="d0">PERSON</data>
      <data key="d1">He, D. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WU, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Wu, J. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PHILIP, S. Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Philip, S. Y. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHANG, W.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhang, W. is an author of the paper "A survey of community detection approaches: From statistical modeling to deep learning"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "A survey of community detection approaches: From statistical modeling to deep learning" was published</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="KANG, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Kang, M. is an author of the paper "Knowledge graph-augmented language models for knowledge-grounded dialogue generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KWAK, J. M.">
      <data key="d0">PERSON</data>
      <data key="d1">Kwak, J. M. is an author of the paper "Knowledge graph-augmented language models for knowledge-grounded dialogue generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BAEK, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Baek, J. is an author of the paper "Knowledge graph-augmented language models for knowledge-grounded dialogue generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HWANG, S. J.">
      <data key="d0">PERSON</data>
      <data key="d1">Hwang, S. J. is an author of the paper "Knowledge graph-augmented language models for knowledge-grounded dialogue generation"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KHATTAB, O.">
      <data key="d0">PERSON</data>
      <data key="d1">Khattab, O. is an author of the paper "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SANTHANAM, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Santhanam, K. is an author of the paper "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LI, X. L.">
      <data key="d0">PERSON</data>
      <data key="d1">Li, X. L. is an author of the paper "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HALL, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Hall, D. is an author of the paper "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIANG, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Liang, P. is an author of the paper "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"
Liang, P. is an author of the paper "Lost in the middle: How language models use long contexts"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="POTTS, C.">
      <data key="d0">PERSON</data>
      <data key="d1">Potts, C. is an author of the paper "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZAHARIA, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Zaharia, M. is an author of the paper "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KIM, G.">
      <data key="d0">PERSON</data>
      <data key="d1">Kim, G. is an author of the paper "Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KIM, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Kim, S. is an author of the paper "Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JEON, B.">
      <data key="d0">PERSON</data>
      <data key="d1">Jeon, B. is an author of the paper "Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PARK, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Park, J. is an author of the paper "Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KANG, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Kang, J. is an author of the paper "Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KLEIN, G.">
      <data key="d0">PERSON</data>
      <data key="d1">Klein, G. is an author of the paper "Making sense of sensemaking 1: Alternative perspectives"Klein, G. isKlein, G. is an author of the paper "Making sense of sensemaking 2: A macrocognitive model"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MOON, B.">
      <data key="d0">PERSON</data>
      <data key="d1">Moon, B. is an author of the paper "Making sense of sensemaking 1: Alternative perspectives"Moon, B. is an author of the paper "Making sense of sensemaking 2: A macrocognitive model"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HOFFMAN, R. R.">
      <data key="d0">PERSON</data>
      <data key="d1">Hoffman, R. R. is an author of the paper "Making sense of sensemaking 2: A macrocognitive model"Hoffman, R. R. is an author of the paper "Making sense of sensemaking 1: Alternative perspectives"</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IEEE INTELLIGENT SYSTEMS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Making sense of sensemaking 1: Alternative perspectives" was publishedThe journal where the paper "Making sense of sensemaking 2: A macrocognitive model" was published</data>
      <data key="d2">aa79049289e6532592eec17b9e76adfb</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="COMPUTATIONAL LINGUISTICS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Domain adaptation with pre-trained transformers for query-focused abstractive text summarization" was published</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
    </node>
    <node id="LEWIS, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Lewis, P. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PEREZ, E.">
      <data key="d0">PERSON</data>
      <data key="d1">Perez, E. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PIKTUS, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Piktus, A. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PETRONI, F.">
      <data key="d0">PERSON</data>
      <data key="d1">Petroni, F. is an author of the paper "Lost in the middle: How language models use long contexts"Petroni, F. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KARPUKHIN, V.">
      <data key="d0">PERSON</data>
      <data key="d1">Karpukhin, V. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GOYAL, N.">
      <data key="d0">PERSON</data>
      <data key="d1">Goyal, N. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="K&#220;TTLER, H.">
      <data key="d0">PERSON</data>
      <data key="d1">K&#252;ttler, H. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LEWIS, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Lewis, M. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIH, W.-T.">
      <data key="d0">PERSON</data>
      <data key="d1">Yih, W.-T. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ROCKT&#196;SCHEL, T.">
      <data key="d0">PERSON</data>
      <data key="d1">Rockt&#228;schel, T. is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIU, N. F.">
      <data key="d0">PERSON</data>
      <data key="d1">Liu, N. F. is an author of the paper "Lost in the middle: How language models use long contexts"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIN, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Lin, K. is an author of the paper "Lost in the middle: How language models use long contexts"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HEWITT, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Hewitt, J. is an author of the paper "Lost in the middle: How language models use long contexts"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PARANJAPE, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Paranjape, A. is an author of the paper "Lost in the middle: How language models use long contexts"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BEVILACQUA, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Bevilacqua, M. is an author of the paper "Lost in the middle: How language models use long contexts"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIU, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Liu, Y. is an author of the paper "Hierarchical transformers for multi-document summarization"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LAPATA, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Lapata, M. is an author of the paper "Hierarchical transformers for multi-document summarization"
Lapata, M. is an author of the paper "Text summarization with latent queries"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MANAKUL, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Manakul, P. is an author of the paper "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIUSIE, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Liusie, A. is an author of the paper "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GALES, M. J.">
      <data key="d0">PERSON</data>
      <data key="d1">Gales, M. J. is an author of the paper "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MAO, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Mao, Y. is an author of the paper "Generation-augmented retrieval for open-domain question answering"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HE, P.">
      <data key="d0">PERSON</data>
      <data key="d1">He, P. is an author of the paper "Generation-augmented retrieval for open-domain question answering"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIU, X.">
      <data key="d0">PERSON</data>
      <data key="d1">Liu, X. is an author of the paper "Generation-augmented retrieval for open-domain question answering"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHEN, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Shen, Y. is an author of the paper "Generation-augmented retrieval for open-domain question answering"Shen, Y. is an author of the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GAO, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Gao, J. is an author of the paper "Generation-augmented retrieval for open-domain question answering"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HAN, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Han, J. is an author of the paper "Generation-augmented retrieval for open-domain question answering"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHEN, W.">
      <data key="d0">PERSON</data>
      <data key="d1">Chen, W. is an author of the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"Chen, W. is an author of the paper "Generation-augmented retrieval for open-domain question answering"
Chen, W. is an author of the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SPIE CONFERENCE ON VISUALIZATION AND DATA ANALYSIS (VDA)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The conference where the paper "Openord: An open-source toolbox for large graph layout" was presented</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="MARTIN, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Martin, S. is an author of the paper "Openord: An open-source toolbox for large graph layout"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BROWN, W. M.">
      <data key="d0">PERSON</data>
      <data key="d1">Brown, W. M. is an author of the paper "Openord: An open-source toolbox for large graph layout"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KLAVANS, R.">
      <data key="d0">PERSON</data>
      <data key="d1">Klavans, R. is an author of the paper "Openord: An open-source toolbox for large graph layout"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BOYACK, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Boyack, K. is an author of the paper "Openord: An open-source toolbox for large graph layout"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NEWMAN, M. E.">
      <data key="d0">PERSON</data>
      <data key="d1">Newman, M. E. is an author of the paper "Modularity and community structure in networks"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Modularity and community structure in networks" was published</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">JOURNAL</data>
    </node>
    <node id="RAM, O.">
      <data key="d0">PERSON</data>
      <data key="d1">Ram, O. is an author of the paper "In-context retrieval-augmented language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LEVINE, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Levine, Y. is an author of the paper "In-context retrieval-augmented language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DALMEDIGOS, I.">
      <data key="d0">PERSON</data>
      <data key="d1">Dalmedigos, I. is an author of the paper "In-context retrieval-augmented language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MUHLGAY, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Muhlgay, D. is an author of the paper "In-context retrieval-augmented language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHASHUA, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Shashua, A. is an author of the paper "In-context retrieval-augmented language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LEYTON-BROWN, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Leyton-Brown, K. is an author of the paper "In-context retrieval-augmented language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHOHAM, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Shoham, Y. is an author of the paper "In-context retrieval-augmented language models"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TRANSACTIONS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "In-context retrieval-augmented language models" was published</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">JOURNAL</data>
    </node>
    <node id="RANADE, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Ranade, P. is an author of the paper "Fabula: Intelligence report generation using retrieval-augmented narrative construction"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOSHI, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Joshi, A. is an author of the paper "Fabula: Intelligence report generation using retrieval-augmented narrative construction"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SARTHI, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Sarthi, P. is an author of the paper "Raptor: Recursive abstractive processing for tree-organized retrieval"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ABDULLAH, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Abdullah, S. is an author of the paper "Raptor: Recursive abstractive processing for tree-organized retrieval"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TULI, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Tuli, A. is an author of the paper "Raptor: Recursive abstractive processing for tree-organized retrieval"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KHANNA, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Khanna, S. is an author of the paper "Raptor: Recursive abstractive processing for tree-organized retrieval"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GOLDIE, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Goldie, A. is an author of the paper "Raptor: Recursive abstractive processing for tree-organized retrieval"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MANNING, C. D.">
      <data key="d0">PERSON</data>
      <data key="d1">Manning, C. D. is an author of the paper "Raptor: Recursive abstractive processing for tree-organized retrieval"
Manning, C. D. is an author of the paper "HotpotQA: A dataset for diverse, explainable multi-hop question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SCOTT, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Scott, K. is the host of the podcast "Behind the Tech"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHAO, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Shao, Z. is an author of the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GONG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Gong, Y. is an author of the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HUANG, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Huang, M. is an author of the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DUAN, N.">
      <data key="d0">PERSON</data>
      <data key="d1">Duan, N. is an author of the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"
Duan, N. is an author of the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SU, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Su, D. is an author of the paper "Caire-covid: A question answering and query-focused multi-document summarization"
Su, D. is an author of the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XU, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Xu, Y. is an author of the paper "Caire-covid: A question answering and query-focused multi-document summarization"
Xu, Y. is an author of the paper "Text summarization with latent queries"Xu, Y. is an author of the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YU, T.">
      <data key="d0">PERSON</data>
      <data key="d1">Yu, T. is an author of the paper "Caire-covid: A question answering and query-focused multi-document summarization"
Yu, T. is an author of the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SIDDIQUE, F. B.">
      <data key="d0">PERSON</data>
      <data key="d1">Siddique, F. B. is an author of the paper "Caire-covid: A question answering and query-focused multi-document summarization"
Siddique, F. B. is an author of the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BAREZI, E. J.">
      <data key="d0">PERSON</data>
      <data key="d1">Barezi, E. J. is an author of the paper "Caire-covid: A question answering and query-focused multi-document summarization"
Barezi, E. J. is an author of the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FUNG, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Fung, P. is an author of the paper "Caire-covid: A question answering and query-focused multi-document summarization"
Fung, P. is an author of the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3,df50c95dff7da074cbb2f68e88686f88</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CANADIAN AI 2020">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models" was presented</data>
      <data key="d2">df50c95dff7da074cbb2f68e88686f88</data>
    </node>
    <node id="SPRINGER">
      <data key="d0">PUBLISHER</data>
      <data key="d1">The publisher of the proceedings for the 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020
Springer is the publisher of the book "Why greatness cannot be planned: The myth of the objective"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,df50c95dff7da074cbb2f68e88686f88</data>
    </node>
    <node id="TANG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Tang, Y. is an author of the paper "MultiHop-RAG: Benchmarking retrieval-augmented generation for multi-hop queries"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YANG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Yang, Y. is an author of the paper "MultiHop-RAG: Benchmarking retrieval-augmented generation for multi-hop queries"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TOUVRON, H.">
      <data key="d0">PERSON</data>
      <data key="d1">Touvron, H. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MARTIN, L.">
      <data key="d0">PERSON</data>
      <data key="d1">Martin, L. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="STONE, K.">
      <data key="d0">PERSON</data>
      <data key="d1">Stone, K. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALBERT, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Albert, P. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALMAHAIRI, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Almahairi, A. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BABAEI, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Babaei, Y. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BASHLYKOV, N.">
      <data key="d0">PERSON</data>
      <data key="d1">Bashlykov, N. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BATRA, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Batra, S. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BHARGAVA, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Bhargava, P. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BHOSALE, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Bhosale, S. is an author of the paper "Llama 2: Open foundation and fine-tuned chat models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TRAAG, V. A.">
      <data key="d0">PERSON</data>
      <data key="d1">Traag, V. A. is an author of the paper "From Louvain to Leiden: guaranteeing well-connected communities"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WALTMAN, L.">
      <data key="d0">PERSON</data>
      <data key="d1">Waltman, L. is an author of the paper "From Louvain to Leiden: guaranteeing well-connected communities"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VAN ECK, N. J.">
      <data key="d0">PERSON</data>
      <data key="d1">Van Eck, N. J. is an author of the paper "From Louvain to Leiden: guaranteeing well-connected communities"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TRAJANOSKA, M.">
      <data key="d0">PERSON</data>
      <data key="d1">Trajanoska, M. is an author of the paper "Enhancing knowledge graph construction using large language models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="STOJANOV, R.">
      <data key="d0">PERSON</data>
      <data key="d1">Stojanov, R. is an author of the paper "Enhancing knowledge graph construction using large language models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TRAJANOV, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Trajanov, D. is an author of the paper "Enhancing knowledge graph construction using large language models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TRIVEDI, H.">
      <data key="d0">PERSON</data>
      <data key="d1">Trivedi, H. is an author of the paper "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BALASUBRAMANIAN, N.">
      <data key="d0">PERSON</data>
      <data key="d1">Balasubramanian, N. is an author of the paper "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KHOT, T.">
      <data key="d0">PERSON</data>
      <data key="d1">Khot, T. is an author of the paper "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SABHARWAL, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Sabharwal, A. is an author of the paper "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WANG, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Wang, J. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIANG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Liang, Y. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MENG, F.">
      <data key="d0">PERSON</data>
      <data key="d1">Meng, F. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SUN, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Sun, Z. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHI, H.">
      <data key="d0">PERSON</data>
      <data key="d1">Shi, H. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LI, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Li, Z. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"Li, Z. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XU, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Xu, J. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QU, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Qu, J. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHOU, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhou, J. is an author of the paper "Is chatgpt a good nlg evaluator? a preliminary study"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WANG, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Wang, S. is an author of the paper "Feb4rag: Evaluating federated search in the context of retrieval augmented generation"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KHRAMTSOVA, E.">
      <data key="d0">PERSON</data>
      <data key="d1">Khramtsova, E. is an author of the paper "Feb4rag: Evaluating federated search in the context of retrieval augmented generation"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHUANG, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhuang, S. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"Zhuang, S. is an author of the paper "Feb4rag: Evaluating federated search in the context of retrieval augmented generation"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZUCCON, G.">
      <data key="d0">PERSON</data>
      <data key="d1">Zuccon, G. is an author of the paper "Feb4rag: Evaluating federated search in the context of retrieval augmented generation"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WANG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Wang, Y. is an author of the paper "Knowledge graph prompting for multi-document question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIPKA, N.">
      <data key="d0">PERSON</data>
      <data key="d1">Lipka, N. is an author of the paper "Knowledge graph prompting for multi-document question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ROSSI, R. A.">
      <data key="d0">PERSON</data>
      <data key="d1">Rossi, R. A. is an author of the paper "Knowledge graph prompting for multi-document question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SIU, A.">
      <data key="d0">PERSON</data>
      <data key="d1">Siu, A. is an author of the paper "Knowledge graph prompting for multi-document question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHANG, R.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhang, R. is an author of the paper "Knowledge graph prompting for multi-document question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DERR, T.">
      <data key="d0">PERSON</data>
      <data key="d1">Derr, T. is an author of the paper "Knowledge graph prompting for multi-document question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YANG, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Yang, Z. is an author of the paper "HotpotQA: A dataset for diverse, explainable multi-hop question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QI, P.">
      <data key="d0">PERSON</data>
      <data key="d1">Qi, P. is an author of the paper "HotpotQA: A dataset for diverse, explainable multi-hop question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHANG, S.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhang, S. is an author of the paper "HotpotQA: A dataset for diverse, explainable multi-hop question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BENGIO, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Bengio, Y. is an author of the paper "HotpotQA: A dataset for diverse, explainable multi-hop question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="COHEN, W. W.">
      <data key="d0">PERSON</data>
      <data key="d1">Cohen, W. W. is an author of the paper "HotpotQA: A dataset for diverse, explainable multi-hop question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SALAKHUTDINOV, R.">
      <data key="d0">PERSON</data>
      <data key="d1">Salakhutdinov, R. is an author of the paper "HotpotQA: A dataset for diverse, explainable multi-hop question answering"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YAO, J.-G.">
      <data key="d0">PERSON</data>
      <data key="d1">Yao, J.-g. is an author of the paper "Recent advances in document summarization"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WAN, X.">
      <data key="d0">PERSON</data>
      <data key="d1">Wan, X. is an author of the paper "Recent advances in document summarization"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIAO, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Xiao, J. is an author of the paper "Recent advances in document summarization"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YAO, L.">
      <data key="d0">PERSON</data>
      <data key="d1">Yao, L. is an author of the paper "Causal graph discovery with retrieval-augmented generation based large language models"Yao, L. is an author of the paper "Exploring large language models for knowledge graph completion"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PENG, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Peng, J. is an author of the paper "Exploring large language models for knowledge graph completion"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MAO, C.">
      <data key="d0">PERSON</data>
      <data key="d1">Mao, C. is an author of the paper "Exploring large language models for knowledge graph completion"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LUO, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Luo, Y. is an author of the paper "Exploring large language models for knowledge graph completion"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHANG, J.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhang, J. is an author of the paper "Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHANG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhang, Y. is an author of the paper "Causal graph discovery with retrieval-augmented generation based large language models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GAN, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Gan, Y. is an author of the paper "Causal graph discovery with retrieval-augmented generation based large language models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WANG, C.">
      <data key="d0">PERSON</data>
      <data key="d1">Wang, C. is an author of the paper "Causal graph discovery with retrieval-augmented generation based large language models"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHENG, L.">
      <data key="d0">PERSON</data>
      <data key="d1">Zheng, L. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHIANG, W.-L.">
      <data key="d0">PERSON</data>
      <data key="d1">Chiang, W.-L. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHENG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Sheng, Y. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WU, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Wu, Z. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHUANG, Y.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhuang, Y. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIN, Z.">
      <data key="d0">PERSON</data>
      <data key="d1">Lin, Z. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LI, D.">
      <data key="d0">PERSON</data>
      <data key="d1">Li, D. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XING, E.">
      <data key="d0">PERSON</data>
      <data key="d1">Xing, E. is an author of the paper "Judging llm-as-a-judge with mt-bench and chatbot arena"</data>
      <data key="d2">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LANGUAGE AGENT TREE SEARCH (LATS)">
      <data key="d0">FRAMEWORK/TECHNOLOGY</data>
      <data key="d1">Language Agent Tree Search (LATS) is a general framework that integrates the capabilities of language models (LMs) in reasoning, acting, and planning by leveraging Monte Carlo Tree Search and LM-powered value functions and self-reflections for proficient exploration and enhanced decision-making.
Language Agent Tree Search (LATS) is a method that unifies reasoning, acting, and planning in language models, addressing the shortcomings of previous techniques like CoT, ToT, and ReAct.
Language Agent Tree Search (LATS) is an algorithm that unifies reasoning, acting, and planning in language models</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,93cb0d0456e0822b5fe30a3e627405f8,9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="ANDY ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Andy Zhou is one of the authors of the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" and is affiliated with the University of Illinois Urbana-Champaign and Lapis Labs.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="KAI YAN">
      <data key="d0">PERSON</data>
      <data key="d1">Kai Yan is one of the authors of the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" and is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="MICHAL SHLAPENTOKH-ROTHMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Michal Shlapentokh-Rothman is one of the authors of the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" and is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="HAOHAN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Haohan Wang is one of the authors of the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" and is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="YU-XIONG WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yu-Xiong Wang is one of the authors of the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" and is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">The University of Illinois Urbana-Champaign is an educational institution where several authors of the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" are affiliated.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="LAPIS LABS">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Lapis Labs is an organization affiliated with Andy Zhou, one of the authors of the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models".</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="GPT-3.5">
      <data key="d0">MODEL</data>
      <data key="d1">GPT-3.5 is a version of OpenAI's language model used in the experimental evaluation of LATS, demonstrating gradient-free performance comparable to gradient-based fine-tuning for web navigation on WebShop.
GPT-3.5 is a version of OpenAI's language model used in conjunction with LATS to achieve high performance on WebShop
A version of OpenAI's language model used in the experiments
GPT-3.5 is a version of OpenAI's language model used in various experiments to evaluate different prompting methods, including LATS, ReAct, Reflexion, and others.
GPT-3.5 is a version of OpenAI's language model used to evaluate the performance of various prompting methods on MBPP
GPT-3.5 is a version of OpenAI's language model used in WebShop for acting-based prompting methods and evaluated on 50 instructions
GPT-3.5 is a version of OpenAI's language model used for evaluating performance on tasks such as HumanEval.
GPT-3.5 is a version of OpenAI's language model used in experiments for the Game of 24
GPT-3.5 is a version of OpenAI's language model used as a baseline for transferring discovered agents to GPT-4
GPT-3.5 is a version of OpenAI's language model used to evaluate discovered agents and baselines in Meta Agent Search
GPT-3.5 (OpenAI, 2022) is a language model used to evaluate the discovered agents and baselines in Meta Agent Search
GPT-3.5 is a version of OpenAI's language model used to evaluate the performance of agents discovered by Meta Agent Search
A version of OpenAI's language model used in the study to evaluate agentic systems
GPT-3.5 is another AI model that Orca-3 outperforms on multiple benchmarks</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,594449768ae2dea9b2efbe677075096b,7de66b94cf868b37b1df51dc545c415f,93cb0d0456e0822b5fe30a3e627405f8,99d90aededb61e04241516ed9ec656cc,b88745a13b69cecbc0ee9c3af41389bf,b8dd0300033963bb4a3e1bad37f8e7b9,bc26e68b0b2783ba912b9e5606d9eb0b,f8e7ed806916bf15245bcb4d52570c26,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="MONTE CARLO TREE SEARCH (MCTS)">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Monte Carlo Tree Search (MCTS) is an algorithm integrated into LATS to enable language models as agents, enhancing their decision-making capabilities.
Monte Carlo Tree Search is an algorithm used in model-based reinforcement learning and adapted in LATS for better exploration and decision-making
Monte Carlo Tree Search (MCTS) is a heuristic search algorithm used in many decision-making environments. It builds a decision tree where every node is a state and every edge is an action. MCTS runs for multiple episodes, expanding the tree by exploring multiple children states and selecting the best ones based on the UCT value.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8,9bb90746134619cad9a3e649b8b35f24,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="REACT">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">ReAct is a prompting technique that augments language models with feedback or observations from an external environment, which LATS expands upon to improve reasoning and decision-making.
ReAct is a prompting technique for language models that LATS outperforms, particularly in interactive environments
ReAct is an acting-based prompting technique that has seen success but is limited by its simplicity and inability to adapt to environment conditions
ReAct extends language models to tasks where the mapping from input to output is enhanced by or requires interactions with an external environment, such as a game or API. It constructs an action space that adds permissible actions to the reasoning traces from CoT, using observations from the environment to improve reasoning and acting.
ReAct is a framework used in LATS where the action space consists of permissible actions and reasoning traces
ReAct is a prompting method used in language models to enhance reasoning and acting capabilities. It is evaluated in various experiments, including those involving HotPotQA.
ReAct is a prompting method that combines reasoning and acting to improve task performance
ReAct is a prompting method used with GPT-3.5 in WebShop, sampling 30 trajectories and providing competitive performance to imitation learning and reinforcement learning techniques
ReAct is a method evaluated in the study, with different configurations for performance and token consumption
ReAct is a method for synergizing reasoning and acting in language models mentioned in the text
ReAct is a simpler prompting method compared to LATS
ReAct is an algorithm compared with LATS in the HotPotQA experiments</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,48e423e2baf2ed485872756f5b4d87d8,594449768ae2dea9b2efbe677075096b,8180bf20b7577f3eee40df5991e2886d,93cb0d0456e0822b5fe30a3e627405f8,99d90aededb61e04241516ed9ec656cc,9bb90746134619cad9a3e649b8b35f24,c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="HUMANEVAL">
      <data key="d0">BENCHMARK</data>
      <data key="d1">HumanEval is a benchmark used to evaluate the programming capabilities of language models, where LATS achieved state-of-the-art pass@1 accuracy with GPT-4.
HumanEval is a dataset where LATS, when used with GPT-4, set the state of the art with a 92.7 Pass@1 rate
HumanEval is a benchmark used to evaluate the Pass@1 accuracy of language models like GPT-3.5 and GPT-4. It involves tasks that require reasoning and acting capabilities.
HumanEval is a dataset used to measure the correctness of synthesized programs in Python from natural language docstrings
HumanEval is a dataset used in experiments with the LATS algorithm
HumanEval is a dataset used for evaluating the performance of models, with a maximum of k=8 trajectories and a sampling size of n=5
HumanEval is a dataset of 164 handwritten programming problems used to evaluate the functional correctness of models for synthesizing programs from natural language descriptions.</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,48e423e2baf2ed485872756f5b4d87d8,93cb0d0456e0822b5fe30a3e627405f8,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="WEBSHOP">
      <data key="d0">BENCHMARK</data>
      <data key="d1">WebShop is a benchmark used to evaluate web navigation capabilities of language models, where LATS demonstrated gradient-free performance with GPT-3.5.
WebShop is a dataset used to evaluate LATS, where it significantly improved performance with GPT-3.5
WebShop is a benchmark used to evaluate reasoning and acting strategies in language models. It is referenced in the context of LATS evaluation.
WebShop is a dataset used to evaluate the performance of various methods on tasks involving reasoning, acting, and planning
WebShop is an online shopping environment composed of a website with 1.18M real-world products and 12k human instructions, where agents must navigate a website through various commands to purchase an item matching a user specification
WebShop is a tool for scalable real-world web interaction with grounded language agents mentioned in the text
WebShop is an environment used in experiments with the LATS algorithm
WebShop is an interactive web-based environment designed to evaluate agents on grounded language understanding and decision-making, simulating an e-commerce shopping task.
WebShop is a platform with an action space that includes various actions like searching, selecting products, and purchasing items
An online platform where users can search for and purchase products</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,594449768ae2dea9b2efbe677075096b,785ad59c6a37896a4676ec5c1689735f,8180bf20b7577f3eee40df5991e2886d,93cb0d0456e0822b5fe30a3e627405f8,99d90aededb61e04241516ed9ec656cc,b8dd0300033963bb4a3e1bad37f8e7b9,f8e7ed806916bf15245bcb4d52570c26,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="PROCEEDINGS OF THE 41ST INTERNATIONAL CONFERENCE ON MACHINE LEARNING">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The conference where the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" was presented.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="PMLR 235">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The volume of the Proceedings of the 41st International Conference on Machine Learning where the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" was published.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="2024">
      <data key="d0">YEAR</data>
      <data key="d1">The year when the paper "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models" was published.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="GITHUB">
      <data key="d0">PLATFORM</data>
      <data key="d1">GitHub is the platform where the code for Language Agent Tree Search (LATS) can be found.
GitHub is the platform where all code, prompts, and experiment results related to the Meta Agent Search algorithm are available
GitHub is the platform where the AutoGPT project is hosted
The platform where the full framework code is available
GitHub is the platform where the repository containing all agents from the experiment is hosted</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b,34d0bb2211fc795fe1096442e086a2b3,449db721e37968e073e3579b59e023b2,93cb0d0456e0822b5fe30a3e627405f8,d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="HTTPS://GITHUB.COM/LAPISROCKS/LANGUAGEAGENTTREESEARCH">
      <data key="d0">URL</data>
      <data key="d1">The URL where the code for Language Agent Tree Search (LATS) is available on GitHub.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="WOOLDRIDGE">
      <data key="d0">PERSON</data>
      <data key="d1">Wooldridge is referenced in the context of general autonomous agents capable of reasoning and decision-making in a variety of environments.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="JENNINGS">
      <data key="d0">PERSON</data>
      <data key="d1">Jennings is referenced in the context of general autonomous agents capable of reasoning and decision-making in a variety of environments.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="CHOWDHERY ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Chowdhery et al. are referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="OPENAI">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">OpenAI is referenced in the context of the rise of language models with strong reasoning and general adaptability.
OpenAI is the organization behind the GPT-4 technical report
OpenAI is the organization that developed the GPT Foundation Model
OpenAI is the organization behind the GPT-3.5 and GPT-4 models
OpenAI is the organization behind ChatGPT and Simple Evals
The organization behind the GPT-4o-2024-05-13 and GPT-3.5-turbo-0125 models
OpenAI is the organization behind the development of the GPT-4o-2024-05-13 and GPT-3.5-turbo-0125 models
OpenAI is the organization behind the development of GPT-4o-2024-05-13 and GPT-3.5-turbo-0125
OpenAI is the organization behind the GPT-4 technical report</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1b1399c76420a477c0c97893d258ae69,2901d5e2711fa4f32d39cd8eea36cd71,2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447,93cb0d0456e0822b5fe30a3e627405f8,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="NALLAPATI ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Nallapati et al. are referenced in the context of language models excelling in standard natural language processing tasks such as summarization.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="BOWMAN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Bowman et al. are referenced in the context of language models excelling in standard natural language processing tasks such as language inference.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="COBBE ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Cobbe et al. are referenced in the context of language models being adapted to tasks requiring advanced common-sense reasoning or quantitative skills.
Cobbe et al. are the authors of the GSM8K dataset</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="SAPAROV AND HE">
      <data key="d0">PERSON</data>
      <data key="d1">Saparov and He are referenced in the context of language models being adapted to tasks requiring advanced common-sense reasoning or quantitative skills.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="DENG ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Deng et al. are referenced in the context of language models performing in complex environments such as web navigation.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="SCHICK ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Schick et al. are referenced in the context of language models performing in complex environments such as tool-use.
Schick et al. are authors who have contributed to the research on tool use in agentic systems
Schick et al. are the authors of the Tool Use method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,93cb0d0456e0822b5fe30a3e627405f8,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="FAN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Fan et al. are referenced in the context of language models performing in complex environments such as open-ended games.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="GAO ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Gao et al. are referenced in the context of prompting techniques that augment language models with feedback or observations from an external environment.
Gao et al. are the authors of the GSM-Hard dataset</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="SHINN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Shinn et al. are referenced in the context of prompting techniques that augment language models with feedback or observations from an external environment.
Shinn et al. are authors who have contributed to the research on self-reflection in agentic systems
Shinn et al. are the authors of the Reflection method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,93cb0d0456e0822b5fe30a3e627405f8,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="SLOMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Sloman is referenced in the context of the limitations of reflexive methods in language models compared to humans' deliberate and thoughtful decision-making characteristics.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="EVANS">
      <data key="d0">PERSON</data>
      <data key="d1">Evans is referenced in the context of the limitations of reflexive methods in language models compared to humans' deliberate and thoughtful decision-making characteristics.</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="XIE ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Xie et al. are referenced in the context of recent search-guided language model work that addresses the issue of planning and multiple reasoning paths.
Xie et al. are authors who have contributed to the understanding of beam search in language models</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="HAO ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Hao et al. are referenced in the context of recent search-guided language model work that addresses the issue of planning and multiple reasoning paths.
Hao et al. are authors who have contributed to the development of techniques like RAP</data>
      <data key="d2">93cb0d0456e0822b5fe30a3e627405f8,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="PMLR">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">PMLR is the organization that published the Proceedings of the 41st International Conference on Machine Learning where the paper on LATS was presented.
PMLR is the organization that hosted the International Conference on Machine LearningPMLR is the organization that hosted the Conference on Robot Learning</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,93cb0d0456e0822b5fe30a3e627405f8</data>
    </node>
    <node id="LATS">
      <data key="d0">FRAMEWORK</data>
      <data key="d1">LATS is a framework based on Monte Carlo Tree Search designed to enhance language model performance by incorporating reasoning, acting, and planning
Language Agent Tree Search (LATS) is a technique that unifies reasoning, acting, and planning in language models, incorporating designs from all three domains
LATS (Language Agent Tree Search) is a search algorithm that unifies reasoning, acting, and planning in language models
Language Agent Tree Search, a method that unifies reasoning, acting, and planning in language models
LATS (Language Agent Tree Search) is a method designed to unify reasoning, acting, and planning in language models. It achieves high performance in various domains by sampling nodes and using trajectories.
LATS (Language Agent Tree Search) is a method that combines internal reasoning and external retrieval strategies to improve performance on tasks like HotPotQA and programming challenges
LATS is a method that improves both score and success rate in WebShop, surpassing RL-based training and using a preconstructed action space of search and click commands, browser feedback, and reflections for observation
LATS (Language Agent Tree Search) is a framework introduced to unify reasoning, acting, and planning in language models
LATS is a framework that enhances language model (LM) performance through interactions with an environment, improving autonomous decision-making and interpretability.
LATS (Language Agent Tree Search) is an algorithm that unifies reasoning, acting, and planning in language models
LATS (Language Agent Tree Search) is an algorithm used for reasoning, acting, and planning in language models
Language Agent Tree Search (LATS) is a method used in language models for reasoning, acting, and planning, with various configurations tested for performance.
LATS (Language Agent Tree Search) is a method used in the Game of 24 to improve performance by incorporating self-consistency scores</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,42de130f5b6144472a86a4c8260a87c7,48e423e2baf2ed485872756f5b4d87d8,4ae237a491bc8a84cc720e40c59a7464,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,b8dd0300033963bb4a3e1bad37f8e7b9,c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="CHAIN-OF-THOUGHT (COT) PROMPTING">
      <data key="d0">METHOD</data>
      <data key="d1">Chain-of-Thought prompting is a method for decomposing complex inputs into sequential steps, often suffering from error propagation
Chain-of-thought (CoT) prompting is a technique that caters to scenarios where the direct mapping from input to output is intricate, such as mathematical queries or challenging questions. It involves creating intermediate thoughts that act as stepping stones between the input and the output.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="SELF-CONSISTENCY">
      <data key="d0">METHOD</data>
      <data key="d1">Self-consistency is a method that employs majority voting over sampled chains to mitigate error propagation in Chain-of-Thought prompting
A heuristic where actions sampled multiple times at the same state tend to be more accurate</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="LEAST-TO-MOST PROMPTING">
      <data key="d0">METHOD</data>
      <data key="d1">Least-to-most prompting is a multi-step decomposition method aimed at improving Chain-of-Thought prompting</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="TREE-OF-THOUGHT (TOT) PROMPTING">
      <data key="d0">METHOD</data>
      <data key="d1">Tree-of-Thought prompting uses depth-first or breadth-first search guided by an LM-generated heuristic to improve Chain-of-Thought prompting
Tree-of-thought (ToT) prompting extends CoT prompting by exploring multiple reasoning paths over thoughts. It frames problems as a search over a tree, where each node represents a partial solution state. Search algorithms like depth-first or breadth-first search are used to explore the tree systematically.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="REASONING VIA PLANNING (RAP)">
      <data key="d0">METHOD</data>
      <data key="d1">Reasoning via Planning uses Monte Carlo Tree Search with rollouts simulated by language models to improve reasoning tasks</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="SELF-REFINE">
      <data key="d0">METHOD</data>
      <data key="d1">Self-refine is a method that uses self-improvement techniques to enhance language model performance in acting tasks
Self-Refine is a technique that uses self-improvement to enhance reasoning and decision-making
Self-Refine is a technique where the meta agent performs iterations of refinement on the novelty and correctness of the proposal
Self-Refine allows iterative self-reflection to correct mistakes made in previous attempts
Self-Refine (Madaan et al., 2024) is a state-of-the-art hand-designed agent used as a baseline in Meta Agent Search
Self-Refine is a manually designed agent method used for comparison in the evaluation of Meta Agent Search
Self-Refine is a method used for improving the performance of models through self-reflection and refinement
Self-Refine is a manually designed agent used for various tasks such as Math, Reading Comprehension, Multi-task, and Science
Self-refine is a tool for iterative refinement with self-feedback
Self-Refine is a state-of-the-art hand-designed agent baseline for experiments on ARC
Self-Refine is a method that allows up to five refinement iterations with an early stop if the critic deems the answer correct</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,1b1399c76420a477c0c97893d258ae69,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="REFLEXION">
      <data key="d0">METHOD</data>
      <data key="d1">Reflexion is a method that uses self-improvement techniques to enhance language model performance in acting tasks
Reflexion is a technique that uses self-improvement to enhance reasoning and decision-making
Reflexion is a method similar to ReAct, focusing on decision-making tasks where reverting between iterations is feasible. It aims to improve reasoning and acting by leveraging environmental feedback.
Reflexion is a prompting method that enhances the reasoning capabilities of language models. It is evaluated in experiments involving HotPotQA and other benchmarks.
Reflexion is a prompting method that incorporates external observations to improve performance on reasoning tasks
Reflexion is a prompting method used in WebShop, similar to ReAct, but its semantic feedback is not as helpful in complex environments like WebShop
Reflexion is a simpler prompting method compared to LATS, mentioned in the limitations
Reflexion is a simpler prompting method compared to LATS
Reflexion is an algorithm compared with LATS in the HumanEval experiments</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,48e423e2baf2ed485872756f5b4d87d8,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,9bb90746134619cad9a3e649b8b35f24,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="YANG ET AL., 2018">
      <data key="d0">REFERENCE</data>
      <data key="d1">Yang et al., 2018 is a reference for the HotPotQA dataset used in evaluating LATS</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="SILVER ET AL., 2017">
      <data key="d0">REFERENCE</data>
      <data key="d1">Silver et al., 2017 is a reference for the success of Monte Carlo Tree Search in model-based reinforcement learning
A reference to a study or paper related to learned heuristics</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="YAO ET AL., 2023B">
      <data key="d0">REFERENCE</data>
      <data key="d1">Yao et al., 2023b is a reference for the ReAct method that LATS outperforms
A paper by Yao et al. published in 2023 that discusses the ReAct method and its application in language models.
Yao et al., 2023b is a publication that discusses the ReAct method and its performance on WebShop
A previous work referenced for setting the maximum depth in HotPotQA experiments</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="YAO ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">Yao et al., 2022 is a reference for the WebShop dataset used in evaluating LATS
Yao et al., 2022 is a publication that discusses the IL and IL+RL methods and their performance on WebShop
A reference to a study or paper by Yao et al. in 2022, related to WebShop and other methods</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="CHEN ET AL., 2021">
      <data key="d0">REFERENCE</data>
      <data key="d1">Chen et al., 2021 is a reference for the HumanEval dataset where LATS set the state of the art with GPT-4
A reference to a study or paper related to programming
A paper by Chen et al. published in 2021 that is referenced in the context of evaluating LATS on programming tasks.
Chen et al., 2021 is a publication that introduced the HumanEval dataset for evaluating the correctness of synthesized programs in Python
A paper discussing safety concerns when executing untrusted model-generated code</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,99d90aededb61e04241516ed9ec656cc,dc55f071b95dec721a9820d39cdb3ccd,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="COBBE ET AL., 2021">
      <data key="d0">REFERENCE</data>
      <data key="d1">Cobbe et al., 2021 is a reference for reasoning in language models involving decomposing complex inputs into sequential steps
A publication that discusses the GSM8K dataset</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="WEI ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">Wei et al., 2022 is a reference for Chain-of-Thought prompting and its variants
A reference to a study or paper related to Chain of Thought
A paper by Wei et al. published in 2022 that discusses the CoT (Chain of Thought) method and its application in language models.
Wei et al., 2022 is a publication that introduced the CoT and ReAct prompting methods
A reference to the authors who introduced the Chain-of-Thought (COT) technique
A publication that discusses the Chain-of-Thought method
A publication by Wei et al. in 2022 related to Chain-of-Thought
Wei et al., 2022 is a publication referenced for the Chain-of-Thought (COT) method</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="KOJIMA ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">Kojima et al., 2022 is a reference for a variant of Chain-of-Thought prompting</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="WANG ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">Wang et al., 2022 is a reference for self-consistency and Chain-of-Thought prompting
A reference to a study or paper related to self-consistency
A paper by Wang et al. published in 2022 that discusses the CoT-SC (Chain of Thought with Self-Consistency) method and its application in language models.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="GUO ET AL., 2018">
      <data key="d0">REFERENCE</data>
      <data key="d1">Guo et al., 2018 is a reference for the issue of error propagation in Chain-of-Thought prompting</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="CHEN ET AL., 2023B">
      <data key="d0">REFERENCE</data>
      <data key="d1">Chen et al., 2023b is a reference for the issue of error propagation in Chain-of-Thought prompting
A paper discussing AgentVerse and its application in optimizing role definition in the prompt</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd,f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="YAO ET AL., 2023A">
      <data key="d0">REFERENCE</data>
      <data key="d1">Yao et al., 2023a is a reference for Tree-of-Thought prompting and search algorithms in Chain-of-Thought prompting
A reference to a study or paper related to Tree of Thought
A paper by Yao et al. published in 2023 that discusses the ToT (Tree of Thoughts) method and its application in language models.
Yao et al., 2023a is a publication that introduced the ToT search method</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="HAO ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">Hao et al., 2023 is a reference for Reasoning via Planning using Monte Carlo Tree Search
Hao et al., 2023 is a reference cited in the context of standard MCTS and RAP relying on internal dynamics models
A reference to a study or paper related to Reasoning and Planning
A paper by Hao et al. published in 2023 that discusses the RAP (ReAct Prompting) method and its application in language models.
Hao et al., 2023 is a publication that introduced the RAP search method
A reference to a study or paper by Hao et al. in 2023, related to RAP and its performance</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,c234cb83764b899335af0950677ad024,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="BESTA ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">Besta et al., 2023 is a reference for search algorithms in Chain-of-Thought prompting</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="AHN ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">Ahn et al., 2022 is a reference for using language models as high-level controllers in robotics</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="HUANG ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">Huang et al., 2022 is a reference for using language models as high-level controllers in robotics</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="DRIESS ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">Driess et al., 2023 is a reference for using language models as high-level controllers in robotics</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="BAKER ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">Baker et al., 2022 is a reference for adapting language model agents to complex multimodal games</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="WANG ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">Wang et al., 2023 is a reference for adapting language model agents to complex multimodal games</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="GUSS ET AL., 2019">
      <data key="d0">REFERENCE</data>
      <data key="d1">Guss et al., 2019 is a reference for the game Minecraft used in adapting language model agents</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="FAN ET AL., 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">Fan et al., 2022 is a reference for the game Minecraft used in adapting language model agents</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="LIU ET AL., 2018">
      <data key="d0">REFERENCE</data>
      <data key="d1">Liu et al., 2018 is a reference for using language models in text-based environments</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="SHRIDHAR ET AL., 2020">
      <data key="d0">REFERENCE</data>
      <data key="d1">Shridhar et al., 2020 is a reference for using language models in text-based environments</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="LIU ET AL., 2024">
      <data key="d0">REFERENCE</data>
      <data key="d1">Liu et al., 2024 is a reference for using language models in text-based environments
A publication by Liu et al. in 2024 related to EoH</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb,f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="MADAAN ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">Madaan et al., 2023 is a reference for the self-refine method
A reference to a study or paper related to self-reflection</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,f8e7ed806916bf15245bcb4d52570c26</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="SHINN ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">Shinn et al., 2023 is a reference for the Reflexion method
A reference to a study or paper related to self-reflection
A paper by Shinn et al. published in 2023 that discusses the Reflexion method and its application in language models.
Shinn et al., 2023 is a publication that introduced the Reflexion prompting method
A reference to a study or paper by Shinn et al. in 2023, related to Reflexion and its performance
A reference to the authors who contributed to the Self-Refine technique
Shinn et al. (2023) is a reference cited in the context of self-reflection and improving the generated agent.
Shinn et al., 2023 is a publication referenced for the Self-Refine method</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,282313a8340c6792e8c35f53ed157cd0,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="LMS FOR ACTING">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="SEARCH ALGORITHMS">
      <data key="d0">METHOD</data>
      <data key="d1">Search algorithms are methods used to explore and find solutions in various tasks, including language models and interactive environments
Search algorithms are used to explore multiple branches of outcomes and determine the best course of action in language models

Search algorithms are techniques used to construct trajectories and incorporate external feedback in LATS</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="NODES">
      <data key="d0">COMPONENT</data>
      <data key="d1">Nodes are components in search algorithms that store and retrieve external feedback, contributing to value assignment heuristics
Nodes refer to the sampled points in the search space used in the evaluation of LATS on Game of 24</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="PROMPTS">
      <data key="d0">COMPONENT</data>
      <data key="d1">Prompts are components in search algorithms that store and retrieve external feedback, contributing to value assignment heuristics
Prompts are specific instructions or few-shot input-output examples provided along with the input to improve reasoning in language models</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="EXTERNAL FEEDBACK">
      <data key="d0">CONCEPT</data>
      <data key="d1">External feedback is information from the environment that is incorporated into search algorithms to improve performance
External feedback is used in LATS to improve performance and efficiency</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="INTERNAL REASONING PERFORMANCE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Internal reasoning performance refers to the ability of a model to reason and solve tasks without external feedback</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="PRETRAINED LMS">
      <data key="d0">MODEL</data>
      <data key="d1">Pretrained language models are models that have been trained on large datasets and are repurposed in LATS for value functions and self-reflections</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="LM-POWERED VALUE FUNCTIONS">
      <data key="d0">COMPONENT</data>
      <data key="d1">LM-powered value functions are components in LATS that use language models to assign values and guide exploration</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="SELF-REFLECTIONS">
      <data key="d0">COMPONENT</data>
      <data key="d1">Self-reflections are components in LATS that allow language models to reflect on their actions and improve exploration</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="IN-CONTEXT LEARNING">
      <data key="d0">CONCEPT</data>
      <data key="d1">In-context learning is the ability of language models to learn and adapt to new tasks based on the context provided
In-context learning leverages the abilities of language models to learn from the context without additional training
A method where the agent learns from trial and error using context</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="ENVIRONMENTAL CONDITIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Environmental conditions refer to the external factors and feedback that influence the performance of language models in LATS</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="REASONING">
      <data key="d0">CONCEPT</data>
      <data key="d1">Reasoning involves decomposing complex inputs into sequential intermediate steps towards a final answer
Reasoning is the process of thinking about something in a logical way to form a conclusion or judgment, a key component in LATS
Reasoning is one of the domains where experiments were conducted
Reasoning is one of the skills covered by the synthetic post-training dataset created by AgentInstruct</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,b88745a13b69cecbc0ee9c3af41389bf,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="ACTING">
      <data key="d0">CONCEPT</data>
      <data key="d1">Acting involves decision-making and performing tasks in interactive environments using language models
Acting refers to the execution of actions based on decisions made by language models, a key component in LATS</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="PLANNING">
      <data key="d0">CONCEPT</data>
      <data key="d1">Planning involves creating strategies and trajectories to achieve goals in language models and interactive environments
Planning refers to the use of a search algorithm to determine the best course of action in language models
Planning in LATS involves organizing information, planning future actions, or injecting internal knowledge to formalize decisions
Planning involves creating a sequence of actions to achieve a specific goal, a key component in LATS</data>
      <data key="d2">c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="AUTONOMOUS REASONING">
      <data key="d0">CONCEPT</data>
      <data key="d1">Autonomous reasoning refers to the ability of language models to reason and make decisions independently</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="DECISION-MAKING">
      <data key="d0">CONCEPT</data>
      <data key="d1">Decision-making involves choosing actions and strategies in interactive environments using language models
Decision-making refers to the process of making choices based on reasoning and planning in language models
Decision-making is the cognitive process of selecting a course of action from multiple alternatives, enhanced by LATS</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="INTERACTIVE QUESTION-ANSWERING (QA)">
      <data key="d0">TASK</data>
      <data key="d1">Interactive question-answering is a task where language models answer questions based on interaction with the environment</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="WEB NAVIGATION">
      <data key="d0">TASK</data>
      <data key="d1">Web navigation is a task where language models interact with web environments to retrieve information and perform actions</data>
      <data key="d2">f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="MATH">
      <data key="d0">TASK</data>
      <data key="d1">Math is a task where language models solve mathematical problems and equations
Math benchmarks test the ability of AI systems to solve mathematical problems
A domain tested by Meta Agent Search using the MGSM benchmark
Math is a domain where FMs possess adequate knowledge to solve questions, and errors could mainly be hallucinations or calculation mistakes
Math is one of the skills covered by the synthetic post-training dataset created by AgentInstruct
Math is another capability evaluated in language models, as indicated in the document's section on evaluation.</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,81c504ffbcc5ed882e234802135295ba,86f77e15d41cbd0cb33f635ccb2cb66b,b88745a13b69cecbc0ee9c3af41389bf,bc26e68b0b2783ba912b9e5606d9eb0b,f8e7ed806916bf15245bcb4d52570c26</data>
    </node>
    <node id="PROGRAMMING">
      <data key="d0">TASK</data>
      <data key="d1">Programming is a task where language models write and debug code
A domain that requires reasoning and acting
Programming is one of the tasks used in experiments with the LATS algorithm
Programming involves writing code to solve problems, often evaluated through datasets like HumanEval and MBPP.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,42de130f5b6144472a86a4c8260a87c7,f8e7ed806916bf15245bcb4d52570c26,fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="COT">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Chain-of-thought (CoT) prompting is a technique that caters to scenarios where the direct mapping from input to output is intricate
CoT (Chain of Thought) is used as the base prompting framework in environments without feedback, such as reasoning tasks
Chain of Thought, a reasoning method
CoT (Chain of Thought) is a prompting method that relies on the agent's existing knowledge to answer questions. It is evaluated in experiments involving HotPotQA and other benchmarks.
CoT (Chain of Thought) is a prompting method that enhances performance on questions requiring reasoning
CoT (Chain of Thought) is a base prompting design used in the Game of 24 with LATS
COT is a method where the FM is prompted to think step by step before answering the question
CoT (Chain of Thought) is a technique showing the performance of models when answering directly without using RAG</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,594449768ae2dea9b2efbe677075096b,97457e990eb6e3c88c11c862f9e3265b,99d90aededb61e04241516ed9ec656cc,ab04427ae0415a1c812a35cf8d3ee1a2,c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="ADAPLANNER">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">AdaPlanner is a technique that incorporates both positive and negative feedback to enhance reasoning and decision-making</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="HUANG ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Huang et al. are authors who suggested that language models cannot self-correct their internal reasoning, making it critical to use external feedback
Huang et al. are authors referenced in the context of multi-objective ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="EXTERNAL TOOLS">
      <data key="d0">RESOURCE</data>
      <data key="d1">External tools such as APIs, search engines, calculators, and other models are used to enhance the reasoning and practical abilities of language models
External tools are tools or APIs used by an agent in reasoning tasks to perform specific actions
External tools are resources or utilities that agents can access to perform tasks, such as search engines, code execution, and database queries</data>
      <data key="d2">c234cb83764b899335af0950677ad024,c3d0436082aada237ee4bee645f16059,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="TREE-BASED SEARCH">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Tree-based search is a method where multiple branches of outcomes are explored during search, widely used in planning and reinforcement learning algorithms</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="MCTS">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Monte Carlo Tree Search (MCTS) is a tree-based search algorithm used to fully unlock the potential of language models
MCTS (Monte Carlo Tree Search) is a search algorithm that requires an environment model to undo previous steps and form a searching tree
Monte Carlo Tree Search, an algorithm used in LATS for principled search
MCTS (Monte Carlo Tree Search) is a search algorithm used in LATS, providing principled search and observed performance gains over other variants like A* and DFS
MCTS (Monte Carlo Tree Search) is a principled search algorithm used as the basis for observed performance gains in the study</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,594449768ae2dea9b2efbe677075096b,c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="LM">
      <data key="d0">MODEL</data>
      <data key="d1">Language Model (LM) is a pre-trained model parameterized to generate outputs corresponding to answers or task completions based on given inputs</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="YA0 ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Yao et al. are authors who have contributed to the development of techniques like ReAct and ToT</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="SELF-REFLECTION">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Self-reflection is the use of language model-generated feedback to improve reasoning and decision-making
A method where the agent reflects on its actions and proposes superior alternatives
Self-reflection is a technique used in LATS to provide additional semantic signals for the agent, improving performance
The process of reviewing and analyzing one's own code to identify errors and areas for improvement
Self-Reflection is a technique used in agentic systems to improve performance by reflecting on past actions
Self-reflection is a process where the meta agent reviews and improves its proposed architecture and implementation through critical thinking and debugging.
A process implemented using the framework to improve task-solving by reflecting on previous attempts and feedback</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,282313a8340c6792e8c35f53ed157cd0,594449768ae2dea9b2efbe677075096b,785ad59c6a37896a4676ec5c1689735f,c3d0436082aada237ee4bee645f16059,c95e02c0dca4a4a36b701cbc7dd14da6,d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="EXTERNAL MEMORY">
      <data key="d0">RESOURCE</data>
      <data key="d1">External memory is used to store past text context for future updates of the solution in language models
External Memory is a method used for improving the performance of models through the use of external memory resources</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="REINFORCEMENT LEARNING">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Reinforcement learning is a type of machine learning algorithm used for its good exploration-exploitation trade-off in planning algorithms
Reinforcement Learning is a method used in search algorithms to explore the search space in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="VODOPIVEC ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Vodopivec et al. are authors who have contributed to the understanding of tree-based search in reinforcement learning</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="WEI ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Wei et al. are authors who have contributed to the development of Chain-of-thought (CoT) prompting
Wei et al. are authors who have contributed to the research on chain-of-thought planning and reasoning
Wei et al. are the authors of the Chain-of-Thought agent
Wei et al. are the authors of the Chain-of-Thought method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b,c3d0436082aada237ee4bee645f16059,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="SWIECHOWSKI ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Swiechowski et al. are authors who have contributed to the understanding of tree-based search in planning algorithms</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="LAVALLE">
      <data key="d0">AUTHOR</data>
      <data key="d1">LaValle is an author who has contributed to the understanding of tree-based search in planning algorithms</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="HAFNER ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Hafner et al. are authors who have contributed to the understanding of tree-based search in reinforcement learning</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="DU ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Du et al. are authors who have contributed to the understanding of tree-based search in reinforcement learning
Du et al. are the authors of the LLM Debate agent
Du et al. are the authors of the LLM Debate method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="WU ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Wu et al. are authors who have contributed to the understanding of tree-based search in reinforcement learning
Wu et al. are the authors of the method for assigning FM modules in the agentic system with different roles and enabling them to collaborate</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </node>
    <node id="SHICK ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Schick et al. are authors who have contributed to the understanding of using external tools to enhance language models</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="SHEN ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Shen et al. are authors who have contributed to the understanding of using external tools to enhance language models</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="SURIS ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Suris et al. are authors who have contributed to the understanding of using external tools to enhance language models</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="LM TASKS">
      <data key="d0">TASK</data>
      <data key="d1">LM tasks refer to the various tasks that language models can perform, such as reasoning, decision-making, and planning
LM tasks refer to tasks involving language models, which can conveniently reset to any step by simply copy-pasting historical text input</data>
      <data key="d2">c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">TASK</data>
    </node>
    <node id="RAP">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">RAP is a technique that incorporates planning and search algorithms to enhance reasoning and decision-making in language models
RAP is a reasoning-based method that relies on the internal representations of the language model. It is used in planning strategies but has limitations in flexibility, sensibility, and adaptability.
RAP (Reinforcement Learning with Augmented Planning) is an algorithm that relies on internal dynamics models to facilitate simulation
Reasoning and Planning, a method used in the experiments
RAP (ReAct Prompting) is a method that combines ReAct prompting with search algorithms to enhance decision-making in language models. It is evaluated in experiments involving HotPotQA and other benchmarks.
RAP (Reasoning and Planning) is a search method that enhances performance by sampling and exploring multiple outputs
RAP is a prompting method used in HotPotQA and Game of 24, showing competitive performance in reasoning tasks
RAP is a method evaluated in the study, with different configurations for performance and token consumption
RAP (Reasoning and Planning) is a method compared to LATS in terms of sample complexity and performance</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,42de130f5b6144472a86a4c8260a87c7,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,9bb90746134619cad9a3e649b8b35f24,c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6,faa2bd677c7f052136479e0175da3e5b,fb9cb0c0984d44c3da881886ed637e55</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="TOT">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">ToT is a technique that incorporates planning and search algorithms to enhance reasoning and decision-making in language models

ToT (Tree of Thoughts) is a prompting method that uses search algorithms to enhance decision-making in language models. It is evaluated in experiments involving HotPotQA and other benchmarks.
ToT (Tree of Thoughts) is a search method that samples and explores multiple outputs to improve performance on reasoning tasks
ToT (Tree of Thoughts) is a prompting method used in HotPotQA and Game of 24, incorporating LM-based heuristics for pruning branches with low values
ToT (Tree of Thoughts) is a method that uses the LM-based heuristic and is compared with other methods in the study
ToT (Tree of Thoughts) is a method compared to LATS in terms of sample complexity and performance</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,42de130f5b6144472a86a4c8260a87c7,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,c95e02c0dca4a4a36b701cbc7dd14da6,faa2bd677c7f052136479e0175da3e5b,fb9cb0c0984d44c3da881886ed637e55</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="BEAM SEARCH">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Beam search is a technique that uses self-improvement to enhance reasoning and decision-making in language models</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="LM INTERNAL REASONING">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">LM internal reasoning refers to the reasoning capabilities of language models without external inputs</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="PROMPT IO">
      <data key="d0">RESOURCE</data>
      <data key="d1">Prompt IO refers to the process where an input prompt is transformed into an output by a language model</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">RESOURCE</data>
    </node>
    <node id="INPUT X">
      <data key="d0">RESOURCE</data>
      <data key="d1">Input X refers to the initial input provided to a language model for reasoning or decision-making
Input x is the initial data or query provided to the language model for processing.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24,c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">RESOURCE</data>
    </node>
    <node id="OUTPUT Y">
      <data key="d0">RESOURCE</data>
      <data key="d1">Output Y refers to the final output generated by a language model based on the given input
Output y is the final result generated by the language model after processing the input and any intermediate steps.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24,c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">RESOURCE</data>
    </node>
    <node id="P&#920;(X)">
      <data key="d0">MODEL</data>
      <data key="d1">P&#952;(X) refers to the pre-trained language model parameterized by &#952; used to generate outputs based on given inputs</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="AUTOREGRESSIVE DECODING">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Autoregressive decoding is a method where the language model generates text sequentially, predicting the next token based on previous tokens</data>
      <data key="d2">c95e02c0dca4a4a36b701cbc7dd14da6</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="LANGUAGE MODEL (LM)">
      <data key="d0">MODEL</data>
      <data key="d1">A language model (LM) is a type of model used to generate outputs based on given inputs. It is used in various prompting techniques like CoT, ToT, and ReAct to improve reasoning and decision-making tasks.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="UPPER CONFIDENCE BOUNDS APPLIED TO TREES (UCT)">
      <data key="d0">METRIC</data>
      <data key="d1">Upper Confidence bounds applied to Trees (UCT) is a metric used in MCTS to select the best child state for expansion. It is calculated based on the value function, the number of visits to a node, and an exploration weight.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="PROMPT">
      <data key="d0">TECHNIQUE/METHOD</data>
      <data key="d1">A prompt is a specific instruction or few-shot input-output example provided along with the input to improve reasoning in language models.
Prompt is a method used in HotPotQA to guide the question answering process
The prompt is used to instruct the meta agent on how to format its output, including sections for thought process, agent name, and code implementation.
A structured format generated by the FM Module by concatenating input Info objects</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0,9bb90746134619cad9a3e649b8b35f24,b8dd0300033963bb4a3e1bad37f8e7b9,d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="THOUGHT Z">
      <data key="d0">CONCEPT</data>
      <data key="d1">Thought z is an intermediate language sequence created during the Chain-of-thought (CoT) prompting process to bridge the gap between input x and output y.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="DEPTH-FIRST SEARCH (DFS)">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Depth-first search (DFS) is a search algorithm used in Tree-of-thought (ToT) prompting to systematically explore the tree of reasoning paths.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="BREADTH-FIRST SEARCH (BFS)">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Breadth-first search (BFS) is a search algorithm used in Tree-of-thought (ToT) prompting to systematically explore the tree of reasoning paths.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="OBSERVATION O">
      <data key="d0">CONCEPT</data>
      <data key="d1">Observation o is the feedback or data received from the external environment during the ReAct process, used to improve reasoning and acting.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="ACTION A">
      <data key="d0">CONCEPT</data>
      <data key="d1">Action a is a permissible action added to the reasoning traces in the ReAct process, generated based on observations from the environment.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="RAP (HAO ET AL., 2023)">
      <data key="d0">TECHNIQUE/METHOD</data>
      <data key="d1">RAP is a reasoning-based method that relies on the internal representations of the language model and is used in planning strategies. It has limitations in flexibility, sensibility, and adaptability.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="ATARI">
      <data key="d0">ENVIRONMENT</data>
      <data key="d1">Atari is a decision-making environment where Monte Carlo Tree Search (MCTS) has been successfully applied.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="GO">
      <data key="d0">ENVIRONMENT</data>
      <data key="d1">Go is a decision-making environment where Monte Carlo Tree Search (MCTS) has been successfully applied.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="KOCSIS AND SZEPESV&#193;RI (2006)">
      <data key="d0">PERSON</data>
      <data key="d1">Kocsis and Szepesv&#225;ri are the researchers who developed the Upper Confidence bounds applied to Trees (UCT) metric used in MCTS.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="SILVER ET AL. (2016)">
      <data key="d0">PERSON</data>
      <data key="d1">Silver et al. are the researchers who demonstrated the success of Monte Carlo Tree Search (MCTS) in the game of Go.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="YE ET AL. (2021)">
      <data key="d0">PERSON</data>
      <data key="d1">Ye et al. are the researchers who demonstrated the success of Monte Carlo Tree Search (MCTS) in the Atari environment.</data>
      <data key="d2">9bb90746134619cad9a3e649b8b35f24</data>
    </node>
    <node id="LM AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">An LM Agent is initialized with a language model to leverage useful language representations for sequential reasoning or decision-making tasks</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="ENVIRONMENT">
      <data key="d0">CONTEXT</data>
      <data key="d1">The environment provides observations to the agent and receives actions from the agent, resulting in feedback
Environment provides the context and rewards for the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="P&#920;">
      <data key="d0">MODEL</data>
      <data key="d1">P&#952; is a language model used as a base decision-maker, agent, state evaluator, and feedback generator in LATS
A language model used to reason about a given state and provide a score</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="UCT ALGORITHM">
      <data key="d0">ALGORITHM/TECHNIQUE</data>
      <data key="d1">The UCT (Upper Confidence bounds applied to Trees) algorithm is used to balance exploration and exploitation in the selection operation of LATS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="SELECTION">
      <data key="d0">OPERATION</data>
      <data key="d1">Selection is the first operation in LATS where the algorithm identifies a segment of the current tree most suitable for subsequent expansion
The process of choosing nodes based on their assigned scalar values</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="EXPANSION">
      <data key="d0">OPERATION</data>
      <data key="d1">Expansion is the second operation in LATS where the tree is expanded by sampling actions from P&#952; and adding new child nodes to the tree
Expansion is a text modification task that involves adding more information to a piece of text to make it more comprehensive.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="SIMULATION">
      <data key="d0">OPERATION</data>
      <data key="d1">Simulation is an operation in LATS where the algorithm simulates the selected node until a terminal node is reached
The process of expanding the currently selected node until a terminal state is reached</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="BACKPROPAGATION">
      <data key="d0">OPERATION</data>
      <data key="d1">Backpropagation is an operation in LATS where the resulting value from the simulation is used to update the value of nodes along the path
The process of updating the values of the tree based on the outcome of a trajectoryThe process of updating the values of nodes based on the outcome of a trajectory</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="REFLECTION">
      <data key="d0">OPERATION</data>
      <data key="d1">Reflection is an operation in LATS where a reflection is generated if the trajectory fails and used as additional context for future trials
The process of using self-reflection to refine decision-making
Reflection involves thinking about past actions and decisions to improve future performance, a component in LATS
An action to review and analyze past actions or decisions
The action of reviewing and analyzing the outcome of the interaction
Reflection is a method used for improving the performance of models through self-reflection and refinement</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,0b6b4880e77d40e284702da16be4ef64,4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7,c234cb83764b899335af0950677ad024,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="LONG-TERM MEMORY STRUCTURE">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">An external structure where the expanded tree is stored in LATS
A structure used to store information over an extended period</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="TASK">
      <data key="d0">CONTEXT</data>
      <data key="d1">A task in LATS is successfully completed when the series of operations result in a solution or a computational limit is reached
A task is an activity or assignment that agents are designed to perform or solve</data>
      <data key="d2">c234cb83764b899335af0950677ad024,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="TRAJECTORY">
      <data key="d0">CONCEPT</data>
      <data key="d1">A trajectory in LATS refers to a sequence of actions and observations sampled from P&#952; to construct the best path for task completion
The sequence of states from the root to the terminal state in the search tree
Trajectory refers to the path taken through the state space in the LATS algorithm
The path or sequence of actions taken during the interaction</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,48e423e2baf2ed485872756f5b4d87d8,5d356b8ff719763a38cecff22c4e17b7,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="FEEDBACK">
      <data key="d0">CONCEPT</data>
      <data key="d1">Feedback in LATS is the response from the environment to the agent's actions, used to guide the search algorithm
Feedback is provided by critics and experts to refine answers in the Meta Agent Search process
The feedback provided by the critic_module
Feedback is the evaluation of the generated code based on correct and wrong examples
Feedback is the information provided about the performance and correctness of the code</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,449db721e37968e073e3579b59e023b2,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="REASONING TASKS">
      <data key="d0">TASK</data>
      <data key="d1">Reasoning tasks are a type of task in LATS where the action space might be limited to a few external tools or APIs</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="DECISION-MAKING TASKS">
      <data key="d0">TASK</data>
      <data key="d1">Decision-making tasks are a type of task in LATS where actions might consist of commands on a website</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="SAMPLING">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Sampling in LATS involves selecting a diverse set of candidates at each step to mitigate the stochastic nature of LM text generation</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="SEARCH ALGORITHM">
      <data key="d0">ALGORITHM/TECHNIQUE</data>
      <data key="d1">A search algorithm in LATS controls the problem-solving process with planning to find the most promising trajectory
An algorithm used to explore and find the most promising regions of a tree
The search algorithm defines how ADAS algorithms explore the search space
Search algorithm is the method used to explore and discover new designs in Meta Agent Search</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="PSEUDOCODE">
      <data key="d0">DOCUMENTATION</data>
      <data key="d1">The full pseudocode of LATS can be found in the Appendix, detailing the operations and process
The pseudocode of the LATS algorithm is provided in the appendix</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="SEC. A">
      <data key="d0">DOCUMENTATION</data>
      <data key="d1">Section A in the Appendix contains the full pseudocode of LATS
Section A of the appendix shows the pseudocode of the proposed algorithm, LATS</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="EVANS, 2010">
      <data key="d0">REFERENCE</data>
      <data key="d1">Evans, 2010 is a reference cited in the context of sampling diverse candidates for complex decision-making tasks</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="MODERN LMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Modern LMs (Language Models) provide useful language representations that facilitate planning in LATS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="EXPLORATION WEIGHT">
      <data key="d0">CONCEPT</data>
      <data key="d1">Exploration weight is a parameter used in the MCTS algorithm to balance exploration and exploitation
Exploration weight is a parameter in the selection formula that affects the performance of the search in HotPotQA</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="PARENT NODE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Parent node is a node in the tree structure from which child nodes are derived in MCTS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="EPISODE">
      <data key="d0">CONCEPT</data>
      <data key="d1">An episode refers to a complete sequence of actions and observations in a task, ending in a terminal state</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="RETURN">
      <data key="d0">CONCEPT</data>
      <data key="d1">Return is the reward or feedback used for updating the value function in MCTS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="VALUE FUNCTION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Value function is a function used to estimate the expected return of a state in MCTS
A function that assigns a value to a state based on certain criteria
Value function is a component of LATS that incorporates self-consistency as an additional heuristic for reasoning tasks
Value function (pV) is used to evaluate states in the LATS algorithm
Value Function is a method used in WebShop and Game of 24 to evaluate the performance of actions</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,48e423e2baf2ed485872756f5b4d87d8,594449768ae2dea9b2efbe677075096b,b8dd0300033963bb4a3e1bad37f8e7b9,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="VOLD(S)">
      <data key="d0">CONCEPT</data>
      <data key="d1">Vold(s) is the old value function before it is updated with the new return in MCTS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="N(S)">
      <data key="d0">CONCEPT</data>
      <data key="d1">N(s) is the number of times a state has been visited in MCTS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="RESET">
      <data key="d0">CONCEPT</data>
      <data key="d1">Reset refers to the ability to revert to a previous state or step in LM tasks</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="ACTION SPACE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Action space refers to the set of all possible actions an agent can take in a given environment
Action space (A) is the set of possible actions in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="PERMISSIBLE ACTIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Permissible actions are the actions that an agent is allowed to take in a given environment</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="REASONING TRACES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Reasoning traces are the language-based thoughts used to formalize decisions in LATS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="OBSERVATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Observation is the feedback received from the environment after an action is taken by the agent
Observation is an action in HotPotQA where the result of an Action is analyzed
Environmental feedback or information received after performing an action in a question answering task
The result or feedback received after performing an action, such as a search in a webshop
The action of noting the system's response to a user's action</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f,5d356b8ff719763a38cecff22c4e17b7,785ad59c6a37896a4676ec5c1689735f,b8dd0300033963bb4a3e1bad37f8e7b9,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="POLICY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Policy is a strategy or rule followed by an agent to decide actions based on observations and past actions</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="COMMANDS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Commands are specific actions taken by an agent in decision-making tasks, such as interacting with a website</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="STOCHASTIC NATURE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Stochastic nature refers to the randomness inherent in LM text generation</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="COMPUTATIONAL LIMIT">
      <data key="d0">CONCEPT</data>
      <data key="d1">Computational limit is the maximum amount of computational resources allocated for completing a task in LATS</data>
      <data key="d2">c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="SCALAR VALUE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Scalar value is a numerical value assigned to a node in LATS to quantify the agent&#8217;s progress in task completion
A numeric value assigned to each new child node for selection and backpropagation</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="HEURISTIC">
      <data key="d0">CONCEPT</data>
      <data key="d1">Heuristic is a method used to guide the search algorithm towards the most promising regions of the tree in LATS
A method used to guide the search algorithm towards the most promising regions of the tree</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="SELF-GENERATED LM SCORE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Self-generated LM score is a component of the value function in LATS, generated by the language model itself
A score generated by the language model to evaluate a state</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,c234cb83764b899335af0950677ad024</data>
    </node>
    <node id="AGENT">
      <data key="d0">ACTOR</data>
      <data key="d1">An entity that performs tasks and makes decisions in a given environment
An agent is a system or entity designed to perform tasks, often using reasoning and planning
An entity discovered by Meta Agent Search that outperforms state-of-the-art hand-designed baselines
An agent is powered by an LLM and can optionally use tools such as search APIs, code interpreter, or a calculator. Each agent has a specific role and set of instructions specified as part of the underlying LLM system message.
An Agent is a tool or process that receives a piece of text and generates a list of questions or modifies text based on predefined tasks such as paraphrasing, expansion, or simplification.</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,1d8835c0ce90e56be22873bcf2740a5d,bc26e68b0b2783ba912b9e5606d9eb0b,c3d0436082aada237ee4bee645f16059,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="TASK COMPLETION">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of finishing a given task successfully</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </node>
    <node id="SELF-CONSISTENCY SCORE">
      <data key="d0">METRIC</data>
      <data key="d1">A score based on the consistency of actions sampled multiple times at the same state
Self-Consistency Score is a metric used in LATS to improve performance in the Game of 24</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="ENVIRONMENTAL FEEDBACK">
      <data key="d0">FEEDBACK</data>
      <data key="d1">Feedback obtained from the environment to improve value assignment</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </node>
    <node id="TERMINAL STATE">
      <data key="d0">STATE</data>
      <data key="d1">A state where the task is either completed successfully or not
Terminal state (st) is a state where the process ends in the LATS algorithm</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,48e423e2baf2ed485872756f5b4d87d8</data>
      <data key="d3">STATE</data>
    </node>
    <node id="REWARD">
      <data key="d0">METRIC</data>
      <data key="d1">A value that reflects the outcome of a simulation
Reward (r) is the feedback from the environment in the LATS algorithm
Reward is a metric in WebShop calculated based on the number of attributes satisfied by the selected item</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf,48e423e2baf2ed485872756f5b4d87d8,b8dd0300033963bb4a3e1bad37f8e7b9</data>
      <data key="d3">METRIC</data>
    </node>
    <node id="UCT FORMULA">
      <data key="d0">FORMULA</data>
      <data key="d1">A formula used to guide the selection of the next node</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
      <data key="d3">FORMULA</data>
    </node>
    <node id="MEMORY">
      <data key="d0">STORAGE</data>
      <data key="d1">A storage for failed trajectories and corresponding reflections</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
      <data key="d3">STORAGE</data>
    </node>
    <node id="EXACT MATCH (EM)">
      <data key="d0">METRIC</data>
      <data key="d1">A metric used to evaluate the accuracy of reasoning</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
      <data key="d3">METRIC</data>
    </node>
    <node id="AUSTIN ET AL.">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper related to programming</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="CAMPBELL ET AL., 2002">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper related to programmed heuristics</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
      <data key="d3">REFERENCE</data>
    </node>
    <node id="HYPERPARAMETER">
      <data key="d0">METRIC</data>
      <data key="d1">A parameter whose value is set before the learning process begins</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </node>
    <node id="PROGRAMMED HEURISTICS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Heuristics that are manually designed and implemented</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </node>
    <node id="LEARNED HEURISTICS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Heuristics that are learned through machine learning techniques</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </node>
    <node id="NODE">
      <data key="d0">CONCEPT</data>
      <data key="d1">A point in the search tree representing a state</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </node>
    <node id="DEPTH LEVEL">
      <data key="d0">METRIC</data>
      <data key="d1">The level of a node in the search tree, indicating its distance from the root</data>
      <data key="d2">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </node>
    <node id="COT-SC">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">CoT-SC (Chain of Thought with Self-Consistency) is a variant of the CoT method that enhances reasoning capabilities in language models. It is evaluated in experiments involving HotPotQA and other benchmarks.
CoT-SC is another method evaluated in the study, with different configurations for performance and token consumption
CoT-SC (Chain of Thought with Self-Consistency) is a method compared to LATS in terms of efficiency
COT-SC (Wang et al., 2023b) is a state-of-the-art hand-designed agent used as a baseline in Meta Agent Search
COT-SC is a manually designed agent method used for comparison in the evaluation of Meta Agent Search
COT-SC is a method used for planning and reasoning in agentic systems
COT-SC is a manually designed agent used for various tasks such as Math, Reading Comprehension, Multi-task, and Science
COT-SC is a method where 5 answers are sampled and then an ensemble is performed using either majority voting or an FM query</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71,42de130f5b6144472a86a4c8260a87c7,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b,faa2bd677c7f052136479e0175da3e5b,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="AUSTIN ET AL., 2022">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper by Austin et al. published in 2022 that is referenced in the context of evaluating LATS on programming tasks.
Austin et al., 2022 is a publication that introduced the MBPP dataset for evaluating the correctness of synthesized programs in Python</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="GAME OF 24">
      <data key="d0">BENCHMARK</data>
      <data key="d1">Game of 24 is a benchmark used to evaluate reasoning and acting strategies in language models. It is referenced in the context of LATS evaluation.
Game of 24 is a mathematical reasoning task where the agent must construct 24 out of a set of numbers and basic operations, used to test the reasoning ability of LATS
Game of 24 is one of the tasks used in experiments with the LATS algorithm
Game of 24 is a mathematical reasoning challenge where the goal is to use basic arithmetic operations to construct the number 24 from four given numbers</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,594449768ae2dea9b2efbe677075096b,b8dd0300033963bb4a3e1bad37f8e7b9,fb9cb0c0984d44c3da881886ed637e55</data>
    </node>
    <node id="MBPP">
      <data key="d0">DATASET</data>
      <data key="d1">MBPP (Mostly Basic Python Problems) is a dataset used to measure the correctness of synthesized programs in Python from natural language docstrings
Mostly Basic Programming Problems (MBPP) is a benchmark containing 974 short Python functions designed to evaluate program synthesis techniques.</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc,fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="IL">
      <data key="d0">METHOD/ALGORITHM</data>
      <data key="d1">IL (Imitation Learning) is a method used to train models based on expert demonstrations</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="RL">
      <data key="d0">METHOD/ALGORITHM</data>
      <data key="d1">RL (Reinforcement Learning) is a method used to train models based on reward signals</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="FINE-TUNING">
      <data key="d0">METHOD/ALGORITHM</data>
      <data key="d1">Fine-tuning is a method used to improve model performance by training on specific datasets
Fine-tuning is a method mentioned in the context of improving performance in WebShop, specifically referenced in Furuta et al., 2024
Fine-tuning is a process where AI models are further trained using synthetic datasets created by AgentInstruct</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="EXPERT">
      <data key="d0">HUMAN</data>
      <data key="d1">Expert refers to human performance benchmarks used for comparison with model performance on WebShop
Expert refers to human performance metrics used as a benchmark in WebShop</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="TAB. 2">
      <data key="d0">DATASET</data>
      <data key="d1">Tab. 2 is a table that shows the performance of internal reasoning and external retrieval strategies on HotPotQA</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="TAB. 3">
      <data key="d0">DATASET</data>
      <data key="d1">Tab. 3 is a table that shows the performance of various methods on HotPotQA in different settings</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="TAB. 4">
      <data key="d0">DATASET</data>
      <data key="d1">Tab. 4 is a table that shows the performance of various methods on programming tasks using HumanEval and MBPP datasets</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="TAB. 5">
      <data key="d0">DATASET</data>
      <data key="d1">Tab. 5 is a table that shows the GPT-3.5 Pass@1 accuracy on MBPP for various prompting methods</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="APPENDIX SEC. D">
      <data key="d0">DATASET</data>
      <data key="d1">Appendix Sec. D provides additional details on the evaluation of LATS and other methods on programming tasks</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="CHEN ET AL., 2023A">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Chen et al., 2023a is a publication that discusses the use of an LM to generate a synthetic test suite for evaluating programming tasks</data>
      <data key="d2">99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="FURUTA ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Furuta et al., 2024 is a publication that discusses the fine-tuning method and its performance on WebShop
A reference to a study or paper by Furuta et al. in 2024, related to fine-tuning methods</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc</data>
    </node>
    <node id="DFS">
      <data key="d0">ALGORITHM</data>
      <data key="d1">DFS (Depth-First Search) is a search algorithm variant compared with MCTS in LATS, used to observe the effects of different search strategies
DFS (Depth-First Search) is another search algorithm mentioned as a variant compared to MCTS</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="ZHUANG ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A reference to a study or paper by Zhuang et al. in 2023, related to search algorithms like A* and DFS
Zhuang et al., 2023 is a reference cited in the context of search algorithms like A* and DFS</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="IMPROVEMENT LEARNING (IL)">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">IL (Improvement Learning) is a method used in WebShop for training agents, mentioned in comparison with other methods like RL-based training</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="REINFORCEMENT LEARNING (RL)">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">RL (Reinforcement Learning) is a method used in WebShop for training agents, mentioned in comparison with other methods like IL and LATS</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="PROMPTING">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">Prompting refers to methods used to guide the behavior of models like GPT-3.5 in WebShop, including techniques like ReAct and Reflexion</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="SEARCH AND CLICK COMMANDS">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">Search and click commands are part of the preconstructed action space used in WebShop for agent navigation</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="BROWSER FEEDBACK">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">Browser feedback is used in WebShop as part of the observation mechanism for agents</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="REFLECTIONS">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">Reflections are used in WebShop as part of the observation mechanism for agents, providing feedback for decision-making</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="SUCCESS RATE (SR)">
      <data key="d0">METRIC</data>
      <data key="d1">Success Rate (SR) is a metric used in WebShop to indicate the frequency with which the chosen product fulfills all given conditions</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="AVERAGE SCORE">
      <data key="d0">METRIC</data>
      <data key="d1">Average Score is a metric used in WebShop to reflect the percentage of user-specified attributes met by the selected product</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="MCTS (MONTE CARLO TREE SEARCH)">
      <data key="d0">ALGORITHM</data>
      <data key="d1">MCTS (Monte Carlo Tree Search) is a search algorithm used in LATS, providing principled search and observed performance gains over other variants like A* and DFS</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b</data>
    </node>
    <node id="A*">
      <data key="d0">ALGORITHM</data>
      <data key="d1">A* is a search algorithm variant compared with MCTS in LATS, used to observe the effects of different search strategies
A* is a search algorithm mentioned as a variant compared to MCTS</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="TOKEN CONSUMPTION">
      <data key="d0">METRIC</data>
      <data key="d1">Token consumption is a metric used in the ablation study of LATS to measure the efficiency of different components
Token consumption is a metric used to measure the number of tokens used by different methods</data>
      <data key="d2">594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="TRAJECTORIES">
      <data key="d0">METRIC</data>
      <data key="d1">Trajectories refer to the sampled paths in the search space used in the evaluation of LATS on Game of 24 and HotPotQA
Trajectories are paths constructed by search algorithms in LATS to enhance decision-making
Trajectories refer to the number of paths sampled in the HumanEval and HotPotQA experiments, with a maximum of k=8
Trajectories refer to the labeled sequences of environmental observations, thoughts, and actions in a question answering task</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f,48e423e2baf2ed485872756f5b4d87d8,594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="LM-BASED HEURISTIC">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">A heuristic used in ToT that prunes branches with low values, removing selection and backpropagation operations</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="LANGUAGE AGENT TREE SEARCH">
      <data key="d0">FRAMEWORK</data>
      <data key="d1">Language Agent Tree Search (LATS) is a framework that unifies reasoning, acting, and planning for enhanced LM problem-solving
A method that unifies reasoning, acting, and planning in language models
Language Agent Tree Search is a method that unifies reasoning, acting, and planning in language models</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad,785ad59c6a37896a4676ec5c1689735f,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="LM AGENTS">
      <data key="d0">AGENT</data>
      <data key="d1">LM agents are language model agents that benefit from the improvements in MCTS and LATS</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="REVERSION PROPERTY">
      <data key="d0">CONCEPT</data>
      <data key="d1">The reversion property allows reverting to earlier states in decision-making environments, which is assumed by LATS</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="SYSTEM-2 LM APPROACHES">
      <data key="d0">CONCEPT</data>
      <data key="d1">System-2 LM approaches refer to advanced language model techniques that involve reasoning and planning, like LATS
System-2 LM approaches involve high-level linguistic reasoning and actions through several rounds of decision-making and reflection, rather than relying on autoregressive generation.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="PERFORMANCE">
      <data key="d0">METRIC</data>
      <data key="d1">Performance is a metric used to evaluate the effectiveness of different methods in the study
Performance is a metric used in the evaluation function to assess an agent's effectiveness
Performance is the primary objective considered in the optimization of ADAS in the paper</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="SAMPLE COMPLEXITY">
      <data key="d0">METRIC</data>
      <data key="d1">Sample complexity is a metric used to measure the asymptotic token cost of different methods</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="NODES EXPANDED">
      <data key="d0">METRIC</data>
      <data key="d1">Nodes expanded is a metric used to measure the number of nodes expanded by different methods upon success</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="COMPUTATIONAL COST">
      <data key="d0">METRIC</data>
      <data key="d1">Computational cost is a metric used to measure the computational resources required by different methods</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="INFERENCE-TIME COMPUTE COSTS">
      <data key="d0">METRIC</data>
      <data key="d1">Inference-time compute costs refer to the computational costs incurred during the inference phase of language models</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="TAB. 8">
      <data key="d0">REFERENCE</data>
      <data key="d1">Tab. 8 is a reference to a table in the document that shows performance comparisons
Table 8 shows the results for HotPotQA</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="TAB. 9">
      <data key="d0">REFERENCE</data>
      <data key="d1">Tab. 9 is a reference to a table in the document that shows sample complexity and token consumption comparisons</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="TAB. 10">
      <data key="d0">REFERENCE</data>
      <data key="d1">Tab. 10 is a reference to a table in the document that shows the cost of different methods on HotPotQA</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="CONCLUSION">
      <data key="d0">SECTION</data>
      <data key="d1">The conclusion section summarizes the contributions and findings of the study</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="LIMITATIONS AND FUTURE DIRECTIONS">
      <data key="d0">SECTION</data>
      <data key="d1">The limitations and future directions section discusses the constraints and potential future work for LATS</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="LANGUAGE MODELS">
      <data key="d0">AGENT</data>
      <data key="d1">Language models (LMs) are AI models capable of understanding and generating human language, used in various decision-making tasks
Language models are used for reasoning, acting, and planning in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="PROMPTING TECHNIQUES">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Prompting techniques are methods used to guide language models in generating responses, such as ReAct and Reflexion
Prompting Techniques are methods used for improving the performance of models through effective prompting</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="TRAJECTORY CONSTRUCTION">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Trajectory construction is the process of creating paths or sequences of actions in LATS to improve decision-making</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="INTERACTION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Interaction refers to the communication between language models and their environment or users, a component in LATS
Interaction refers to the engagement between agents and their environments in real-world applications</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479,faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="AUTONOMOUS DECISION-MAKING">
      <data key="d0">CONCEPT</data>
      <data key="d1">Autonomous decision-making is the ability of language models to make decisions without human intervention, enabled by LATS</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="COMPUTATIONAL BUDGET">
      <data key="d0">CONCEPT</data>
      <data key="d1">Computational budget refers to the limit on computational resources available for a task, relevant to the efficiency of LATS</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="GROUND-TRUTH FEEDBACK">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Ground-truth feedback is accurate information used to guide and improve the performance of language models</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="SYSTEM-1 LM APPROACHES">
      <data key="d0">CONCEPT</data>
      <data key="d1">System-1 LM approaches refer to simpler, more intuitive language model techniques that do not involve complex reasoning or planning</data>
      <data key="d2">faa2bd677c7f052136479e0175da3e5b</data>
    </node>
    <node id="DANIEL CAMPOS">
      <data key="d0">PERSON</data>
      <data key="d1">Daniel Campos provided useful feedback on earlier versions of the paper.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="NSF GRANT 2106825">
      <data key="d0">FUNDING</data>
      <data key="d1">A grant from the National Science Foundation that supported the work.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="NIFA AWARD 2020-67021-32799">
      <data key="d0">FUNDING</data>
      <data key="d1">An award from the National Institute of Food and Agriculture that supported the work.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="JUMP ARCHES ENDOWMENT">
      <data key="d0">FUNDING</data>
      <data key="d1">An endowment through the Health Care Engineering Systems Center at Illinois and the OSF Foundation that supported the work.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="IBM-ILLINOIS DISCOVERY ACCELERATOR INSTITUTE">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">An institute that supported the work.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="NVIDIA GPUS">
      <data key="d0">HARDWARE</data>
      <data key="d1">NVIDIA GPUs were used at NCSA Delta through allocations from the ACCESS program.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="NCSA DELTA">
      <data key="d0">FACILITY</data>
      <data key="d1">A facility where NVIDIA GPUs were used for the work.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="ACCESS PROGRAM">
      <data key="d0">PROGRAM</data>
      <data key="d1">A program that provided allocations for using NVIDIA GPUs at NCSA Delta.</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="MICHAEL AHN">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Ahn is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="ANTHONY BROHAN">
      <data key="d0">PERSON</data>
      <data key="d1">Anthony Brohan is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NOAH BROWN">
      <data key="d0">PERSON</data>
      <data key="d1">Noah Brown is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."
Noah Brown is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YEVGEN CHEBOTAR">
      <data key="d0">PERSON</data>
      <data key="d1">Yevgen Chebotar is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."
Yevgen Chebotar is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="OMAR CORTES">
      <data key="d0">PERSON</data>
      <data key="d1">Omar Cortes is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BYRON DAVID">
      <data key="d0">PERSON</data>
      <data key="d1">Byron David is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHELSEA FINN">
      <data key="d0">PERSON</data>
      <data key="d1">Chelsea Finn is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."
Chelsea Finn is an author of the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHUYUAN FU">
      <data key="d0">PERSON</data>
      <data key="d1">Chuyuan Fu is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."
Chuyuan Fu is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KEERTHANA GOPALAKRISHNAN">
      <data key="d0">PERSON</data>
      <data key="d1">Keerthana Gopalakrishnan is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KAROL HAUSMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Karol Hausman is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."
Karol Hausman is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALEX HERZOG">
      <data key="d0">PERSON</data>
      <data key="d1">Alex Herzog is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DANIEL HO">
      <data key="d0">PERSON</data>
      <data key="d1">Daniel Ho is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JASMINE HSU">
      <data key="d0">PERSON</data>
      <data key="d1">Jasmine Hsu is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JULIAN IBARZ">
      <data key="d0">PERSON</data>
      <data key="d1">Julian Ibarz is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BRIAN ICHTER">
      <data key="d0">PERSON</data>
      <data key="d1">Brian Ichter is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."
Brian Ichter is an author of the paper "Inner monologue: Embodied reasoning through planning withBrian Ichter is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALEX IRPAN">
      <data key="d0">PERSON</data>
      <data key="d1">Alex Irpan is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ROSARIO JAUREGUI RUANO">
      <data key="d0">PERSON</data>
      <data key="d1">Rosario Jauregui Ruano is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KYLE JEFFREY">
      <data key="d0">PERSON</data>
      <data key="d1">Kyle Jeffrey is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SALLY JESMONTH">
      <data key="d0">PERSON</data>
      <data key="d1">Sally Jesmonth is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NIKHIL J JOSHI">
      <data key="d0">PERSON</data>
      <data key="d1">Nikhil J Joshi is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RYAN JULIAN">
      <data key="d0">PERSON</data>
      <data key="d1">Ryan Julian is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DMITRY KALASHNIKOV">
      <data key="d0">PERSON</data>
      <data key="d1">Dmitry Kalashnikov is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUHENG KUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yuheng Kuang is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KUANG-HUEI LEE">
      <data key="d0">PERSON</data>
      <data key="d1">Kuang-Huei Lee is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."
Kuang-Huei Lee is an author of the paper "Multimodal web navigation with instruction-finetuned foundation models"
Kuang-Huei Lee is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SERGEY LEVINE">
      <data key="d0">PERSON</data>
      <data key="d1">Sergey Levine is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."
Sergey Levine is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"
Sergey Levine is an author of the paper "The false promise of imitating proprietary llms"</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YAO LU">
      <data key="d0">PERSON</data>
      <data key="d1">Yao Lu is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LINDA LUU">
      <data key="d0">PERSON</data>
      <data key="d1">Linda Luu is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."
Linda Luu is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CAROLINA PARADA">
      <data key="d0">PERSON</data>
      <data key="d1">Carolina Parada is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PETER PASTOR">
      <data key="d0">PERSON</data>
      <data key="d1">Peter Pastor is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JORNELL QUIAMBAO">
      <data key="d0">PERSON</data>
      <data key="d1">Jornell Quiambao is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KANISHKA RAO">
      <data key="d0">PERSON</data>
      <data key="d1">Kanishka Rao is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAREK RETTINGHOUSE">
      <data key="d0">PERSON</data>
      <data key="d1">Jarek Rettinghouse is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DIEGO REYES">
      <data key="d0">PERSON</data>
      <data key="d1">Diego Reyes is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PIERRE SERMANET">
      <data key="d0">PERSON</data>
      <data key="d1">Pierre Sermanet is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."
Pierre Sermanet is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NICOLAS SIEVERS">
      <data key="d0">PERSON</data>
      <data key="d1">Nicolas Sievers is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CLAYTON TAN">
      <data key="d0">PERSON</data>
      <data key="d1">Clayton Tan is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALEXANDER TOSHEV">
      <data key="d0">PERSON</data>
      <data key="d1">Alexander Toshev is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VINCENT VANHOUCKE">
      <data key="d0">PERSON</data>
      <data key="d1">Vincent Vanhoucke is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FEI XIA">
      <data key="d0">PERSON</data>
      <data key="d1">Fei Xia is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."
Fei Xia is an author of the paper "PaLM-E: An embodied multimodal language model" presented at ICML in 2023
Fei Xia is an author of the paper "Chain-of-thought prompting elicits reasoning in large language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TED XIAO">
      <data key="d0">PERSON</data>
      <data key="d1">Ted Xiao is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."
Ted Xiao is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PENG XU">
      <data key="d0">PERSON</data>
      <data key="d1">Peng Xu is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SICHUN XU">
      <data key="d0">PERSON</data>
      <data key="d1">Sichun Xu is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MENGYUAN YAN">
      <data key="d0">PERSON</data>
      <data key="d1">Mengyuan Yan is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANDY ZENG">
      <data key="d0">PERSON</data>
      <data key="d1">Andy Zeng is an author of the paper "Do as I can, not as I say: Grounding language in robotic affordances."
Andy Zeng is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JACOB AUSTIN">
      <data key="d0">PERSON</data>
      <data key="d1">Jacob Austin is an author of the paper "Program synthesis with large language models."
Jacob Austin is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AUGUSTUS ODENA">
      <data key="d0">PERSON</data>
      <data key="d1">Augustus Odena is an author of the paper "Program synthesis with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MAXWELL NYE">
      <data key="d0">PERSON</data>
      <data key="d1">Maxwell Nye is an author of the paper "Program synthesis with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MAARTEN BOSMA">
      <data key="d0">PERSON</data>
      <data key="d1">Maarten Bosma is an author of the paper "Program synthesis with large language models."
Maarten Bosma is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023
Maarten Bosma is an author mentioned in the text
Maarten Bosma is an author of the paper "Chain-of-thought prompting elicits reasoning in large language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HENRYK MICHALEWSKI">
      <data key="d0">PERSON</data>
      <data key="d1">Henryk Michalewski is an author of the paper "Program synthesis with large language models."
Henryk Michalewski is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAVID DOHAN">
      <data key="d0">PERSON</data>
      <data key="d1">David Dohan is an author of the paper "Program synthesis with large language models."
David Dohan is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ELLEN JIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Ellen Jiang is an author of the paper "Program synthesis with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CARRIE CAI">
      <data key="d0">PERSON</data>
      <data key="d1">Carrie Cai is an author of the paper "Program synthesis with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MICHAEL TERRY">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Terry is an author of the paper "Program synthesis with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QUOC LE">
      <data key="d0">PERSON</data>
      <data key="d1">Quoc Le is an author of the paper "Program synthesis with large language models."
Quoc Le is an author mentioned in the text
Quoc Le is an author of the paper "Least-to-most prompting enables complex reasoning in large language models"</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,4ae237a491bc8a84cc720e40c59a7464,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHARLES SUTTON">
      <data key="d0">PERSON</data>
      <data key="d1">Charles Sutton is an author of the paper "Program synthesis with large language models."
Charles Sutton is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BOWEN BAKER">
      <data key="d0">PERSON</data>
      <data key="d1">Bowen Baker is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ILGE AKKAYA">
      <data key="d0">PERSON</data>
      <data key="d1">Ilge Akkaya is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PETER ZHOKHOV">
      <data key="d0">PERSON</data>
      <data key="d1">Peter Zhokhov is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOOST HUIZINGA">
      <data key="d0">PERSON</data>
      <data key="d1">Joost Huizinga is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIE TANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jie Tang is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ADRIEN ECOFFET">
      <data key="d0">PERSON</data>
      <data key="d1">Adrien Ecoffet is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."
Adrien Ecoffet is an author of the paper "Open questions in creating safe open-ended AI: Tensions between control and creativity"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BRANDON HOUGHTON">
      <data key="d0">PERSON</data>
      <data key="d1">Brandon Houghton is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."
Brandon Houghton is an author of the paper "MineRL: A large-scale dataset of Minecraft demonstrations"</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RAUL SAMPEDRO">
      <data key="d0">PERSON</data>
      <data key="d1">Raul Sampedro is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JEFF CLUNE">
      <data key="d0">PERSON</data>
      <data key="d1">Jeff Clune is an author of the paper "Video pretraining (VPT): Learning to act by watching unlabeled online videos."
Jeff Clune is a researcher involved in the study of Automated Design of Agentic Systems and is affiliated with the University of British Columbia, the Vector Institute, and holds a Canada CIFAR AI Chair
Jeff Clune is the author of the paper "Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artificial intelligence"Jeff Clune is an author of the paper "Open questions in creating safe open-ended AI: Tensions between control and creativity"
Jeff Clune is an author of the paper "Thought Cloning: Learning to think while acting by imitating human thinking"
Jeff Clune is an author of the paper "Intelligent go-explore: Standing on the shoulders of giant foundation models"
Jeff Clune is an author of the paper "Designing neural networks through neuroevolution"
Jeff Clune is an author of the papers "Poet: open-ended coevolution of environments and their optimized solutions" and "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions"Jeff Clune is an author of the paper "OMNI: Open-endedness via models of human notions of interestingness"
Jeff Clune is an author of the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,1b1399c76420a477c0c97893d258ae69,2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,4ae237a491bc8a84cc720e40c59a7464,6109537356a2ce2339f77c827aa3668e,c3d0436082aada237ee4bee645f16059,cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MACIEJ BESTA">
      <data key="d0">PERSON</data>
      <data key="d1">Maciej Besta is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NILS BLACH">
      <data key="d0">PERSON</data>
      <data key="d1">Nils Blach is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALES KUBICEK">
      <data key="d0">PERSON</data>
      <data key="d1">Ales Kubicek is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ROBERT GERSTENBERGER">
      <data key="d0">PERSON</data>
      <data key="d1">Robert Gerstenberger is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LUKAS GIANINAZZI">
      <data key="d0">PERSON</data>
      <data key="d1">Lukas Gianinazzi is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOANNA GAJDA">
      <data key="d0">PERSON</data>
      <data key="d1">Joanna Gajda is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TOMASZ LEHMANN">
      <data key="d0">PERSON</data>
      <data key="d1">Tomasz Lehmann is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MICHAL PODSTAWSKI">
      <data key="d0">PERSON</data>
      <data key="d1">Michal Podstawski is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HUBERT NIEWIADOMSKI">
      <data key="d0">PERSON</data>
      <data key="d1">Hubert Niewiadomski is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PIOTR NYCZYK">
      <data key="d0">PERSON</data>
      <data key="d1">Piotr Nyczyk is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TORSTEN HOEFLER">
      <data key="d0">PERSON</data>
      <data key="d1">Torsten Hoefler is an author of the paper "Graph of thoughts: Solving elaborate problems with large language models."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAMUEL R BOWMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Samuel R Bowman is an author of the paper "A large annotated corpus for learning natural language inference."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GABOR ANGELI">
      <data key="d0">PERSON</data>
      <data key="d1">Gabor Angeli is an author of the paper "A large annotated corpus for learning natural language inference."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHRISTOPHER POTTS">
      <data key="d0">PERSON</data>
      <data key="d1">Christopher Potts is an author of the paper "A large annotated corpus for learning natural language inference."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHRISTOPHER D MANNING">
      <data key="d0">PERSON</data>
      <data key="d1">Christopher D Manning is an author of the paper "A large annotated corpus for learning natural language inference."
Christopher D Manning is an author mentioned in the text
Christopher D Manning is an author of the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,4ae237a491bc8a84cc720e40c59a7464,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TOM B. BROWN">
      <data key="d0">PERSON</data>
      <data key="d1">Tom B. Brown is an author of the paper "Language models are few-shot learners."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BENJAMIN MANN">
      <data key="d0">PERSON</data>
      <data key="d1">Benjamin Mann is an author of the paper "Language models are few-shot learners."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NICK RYDER">
      <data key="d0">PERSON</data>
      <data key="d1">Nick Ryder is an author of the paper "Language models are few-shot learners."
Nick Ryder is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MELANIE SUBBIAH">
      <data key="d0">PERSON</data>
      <data key="d1">Melanie Subbiah is an author of the paper "Language models are few-shot learners."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="JARED KAPLAN">
      <data key="d0">PERSON</data>
      <data key="d1">Jared Kaplan is an author of the paper "Language models are few-shot learners."
Jared Kaplan is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021
An author of the paper "Evaluating Large Language Models Trained on Code"</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126,7de66b94cf868b37b1df51dc545c415f</data>
    </node>
    <node id="PRAFULLA DHARIWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Prafulla Dhariwal is an author of the paper "Language models are few-shot learners."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="ARVIND NEELAKANTAN">
      <data key="d0">PERSON</data>
      <data key="d1">Arvind Neelakantan is an author of the paper "Language models are few-shot learners."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="PRANAV SHYAM">
      <data key="d0">PERSON</data>
      <data key="d1">Pranav Shyam is an author of the paper "Language models are few-shot learners."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="GIRISH SASTRY">
      <data key="d0">PERSON</data>
      <data key="d1">Girish Sastry is an author of the paper "Language models are few-shot learners."
Girish Sastry is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="AMANDA ASKELL">
      <data key="d0">PERSON</data>
      <data key="d1">Amanda Askell is an author of the paper "Language models are few-shot learners."
An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7de66b94cf868b37b1df51dc545c415f</data>
    </node>
    <node id="SANDHINI AGARWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Sandhini Agarwal is an author of the paper "Language models are few-shot learners."</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464</data>
    </node>
    <node id="ARIEL HERBERT-VOSS">
      <data key="d0">PERSON</data>
      <data key="d1">Ariel Herbert-Voss is an author of the paper "Language models are few-shot learners."
Ariel Herbert-Voss is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="GRETCHEN KRUEGER">
      <data key="d0">PERSON</data>
      <data key="d1">Gretchen Krueger is an author of the paper "Language models are few-shot learners."
Gretchen Krueger is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">4ae237a491bc8a84cc720e40c59a7464,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="AMODEI">
      <data key="d0">PERSON</data>
      <data key="d1">Amodei is an author of the paper "Language models are few-shot learners" presented at NeurIPS in 2020</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="NEURIPS">
      <data key="d0">CONFERENCE</data>
      <data key="d1">NeurIPS is a conference where the paper "Language models are few-shot learners" was presented in 2020
NeurIPS is the conference where the paper "Learning universal policies via text-guided video generation" was presentedNeurIPS is the conference where the paper "Large language models are zero-shot reasoners" was presented
NeurIPS is the conference where the paper "LLaMA: Open and efficient foundation language models" was presentedNeurIPS is the conference where the paper "AdaPlanner: Adaptive planning from feedback with language models" was presentedNeurIPS is the conference where the paper "Self-refine: Iterative refinement with self-feedback" was presented
NeurIPS is a conference where some of the mentioned papers were presented</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,68e5573b596d253a03047b1e41988598,7a48515e86161237c03c9a8373197126,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="MURRAY CAMPBELL">
      <data key="d0">PERSON</data>
      <data key="d1">Murray Campbell is an author of the paper "Deep blue" published in Artificial Intelligence in 2002</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="A JOSEPH HOANE JR">
      <data key="d0">PERSON</data>
      <data key="d1">A Joseph Hoane Jr is an author of the paper "Deep blue" published in Artificial Intelligence in 2002</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="FENG-HSIUNG HSU">
      <data key="d0">PERSON</data>
      <data key="d1">Feng-hsiung Hsu is an author of the paper "Deep blue" published in Artificial Intelligence in 2002</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ARTIFICIAL INTELLIGENCE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Artificial Intelligence is the journal where the paper "Deep blue" was published in 2002</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="BEI CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Bei Chen is an author of the paper "CodeT: Code generation with generated tests" presented at ICLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="FENGJI ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Fengji Zhang is an author of the paper "CodeT: Code generation with generated tests" presented at ICLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ANH NGUYEN">
      <data key="d0">PERSON</data>
      <data key="d1">Anh Nguyen is an author of the paper "CodeT: Code generation with generated tests" presented at ICLR in 2023
An author of the Phi-3 technical reportAnh Nguyen is an author of the Phi-3 technical report</data>
      <data key="d2">7a48515e86161237c03c9a8373197126,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="DAOGUANG ZAN">
      <data key="d0">PERSON</data>
      <data key="d1">Daoguang Zan is an author of the paper "CodeT: Code generation with generated tests" presented at ICLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ZEQI LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Zeqi Lin is an author of the paper "CodeT: Code generation with generated tests" presented at ICLR in 2023
An author of the Phi-3 technical reportZeqi Lin is an author of the Phi-3 technical report</data>
      <data key="d2">7a48515e86161237c03c9a8373197126,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="JIAN-GUANG LOU">
      <data key="d0">PERSON</data>
      <data key="d1">Jian-Guang Lou is an author of the paper "CodeT: Code generation with generated tests" presented at ICLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="WEIZHU CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Weizhu Chen is an author of the paper "CodeT: Code generation with generated tests" presented at ICLR in 2023
An author of the Phi-3 technical reportWeizhu Chen is an author of the Phi-3 technical report
Weizhu Chen is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">7a48515e86161237c03c9a8373197126,dd9a46950237e49ef9b1c7ef08e08d42,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ICLR">
      <data key="d0">CONFERENCE</data>
      <data key="d1">ICLR is a conference where the paper "CodeT: Code generation with generated tests" was presented in 2023
ICLR is the conference where the paper "Reinforcement learning on web interfaces using workflow-guided exploration" was presentedICLR is the conference where the paper "Multimodal web navigation with instruction-finetuned foundation models" was presentedICLR is the conference where the paper "Large language models cannot self-correct reasoning yet" was presented
ICLR is the conference where the paper "ALFWorld: Aligning text and embodied environments for interactive learning" was presentedICLR is the conference where the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs" was presented
ICLR is a conference where some of the mentioned papers were presented</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,68e5573b596d253a03047b1e41988598,7a48515e86161237c03c9a8373197126,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="MARK CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Mark Chen is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021Mark Chen is an author of the paper "Training verifiers to solve math word problems" published on arXiv in 2021
An author of the paper "Evaluating Large Language Models Trained on Code"
Mark Chen is an author of the paper "Training verifiers to solve math word problems"
Mark Chen is an author of the paper "Training verifiers to solve math word problems"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126,7de66b94cf868b37b1df51dc545c415f,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JERRY TWOREK">
      <data key="d0">PERSON</data>
      <data key="d1">Jerry Tworek is an author of the paper "Training verifiers to solve math word problems" published on arXiv in 2021Jerry Tworek is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021
An author of the paper "Evaluating Large Language Models Trained on Code"
Jerry Tworek is an author of the paper "Training verifiers to solve math word problems"
Jerry Tworek is an author of the paper "Training verifiers to solve math word problems"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126,7de66b94cf868b37b1df51dc545c415f,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HEEWOO JUN">
      <data key="d0">PERSON</data>
      <data key="d1">Heewoo Jun is an author of the paper "Training verifiers to solve math word problems" published on arXiv in 2021Heewoo Jun is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021
An author of the paper "Evaluating Large Language Models Trained on Code"
Heewoo Jun is an author of the paper "Training verifiers to solve math word problems"
Heewoo Jun is an author of the paper "Training verifiers to solve math word problems"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126,7de66b94cf868b37b1df51dc545c415f,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QIMING YUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Qiming Yuan is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021
An author of the paper "Evaluating Large Language Models Trained on Code"</data>
      <data key="d2">7a48515e86161237c03c9a8373197126,7de66b94cf868b37b1df51dc545c415f</data>
    </node>
    <node id="HENRIQUE PONDE">
      <data key="d0">PERSON</data>
      <data key="d1">Henrique Ponde is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="HARRISON EDWARDS">
      <data key="d0">PERSON</data>
      <data key="d1">Harrison Edwards is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="YURA BURDA">
      <data key="d0">PERSON</data>
      <data key="d1">Yura Burda is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="NICHOLAS JOSEPH">
      <data key="d0">PERSON</data>
      <data key="d1">Nicholas Joseph is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021
An author of the paper "Evaluating Large Language Models Trained on Code"</data>
      <data key="d2">7a48515e86161237c03c9a8373197126,7de66b94cf868b37b1df51dc545c415f</data>
    </node>
    <node id="GREG BROCKMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Greg Brockman is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021
An author of the paper "Evaluating Large Language Models Trained on Code"</data>
      <data key="d2">7a48515e86161237c03c9a8373197126,7de66b94cf868b37b1df51dc545c415f</data>
    </node>
    <node id="ALEX RAY">
      <data key="d0">PERSON</data>
      <data key="d1">Alex Ray is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="RAUL PURI">
      <data key="d0">PERSON</data>
      <data key="d1">Raul Puri is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MICHAEL PETROV">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Petrov is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="HEIDY KHLAAF">
      <data key="d0">PERSON</data>
      <data key="d1">Heidy Khlaaf is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="PAMELA MISHKIN">
      <data key="d0">PERSON</data>
      <data key="d1">Pamela Mishkin is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="BROOKE CHAN">
      <data key="d0">PERSON</data>
      <data key="d1">Brooke Chan is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SCOTT GRAY">
      <data key="d0">PERSON</data>
      <data key="d1">Scott Gray is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MIKHAIL PAVLOV">
      <data key="d0">PERSON</data>
      <data key="d1">Mikhail Pavlov is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ALETHEA POWER">
      <data key="d0">PERSON</data>
      <data key="d1">Alethea Power is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="LUKASZ KAISER">
      <data key="d0">PERSON</data>
      <data key="d1">Lukasz Kaiser is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021Lukasz Kaiser is an author of the paper "Training verifiers to solve math word problems" published on arXiv in 2021
Lukasz Kaiser is an author of the paper "Training verifiers to solve math word problems"
Lukasz Kaiser is an author of the paper "Training verifiers to solve math word problems"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MOHAMMAD BAVARIAN">
      <data key="d0">PERSON</data>
      <data key="d1">Mohammad Bavarian is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021Mohammad Bavarian is an author of the paper "Training verifiers to solve math word problems" published on arXiv in 2021
Mohammad Bavarian is an author of the paper "Training verifiers to solve math word problems"
Mohammad Bavarian is an author of the paper "Training verifiers to solve math word problems"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CLEMENS WINTER">
      <data key="d0">PERSON</data>
      <data key="d1">Clemens Winter is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="PHILIPPE TILLET">
      <data key="d0">PERSON</data>
      <data key="d1">Philippe Tillet is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="FELIPE PETROSKI SUCH">
      <data key="d0">PERSON</data>
      <data key="d1">Felipe Petroski Such is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="DAVID W. CUMMINGS">
      <data key="d0">PERSON</data>
      <data key="d1">David W. Cummings is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MATTHIAS PLAPPERT">
      <data key="d0">PERSON</data>
      <data key="d1">Matthias Plappert is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021Matthias Plappert is an author of the paper "Training verifiers to solve math word problems" published on arXiv in 2021
Matthias Plappert is an author of the paper "Training verifiers to solve math word problems"
Matthias Plappert is an author of the paper "Training verifiers to solve math word problems"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FOTIOS CHANTZIS">
      <data key="d0">PERSON</data>
      <data key="d1">Fotios Chantzis is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ELIZABETH BARNES">
      <data key="d0">PERSON</data>
      <data key="d1">Elizabeth Barnes is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="WILLIAM H. GUSS">
      <data key="d0">PERSON</data>
      <data key="d1">William H. Guss is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021
William H. Guss is an author of the paper "MineRL: A large-scale dataset of Minecraft demonstrations"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ALEX NICHOL">
      <data key="d0">PERSON</data>
      <data key="d1">Alex Nichol is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="IGOR BABUSCHKIN">
      <data key="d0">PERSON</data>
      <data key="d1">Igor Babuschkin is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SUCHIR BALAJI">
      <data key="d0">PERSON</data>
      <data key="d1">Suchir Balaji is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021
Suchir Balaji is an author of the paper "Webgpt: Browser-assisted question-answering with human feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SHANTANU JAIN">
      <data key="d0">PERSON</data>
      <data key="d1">Shantanu Jain is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021
Shantanu Jain is an author of the paper "Webgpt: Browser-assisted question-answering with human feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ANDREW CARR">
      <data key="d0">PERSON</data>
      <data key="d1">Andrew Carr is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="JAN LEIKE">
      <data key="d0">PERSON</data>
      <data key="d1">Jan Leike is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="JOSHUA ACHIAM">
      <data key="d0">PERSON</data>
      <data key="d1">Joshua Achiam is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="VEDANT MISRA">
      <data key="d0">PERSON</data>
      <data key="d1">Vedant Misra is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021Vedant Misra is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="EVAN MORIKAWA">
      <data key="d0">PERSON</data>
      <data key="d1">Evan Morikawa is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ALEC RADFORD">
      <data key="d0">PERSON</data>
      <data key="d1">Alec Radford is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MATTHEW M. KNIGHT">
      <data key="d0">PERSON</data>
      <data key="d1">Matthew M. Knight is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MILES BRUNDAGE">
      <data key="d0">PERSON</data>
      <data key="d1">Miles Brundage is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MIRA MURATI">
      <data key="d0">PERSON</data>
      <data key="d1">Mira Murati is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="KATIE MAYER">
      <data key="d0">PERSON</data>
      <data key="d1">Katie Mayer is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="PETER WELINDER">
      <data key="d0">PERSON</data>
      <data key="d1">Peter Welinder is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="BOB MCGREW">
      <data key="d0">PERSON</data>
      <data key="d1">Bob McGrew is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="DARIO AMODEI">
      <data key="d0">PERSON</data>
      <data key="d1">Dario Amodei is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SAM MCCANDLISH">
      <data key="d0">PERSON</data>
      <data key="d1">Sam McCandlish is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ILYA SUTSKEVER">
      <data key="d0">PERSON</data>
      <data key="d1">Ilya Sutskever is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021
Ilya Sutskever is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Ilya Sutskever is an author of the paper "Mastering the game of Go with deep neural networks and tree search"
Ilya Sutskever is an author of the paper "RL^2: Fast reinforcement learning via slow reinforcement learning"
Ilya Sutskever is an author of the paper "Imagenet classification with deep convolutional neural networks"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,2d4672dfb7bd4283f0b5f23ab4f26653,6109537356a2ce2339f77c827aa3668e,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="WOJCIECH ZAREMBA">
      <data key="d0">PERSON</data>
      <data key="d1">Wojciech Zaremba is an author of the paper "Evaluating large language models trained on code" published on arXiv in 2021</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="WENHU CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Wenhu Chen is an author of the paper "Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks" published in TMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="XUEGUANG MA">
      <data key="d0">PERSON</data>
      <data key="d1">Xueguang Ma is an author of the paper "Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks" published in TMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="XINYI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Xinyi Wang is an author of the paper "Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks" published in TMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="WILLIAM W. COHEN">
      <data key="d0">PERSON</data>
      <data key="d1">William W. Cohen is an author of the paper "Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks" published in TMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="TMLR">
      <data key="d0">PUBLICATION</data>
      <data key="d1">TMLR is the journal where the paper "Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks" was published in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="AAKANKSHA CHOWDHERY">
      <data key="d0">PERSON</data>
      <data key="d1">Aakanksha Chowdhery is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023Aakanksha Chowdhery is an author of the paper "
Aakanksha Chowdhery is an author of the paper "Self-consistency improves chain of thought reasoning in language models"
Aakanksha Chowdhery is an author of the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,7a48515e86161237c03c9a8373197126,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHARAN NARANG">
      <data key="d0">PERSON</data>
      <data key="d1">Sharan Narang is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023
Sharan Narang is an author mentioned in the text
Sharan Narang is an author of the paper "Self-consistency improves chain of thought reasoning in language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,7a48515e86161237c03c9a8373197126,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="JACOB DEVLIN">
      <data key="d0">PERSON</data>
      <data key="d1">Jacob Devlin is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="GAURAV MISHRA">
      <data key="d0">PERSON</data>
      <data key="d1">Gaurav Mishra is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ADAM ROBERTS">
      <data key="d0">PERSON</data>
      <data key="d1">Adam Roberts is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="PAUL BARHAM">
      <data key="d0">PERSON</data>
      <data key="d1">Paul Barham is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="HYUNG WON CHUNG">
      <data key="d0">PERSON</data>
      <data key="d1">Hyung Won Chung is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023
Hyung Won Chung is an author of the paper "Language models are multilingual chain-of-thought reasoners"
Hyung Won Chung is an author of the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,7a48515e86161237c03c9a8373197126,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="SEBASTIAN GEHRMANN">
      <data key="d0">PERSON</data>
      <data key="d1">Sebastian Gehrmann is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023
Sebastian Gehrmann is an author of the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d2">7a48515e86161237c03c9a8373197126,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JOSHUA MAYNEZ">
      <data key="d0">PERSON</data>
      <data key="d1">Joshua Maynez is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ABHISHEK RAO">
      <data key="d0">PERSON</data>
      <data key="d1">Abhishek Rao is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="PARKER BARNES">
      <data key="d0">PERSON</data>
      <data key="d1">Parker Barnes is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="YI TAY">
      <data key="d0">PERSON</data>
      <data key="d1">Yi Tay is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023
Yi Tay is an author of the paper "Language models are multilingual chain-of-thought reasoners"
Yi Tay is an author of the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,7a48515e86161237c03c9a8373197126,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="NOAM SHAZEER">
      <data key="d0">PERSON</data>
      <data key="d1">Noam Shazeer is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="VINODKUMAR PRABHAKARAN">
      <data key="d0">PERSON</data>
      <data key="d1">Vinodkumar Prabhakaran is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="EMILY REIF">
      <data key="d0">PERSON</data>
      <data key="d1">Emily Reif is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="NAN DU">
      <data key="d0">PERSON</data>
      <data key="d1">Nan Du is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023
Nan Du is an author of the paper "React: Synergizing reasoning and acting in language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="BEN HUTCHINSON">
      <data key="d0">PERSON</data>
      <data key="d1">Ben Hutchinson is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="REINER POPE">
      <data key="d0">PERSON</data>
      <data key="d1">Reiner Pope is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="JAMES BRADBURY">
      <data key="d0">PERSON</data>
      <data key="d1">James Bradbury is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MICHAEL ISARD">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Isard is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="GUY GUR-ARI">
      <data key="d0">PERSON</data>
      <data key="d1">Guy Gur-Ari is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="PENGCHENG YIN">
      <data key="d0">PERSON</data>
      <data key="d1">Pengcheng Yin is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="TOJU DUKE">
      <data key="d0">PERSON</data>
      <data key="d1">Toju Duke is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ANSELM LEVSKAYA">
      <data key="d0">PERSON</data>
      <data key="d1">Anselm Levskaya is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SANJAY GHEMAWAT">
      <data key="d0">PERSON</data>
      <data key="d1">Sanjay Ghemawat is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SUNIPA DEV">
      <data key="d0">PERSON</data>
      <data key="d1">Sunipa Dev is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="XAVIER GARCIA">
      <data key="d0">PERSON</data>
      <data key="d1">Xavier Garcia is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="KEVIN ROBINSON">
      <data key="d0">PERSON</data>
      <data key="d1">Kevin Robinson is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="LIAM FEDUS">
      <data key="d0">PERSON</data>
      <data key="d1">Liam Fedus is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="DENNY ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Denny Zhou is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023
Denny Zhou is an author of the paper "Large language models cannot self-correct reasoning yet"
Denny Zhou is an author mentioned in the text
Denny Zhou is an author of the paper "Language models are multilingual chain-of-thought reasoners"
Denny Zhou is an author of the papers "Self-consistency improves chain of thought reasoning in language models" and "Chain-of-thought prompting elicits reasoning in large language models"Denny Zhou is an author of the paper "Self-consistency improves chain of thought reasoning in language models"
Denny Zhou is an author of the paper "Take a step back: Evoking reasoning via abstraction in large language models"
Denny Zhou is an author of the paper "Instruction-following evaluation for large language models"Denny Zhou is an author of the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,68e5573b596d253a03047b1e41988598,7a48515e86161237c03c9a8373197126,8180bf20b7577f3eee40df5991e2886d,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="DAPHNE IPPOLITO">
      <data key="d0">PERSON</data>
      <data key="d1">Daphne Ippolito is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="DAVID LUAN">
      <data key="d0">PERSON</data>
      <data key="d1">David Luan is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="HYEONTAEK LIM">
      <data key="d0">PERSON</data>
      <data key="d1">Hyeontaek Lim is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="BARRET ZOPH">
      <data key="d0">PERSON</data>
      <data key="d1">Barret Zoph is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ALEXANDER SPIRIDONOV">
      <data key="d0">PERSON</data>
      <data key="d1">Alexander Spiridonov is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="RYAN SEPASSI">
      <data key="d0">PERSON</data>
      <data key="d1">Ryan Sepassi is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SHIVANI AGRAWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Shivani Agrawal is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MARK OMERNICK">
      <data key="d0">PERSON</data>
      <data key="d1">Mark Omernick is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ANDREW M. DAI">
      <data key="d0">PERSON</data>
      <data key="d1">Andrew M. Dai is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="THANUMALAYAN SANKARANARAYANA PILLAI">
      <data key="d0">PERSON</data>
      <data key="d1">Thanumalayan Sankaranarayana Pillai is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MARIE PELLAT">
      <data key="d0">PERSON</data>
      <data key="d1">Marie Pellat is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="AITOR LEWKOWYCZ">
      <data key="d0">PERSON</data>
      <data key="d1">Aitor Lewkowycz is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ERICA MOREIRA">
      <data key="d0">PERSON</data>
      <data key="d1">Erica Moreira is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="REWON CHILD">
      <data key="d0">PERSON</data>
      <data key="d1">Rewon Child is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="OLEKSANDR POLOZOV">
      <data key="d0">PERSON</data>
      <data key="d1">Oleksandr Polozov is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="KATHERINE LEE">
      <data key="d0">PERSON</data>
      <data key="d1">Katherine Lee is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ZONGWEI ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Zongwei Zhou is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="XUEZHI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Xuezhi Wang is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023
Xuezhi Wang is an author mentioned in the text
Xuezhi Wang is an author of the paper "Language models are multilingual chain-of-thought reasoners"
Xuezhi Wang is an author of the papers "Self-consistency improves chain of thought reasoning in language models" and "Chain-of-thought prompting elicits reasoning in large language models"Xuezhi Wang is an author of the paper "Self-consistency improves chain of thought reasoning in language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,7a48515e86161237c03c9a8373197126,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="BRENNAN SAETA">
      <data key="d0">PERSON</data>
      <data key="d1">Brennan Saeta is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MARK DIAZ">
      <data key="d0">PERSON</data>
      <data key="d1">Mark Diaz is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="ORHAN FIRAT">
      <data key="d0">PERSON</data>
      <data key="d1">Orhan Firat is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MICHELE CATASTA">
      <data key="d0">PERSON</data>
      <data key="d1">Michele Catasta is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="JASON WEI">
      <data key="d0">PERSON</data>
      <data key="d1">Jason Wei is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023
Jason Wei is an author mentioned in the text
Jason Wei is an author of the paper "Language models are multilingual chain-of-thought reasoners"
Jason Wei is an author of the paper "Self-consistency improves chain of thought reasoning in language models"Jason Wei is an author of the papers "Self-consistency improves chain of thought reasoning in language models" and "Chain-of-thought prompting elicits reasoning in large language models"
Jason Wei is an author of the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,7a48515e86161237c03c9a8373197126,8180bf20b7577f3eee40df5991e2886d,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="KATHY MEIER-HELLSTERN">
      <data key="d0">PERSON</data>
      <data key="d1">Kathy Meier-Hellstern is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="DOUGLAS ECK">
      <data key="d0">PERSON</data>
      <data key="d1">Douglas Eck is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="JEFF DEAN">
      <data key="d0">PERSON</data>
      <data key="d1">Jeff Dean is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SLAV PETROV">
      <data key="d0">PERSON</data>
      <data key="d1">Slav Petrov is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="NOAH FIEDEL">
      <data key="d0">PERSON</data>
      <data key="d1">Noah Fiedel is an author of the paper "PaLM: Scaling language modeling with pathways" published in JMLR in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="JMLR">
      <data key="d0">PUBLICATION</data>
      <data key="d1">JMLR is the journal where the paper "PaLM: Scaling language modeling with pathways" was published in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="KARL COBBE">
      <data key="d0">PERSON</data>
      <data key="d1">Karl Cobbe is an author of the paper "Training verifiers to solve math word problems" published on arXiv in 2021
Karl Cobbe is an author of the paper "Training verifiers to solve math word problems"
Karl Cobbe is an author of the paper "Training verifiers to solve math word problems"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="VINEET KOSARAJU">
      <data key="d0">PERSON</data>
      <data key="d1">Vineet Kosaraju is an author of the paper "Training verifiers to solve math word problems" published on arXiv in 2021
Vineet Kosaraju is an author of the paper "Training verifiers to solve math word problems"
Vineet Kosaraju is an author of the paper "Webgpt: Browser-assisted question-answering with human feedback"
Vineet Kosaraju is an author of the paper "Training verifiers to solve math word problems"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,1b1399c76420a477c0c97893d258ae69,7a48515e86161237c03c9a8373197126,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="JACOB HILTON">
      <data key="d0">PERSON</data>
      <data key="d1">Jacob Hilton is an author of the paper "Training verifiers to solve math word problems" published on arXiv in 2021
Jacob Hilton is an author of the paper "Training verifiers to solve math word problems"
Jacob Hilton is an author of the paper "Webgpt: Browser-assisted question-answering with human feedback"
Jacob Hilton is an author of the paper "Training verifiers to solve math word problems"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,1b1399c76420a477c0c97893d258ae69,7a48515e86161237c03c9a8373197126,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="REIICHIRO NAKANO">
      <data key="d0">PERSON</data>
      <data key="d1">Reiichiro Nakano is an author of the paper "Training verifiers to solve math word problems" published on arXiv in 2021
Reiichiro Nakano is an author of the paper "Training verifiers to solve math word problems"
Reiichiro Nakano is an author of the paper "Webgpt: Browser-assisted question-answering with human feedback"
Reiichiro Nakano is an author of the paper "Training verifiers to solve math word problems"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,1b1399c76420a477c0c97893d258ae69,7a48515e86161237c03c9a8373197126,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CHRISTOPHER HESSE">
      <data key="d0">PERSON</data>
      <data key="d1">Christopher Hesse is an author of the paper "Training verifiers to solve math word problems" published on arXiv in 2021
Christopher Hesse is an author of the paper "Webgpt: Browser-assisted question-answering with human feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="JOHN SCHULMAN">
      <data key="d0">PERSON</data>
      <data key="d1">John Schulman is an author of the paper "Training verifiers to solve math word problems" published on arXiv in 2021
John Schulman is an author of the paper "RL^2: Fast reinforcement learning via slow reinforcement learning"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="XIANG DENG">
      <data key="d0">PERSON</data>
      <data key="d1">Xiang Deng is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="YU GU">
      <data key="d0">PERSON</data>
      <data key="d1">Yu Gu is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="BOYUAN ZHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Boyuan Zheng is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SHIJIE CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Shijie Chen is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="SAMUEL STEVENS">
      <data key="d0">PERSON</data>
      <data key="d1">Samuel Stevens is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="BOSHI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Boshi Wang is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="HUAN SUN">
      <data key="d0">PERSON</data>
      <data key="d1">Huan Sun is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="YU SU">
      <data key="d0">PERSON</data>
      <data key="d1">Yu Su is an author of the paper "Mind2Web: Towards a generalist agent for the web" presented at NeurIPS Datasets and Benchmarks Track in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="NEURIPS DATASETS AND BENCHMARKS TRACK">
      <data key="d0">CONFERENCE</data>
      <data key="d1">NeurIPS Datasets and Benchmarks Track is a conference where the paper "Mind2Web: Towards a generalist agent for the web" was presented in 2023
NeurIPS Datasets and Benchmarks Track is the conference where the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge" was presented</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="DANNY DRIESS">
      <data key="d0">PERSON</data>
      <data key="d1">Danny Driess is an author of the paper "PaLM-E: An embodied multimodal language model" presented at ICML in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MEHDI S. M. SAJJADI">
      <data key="d0">PERSON</data>
      <data key="d1">Mehdi S. M. Sajjadi is an author of the paper "PaLM-E: An embodied multimodal language model" presented at ICML in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="COREY LYNCH">
      <data key="d0">PERSON</data>
      <data key="d1">Corey Lynch is an author of the paper "PaLM-E: An embodied multimodal language model" presented at ICML in 2023</data>
      <data key="d2">7a48515e86161237c03c9a8373197126</data>
    </node>
    <node id="MENGJIAO YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Mengjiao Yang is an author of the paper "Learning universal policies via text-guided video generation"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="BO DAI">
      <data key="d0">PERSON</data>
      <data key="d1">Bo Dai is an author of the paper "Learning universal policies via text-guided video generation"
Bo Dai is an author of the paper "AdaPlanner: Adaptive planning from feedback with language models"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="HANJUN DAI">
      <data key="d0">PERSON</data>
      <data key="d1">Hanjun Dai is an author of the paper "Learning universal policies via text-guided video generation"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="OFIR NACHUM">
      <data key="d0">PERSON</data>
      <data key="d1">Ofir Nachum is an author of the paper "Multimodal web navigation with instruction-finetuned foundation models"Ofir Nachum is an author of the paper "Learning universal policies via text-guided video generation"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOSHUA B. TENENBAUM">
      <data key="d0">PERSON</data>
      <data key="d1">Joshua B. Tenenbaum is an author of the paper "Learning universal policies via text-guided video generation"
Joshua B. Tenenbaum is an author of the paper "Improving factuality and reasoning in language models through multiagent debate"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="DALE SCHUURMANS">
      <data key="d0">PERSON</data>
      <data key="d1">Dale Schuurmans is an author of the paper "Learning universal policies via text-guided video generation"
Dale Schuurmans is an author mentioned in the text
Dale Schuurmans is an author of the papers "Self-consistency improves chain of thought reasoning in language models" and "Chain-of-thought prompting elicits reasoning in large language models"Dale Schuurmans is an author of the paper "Self-consistency improves chain of thought reasoning in language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="PIETER ABBEEL">
      <data key="d0">PERSON</data>
      <data key="d1">Pieter Abbeel is an author of the paper "Learning universal policies via text-guided video generation"
Pieter Abbeel is an author mentioned in the text
An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"
Pieter Abbeel is an author of the paper "RL^2: Fast reinforcement learning via slow reinforcement learning"
Pieter Abbeel is an author of the paper "The false promise of imitating proprietary llms"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,68e5573b596d253a03047b1e41988598,7de66b94cf868b37b1df51dc545c415f,8180bf20b7577f3eee40df5991e2886d,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="JONATHAN ST BT EVANS">
      <data key="d0">PERSON</data>
      <data key="d1">Jonathan St BT Evans is the author of the paper "Intuition and reasoning: A dual-process perspective"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PSYCHOLOGICAL INQUIRY">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Psychological Inquiry is the journal where the paper "Intuition and reasoning: A dual-process perspective" was published</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="LINXI FAN">
      <data key="d0">PERSON</data>
      <data key="d1">Linxi Fan is an author of the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge"
Linxi Fan is an author mentioned in the text
Linxi Fan is an author of the paper "Eureka: Human-level reward design via coding large language models"
Linxi Fan is an author of the paper "Voyager: An open-ended embodied agent with large language models"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,34d0bb2211fc795fe1096442e086a2b3,68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="GUANZHI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Guanzhi Wang is an author of the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge"
Guanzhi Wang is an author mentioned in the text
Guanzhi Wang is an author of the paper "Eureka: Human-level reward design via coding large language models"
Guanzhi Wang is an author of the paper "Voyager: An open-ended embodied agent with large language models"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,34d0bb2211fc795fe1096442e086a2b3,68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YUNFAN JIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yunfan Jiang is an author of the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge"
Yunfan Jiang is an author mentioned in the text
Yunfan Jiang is an author of the paper "Voyager: An open-ended embodied agent with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="AJAY MANDLEKAR">
      <data key="d0">PERSON</data>
      <data key="d1">Ajay Mandlekar is an author of the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge"
Ajay Mandlekar is an author mentioned in the text
Ajay Mandlekar is an author of the paper "Voyager: An open-ended embodied agent with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YUNCONG YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yuncong Yang is an author of the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="HAOYI ZHU">
      <data key="d0">PERSON</data>
      <data key="d1">Haoyi Zhu is an author of the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="ANDREW TANG">
      <data key="d0">PERSON</data>
      <data key="d1">Andrew Tang is an author of the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="DE-AN HUANG">
      <data key="d0">PERSON</data>
      <data key="d1">De-An Huang is an author of the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge"
De-An Huang is an author of the paper "Eureka: Human-level reward design via coding large language models"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="YUKE ZHU">
      <data key="d0">PERSON</data>
      <data key="d1">Yuke Zhu is an author of the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge"
Yuke Zhu is an author mentioned in the text
Yuke Zhu is an author of the paper "Eureka: Human-level reward design via coding large language models"
Yuke Zhu is an author of the paper "Voyager: An open-ended embodied agent with large language models"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,34d0bb2211fc795fe1096442e086a2b3,68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ANIMA ANANDKUMAR">
      <data key="d0">PERSON</data>
      <data key="d1">Anima Anandkumar is an author of the paper "MineDojo: Building open-ended embodied agents with internet-scale knowledge"
Anima Anandkumar is an author mentioned in the text
Anima Anandkumar is an author of the paper "Eureka: Human-level reward design via coding large language models"
Anima Anandkumar is an author of the paper "Voyager: An open-ended embodied agent with large language models"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,34d0bb2211fc795fe1096442e086a2b3,68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="HIROKI FURUTA">
      <data key="d0">PERSON</data>
      <data key="d1">Hiroki Furuta is an author of the paper "Multimodal web navigation with instruction-finetuned foundation models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUTAKA MATSUO">
      <data key="d0">PERSON</data>
      <data key="d1">Yutaka Matsuo is an author of the paper "Large language models are zero-shot reasoners"Yutaka Matsuo is an author of the paper "Multimodal web navigation with instruction-finetuned foundation models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHIXIANG SHANE GU">
      <data key="d0">PERSON</data>
      <data key="d1">Shixiang Shane Gu is an author of the paper "Multimodal web navigation with instruction-finetuned foundation models"Shixiang Shane Gu is an author of the paper "Large language models are zero-shot reasoners"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IZZEDDIN GUR">
      <data key="d0">PERSON</data>
      <data key="d1">Izzeddin Gur is an author of the paper "Multimodal web navigation with instruction-finetuned foundation models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LUYU GAO">
      <data key="d0">PERSON</data>
      <data key="d1">Luyu Gao is an author of the paper "PAL: Program-aided language models"
Luyu Gao is an author of the paper "Self-refine: Iterative refinement with self-feedback"
Luyu Gao is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AMAN MADAAN">
      <data key="d0">PERSON</data>
      <data key="d1">Aman Madaan is an author of the paper "PAL: Program-aided language models"
Aman Madaan is an author of the paper "Self-refine: Iterative refinement with self-feedback"
Aman Madaan is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHUYAN ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Shuyan Zhou is an author of the paper "PAL: Program-aided language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="URI ALON">
      <data key="d0">PERSON</data>
      <data key="d1">Uri Alon is an author of the paper "PAL: Program-aided language models"
Uri Alon is an author of the paper "Self-refine: Iterative refinement with self-feedback"
Uri Alon is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PENGFEI LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Pengfei Liu is an author of the paper "PAL: Program-aided language models"
Pengfei Liu is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIMING YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yiming Yang is an author of the paper "PAL: Program-aided language models"
Yiming Yang is an author of the paper "Self-refine: Iterative refinement with self-feedback"
Yiming Yang is an author of the paper "Pal: Program-aided language models"
Yiming Yang is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653,6109537356a2ce2339f77c827aa3668e,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAMIE CALLAN">
      <data key="d0">PERSON</data>
      <data key="d1">Jamie Callan is an author of the paper "PAL: Program-aided language models"
Jamie Callan is an author of the paper "Pal: Program-aided language models"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GRAHAM NEUBIG">
      <data key="d0">PERSON</data>
      <data key="d1">Graham Neubig is an author of the paper "PAL: Program-aided language models"
Graham Neubig is an author of the paper "Pal: Program-aided language models"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ICML">
      <data key="d0">CONFERENCE</data>
      <data key="d1">ICML is the conference where the paper "PAL: Program-aided language models" was presentedICML is the conference where the paper "An embodied multi-modal language model" was presentedICML is the conference where the paper "Learning latent dynamics for planning from pixels" was presented</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="JIANXIAN GUO">
      <data key="d0">PERSON</data>
      <data key="d1">Jiaxian Guo is an author of the paper "Long text generation via adversarial training with leaked information"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="SIDI LU">
      <data key="d0">PERSON</data>
      <data key="d1">Sidi Lu is an author of the paper "Long text generation via adversarial training with leaked information"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HAN CAI">
      <data key="d0">PERSON</data>
      <data key="d1">Han Cai is an author of the paper "Long text generation via adversarial training with leaked information"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEINAN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Weinan Zhang is an author of the paper "Long text generation via adversarial training with leaked information"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YONG YU">
      <data key="d0">PERSON</data>
      <data key="d1">Yong Yu is an author of the paper "Long text generation via adversarial training with leaked information"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JUN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jun Wang is an author of the paper "Long text generation via adversarial training with leaked information"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AAAI">
      <data key="d0">CONFERENCE</data>
      <data key="d1">AAAI is the conference where the paper "Long text generation via adversarial training with leaked information" was presented</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="NICHOLAY TOPIN">
      <data key="d0">PERSON</data>
      <data key="d1">Nicholay Topin is an author of the paper "MineRL: A large-scale dataset of Minecraft demonstrations"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PHILLIP WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Phillip Wang is an author of the paper "MineRL: A large-scale dataset of Minecraft demonstrations"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CAYDEN CODEL">
      <data key="d0">PERSON</data>
      <data key="d1">Cayden Codel is an author of the paper "MineRL: A large-scale dataset of Minecraft demonstrations"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MANUELA VELOSO">
      <data key="d0">PERSON</data>
      <data key="d1">Manuela Veloso is an author of the paper "MineRL: A large-scale dataset of Minecraft demonstrations"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RUSLAN SALAKHUTDINOV">
      <data key="d0">PERSON</data>
      <data key="d1">Ruslan Salakhutdinov is an author of the paper "MineRL: A large-scale dataset of Minecraft demonstrations"
Ruslan Salakhutdinov is an author mentioned in the text</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IJCAI">
      <data key="d0">CONFERENCE</data>
      <data key="d1">IJCAI is the conference where the paper "MineRL: A large-scale dataset of Minecraft demonstrations" was presented</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="DANIJAR HAFNER">
      <data key="d0">PERSON</data>
      <data key="d1">Danijar Hafner is an author of the paper "Mastering diverse domains through world models"Danijar Hafner is an author of the paper "Learning latent dynamics for planning from pixels"
Danijar Hafner is an author mentioned in the text</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TIMOTHY LILLICRAP">
      <data key="d0">PERSON</data>
      <data key="d1">Timothy Lillicrap is an author of the paper "Learning latent dynamics for planning from pixels"Timothy Lillicrap is an author of the paper "Mastering diverse domains through world models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IAN FISCHER">
      <data key="d0">PERSON</data>
      <data key="d1">Ian Fischer is an author of the paper "Learning latent dynamics for planning from pixels"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RUBEN VILLEGAS">
      <data key="d0">PERSON</data>
      <data key="d1">Ruben Villegas is an author of the paper "Learning latent dynamics for planning from pixels"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAVID HA">
      <data key="d0">PERSON</data>
      <data key="d1">David Ha is an author of the paper "Learning latent dynamics for planning from pixels"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HONGLAK LEE">
      <data key="d0">PERSON</data>
      <data key="d1">Honglak Lee is an author of the paper "Learning latent dynamics for planning from pixels"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAMES DAVIDSON">
      <data key="d0">PERSON</data>
      <data key="d1">James Davidson is an author of the paper "Learning latent dynamics for planning from pixels"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JURGIS PASUKONIS">
      <data key="d0">PERSON</data>
      <data key="d1">Jurgis Pasukonis is an author of the paper "Mastering diverse domains through world models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIMMY BA">
      <data key="d0">PERSON</data>
      <data key="d1">Jimmy Ba is an author of the paper "Mastering diverse domains through world models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHIBO HAO">
      <data key="d0">PERSON</data>
      <data key="d1">Shibo Hao is an author of the paper "Reasoning with language model is planning with world model"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YI GU">
      <data key="d0">PERSON</data>
      <data key="d1">Yi Gu is an author of the paper "Reasoning with language model is planning with world model"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HAODI MA">
      <data key="d0">PERSON</data>
      <data key="d1">Haodi Ma is an author of the paper "Reasoning with language model is planning with world model"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOSHUA JIAHUA HONG">
      <data key="d0">PERSON</data>
      <data key="d1">Joshua Jiahua Hong is an author of the paper "Reasoning with language model is planning with world model"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHEN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhen Wang is an author of the paper "Reasoning with language model is planning with world model"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAISY ZHE WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Daisy Zhe Wang is an author of the paper "Reasoning with language model is planning with world model"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHTING HU">
      <data key="d0">PERSON</data>
      <data key="d1">Zhiting Hu is an author of the paper "Reasoning with language model is planning with world model"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="EMNLP">
      <data key="d0">CONFERENCE</data>
      <data key="d1">EMNLP is the conference where the paper "Reasoning with language model is planning with world model" was presented
EMNLP is a conference where the paper on HotpotQA was presented</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="JIE HUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jie Huang is an author of the paper "Large language models cannot self-correct reasoning yet"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XINYUN CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Xinyun Chen is an author of the paper "Large language models cannot self-correct reasoning yet"
Xinyun Chen is an author of the paper "Large language models as optimizers"
Xinyun Chen is an author of the paper "Take a step back: Evoking reasoning via abstraction in large language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,68e5573b596d253a03047b1e41988598,cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SWAROOP MISHRA">
      <data key="d0">PERSON</data>
      <data key="d1">Swaroop Mishra is an author of the paper "Large language models cannot self-correct reasoning yet"
Swaroop Mishra is an author of the paper "Take a step back: Evoking reasoning via abstraction in large language models"
Swaroop Mishra is an author of the paper "Instruction-following evaluation for large language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HUAIXIU STEVEN ZHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Huaixiu Steven Zheng is an author of the paper "Large language models cannot self-correct reasoning yet"
Huaixiu Steven Zheng is an author of the paper "Take a step back: Evoking reasoning via abstraction in large language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ADAMS WEI YU">
      <data key="d0">PERSON</data>
      <data key="d1">Adams Wei Yu is an author of the paper "Large language models cannot self-correct reasoning yet"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XINYING SONG">
      <data key="d0">PERSON</data>
      <data key="d1">Xinying Song is an author of the paper "Large language models cannot self-correct reasoning yet"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WENLONG HUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Wenlong Huang is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="F. XIA">
      <data key="d0">PERSON</data>
      <data key="d1">F. Xia is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HARRIS CHAN">
      <data key="d0">PERSON</data>
      <data key="d1">Harris Chan is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JACKY LIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jacky Liang is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PETER R. FLORENCE">
      <data key="d0">PERSON</data>
      <data key="d1">Peter R. Florence is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JONATHAN TOMPSON">
      <data key="d0">PERSON</data>
      <data key="d1">Jonathan Tompson is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IGOR MORDATCH">
      <data key="d0">PERSON</data>
      <data key="d1">Igor Mordatch is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"
Igor Mordatch is an author of the paper "Improving factuality and reasoning in language models through multiagent debate"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TOMAS JACKSON">
      <data key="d0">PERSON</data>
      <data key="d1">Tomas Jackson is an author of the paper "Inner monologue: Embodied reasoning through planning with language models"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIAXIAN GUO">
      <data key="d0">PERSON</data>
      <data key="d1">Jiaxian Guo is an author of the paper "Long text generation via adversarial training with leaked information"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="CORL">
      <data key="d0">CONFERENCE</data>
      <data key="d1">CoRL is the conference where the paper "Inner monologue: Embodied reasoning through planning with language models" was presented
CoRL is a conference where the paper on Daydreamer was presented</data>
      <data key="d2">68e5573b596d253a03047b1e41988598,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="LEVENTE KOCSIS">
      <data key="d0">PERSON</data>
      <data key="d1">Levente Kocsis is an author of the paper "Bandit based monte-carlo planning"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="CSABA SZEPESVARI">
      <data key="d0">PERSON</data>
      <data key="d1">Csaba Szepesvari is an author of the paper "Bandit based monte-carlo planning"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="ECML">
      <data key="d0">CONFERENCE</data>
      <data key="d1">ECML is the conference where the paper "Bandit based monte-carlo planning" was presented</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="TAKESHI KOJIMA">
      <data key="d0">PERSON</data>
      <data key="d1">Takeshi Kojima is an author of the paper "Large language models are zero-shot reasoners"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="MACHEL REID">
      <data key="d0">PERSON</data>
      <data key="d1">Machel Reid is an author of the paper "Large language models are zero-shot reasoners"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="YUSUKE IWASAWA">
      <data key="d0">PERSON</data>
      <data key="d1">Yusuke Iwasawa is an author of the paper "Large language models are zero-shot reasoners"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="STEVEN M. LAVALLE">
      <data key="d0">PERSON</data>
      <data key="d1">Steven M. LaValle is the author of the paper "Rapidly-exploring random trees: A new tool for path planning"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="THE ANNUAL RESEARCH REPORT">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The Annual Research Report is the publication where the paper "Rapidly-exploring random trees: A new tool for path planning" was published</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="EVAN ZHERAN LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Evan Zheran Liu is an author of the paper "Reinforcement learning on web interfaces using workflow-guided exploration"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="KELVIN GUU">
      <data key="d0">PERSON</data>
      <data key="d1">Kelvin Guu is an author of the paper "Reinforcement learning on web interfaces using workflow-guided exploration"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="PANUPONG PASUPAT">
      <data key="d0">PERSON</data>
      <data key="d1">Panupong Pasupat is an author of the paper "Reinforcement learning on web interfaces using workflow-guided exploration"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="TIANLIN SHI">
      <data key="d0">PERSON</data>
      <data key="d1">Tianlin Shi is an author of the paper "Reinforcement learning on web interfaces using workflow-guided exploration"</data>
      <data key="d2">68e5573b596d253a03047b1e41988598</data>
    </node>
    <node id="PERCY LIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Percy Liang is an author of the paper "Reinforcement learning on web interfaces using workflow-guided exploration"
Percy Liang is an author of the paper "Generative agents: Interactive simulacra of human behavior"
Percy Liang is an author of the paper "Alpaca"
Percy Liang is an author of the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,3d1f6634f93f8a4c296dc8df7e59859e,68e5573b596d253a03047b1e41988598,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="NIKET TANDON">
      <data key="d0">PERSON</data>
      <data key="d1">Niket Tandon is an author of the paper "Self-refine: Iterative refinement with self-feedback"
Niket Tandon is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="PRAKHAR GUPTA">
      <data key="d0">PERSON</data>
      <data key="d1">Prakhar Gupta is an author of the paper "Self-refine: Iterative refinement with self-feedback"
Prakhar Gupta is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="SKYLER HALLINAN">
      <data key="d0">PERSON</data>
      <data key="d1">Skyler Hallinan is an author of the paper "Self-refine: Iterative refinement with self-feedback"
Skyler Hallinan is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="SARAH WIEGREFFE">
      <data key="d0">PERSON</data>
      <data key="d1">Sarah Wiegreffe is an author of the paper "Self-refine: Iterative refinement with self-feedback"
Sarah Wiegreffe is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="NOUHA DZIRI">
      <data key="d0">PERSON</data>
      <data key="d1">Nouha Dziri is an author of the paper "Self-refine: Iterative refinement with self-feedback"
Nouha Dziri is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="SHRIMAI PRABHUMOYE">
      <data key="d0">PERSON</data>
      <data key="d1">Shrimai Prabhumoye is an author of the paper "Self-refine: Iterative refinement with self-feedback"
Shrimai Prabhumoye is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="SHASHANK GUPTA">
      <data key="d0">PERSON</data>
      <data key="d1">Shashank Gupta is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="BODHISATTWA PRASAD MAJUMDER">
      <data key="d0">PERSON</data>
      <data key="d1">Bodhisattwa Prasad Majumder is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="KATHERINE HERMANN">
      <data key="d0">PERSON</data>
      <data key="d1">Katherine Hermann is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="SEAN WELLECK">
      <data key="d0">PERSON</data>
      <data key="d1">Sean Welleck is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="AMIR YAZDANBAKHSH">
      <data key="d0">PERSON</data>
      <data key="d1">Amir Yazdanbakhsh is an author of the paper "Self-refine: Iterative refinement with self-feedback"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="PETER CLARK">
      <data key="d0">PERSON</data>
      <data key="d1">Peter Clark is an author of the paper "Self-refine: Iterative refinement with self-feedback"
Peter Clark is an author of the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="RAMESH NALLAPATI">
      <data key="d0">PERSON</data>
      <data key="d1">Ramesh Nallapati is an author of the paper "Abstractive text summarization using sequence-to-sequence RNNs and beyond"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="BOWEN ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Bowen Zhou is an author of the paper "Abstractive text summarization using sequence-to-sequence RNNs and beyond"
Bowen Zhou is an author of the paper "Enhancing chat language models by scaling high-quality instructional conversations"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CICERO DOS SANTOS">
      <data key="d0">PERSON</data>
      <data key="d1">Cicero dos Santos is an author of the paper "Abstractive text summarization using sequence-to-sequence RNNs and beyond"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="CAGLAR GULCEHRE">
      <data key="d0">PERSON</data>
      <data key="d1">Caglar Gulcehre is an author of the paper "Abstractive text summarization using sequence-to-sequence RNNs and beyond"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="BING XIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Bing Xiang is an author of the paper "Abstractive text summarization using sequence-to-sequence RNNs and beyond"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="SPECIAL INTEREST GROUP ON NATURAL LANGUAGE LEARNING">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "Abstractive text summarization using sequence-to-sequence RNNs and beyond" was presented</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="YUJIA QIN">
      <data key="d0">PERSON</data>
      <data key="d1">Yujia Qin is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Yujia Qin is an author of the paper "Enhancing chat language models by scaling high-quality instructional conversations"
Yujia Qin is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="SHIHAO LIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Shihao Liang is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Shihao Liang is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="YINING YE">
      <data key="d0">PERSON</data>
      <data key="d1">Yining Ye is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Yining Ye is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="KUNLUN ZHU">
      <data key="d0">PERSON</data>
      <data key="d1">Kunlun Zhu is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Kunlun Zhu is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="LAN YAN">
      <data key="d0">PERSON</data>
      <data key="d1">Lan Yan is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Lan Yan is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="YAXI LU">
      <data key="d0">PERSON</data>
      <data key="d1">Yaxi Lu is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"
Yaxi Lu is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e,7de66b94cf868b37b1df51dc545c415f</data>
    </node>
    <node id="YANKAI LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Yankai Lin is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Yankai Lin is an author of the paper "A survey on large language model based autonomous agents"
Yankai Lin is an author of the paper "A survey on large language model based autonomous agents"
Yankai Lin is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="XIN CONG">
      <data key="d0">PERSON</data>
      <data key="d1">Xin Cong is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Xin Cong is an author of the paper "Communicative agents for software development"
Xin Cong is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="XIANGRU TANG">
      <data key="d0">PERSON</data>
      <data key="d1">Xiangru Tang is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Xiangru Tang is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="BILL QIAN">
      <data key="d0">PERSON</data>
      <data key="d1">Bill Qian is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Bill Qian is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SIHAN ZHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Sihan Zhao is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Sihan Zhao is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="RUNCHU TIAN">
      <data key="d0">PERSON</data>
      <data key="d1">Runchu Tian is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Runchu Tian is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="RUOBING XIE">
      <data key="d0">PERSON</data>
      <data key="d1">Ruobing Xie is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Ruobing Xie is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="JIE ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Jie Zhou is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Jie Zhou is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="MARK GERSTEIN">
      <data key="d0">PERSON</data>
      <data key="d1">Mark Gerstein is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Mark Gerstein is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="DAHAI LI">
      <data key="d0">PERSON</data>
      <data key="d1">Dahai Li is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Dahai Li is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ZHIYUAN LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Zhiyuan Liu is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Zhiyuan Liu is an author of the paper "Communicative agents for software development"
Zhiyuan Liu is an author of the paper "Enhancing chat language models by scaling high-quality instructional conversations"
Zhiyuan Liu is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="MAOSONG SUN">
      <data key="d0">PERSON</data>
      <data key="d1">Maosong Sun is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"
Maosong Sun is an author of the paper "Communicative agents for software development"
Maosong Sun is an author of the paper "Enhancing chat language models by scaling high-quality instructional conversations"
Maosong Sun is an author of the paper "ToolLLM: Facilitating large language models to master 16000+ real-world APIs"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2d4672dfb7bd4283f0b5f23ab4f26653,3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ABULHAIR SAPAROV">
      <data key="d0">PERSON</data>
      <data key="d1">Abulhair Saparov is an author of the paper "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="HE HE">
      <data key="d0">PERSON</data>
      <data key="d1">He He is an author of the paper "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="TIMO SCHICK">
      <data key="d0">PERSON</data>
      <data key="d1">Timo Schick is an author of the paper "Toolformer: Language models can teach themselves to use tools"
Timo Schick is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JANE DWIVEDI-YU">
      <data key="d0">PERSON</data>
      <data key="d1">Jane Dwivedi-Yu is an author of the paper "Toolformer: Language models can teach themselves to use tools"
Jane Dwivedi-Yu is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ROBERTO DESSI">
      <data key="d0">PERSON</data>
      <data key="d1">Roberto Dessi is an author of the paper "Toolformer: Language models can teach themselves to use tools"
Roberto Dessi is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ROBERTA RAILEANU">
      <data key="d0">PERSON</data>
      <data key="d1">Roberta Raileanu is an author of the paper "Toolformer: Language models can teach themselves to use tools"
Roberta Raileanu is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MARIA LOMELI">
      <data key="d0">PERSON</data>
      <data key="d1">Maria Lomeli is an author of the paper "Toolformer: Language models can teach themselves to use tools"
Maria Lomeli is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="LUKE ZETTLEMOYER">
      <data key="d0">PERSON</data>
      <data key="d1">Luke Zettlemoyer is an author of the paper "Toolformer: Language models can teach themselves to use tools"
Luke Zettlemoyer is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="NICOLA CANCEDDA">
      <data key="d0">PERSON</data>
      <data key="d1">Nicola Cancedda is an author of the paper "Toolformer: Language models can teach themselves to use tools"
Nicola Cancedda is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="THOMAS SCIALOM">
      <data key="d0">PERSON</data>
      <data key="d1">Thomas Scialom is an author of the paper "Toolformer: Language models can teach themselves to use tools"
Thomas Scialom is an author mentioned in the text
Thomas Scialom is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YONGLIANG SHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Yongliang Shen is an author of the paper "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="KAITAO SONG">
      <data key="d0">PERSON</data>
      <data key="d1">Kaitao Song is an author of the paper "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face"
Kaitao Song is an author of theKaitao Song is an author of the paper "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="XU TAN">
      <data key="d0">PERSON</data>
      <data key="d1">Xu Tan is an author of the paper "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face"
Xu Tan is an author of the paper "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="DONGSHENG LI">
      <data key="d0">PERSON</data>
      <data key="d1">Dongsheng Li is an author of the paper "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face"
Dongsheng Li is an author of the paper "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="WEIMING LU">
      <data key="d0">PERSON</data>
      <data key="d1">Weiming Lu is an author of the paper "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="YUETING ZHUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yueting Zhuang is an author of the paper "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="NOAH SHINN">
      <data key="d0">PERSON</data>
      <data key="d1">Noah Shinn is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"
Noah Shinn is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="FEDERICO CASSANO">
      <data key="d0">PERSON</data>
      <data key="d1">Federico Cassano is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"
Federico Cassano is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="BECK LABASH">
      <data key="d0">PERSON</data>
      <data key="d1">Beck Labash is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="ASHWIN GOPINATH">
      <data key="d0">PERSON</data>
      <data key="d1">Ashwin Gopinath is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"
Ashwin Gopinath is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="KARTHIK NARASIMHAN">
      <data key="d0">PERSON</data>
      <data key="d1">Karthik Narasimhan is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"
Karthik Narasimhan is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SHUNYU YAO">
      <data key="d0">PERSON</data>
      <data key="d1">Shunyu Yao is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"
Shunyu Yao is an author mentioned in the text
Shunyu Yao is an author of the paper "Reflexion: Language agents with verbal reinforcement learning"
Shunyu Yao is an author of the paper "React: Synergizing reasoning and acting in language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="MOHIT SHRIDHAR">
      <data key="d0">PERSON</data>
      <data key="d1">Mohit Shridhar is an author of the paper "ALFWorld: Aligning text and embodied environments for interactive learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="XINGDI YUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Xingdi Yuan is an author of the paper "ALFWorld: Aligning text and embodied environments for interactive learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="MARC-ALEXANDRE COTE">
      <data key="d0">PERSON</data>
      <data key="d1">Marc-Alexandre Cote is an author of the paper "ALFWorld: Aligning text and embodied environments for interactive learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="YONATAN BISK">
      <data key="d0">PERSON</data>
      <data key="d1">Yonatan Bisk is an author of the paper "ALFWorld: Aligning text and embodied environments for interactive learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="ADAM TRISCHLER">
      <data key="d0">PERSON</data>
      <data key="d1">Adam Trischler is an author of the paper "ALFWorld: Aligning text and embodied environments for interactive learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="MATTHEW HAUSKNECHT">
      <data key="d0">PERSON</data>
      <data key="d1">Matthew Hausknecht is an author of the paper "ALFWorld: Aligning text and embodied environments for interactive learning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="DAVID SILVER">
      <data key="d0">PERSON</data>
      <data key="d1">David Silver is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"David Silver is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AJA HUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Aja Huang is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Aja Huang is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHRIS J. MADDISON">
      <data key="d0">PERSON</data>
      <data key="d1">Chris J. Maddison is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Chris J. Maddison is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ARTHUR GUEZ">
      <data key="d0">PERSON</data>
      <data key="d1">Arthur Guez is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Arthur Guez is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="L. SIFRE">
      <data key="d0">PERSON</data>
      <data key="d1">L. Sifre is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"L. Sifre is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GEORGE VAN DEN DRIESSCHE">
      <data key="d0">PERSON</data>
      <data key="d1">George van den Driessche is an author of the paper "Mastering the game of Go with deep neural networks and tree search"George van den Driessche is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JULIAN SCHRITTWIESER">
      <data key="d0">PERSON</data>
      <data key="d1">Julian Schrittwieser is an author of the paper "Mastering the game of Go with deep neural networks and tree search"Julian Schrittwieser is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IOANNIS ANTONOGLOU">
      <data key="d0">PERSON</data>
      <data key="d1">Ioannis Antonoglou is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Ioannis Antonoglou is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VEDAVYAS PANNEERSHELVAM">
      <data key="d0">PERSON</data>
      <data key="d1">Vedavyas Panneershelvam is an author of the paper "Mastering the game of Go with deep neural networks and tree search"Vedavyas Panneershelvam is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MARC LANCTOT">
      <data key="d0">PERSON</data>
      <data key="d1">Marc Lanctot is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Marc Lanctot is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SANDER DIELEMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Sander Dieleman is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Sander Dieleman is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DOMINIK GREWE">
      <data key="d0">PERSON</data>
      <data key="d1">Dominik Grewe is an author of the paper "Mastering the game of Go with deep neural networks and tree search"Dominik Grewe is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOHN NHAM">
      <data key="d0">PERSON</data>
      <data key="d1">John Nham is an author of the paper "Mastering the game of Go with deep neural networks and tree search"John Nham is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NAL KALCHBRENNER">
      <data key="d0">PERSON</data>
      <data key="d1">Nal Kalchbrenner is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Nal Kalchbrenner is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TIMOTHY P. LILLICRAP">
      <data key="d0">PERSON</data>
      <data key="d1">Timothy P. Lillicrap is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Timothy P. Lillicrap is an author of the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MADELEINE LEACH">
      <data key="d0">PERSON</data>
      <data key="d1">Madeleine Leach is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"Madeleine Leach is an author of the paper "Master</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NATURE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Nature is the journal where the paper "Mastering the game of Go with deep neural networks and tree search" was published
Nature is the journal where the paper "Mathematical discoveries from program search with large language models" was published</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="KORAY KAVUKCUOGLU">
      <data key="d0">PERSON</data>
      <data key="d1">Koray Kavukcuoglu is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="THORE GRAEPEL">
      <data key="d0">PERSON</data>
      <data key="d1">Thore Graepel is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="DEMIS HASSABIS">
      <data key="d0">PERSON</data>
      <data key="d1">Demis Hassabis is an author of the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="STEVEN A. SLOMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Steven A. Sloman is the author of the paper "The empirical case for two systems of reasoning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="PSYCHOLOGICAL BULLETIN">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Psychological Bulletin is the journal where the paper "The empirical case for two systems of reasoning" was published</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="HAOTIAN SUN">
      <data key="d0">PERSON</data>
      <data key="d1">Haotian Sun is an author of the paper "AdaPlanner: Adaptive planning from feedback with language models"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="YUCHEN ZHUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yuchen Zhuang is an author of the paper "AdaPlanner: Adaptive planning from feedback with language models"
Yuchen Zhuang is an author mentioned in the text
Yuchen Zhuang is an author of the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="LINGKAI KONG">
      <data key="d0">PERSON</data>
      <data key="d1">Lingkai Kong is an author of the paper "AdaPlanner: Adaptive planning from feedback with language models"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="CHAO ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Chao Zhang is an author of the paper "AdaPlanner: Adaptive planning from feedback with language models"
Chao Zhang is an author mentioned in the text
Chao Zhang is an author of the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="DIDAC SURIS">
      <data key="d0">PERSON</data>
      <data key="d1">Didac Suris is an author of the paper "ViperGPT: Visual inference via Python execution for reasoning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="SACHIT MENON">
      <data key="d0">PERSON</data>
      <data key="d1">Sachit Menon is an author of the paper "ViperGPT: Visual inference via Python execution for reasoning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="CARL VONDRICK">
      <data key="d0">PERSON</data>
      <data key="d1">Carl Vondrick is an author of the paper "ViperGPT: Visual inference via Python execution for reasoning"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="ICCV">
      <data key="d0">CONFERENCE</data>
      <data key="d1">ICCV is the conference where the paper "ViperGPT: Visual inference via Python execution for reasoning" was presented</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="MACIEJ SWIECHOWSKI">
      <data key="d0">PERSON</data>
      <data key="d1">Maciej Swiechowski is an author of the paper "Monte Carlo tree search: A review of recent modifications and applications"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="KONRAD GODLEWSKI">
      <data key="d0">PERSON</data>
      <data key="d1">Konrad Godlewski is an author of the paper "Monte Carlo tree search: A review of recent modifications and applications"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="BARTOSZ SAWICKI">
      <data key="d0">PERSON</data>
      <data key="d1">Bartosz Sawicki is an author of the paper "Monte Carlo tree search: A review of recent modifications and applications"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="JACEK MA'NDZIUK">
      <data key="d0">PERSON</data>
      <data key="d1">Jacek Ma'ndziuk is an author of the paper "Monte Carlo tree search: A review of recent modifications and applications"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="ARTIFICIAL INTELLIGENCE REVIEW">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Artificial Intelligence Review is the journal where the paper "Monte Carlo tree search: A review of recent modifications and applications" was published</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="HUGO TOUVRON">
      <data key="d0">PERSON</data>
      <data key="d1">Hugo Touvron is an author of the paper "LLaMA: Open and efficient foundation language models"
Hugo Touvron is an author mentioned in the text</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="LOUIS MARTIN">
      <data key="d0">PERSON</data>
      <data key="d1">Louis Martin is an author of the paper "LLaMA: Open and efficient foundation language models"
Louis Martin is an author mentioned in the text</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="KEVIN R. STONE">
      <data key="d0">PERSON</data>
      <data key="d1">Kevin R. Stone is an author of the paper "LLaMA: Open and efficient foundation language models"
Kevin R. Stone is an author mentioned in the text</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="PETER ALBERT">
      <data key="d0">PERSON</data>
      <data key="d1">Peter Albert is an author of the paper "LLaMA: Open and efficient foundation language models"
Peter Albert is an author mentioned in the text</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="AMJAD ALMAHAIRI">
      <data key="d0">PERSON</data>
      <data key="d1">Amjad Almahairi is an author of the paper "LLaMA: Open and efficient foundation language models"
Amjad Almahairi is an author mentioned in the text</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YASMINE BABAEI">
      <data key="d0">PERSON</data>
      <data key="d1">Yasmine Babaei is an author of the paper "LLaMA: Open and efficient foundation language models"
Yasmine Babaei is an author mentioned in the text</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="NIKOLAY BASHLYKOV">
      <data key="d0">PERSON</data>
      <data key="d1">Nikolay Bashlykov is an author of the paper "LLaMA: Open and efficient foundation language models"
Nikolay Bashlykov is an author mentioned in the text</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="SOUMYA BATRA">
      <data key="d0">PERSON</data>
      <data key="d1">Soumya Batra is an author of the paper "LLaMA: Open and efficient foundation language models"
Soumya Batra is an author mentioned in the text</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="PRAJJWAL BHARGAVA">
      <data key="d0">PERSON</data>
      <data key="d1">Prajjwal Bhargava is an author of the paper "LLaMA: Open and efficient foundation language models"
Prajjwal Bhargava is an author mentioned in the text</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="SHRUTI BHOSALE">
      <data key="d0">PERSON</data>
      <data key="d1">Shruti Bhosale is an author of the paper "LLaMA: Open and efficient foundation language models"
Shruti Bhosale is an author mentioned in the text</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="DANIEL M. BIKEL">
      <data key="d0">PERSON</data>
      <data key="d1">Daniel M. Bikel is an author of the paper "LLaMA: Open and efficient foundation language models"
Daniel M. Bikel is an author mentioned in the text</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="LUKAS BLECHER">
      <data key="d0">PERSON</data>
      <data key="d1">Lukas Blecher is an author of the paper "LLaMA: Open and efficient foundation language models"
Lukas Blecher is an author mentioned in the text</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="CRISTIAN CANTON FERRER">
      <data key="d0">PERSON</data>
      <data key="d1">Cristian Canton Ferrer is an author of the paper "LLaMA: Open and efficient foundation language models"</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </node>
    <node id="MOYA CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Moya Chen is an author of the paper "LLaMA: Open and efficient foundation language models"
Moya Chen is an author mentioned in the text</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="GUILLEM CUCURULL">
      <data key="d0">PERSON</data>
      <data key="d1">Guillem Cucurull is an author of the paper "LLaMA: Open and efficient foundation language models"
Guillem Cucurull is an author mentioned in the text</data>
      <data key="d2">2d4672dfb7bd4283f0b5f23ab4f26653,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="CRISTIAN CANT&#211;N FERRER">
      <data key="d0">PERSON</data>
      <data key="d1">Cristian Cant&#243;n Ferrer is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="DAVID ESIOBU">
      <data key="d0">PERSON</data>
      <data key="d1">David Esiobu is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="JUDE FERNANDES">
      <data key="d0">PERSON</data>
      <data key="d1">Jude Fernandes is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="JEREMY FU">
      <data key="d0">PERSON</data>
      <data key="d1">Jeremy Fu is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="WENYIN FU">
      <data key="d0">PERSON</data>
      <data key="d1">Wenyin Fu is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="BRIAN FULLER">
      <data key="d0">PERSON</data>
      <data key="d1">Brian Fuller is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="CYNTHIA GAO">
      <data key="d0">PERSON</data>
      <data key="d1">Cynthia Gao is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="VEDANUJ GOSWAMI">
      <data key="d0">PERSON</data>
      <data key="d1">Vedanuj Goswami is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="NAMAN GOYAL">
      <data key="d0">PERSON</data>
      <data key="d1">Naman Goyal is an author mentioned in the text
Naman Goyal is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ANTHONY S. HARTSHORN">
      <data key="d0">PERSON</data>
      <data key="d1">Anthony S. Hartshorn is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="SAGHAR HOSSEINI">
      <data key="d0">PERSON</data>
      <data key="d1">Saghar Hosseini is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="RUI HOU">
      <data key="d0">PERSON</data>
      <data key="d1">Rui Hou is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="HAKAN INAN">
      <data key="d0">PERSON</data>
      <data key="d1">Hakan Inan is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="MARCIN KARDAS">
      <data key="d0">PERSON</data>
      <data key="d1">Marcin Kardas is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="VIKTOR KERKEZ">
      <data key="d0">PERSON</data>
      <data key="d1">Viktor Kerkez is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="MADIAN KHABSA">
      <data key="d0">PERSON</data>
      <data key="d1">Madian Khabsa is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ISABEL M. KLOUMANN">
      <data key="d0">PERSON</data>
      <data key="d1">Isabel M. Kloumann is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="A. V. KORENEV">
      <data key="d0">PERSON</data>
      <data key="d1">A. V. Korenev is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="PUNIT SINGH KOURA">
      <data key="d0">PERSON</data>
      <data key="d1">Punit Singh Koura is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="MARIE-ANNE LACHAUX">
      <data key="d0">PERSON</data>
      <data key="d1">Marie-Anne Lachaux is an author mentioned in the text
Marie-Anne Lachaux is an author of the paper "Mistral 7b"</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="THIBAUT LAVRIL">
      <data key="d0">PERSON</data>
      <data key="d1">Thibaut Lavril is an author mentioned in the text
Thibaut Lavril is an author of the paper "Mistral 7b"</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="JENYA LEE">
      <data key="d0">PERSON</data>
      <data key="d1">Jenya Lee is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="DIANA LISKOVICH">
      <data key="d0">PERSON</data>
      <data key="d1">Diana Liskovich is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YINGHAI LU">
      <data key="d0">PERSON</data>
      <data key="d1">Yinghai Lu is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YUNING MAO">
      <data key="d0">PERSON</data>
      <data key="d1">Yuning Mao is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="XAVIER MARTINET">
      <data key="d0">PERSON</data>
      <data key="d1">Xavier Martinet is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="TODOR MIHAYLOV">
      <data key="d0">PERSON</data>
      <data key="d1">Todor Mihaylov is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="PUSHKAR MISHRA">
      <data key="d0">PERSON</data>
      <data key="d1">Pushkar Mishra is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="IGOR MOLYBOG">
      <data key="d0">PERSON</data>
      <data key="d1">Igor Molybog is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YIXIN NIE">
      <data key="d0">PERSON</data>
      <data key="d1">Yixin Nie is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ANDREW POULTON">
      <data key="d0">PERSON</data>
      <data key="d1">Andrew Poulton is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="JEREMY REIZENSTEIN">
      <data key="d0">PERSON</data>
      <data key="d1">Jeremy Reizenstein is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="RASHI RUNGTA">
      <data key="d0">PERSON</data>
      <data key="d1">Rashi Rungta is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="KALYAN SALADI">
      <data key="d0">PERSON</data>
      <data key="d1">Kalyan Saladi is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ALAN SCHELTEN">
      <data key="d0">PERSON</data>
      <data key="d1">Alan Schelten is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="RUAN SILVA">
      <data key="d0">PERSON</data>
      <data key="d1">Ruan Silva is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ERIC MICHAEL SMITH">
      <data key="d0">PERSON</data>
      <data key="d1">Eric Michael Smith is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="R. SUBRAMANIAN">
      <data key="d0">PERSON</data>
      <data key="d1">R. Subramanian is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="XIA TAN">
      <data key="d0">PERSON</data>
      <data key="d1">Xia Tan is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="BINH TANG">
      <data key="d0">PERSON</data>
      <data key="d1">Binh Tang is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ROSS TAYLOR">
      <data key="d0">PERSON</data>
      <data key="d1">Ross Taylor is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ADINA WILLIAMS">
      <data key="d0">PERSON</data>
      <data key="d1">Adina Williams is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="JIAN XIANG KUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Jian Xiang Kuan is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="PUXIN XU">
      <data key="d0">PERSON</data>
      <data key="d1">Puxin Xu is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ZHENGXU YAN">
      <data key="d0">PERSON</data>
      <data key="d1">Zhengxu Yan is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ILIYAN ZAROV">
      <data key="d0">PERSON</data>
      <data key="d1">Iliyan Zarov is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YUCHEN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yuchen Zhang is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ANGELA FAN">
      <data key="d0">PERSON</data>
      <data key="d1">Angela Fan is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="MELANIE KAMBADUR">
      <data key="d0">PERSON</data>
      <data key="d1">Melanie Kambadur is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="AURELIEN RODRIGUEZ">
      <data key="d0">PERSON</data>
      <data key="d1">Aurelien Rodriguez is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ROBERT STOJNIC">
      <data key="d0">PERSON</data>
      <data key="d1">Robert Stojnic is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="SERGEY EDUNOV">
      <data key="d0">PERSON</data>
      <data key="d1">Sergey Edunov is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="LLAMA 2">
      <data key="d0">MODEL</data>
      <data key="d1">Llama 2 is an open foundation and fine-tuned chat model mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="TOM VODOPIVEC">
      <data key="d0">PERSON</data>
      <data key="d1">Tom Vodopivec is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SPYRIDON SAMOTHRAKIS">
      <data key="d0">PERSON</data>
      <data key="d1">Spyridon Samothrakis is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BRANKO STER">
      <data key="d0">PERSON</data>
      <data key="d1">Branko Ster is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUQI XIE">
      <data key="d0">PERSON</data>
      <data key="d1">Yuqi Xie is an author mentioned in the text
Yuqi Xie is an author of the paper "Voyager: An open-ended embodied agent with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHAOWEI XIAO">
      <data key="d0">PERSON</data>
      <data key="d1">Chaowei Xiao is an author mentioned in the text
Chaowei Xiao is an author of the paper "Voyager: An open-ended embodied agent with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VOYAGER">
      <data key="d0">TOOL/AGENT</data>
      <data key="d1">Voyager is an open-ended embodied agent with large language models mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">TOOL/AGENT</data>
    </node>
    <node id="ED CHI">
      <data key="d0">PERSON</data>
      <data key="d1">Ed Chi is an author mentioned in the text
Ed Chi is an author of the paper "Least-to-most prompting enables complex reasoning in large language models"</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MICHAEL WOOLDRIDGE">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Wooldridge is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NICHOLAS R JENNINGS">
      <data key="d0">PERSON</data>
      <data key="d1">Nicholas R Jennings is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THE KNOWLEDGE ENGINEERING REVIEW">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The Knowledge Engineering Review is a publication where the paper by Michael Wooldridge and Nicholas R Jennings was published</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="PHILIPP WU">
      <data key="d0">PERSON</data>
      <data key="d1">Philipp Wu is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALEJANDRO ESCONTRELA">
      <data key="d0">PERSON</data>
      <data key="d1">Alejandro Escontrela is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KEN GOLDBERG">
      <data key="d0">PERSON</data>
      <data key="d1">Ken Goldberg is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAYDREAMER">
      <data key="d0">TOOL/AGENT</data>
      <data key="d1">Daydreamer is a world model for physical robot learning mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">TOOL/AGENT</data>
    </node>
    <node id="YUXI XIE">
      <data key="d0">PERSON</data>
      <data key="d1">Yuxi Xie is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KENJI KAWAGUCHI">
      <data key="d0">PERSON</data>
      <data key="d1">Kenji Kawaguchi is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIRAN ZHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Yiran Zhao is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XU ZHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Xu Zhao is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MIN-YEN KAN">
      <data key="d0">PERSON</data>
      <data key="d1">Min-Yen Kan is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JUNXIAN HE">
      <data key="d0">PERSON</data>
      <data key="d1">Junxian He is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QIZHE XIE">
      <data key="d0">PERSON</data>
      <data key="d1">Qizhe Xie is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHILIN YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhilin Yang is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PENG QI">
      <data key="d0">PERSON</data>
      <data key="d1">Peng Qi is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAIZHENG ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Saizheng Zhang is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YOSHUA BENGIO">
      <data key="d0">PERSON</data>
      <data key="d1">Yoshua Bengio is an author mentioned in the text
An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WILLIAM W COHEN">
      <data key="d0">PERSON</data>
      <data key="d1">William W Cohen is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HOWARD CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Howard Chen is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOHN YANG">
      <data key="d0">PERSON</data>
      <data key="d1">John Yang is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KARTHIK R NARASIMHAN">
      <data key="d0">PERSON</data>
      <data key="d1">Karthik R Narasimhan is an author mentioned in the text
Karthik R Narasimhan is an author of the paper "React: Synergizing reasoning and acting in language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DIAN YU">
      <data key="d0">PERSON</data>
      <data key="d1">Dian Yu is an author mentioned in the text
Dian Yu is an author of the paper "React: Synergizing reasoning and acting in language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JEFFREY ZHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Jeffrey Zhao is an author mentioned in the text
Jeffrey Zhao is an author of the paper "React: Synergizing reasoning and acting in language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IZHAK SHAFRAN">
      <data key="d0">PERSON</data>
      <data key="d1">Izhak Shafran is an author mentioned in the text
Izhak Shafran is an author of the paper "React: Synergizing reasoning and acting in language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THOMAS L. GRIFFITHS">
      <data key="d0">PERSON</data>
      <data key="d1">Thomas L. Griffiths is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUAN CAO">
      <data key="d0">PERSON</data>
      <data key="d1">Yuan Cao is an author mentioned in the text
Yuan Cao is an author of the paper "React: Synergizing reasoning and acting in language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TREE OF THOUGHTS">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">Tree of Thoughts is a deliberate problem-solving method with large language models mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
      <data key="d3">TOOL/PROCESS</data>
    </node>
    <node id="WEIRUI YE">
      <data key="d0">PERSON</data>
      <data key="d1">Weirui Ye is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="SHAOHUAI LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Shaohuai Liu is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="THANARD KURUTACH">
      <data key="d0">PERSON</data>
      <data key="d1">Thanard Kurutach is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="YANG GAO">
      <data key="d0">PERSON</data>
      <data key="d1">Yang Gao is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="NATHANAEL SCHARLI">
      <data key="d0">PERSON</data>
      <data key="d1">Nathanael Sch&#228;rli is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="LE HOU">
      <data key="d0">PERSON</data>
      <data key="d1">Le Hou is an author mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="NATHAN SCALES">
      <data key="d0">PERSON</data>
      <data key="d1">Nathan Scales is an author mentioned in the text
Nathan Scales is an author of the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="OLIVIER BOUSQUET">
      <data key="d0">PERSON</data>
      <data key="d1">Olivier Bousquet is an author mentioned in the text
Olivier Bousquet is an author of the paper "Least-to-most prompting enables complex reasoning in large language models"</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="XIANG CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Xiang Chen is an author mentioned in the text
Xiang Chen is an author of the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="TONG YU">
      <data key="d0">PERSON</data>
      <data key="d1">Tong Yu is an author mentioned in the text
Tong Yu is an author of the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="SAAYAN MITRA">
      <data key="d0">PERSON</data>
      <data key="d1">Saayan Mitra is an author mentioned in the text
Saayan Mitra is an author of the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="VICTOR BURSZTYN">
      <data key="d0">PERSON</data>
      <data key="d1">Victor Bursztyn is an author mentioned in the text
Victor Bursztyn is an author of the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="RYAN A. ROSSI">
      <data key="d0">PERSON</data>
      <data key="d1">Ryan A. Rossi is an author mentioned in the text
Ryan A. Rossi is an author of the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="SOMDEB SARKHEL">
      <data key="d0">PERSON</data>
      <data key="d1">Somdeb Sarkhel is an author mentioned in the text
Somdeb Sarkhel is an author of the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="TOOLCHAIN*">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">ToolChain* is a method for efficient action space navigation in large language models with A* search mentioned in the text</data>
      <data key="d2">8180bf20b7577f3eee40df5991e2886d</data>
    </node>
    <node id="ICLR 2022">
      <data key="d0">EVENT</data>
      <data key="d1">The conference where the paper "Least-to-most prompting enables complex reasoning in large language models" was presented</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="ICLR 2023">
      <data key="d0">EVENT</data>
      <data key="d1">The conference where the paper "ToolChain*: Efficient action space navigation in large language models with A* search" was presented</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="MINECRAFT">
      <data key="d0">ENVIRONMENT</data>
      <data key="d1">Minecraft is an environment suggested for future work with planning-based prompting methods like LATS</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="FAN ET AL. 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper by Fan et al. in 2022, related to the use of LATS in environments like Minecraft</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="ALFWORLD">
      <data key="d0">ENVIRONMENT</data>
      <data key="d1">Alfworld is an environment used in experiments with the LATS algorithm</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="TOOLBENCH">
      <data key="d0">TOOL</data>
      <data key="d1">ToolBench is a tool used in experiments with the LATS algorithm</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="QIN ET AL. 2024">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper by Qin et al. in 2024, related to the use of LATS in environments like ToolBench</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="CHEN ET AL. 2021">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper by Chen et al. in 2021, related to the use of LATS in environments like HumanEval</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="YAO ET AL. 2022">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper by Yao et al. in 2022, related to the use of LATS in environments like WebShop</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="SHRIDHAR ET AL. 2020">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper by Shridhar et al. in 2020, related to the use of LATS in environments like Alfworld</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="HAO ET AL. 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper by Hao et al. in 2023, related to the use of LATS in environments like ToolBench</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="LIU ET AL. 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a study or paper by Liu et al. in 2023, related to the use of LATS in environments like ToolBench</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="SEC. B">
      <data key="d0">SECTION</data>
      <data key="d1">Section B of the appendix provides further discussion of the limitations of the LATS method</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="SEC. C">
      <data key="d0">SECTION</data>
      <data key="d1">Section C of the appendix presents additional experimental results of the LATS method</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="SEC. D">
      <data key="d0">SECTION</data>
      <data key="d1">Section D of the appendix specifies the environment details in the experiments with the LATS method</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="SEC. E">
      <data key="d0">SECTION</data>
      <data key="d1">Section E of the appendix lists the prompts used for the HotPotQA environment in the experiments with the LATS method</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="SEC. F">
      <data key="d0">SECTION</data>
      <data key="d1">Section F of the appendix lists the prompts used for the Programming environment in the experiments with the LATS method</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="SEC. G">
      <data key="d0">SECTION</data>
      <data key="d1">Section G of the appendix lists the prompts used for the WebShop environment in the experiments with the LATS method</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="MONTE CARLO TREE SEARCH">
      <data key="d0">METHOD</data>
      <data key="d1">Monte Carlo Tree Search is a method used in the LATS algorithm for decision-making tasks</data>
      <data key="d2">42de130f5b6144472a86a4c8260a87c7</data>
    </node>
    <node id="DEPTH">
      <data key="d0">PARAMETER</data>
      <data key="d1">Depth is a parameter that defines the maximum number of steps in the search process for HotPotQA</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="LM VALUE FUNCTION">
      <data key="d0">FUNCTION</data>
      <data key="d1">The LM value function scores states based on expected future reward and guides the search process</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="WIKIPEDIA WEB API">
      <data key="d0">TOOL</data>
      <data key="d1">The Wikipedia web API is used for interactive information retrieval in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="CROWDWORKERS">
      <data key="d0">PERSON</data>
      <data key="d1">Crowdworkers are individuals who crafted the question-answer pairs in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="WIKIPEDIA">
      <data key="d0">SOURCE</data>
      <data key="d1">Wikipedia is the source of the documents used in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="HYPERPARAMETERS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Hyperparameters are settings used in the value function for the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="FIG. 3">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 3 shows the results of the HumanEval experiments and the performance of LATS over time</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="FIG. 4">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 4 illustrates how ReAct and LATS work on an example task of HotPotQA</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="ALGORITHM 1">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Algorithm 1 describes the LATS process, including initialization, expansion, simulation, evaluation, selection, and backpropagation</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SAMPLING SIZE">
      <data key="d0">PARAMETER</data>
      <data key="d1">Sampling size is a parameter used in the HumanEval dataset, with a value of n=5</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SELECTION FORMULA">
      <data key="d0">PARAMETER</data>
      <data key="d1">Selection formula is used in the LATS algorithm to choose actions based on exploration weight and value function</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="STATE SPACE">
      <data key="d0">PARAMETER</data>
      <data key="d1">State space refers to the complexity of the environment in which the LATS algorithm operates</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="ROLL-OUTS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Roll-outs refer to the number of iterations (K) in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="CONTEXT">
      <data key="d0">PARAMETER</data>
      <data key="d1">Context (c) is part of the state in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="VISIT COUNTER">
      <data key="d0">PARAMETER</data>
      <data key="d1">Visit counter (N) is used in the LATS algorithm to track the number of visits to each state</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="OBSERVATION SPACE">
      <data key="d0">PARAMETER</data>
      <data key="d1">Observation space (O) is the set of possible observations in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="INITIAL STATE">
      <data key="d0">PARAMETER</data>
      <data key="d1">Initial state (s) is the starting point in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="ACTION GENERATOR">
      <data key="d0">PARAMETER</data>
      <data key="d1">Action generator (p&#952;) is used to sample actions in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="REFLECTION GENERATOR">
      <data key="d0">PARAMETER</data>
      <data key="d1">Reflection generator (pref) is used to generate reflections in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SUCCESS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Success is a condition checked in the reflection phase of the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="EVALUATION OPERATION">
      <data key="d0">PARAMETER</data>
      <data key="d1">Evaluation operation is used to score states in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="ITERATIONS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Iterations refer to the number of times the LATS algorithm is run
Iterations refer to the repeated cycles of refining instructions to enhance their complexity and quality in the Instruction Refinement Flow.</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SUPPORTING FACTS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Supporting facts are provided by crowdworkers in the HotPotQA dataset to justify answers</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="ENTITY">
      <data key="d0">PARAMETER</data>
      <data key="d1">Entity is a type of question in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="LOCATION">
      <data key="d0">PARAMETER</data>
      <data key="d1">Location is a type of question in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="DATE">
      <data key="d0">PARAMETER</data>
      <data key="d1">Date is a type of question in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="COMPARISON">
      <data key="d0">PARAMETER</data>
      <data key="d1">Comparison is a type of question in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SHARED PROPERTIES">
      <data key="d0">PARAMETER</data>
      <data key="d1">Shared properties are compared between two entities in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SUPPORTING DOCUMENTS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Supporting documents are used to answer questions in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="QUESTION-ANSWER PAIRS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Question-answer pairs are crafted by crowdworkers in the HotPotQA dataset</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="WIKIPEDIA PARAGRAPHS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Wikipedia paragraphs are used in the HotPotQA benchmark setting</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SUBSET OF 100 QUESTIONS">
      <data key="d0">PARAMETER</data>
      <data key="d1">A randomly selected subset of 100 questions is used in the HotPotQA experiments</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="MAXIMUM DEPTH LIMIT">
      <data key="d0">PARAMETER</data>
      <data key="d1">Maximum depth limit is set to 6 in the HotPotQA experiments</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="VALUE FUNCTION HYPERPARAMETERS">
      <data key="d0">PARAMETER</data>
      <data key="d1">Value function hyperparameters include &#955;=0.5 for the LM score and self-consistency score in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SEARCH [ENTITY]">
      <data key="d0">PARAMETER</data>
      <data key="d1">Search [entity] is an action type in the Wikipedia web API used in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="LOOKUP [STRING]">
      <data key="d0">PARAMETER</data>
      <data key="d1">Lookup [string] is an action type in the Wikipedia web API used in the LATS algorithm</data>
      <data key="d2">48e423e2baf2ed485872756f5b4d87d8</data>
    </node>
    <node id="SEARCH">
      <data key="d0">ACTION/COMMAND</data>
      <data key="d1">The search command returns the first 5 sentences from the corresponding entity wiki page if it exists, or suggests top-5 similar entities from the Wikipedia search engine.
Search is an action in HotPotQA where an entity is searched on Wikipedia
The action of looking for specific items or information, such as products in a webshop
An action to look for specific products or information
The action of looking for products using specific keywords
Search is the process of exploring and discovering new designs in ADAS</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7,6bdf681c0bd9e401ac72344a6a0ae479,785ad59c6a37896a4676ec5c1689735f,b8dd0300033963bb4a3e1bad37f8e7b9,fb2b4544aedd793e4d4ec3147320a51c</data>
      <data key="d3">ACTION/COMMAND</data>
    </node>
    <node id="LOOKUP">
      <data key="d0">ACTION/COMMAND</data>
      <data key="d1">The lookup command returns the next sentence in the page containing the specified string.
Lookup is an action in HotPotQA where the next sentence containing a keyword in the current passage is returned</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9,fb2b4544aedd793e4d4ec3147320a51c</data>
      <data key="d3">ACTION/COMMAND</data>
    </node>
    <node id="FINISH">
      <data key="d0">ACTION/COMMAND</data>
      <data key="d1">The finish command completes the current task with the provided answer.
Finish is an action in HotPotQA where the final answer is provided and the task is completed</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9,fb2b4544aedd793e4d4ec3147320a51c</data>
      <data key="d3">ACTION/COMMAND</data>
    </node>
    <node id="AMAZON">
      <data key="d0">COMPANY/PLATFORM</data>
      <data key="d1">Amazon is an e-commerce platform from which over 1 million real-world products were scraped for the WebShop environment.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
      <data key="d3">COMPANY/PLATFORM</data>
    </node>
    <node id="TASK SCORE">
      <data key="d0">METRIC</data>
      <data key="d1">Task Score is an evaluation metric in WebShop defined as 100 times the average reward obtained across episodes.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
      <data key="d3">METRIC</data>
    </node>
    <node id="SUCCESS RATE">
      <data key="d0">METRIC</data>
      <data key="d1">Success Rate (SR) is an evaluation metric in WebShop defined as the portion of successful episodes.
Success Rate (SR) is a metric in WebShop defined as the portion of instructions where the reward equals 1</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9,fb2b4544aedd793e4d4ec3147320a51c</data>
      <data key="d3">METRIC</data>
    </node>
    <node id="INTERACTIVE INFORMATION RETRIEVAL">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="FUNCTION SIGNATURE">
      <data key="d0">COMPONENT</data>
      <data key="d1">A function signature is part of a programming problem, including the function name and parameters.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="DOCSTRING">
      <data key="d0">COMPONENT</data>
      <data key="d1">A docstring is a description of a function's purpose and behavior, included in programming problems.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="REFERENCE IMPLEMENTATION">
      <data key="d0">COMPONENT</data>
      <data key="d1">A reference implementation is a sample solution provided for a programming problem.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="UNIT TESTS">
      <data key="d0">COMPONENT</data>
      <data key="d1">Unit tests are tests provided to check the correctness of a function implementation in programming problems.
Tests that are used to validate the correctness of a function implementation</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f,fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="NATURAL LANGUAGE DESCRIPTION">
      <data key="d0">COMPONENT</data>
      <data key="d1">A natural language description explains a programming task in plain language, used in datasets like HumanEval and MBPP.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="ALGORITHMS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Algorithms are step-by-step procedures for solving problems, often evaluated in programming tasks.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="BASIC MATHEMATICS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Basic mathematics involves fundamental mathematical operations, often required in programming tasks.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="CROWDSOURCING">
      <data key="d0">PROCESS</data>
      <data key="d1">Crowdsourcing involves gathering input or data from a large group of people, used to create datasets like MBPP.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="QUERY SEARCHES">
      <data key="d0">ACTION/COMMAND</data>
      <data key="d1">Query searches are actions in WebShop that allow agents to search for products based on specified criteria.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="BUTTON CLICKS">
      <data key="d0">ACTION/COMMAND</data>
      <data key="d1">Button clicks are actions in WebShop that allow agents to interact with the web interface, such as selecting products or navigating pages.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="HTML MODE">
      <data key="d0">MODE</data>
      <data key="d1">HTML mode in WebShop provides pixel-level observations with interactive elements for agents to interact with.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="SIMPLE MODE">
      <data key="d0">MODE</data>
      <data key="d1">Simple mode in WebShop converts raw HTML into structured text observations for easier training of agents.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="LEXICAL MATCHING">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Lexical matching is a technique used in WebShop to compare the product purchased by the agent against specified attributes and options.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="SEMANTIC SIMILARITY">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Semantic similarity is a technique used in WebShop to compare the product purchased by the agent against specified attributes and options based on meaning.</data>
      <data key="d2">fb2b4544aedd793e4d4ec3147320a51c</data>
    </node>
    <node id="BAUER MEDIA GROUP">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Bauer Media Group is the publisher of the magazine First for Women</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="ARTHUR'S MAGAZINE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Arthur's Magazine was an American literary periodical published in Philadelphia in the 19th century</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="FIRST FOR WOMEN">
      <data key="d0">PUBLICATION</data>
      <data key="d1">First for Women is a woman's magazine published by Bauer Media Group in the USA, started in 1989
A magazine that is part of the question "Which magazine was started first Arthur&#8217;s Magazine or First for Women?"</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f,b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="GODEY'S LADY'S BOOK">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Godey's Lady's Book was a magazine that Arthur's Magazine was merged into in May 1846</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="QUERY">
      <data key="d0">ACTION</data>
      <data key="d1">Query is an action in WebShop where a search is performed</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="RESULTS">
      <data key="d0">STATE</data>
      <data key="d1">Results is a state in WebShop where search results are displayed</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="ITEM">
      <data key="d0">STATE</data>
      <data key="d1">Item is a state in WebShop where a specific product is selected</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="ITEM-DETAIL">
      <data key="d0">STATE</data>
      <data key="d1">Item-Detail is a state in WebShop where detailed information about a selected item is displayed</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="EPISODE END">
      <data key="d0">STATE</data>
      <data key="d1">Episode End is a state in WebShop where the buying process is completed</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="THOUGHT">
      <data key="d0">ACTION</data>
      <data key="d1">Thought is an action in HotPotQA where reasoning about the current situation is performed
A reasoning process about the current situation in a question answering task
The "thought" section captures the meta agent's reasoning, overall concept, and implementation steps for designing the next function or agent.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0,357f3442ba581c9d2bdf84d90509056f,b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="ACTION">
      <data key="d0">ACTION</data>
      <data key="d1">Action is an action in HotPotQA where a specific task is performed based on the Thought
An operation performed to progress in a question answering task, such as searching or looking up information
An action taken by the user during the interaction with the system</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f,5d356b8ff719763a38cecff22c4e17b7,b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="COLORADO OROGENY">
      <data key="d0">ENTITY</data>
      <data key="d1">Colorado Orogeny is a geological event mentioned in a question in HotPotQA</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="HIGH PLAINS">
      <data key="d0">ENTITY</data>
      <data key="d1">High Plains is a region mentioned in a question in HotPotQA</data>
      <data key="d2">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </node>
    <node id="VALUE FUNCTION PROMPT">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">A prompt that asks to analyze the trajectories of a solution to a question answering task, focusing on thoughts, actions, and observations
A prompt that asks to analyze a purchasing trajectory and conclude a correctness score from 1 to 10</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f,6f486e20e3102c7a285e357d356417ad</data>
      <data key="d3">INSTRUCTION</data>
    </node>
    <node id="SEARCH[ENTITY]">
      <data key="d0">ACTION</data>
      <data key="d1">An action type that searches for the exact entity on Wikipedia and returns the first paragraph if it exists</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
      <data key="d3">ACTION</data>
    </node>
    <node id="LOOKUP[KEYWORD]">
      <data key="d0">ACTION</data>
      <data key="d1">An action type that returns the next sentence containing the specified keyword in the current passage</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
      <data key="d3">ACTION</data>
    </node>
    <node id="FINISH[ANSWER]">
      <data key="d0">ACTION</data>
      <data key="d1">An action type that returns the answer and finishes the task</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
      <data key="d3">ACTION</data>
    </node>
    <node id="ARTHUR&#8217;S MAGAZINE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">An American literary periodical published in Philadelphia in the 19th century, edited by Timothy Shay Arthur</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="REFLECTION PROMPT">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">A prompt that asks to analyze the trajectories of a solution to a question-answering task, focusing on thoughts, actions, and observations
A prompt given to an AI Python assistant to guide the reflection on why a function implementation is wrong based on unit test results
A prompt that asks an advanced reasoning agent to diagnose a failure in a previous trial and devise a new plan to mitigate the failure</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f,6f486e20e3102c7a285e357d356417ad,785ad59c6a37896a4676ec5c1689735f</data>
      <data key="d3">INSTRUCTION</data>
    </node>
    <node id="PROGRAMMING PROMPTS">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">Prompts related to programming tasks, such as implementing a function or analyzing code</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
      <data key="d3">INSTRUCTION</data>
    </node>
    <node id="HUMANEVAL FUNCTION IMPLEMENTATION EXAMPLE">
      <data key="d0">EXAMPLE</data>
      <data key="d1">An example of a function implementation for evaluating human-like performance in programming tasks</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
      <data key="d3">EXAMPLE</data>
    </node>
    <node id="MINIMUM SUBARRAY SUM">
      <data key="d0">FUNCTION</data>
      <data key="d1">A function that finds the minimum sum of any non-empty sub-array of integers</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
      <data key="d3">FUNCTION</data>
    </node>
    <node id="GODEY&#8217;S LADY&#8217;S BOOK">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A magazine into which Arthur&#8217;s Magazine was merged in May 1846</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
    </node>
    <node id="TIMOTHY SHAY ARTHUR">
      <data key="d0">PERSON</data>
      <data key="d1">The editor of Arthur&#8217;s Magazine</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
    </node>
    <node id="EDGAR A. POE">
      <data key="d0">PERSON</data>
      <data key="d1">A contributor to Arthur&#8217;s Magazine</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
    </node>
    <node id="J.H. INGRAHAM">
      <data key="d0">PERSON</data>
      <data key="d1">A contributor to Arthur&#8217;s Magazine</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
    </node>
    <node id="SARAH JOSEPHA HALE">
      <data key="d0">PERSON</data>
      <data key="d1">A contributor to Arthur&#8217;s Magazine</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
    </node>
    <node id="THOMAS G. SPEAR">
      <data key="d0">PERSON</data>
      <data key="d1">A contributor to Arthur&#8217;s Magazine</data>
      <data key="d2">357f3442ba581c9d2bdf84d90509056f</data>
    </node>
    <node id="AI PYTHON ASSISTANT">
      <data key="d0">TOOL/ROLE</data>
      <data key="d1">An AI assistant designed to help with Python programming tasks, including implementing functions, running unit tests, and reflecting on code</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="FUNCTION IMPLEMENTATION">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">The process of writing and defining a function in Python, including its signature and body</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="UNIT TEST">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">A type of software testing where individual units or components of a software are tested to validate that each unit performs as expected</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="BRIGHT CITRUS DEODORANT">
      <data key="d0">PRODUCT</data>
      <data key="d1">A type of deodorant with a bright citrus scent, suitable for sensitive skin
Bright Citrus Deodorant by Earth Mama is a natural and safe deodorant for sensitive skin, pregnancy, and breastfeeding, containing organic calendula, available in a 3-ounce size</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad,785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="EARTH MAMA">
      <data key="d0">BRAND</data>
      <data key="d1">A brand that produces natural and safe products for sensitive skin, pregnancy, and breastfeeding
Earth Mama is the brand that produces the Bright Citrus Deodorant</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad,785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="GINGER FRESH DEODORANT">
      <data key="d0">PRODUCT</data>
      <data key="d1">A type of deodorant with a ginger fresh scent, suitable for sensitive skin
Ginger Fresh Deodorant by Earth Mama is a natural and safe deodorant for sensitive skin, pregnancy, and breastfeeding, containing organic calendula, available in a 3-ounce size</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad,785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="BARREL AND OAK">
      <data key="d0">BRAND</data>
      <data key="d1">A brand that produces aluminum-free deodorants and other personal care products</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="CEDAR &amp; PATCHOULI DEODORANT">
      <data key="d0">PRODUCT</data>
      <data key="d1">A type of deodorant with a cedar and patchouli blend, suitable for sensitive skin</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="MIN SUM">
      <data key="d0">CONCEPT</data>
      <data key="d1">A variable used in a function to keep track of the minimum sum encountered during iteration</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="CURRENT SUM">
      <data key="d0">CONCEPT</data>
      <data key="d1">A variable used in a function to keep track of the current sum during iteration</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="NUMS">
      <data key="d0">CONCEPT</data>
      <data key="d1">A list of numbers that the function iterates over to calculate sums</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="BASE ACTING/REASONING PROMPT">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A prompt given to an AI Python assistant to guide the implementation of a function based on previous code, unit tests, and self-reflection</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="IMPROVED IMPLEMENTATION">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">The revised version of a function after identifying and fixing errors from the previous implementation</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="TEST CASE GENERATION PROMPT">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A prompt given to an AI coding assistant to write unique and diverse unit tests for functions</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="ACTING PROMPT">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A prompt given to an AI assistant to guide actions in a specific scenario, such as searching for products in a webshop</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="THINK">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">The process of considering or reasoning about information before taking further action
An action to reflect or consider information before making a decision
The action of considering or reflecting on the current situation or next steps</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7,785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="CLICK">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">The action of selecting an item or link, such as a product in a webshop
An action to select or interact with a specific item or option
The action of selecting an option or button in the user interface</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7,785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="PRICE">
      <data key="d0">CONCEPT</data>
      <data key="d1">The cost of a product, such as deodorant in a webshop</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="SIZE">
      <data key="d0">CONCEPT</data>
      <data key="d1">The dimensions or volume of a product, such as a 3-ounce bottle of deodorant</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="SCENT">
      <data key="d0">CONCEPT</data>
      <data key="d1">The fragrance or smell of a product, such as bright citrus or ginger fresh</data>
      <data key="d2">785ad59c6a37896a4676ec5c1689735f</data>
    </node>
    <node id="ENJOY LIFE FOODS">
      <data key="d0">BRAND</data>
      <data key="d1">Enjoy Life Foods is a brand that produces various dairy-free, nut-free, soy-free, gluten-free, and vegan food products
A brand that offers various allergen-free food products, including dairy-free, nut-free, soy-free, and gluten-free options</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,6f486e20e3102c7a285e357d356417ad</data>
      <data key="d3">BRAND</data>
    </node>
    <node id="DAIRY FREE AND APPLE VARIETY PACK OF CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">A variety pack of chips that is dairy-free and includes apple flavor, produced by Enjoy Life Foods
A variety pack of chips that is dairy-free and includes apple flavor</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,6f486e20e3102c7a285e357d356417ad</data>
      <data key="d3">PRODUCT</data>
    </node>
    <node id="WEB SHOP">
      <data key="d0">PLATFORM</data>
      <data key="d1">An online shopping platform where users can search for and purchase items</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
      <data key="d3">PLATFORM</data>
    </node>
    <node id="CALMING LAVENDER DEODORANT">
      <data key="d0">PRODUCT</data>
      <data key="d1">Calming Lavender Deodorant by Earth Mama is a natural and safe deodorant for sensitive skin, pregnancy, and breastfeeding, containing organic calendula, available in a 3-ounce size</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="SIMPLY NON-SCENTS DEODORANT">
      <data key="d0">PRODUCT</data>
      <data key="d1">Simply Non-Scents Deodorant by Earth Mama is a natural and safe deodorant for sensitive skin, pregnancy, and breastfeeding, containing organic calendula, available in a 3-ounce size</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="ENJOY LIFE FOODS SOFT BAKED OVALS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Enjoy Life Foods Soft Baked Ovals are breakfast bars that are nut-free, soy-free, dairy-free, non-GMO, gluten-free, and vegan, available in a variety pack of 4 boxes
A specific product from Enjoy Life Foods that includes breakfast bars which are nut-free, soy-free, dairy-free, non-GMO, gluten-free, and vegan</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="ENJOY LIFE SOFT BAKED CHEWY BARS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Enjoy Life Soft Baked Chewy Bars are nut-free, soy-free, dairy-free, gluten-free bars available in a variety pack of 6 boxes
A specific product from Enjoy Life Foods that includes chewy bars which are nut-free, soy-free, dairy-free, and gluten-free</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="ENJOY LIFE LENTIL CHIPS VARIETY PACK">
      <data key="d0">PRODUCT</data>
      <data key="d1">Enjoy Life Lentil Chips Variety Pack are dairy-free, soy-free, nut-free, non-GMO, vegan, gluten-free chips available in a pack of 24 bags</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="TRAVEL SET (4-PACK)">
      <data key="d0">PRODUCT</data>
      <data key="d1">A travel set of deodorants by Earth Mama, available in a pack of 4</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="3 OUNCE (PACK OF 1)">
      <data key="d0">PRODUCT</data>
      <data key="d1">A 3-ounce bottle of deodorant by Earth Mama, available in a pack of 1</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="3-OUNCE (2-PACK)">
      <data key="d0">PRODUCT</data>
      <data key="d1">A 3-ounce bottle of deodorant by Earth Mama, available in a pack of 2</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="DAIRY FREE CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Dairy-free chips produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
      <data key="d3">PRODUCT</data>
    </node>
    <node id="NUT FREE BARS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Nut-free bars produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="SOY FREE BARS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Soy-free bars produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="GLUTEN FREE BARS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Gluten-free bars produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="VEGAN BARS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Vegan bars produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="NON GMO BARS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Non-GMO bars produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="SOY FREE CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Soy-free chips produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="NUT FREE CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Nut-free chips produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="NON GMO CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Non-GMO chips produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="VEGAN CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Vegan chips produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="GLUTEN FREE CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Gluten-free chips produced by Enjoy Life Foods</data>
      <data key="d2">6f486e20e3102c7a285e357d356417ad</data>
    </node>
    <node id="SOFT BAKED OVALS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Breakfast bars from Enjoy Life Foods that are nut-free, soy-free, dairy-free, non-GMO, gluten-free, and vegan</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="SOFT BAKED CHEWY BARS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Chewy bars from Enjoy Life Foods that are nut-free, soy-free, dairy-free, and gluten-free</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="LENTIL CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">Lentil chips from Enjoy Life Foods that are dairy-free, soy-free, nut-free, non-GMO, vegan, and gluten-free</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="VARIETY PACK">
      <data key="d0">OPTION</data>
      <data key="d1">An option for the Enjoy Life Lentil Chips that includes multiple flavors</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="0.8 OUNCE (PACK OF 24)">
      <data key="d0">SIZE</data>
      <data key="d1">A size option for the Enjoy Life Lentil Chips variety pack</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="BUY NOW">
      <data key="d0">ACTION</data>
      <data key="d1">An action to purchase a product immediately</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON">
      <data key="d0">PRODUCT</data>
      <data key="d1">A gluten-free, vegetarian product with smoked peppered bacon flavor</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="SMOKED BACON SEA SALT">
      <data key="d0">PRODUCT</data>
      <data key="d1">A 3-pack of smoked bacon sea salt flavors that are gluten-free, non-GMO, and contain no MSG</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="SPICY HOT PEPPER SEA SALT">
      <data key="d0">PRODUCT</data>
      <data key="d1">A 3-pack of spicy hot pepper sea salt flavors that are gluten-free, kosher, non-GMO, and contain no MSG</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="LOUISVILLE VEGAN JERKY">
      <data key="d0">PRODUCT</data>
      <data key="d1">A 5-flavor variety pack of vegan jerky that is non-GMO, soy protein-based, and gluten-free
A 5-flavor variety pack of vegan jerky made from non-GMO soy protein and gluten-free. Flavors include Black Pepper, Buffalo Dill, Pepperoni, Maple Bacon, and Carolina BBQ</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="PREVIOUS TRIAL INSTRUCTION">
      <data key="d0">ACTION</data>
      <data key="d1">An instruction given for a previous search or task</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="ENJOY LIFE LENTIL CHIPS">
      <data key="d0">PRODUCT</data>
      <data key="d1">A specific product from Enjoy Life Foods that includes lentil chips which are dairy-free, soy-free, nut-free, non-GMO, vegan, and gluten-free</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="SMOKED BACON CHIPOTLE">
      <data key="d0">PRODUCT</data>
      <data key="d1">A flavor option in the Smoked Bacon Sea Salt 3-Pack that is gluten-free, non-GMO, and contains no MSG</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="SMOKED BACON AND ONION">
      <data key="d0">PRODUCT</data>
      <data key="d1">A flavor option in the Smoked Bacon Sea Salt 3-Pack that is gluten-free, non-GMO, and contains no MSG</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </node>
    <node id="GHOST PEPPER">
      <data key="d0">PRODUCT</data>
      <data key="d1">A flavor option in the Spicy Hot Pepper Sea Salt 3-Pack that is gluten-free, kosher, non-GMO, and contains no MSG
A type of hot pepper used in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="JALAPENO">
      <data key="d0">PRODUCT</data>
      <data key="d1">A flavor option in the Spicy Hot Pepper Sea Salt 3-Pack that is gluten-free, kosher, non-GMO, and contains no MSG
A type of hot pepper used in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="HABANERO">
      <data key="d0">PRODUCT</data>
      <data key="d1">A flavor option in the Spicy Hot Pepper Sea Salt 3-Pack that is gluten-free, kosher, non-GMO, and contains no MSG
A type of hot pepper used in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="BLACK PEPPER">
      <data key="d0">PRODUCT</data>
      <data key="d1">A flavor option in the Louisville Vegan Jerky 5-Flavor Variety Pack that is non-GMO, soy protein-based, and gluten-free
One of the flavors in the Louisville Vegan Jerky variety pack</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="BUFFALO DILL">
      <data key="d0">PRODUCT</data>
      <data key="d1">A flavor option in the Louisville Vegan Jerky 5-Flavor Variety Pack that is non-GMO, soy protein-based, and gluten-free
One of the flavors in the Louisville Vegan Jerky variety pack</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="PEPPERONI">
      <data key="d0">PRODUCT</data>
      <data key="d1">A flavor option in the Louisville Vegan Jerky 5-Flavor Variety Pack that is non-GMO, soy protein-based, and gluten-free
One of the flavors in the Louisville Vegan Jerky variety pack</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="MAPLE BACON">
      <data key="d0">PRODUCT</data>
      <data key="d1">A flavor option in the Louisville Vegan Jerky 5-Flavor Variety Pack that is non-GMO, soy protein-based, and gluten-free
One of the flavors in the Louisville Vegan Jerky variety pack</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="CAROLINA BBQ">
      <data key="d0">PRODUCT</data>
      <data key="d1">A flavor option in the Louisville Vegan Jerky 5-Flavor Variety Pack that is non-GMO, soy protein-based, and gluten-free
One of the flavors in the Louisville Vegan Jerky variety pack</data>
      <data key="d2">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="NON-GMO">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Non-GMO refers to products that are not genetically modified</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="SPICY HOT PEPPER SEA SALT 3-PACK">
      <data key="d0">PRODUCT</data>
      <data key="d1">A 3-pack of spicy hot pepper sea salt including Ghost Pepper, Jalapeno, and Habanero flavors. It is all-natural, gluten-free, kosher, no MSG, and non-GMO</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="GLUTEN-FREE">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Indicates that the product does not contain gluten</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="KOSHER">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Indicates that the product meets kosher dietary standards</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="NO MSG">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Indicates that the product does not contain monosodium glutamate</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="SOY PROTEIN">
      <data key="d0">INGREDIENT</data>
      <data key="d1">A plant-based protein used in Louisville Vegan Jerky</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="REFINE SEARCH">
      <data key="d0">ACTION</data>
      <data key="d1">The process of narrowing down search results to better match desired criteria</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="VEGETARIAN BACON">
      <data key="d0">PRODUCT</data>
      <data key="d1">A type of bacon alternative made from vegetarian ingredients</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="GLUTEN-FREE AND 4 OUNCE PACK OF 2">
      <data key="d0">CONSTRAINT</data>
      <data key="d1">Specific requirements for the product being searched for</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="STATUS">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">The current state or condition of the system or process</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="FAIL">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Indicates that the attempt or action was unsuccessful</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="PREVIOUS TRIAL">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Refers to the earlier attempt or session in the process</data>
      <data key="d2">5d356b8ff719763a38cecff22c4e17b7</data>
    </node>
    <node id="SHENGRAN HU">
      <data key="d0">PERSON</data>
      <data key="d1">Shengran Hu is a researcher involved in the study of Automated Design of Agentic Systems and is affiliated with the University of British Columbia and the Vector Institute
Shengran Hu is the author of the paper on Automated Design of Agentic Systems and the creator of the Meta Agent Search algorithm
Shengran Hu is an author of the paper "Thought Cloning: Learning to think while acting by imitating human thinking"
Shengran Hu is an author of the paper "Intelligent go-explore: Standing on the shoulders of giant foundation models"
The author or maintainer of the framework code available on GitHub
Shengran Hu is associated with the experiment and the repository containing all agents from the experiment
Shengran Hu is associated with the implementation of baselines and the repository at https://github.com/ShengranHu/ADAS</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,24d7b89ae9522ae60d2317984951355b,449db721e37968e073e3579b59e023b2,6109537356a2ce2339f77c827aa3668e,97457e990eb6e3c88c11c862f9e3265b,c3d0436082aada237ee4bee645f16059,d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="CONG LU">
      <data key="d0">PERSON</data>
      <data key="d1">Cong Lu is a researcher involved in the study of Automated Design of Agentic Systems and is affiliated with the University of British Columbia and the Vector Institute
Cong Lu is an author of the paper "The AI Scientist: Towards fully automated open-ended scientific discovery"
Cong Lu is an author of the paper "Intelligent go-explore: Standing on the shoulders of giant foundation models"
Cong Lu is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,6109537356a2ce2339f77c827aa3668e,c3d0436082aada237ee4bee645f16059,cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </node>
    <node id="UNIVERSITY OF BRITISH COLUMBIA">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">The University of British Columbia is an educational institution where some of the researchers involved in the study of Automated Design of Agentic Systems are affiliated</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="VECTOR INSTITUTE">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">The Vector Institute is a research institution where some of the researchers involved in the study of Automated Design of Agentic Systems are affiliated
An organization that supported the work on Automated Design of Agentic Systems</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="CANADA CIFAR AI CHAIR">
      <data key="d0">TITLE</data>
      <data key="d1">The Canada CIFAR AI Chair is a title held by Jeff Clune, one of the researchers involved in the study of Automated Design of Agentic Systems</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)">
      <data key="d0">RESEARCH AREA</data>
      <data key="d1">Automated Design of Agentic Systems (ADAS) is a research area focused on automatically creating powerful agentic system designs, including inventing novel building blocks and/or combining them in new ways</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="META AGENT SEARCH">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Meta Agent Search is an algorithm that iteratively programs new agents, tests their performance on tasks, and adds them to an archive of discovered agents to inform subsequent iterations
Meta Agent Search is an algorithm in ADAS that enables the complete design of agentic systems in code space
Meta Agent Search is an algorithm that demonstrates the approach of defining and searching for agents in the context of ADAS
Meta Agent Search is an algorithm that defines and searches for agents in code by adopting FMs as meta agents to iteratively program new agents based on an ever-growing archive of previous discoveries
Meta Agent Search is a process that progressively discovers high-performance agents based on an ever-growing archive of previous discoveries
Meta Agent Search is a method used to discover agents that outperform state-of-the-art hand-designed baselines across multiple domains such as reading comprehension, math, multi-task problem solving, and science
Meta Agent Search is a process used to discover effective agents tailored to specific domains, showcasing its effectiveness across various domains
Meta Agent Search is a method used to discover generalizable design patterns and agentic systems that can be transferred across different domains
Meta Agent Search involves executing untrusted model-generated code and raises safety concerns
Meta Agent Search is an algorithm that can be asked to be safe during training and create helpful, harmless, honest agents
A process where a meta agent iteratively builds on previous discoveries to program new agents

Meta Agent Search is a method used to discover agents in various domains
A process that aims to improve results and reduce costs in agent discovery and evaluation</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,449db721e37968e073e3579b59e023b2,4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479,7de66b94cf868b37b1df51dc545c415f,81c504ffbcc5ed882e234802135295ba,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b,c3d0436082aada237ee4bee645f16059,dc55f071b95dec721a9820d39cdb3ccd,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="FOUNDATION MODELS (FMS)">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Foundation Models (FMs) are used as modules within agentic systems for tasks that need flexible reasoning and planning
Foundation Models (FMs) are modules used in the control flow of agentic systems to solve tasks by planning, using tools, and carrying out multiple, iterative steps of processing
Foundation Models are models queried by the meta agent within the framework to assist in generating and improving agent architectures.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0,4884e8429ca1e567dadf5e22b4b68274,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="CLAUDE">
      <data key="d0">MODEL</data>
      <data key="d1">Claude is a Foundation Model developed by Anthropic, used for general-purpose agentic tasks</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="CHAIN-OF-THOUGHT">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Chain-of-Thought is a planning and reasoning technique used as a building block in agentic systems
Chain-of-Thought (COT) is a technique used to generate possible answers, refine them, and ensemble the best answers in the Meta Agent Search algorithm
Chain-of-Thought (Wei et al., 2022) is a state-of-the-art hand-designed agent used as a baseline in Meta Agent Search
Chain-of-Thought is a manually designed agent method used for comparison in the evaluation of Meta Agent Search
Chain-of-Thought is a method used for planning and reasoning in agentic systems
Chain-of-Thought is a manually designed agent used for various tasks such as Math, Reading Comprehension, Multi-task, and Science</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb,bc26e68b0b2783ba912b9e5606d9eb0b,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="TOOLFORMER">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Toolformer is a technique used in agentic systems to enable the use of external tools</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="META AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">A meta agent is an agent that programs other agents, tests their performance, and iteratively improves them
A meta agent is an agent that creates other agents, often used in the context of ADAS
Meta Agent is a concept where a meta agent programs better agents in code, enhancing the ADAS algorithm's ability to discover new agentic systems
Meta agent is an agent used in ADAS to program new agents in code
The meta agent is a system designed to generate and improve agent architectures through iterative self-reflection and debugging processes.
An agent designed to generate code solutions for the ARC challenge tasks
Meta agent is an agent using the "gpt-4o-2024-05-13" model for evaluation
The meta agent uses GPT-4o-2024-05-13 to find optimal agents for various benchmarks</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,282313a8340c6792e8c35f53ed157cd0,4884e8429ca1e567dadf5e22b4b68274,4b43decac6833d1515992f8869ecada7,6bdf681c0bd9e401ac72344a6a0ae479,81c504ffbcc5ed882e234802135295ba,84317ae35cc75d612287186d93461447,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="AGENT ARCHIVE">
      <data key="d0">REPOSITORY</data>
      <data key="d1">Agent Archive is a repository where discovered agents are stored and used to inform the meta agent in subsequent iterations</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="MULTI-STEP PEER REVIEW AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">Multi-Step Peer Review Agent is an example of an agent discovered by the Meta Agent Search algorithm
Multi-Step Peer Review Agent is an agent discovered during the search in the Reading Comprehension domain</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="VERIFIED MULTIMODAL AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">Verified Multimodal Agent is an example of an agent discovered by the Meta Agent Search algorithm
Verified Multimodal Agent is an agent discovered during the search in the Math domain
An agent designed to solve problems using visual representations and verification processes</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,c3d0436082aada237ee4bee645f16059,ef75d2c866bee783577ed9f65707cf13</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="DIVIDE AND CONQUER AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">Divide and Conquer Agent is an example of an agent discovered by the Meta Agent Search algorithm
Divide and Conquer Agent is an agent discovered during the search in the Reading Comprehension domain</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="HOG">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">HOG (Histogram of Oriented Gradients) is a hand-designed feature used in computer vision that was eventually replaced by learned features from Convolutional Neural Networks</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="CONVOLUTIONAL NEURAL NETWORKS (CNNS)">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Convolutional Neural Networks (CNNs) are a type of neural network used in computer vision that replaced hand-designed features like HOG</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="NEURAL ARCHITECTURE SEARCH">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Neural Architecture Search is a method used to automatically design neural network architectures, leading to the best-performing CNN models
Neural Architecture Search is a method for automating the design of neural network architectures
Neural Architecture Search is a research area related to ADAS, focusing on optimizing neural network architectures
Neural Architecture Search is a technique aimed at automating the design of neural network architectures
A method that shows insights into Neural Networks by observing the emerged architecture</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,7c08d98f503d722d7de13be55375c8cb,7de66b94cf868b37b1df51dc545c415f,81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="AUTOML">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">AutoML (Automated Machine Learning) is a method that automates the process of applying machine learning to real-world problems
AutoML methods are automated machine learning techniques that optimize the process of applying machine learning to real-world problems
AutoML (Automated Machine Learning) is a research area related to ADAS
AutoML (Automated Machine Learning) is a method used for automating the process of machine learning model development
AutoML is a field of research that aims to automate the process of machine learning model development</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,4884e8429ca1e567dadf5e22b4b68274,7c08d98f503d722d7de13be55375c8cb,81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="AI-GENERATING ALGORITHMS (AI-GAS)">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">AI-Generating Algorithms (AI-GAs) are methods that automatically generate AI systems, demonstrating the superiority of learned AI systems over hand-designed ones
AI-Generating Algorithms are methods that automatically generate AI systems</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">TECHNIQUE</data>
    </node>
    <node id="SHENGRAN HU'S GITHUB">
      <data key="d0">RESOURCE</data>
      <data key="d1">Shengran Hu's GitHub is a resource where the code related to the Automated Design of Agentic Systems can be found</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">RESOURCE</data>
    </node>
    <node id="ANTHROPIC">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Anthropic is the organization that developed the Claude Foundation Model
Anthropic is the organization behind the Claude-Haiku and Claude-Sonnet models
An organization that introduced the next generation of Claude and Claude 3.5 Sonnet</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,7de66b94cf868b37b1df51dc545c415f,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">ORGANIZATION</data>
    </node>
    <node id="WANG ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Wang et al. are authors who have contributed to the research on Foundation Models and agentic systems
Wang et al. are the authors of the COT-SC agent
Wang et al. are the authors of the method for developing new skills for embodied agents in codeWang et al. are the authors of the COT-SC method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="ROCKT&#196;SCHEL">
      <data key="d0">AUTHOR</data>
      <data key="d1">Rockt&#228;schel is an author who has contributed to the research on compound agentic systems</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="ZAHARIA ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Zaharia et al. are authors who have contributed to the research on compound agentic systems</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="HU &amp; CLUNE">
      <data key="d0">AUTHOR</data>
      <data key="d1">Hu &amp; Clune are authors who have contributed to the research on chain-of-thought planning and reasoning
Hu &amp; Clune are the authors of the Chain-of-Thought-based planning and reasoning methods</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="ZHANG ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Zhang et al. are authors who have contributed to the research on memory structures in agentic systems
Zhang et al. are the authors of the External Memory and RAG methods
Zhang et al. are authors referenced in the context of Open-ended Algorithms</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,6bdf681c0bd9e401ac72344a6a0ae479,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="QU ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Qu et al. are authors who have contributed to the research on tool use in agentic systems
Qu et al. are the authors of the Tool Use method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="MADAAN ET AL.">
      <data key="d0">AUTHOR</data>
      <data key="d1">Madaan et al. are authors who have contributed to the research on self-reflection in agentic systems
Madaan et al. are the authors of the Self-Refine agent
Madaan et al. are the authors of the Self-Refine method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="CLUNE (2019)">
      <data key="d0">AUTHOR</data>
      <data key="d1">Clune (2019) is an author who has contributed to the research on the history of machine learning and the replacement of hand-designed solutions with learned solutions
A publication by Clune in 2019 that discusses AI-Generating Algorithms</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="DALAL &amp; TRIGGS (2005)">
      <data key="d0">AUTHOR</data>
      <data key="d1">Dalal &amp; Triggs (2005) are authors who have contributed to the research on hand-designed features in computer vision</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="KRIZHEVSKY ET AL. (2012)">
      <data key="d0">AUTHOR</data>
      <data key="d1">Krizhevsky et al. (2012) are authors who have contributed to the research on Convolutional Neural Networks
A publication by Krizhevsky et al. in 2012 that discusses Convolutional Neural Networks</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="HUTTER ET AL. (2019)">
      <data key="d0">AUTHOR</data>
      <data key="d1">Hutter et al. (2019) are authors who have contributed to the research on AutoML methods
A publication by Hutter et al. in 2019 that discusses AutoML methods</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="ELSKEN">
      <data key="d0">AUTHOR</data>
      <data key="d1">Elsken is an author who has contributed to the research on Neural Architecture Search
Elsken is an author referenced in the context of Neural Architecture Search</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">AUTHOR</data>
    </node>
    <node id="MEMORY STRUCTURES">
      <data key="d0" />
      <data key="d1">Memory structures are components in agentic systems that store information for future use, aiding in reasoning and planning</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="TOOL USE">
      <data key="d0" />
      <data key="d1">
Tool use is a component of agentic systems that involves using external tools to accomplish tasks
Tool Use is a method used for improving the performance of models through the use of external tools
Tool use is one of the skills covered by the synthetic post-training dataset created by AgentInstruct
Tool use involves the employment of functions or APIs to perform tasks or solve problems.
Tool use involves the manipulation of tools to achieve goals. In AI, it refers to the ability of an AI system to use available resources or auxiliary systems to solve complex tasks.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,0c212c1467564ad33330b1f655a8e27e,4884e8429ca1e567dadf5e22b4b68274,b88745a13b69cecbc0ee9c3af41389bf,c3d0436082aada237ee4bee645f16059,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="BUILDING BLOCKS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Building blocks are fundamental components used to construct agentic systems, such as chain-of-thought, self-reflection, and tool use
Building blocks refer to the fundamental components used to construct agentic systems
Building blocks refer to existing components that can be used to seed ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479,81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="COMPOUND AGENTIC SYSTEM">
      <data key="d0">CONCEPT</data>
      <data key="d1">A compound agentic system is an agentic system composed of multiple components or building blocks to solve complex tasks</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="MONOLITHIC MODEL QUERY">
      <data key="d0">CONCEPT</data>
      <data key="d1">A monolithic model query is a single, standalone model used to solve a task, as opposed to a compound agentic system</data>
      <data key="d2">c3d0436082aada237ee4bee645f16059</data>
    </node>
    <node id="APPENDIX F">
      <data key="d0">DOCUMENT SECTION</data>
      <data key="d1">Appendix F contains the detailed code of example agents</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="CNN">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Convolutional Neural Networks (CNNs) are a type of deep learning model used in various AI applications</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="ELSKEN ET AL. (2019)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Elsken et al. in 2019 that discusses Neural Architecture Search</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="SHEN ET AL. (2023)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Shen et al. in 2023 that discusses Neural Architecture Search</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="LLM ALIGNMENT">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">LLM alignment refers to aligning large language models to specific tasks or goals</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="LEARNED LOSS FUNCTIONS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Learned loss functions are loss functions that are optimized through learning rather than manually designed</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="LU ET AL. (2024A)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Lu et al. in 2024 that discusses learned loss functions in LLM alignment</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="DPO">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">DPO is a hand-designed loss function used in LLM alignment</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="RAFAILOV ET AL. (2024)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Rafailov et al. in 2024 that discusses DPO</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="AI SCIENTIST">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">AI Scientist is an automated research pipeline for developing novel machine learning algorithms</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="LU ET AL. (2024B)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Lu et al. in 2024 that discusses the AI Scientist</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="OMNI-EPIC">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">OMNI-EPIC is a system for automatically generating robotics learning environments
OMNI-EPIC is a technique that enables the generation of learning environments in an open-ended manner
OMNI-EPIC enables FMs to create robotics learning environments by programming in code</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb,81c504ffbcc5ed882e234802135295ba,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="FALDOR ET AL. (2024)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Faldor et al. in 2024 that discusses OMNI-EPIC</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="ADAS">
      <data key="d0">RESEARCH AREA</data>
      <data key="d1">Automated Design of Agentic Systems (ADAS) is a research area focused on automating the invention of novel building blocks and designing powerful agentic systems
Automated Design of Agentic Systems (ADAS) is a research area that involves using a search algorithm to discover agentic systems across a search space that optimize an evaluation function
ADAS (Automated Design of Agentic Systems) is a field of study that involves using programming languages as the search space for designing agentic systems
ADAS (Automated Design of Agentic Systems) is a process that progressively discovers agents that outperform state-of-the-art hand-designed baselines and invent novel design patterns through the innovation and combination of various stepping stones
ADAS (Automated Design of Agentic Systems) is a research area focused on inventing novel building blocks and designing powerful agentic systems in an automated manner
ADAS (Automated Design of Agentic Systems) is a proposed research area aimed at inventing novel building blocks and designing powerful agentic systems in an automated manner
Automated Design of Agentic Systems (ADAS) involves learning more components in agents than just prompts
ADAS stands for Automated Design of Agentic Systems, which are algorithms that can be programmed using powerful FMs without expensive hardware
Automated Design of Agentic Systems, a research problem aiming to automatically invent novel building blocks and design powerful agentic systems
ADAS is a repository where detailed implementations of all baselines can be found</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479,7c08d98f503d722d7de13be55375c8cb,7de66b94cf868b37b1df51dc545c415f,81c504ffbcc5ed882e234802135295ba,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ARC LOGIC PUZZLE TASK">
      <data key="d0">BENCHMARK</data>
      <data key="d1">The ARC logic puzzle task is a benchmark that tests the general intelligence of an AI system</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="CHOLLET (2019)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Chollet in 2019 that discusses the ARC logic puzzle task</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="DROP">
      <data key="d0">BENCHMARK</data>
      <data key="d1">DROP is a benchmark for reading comprehension tasks
DROP (Discrete Reasoning Over Paragraphs) is a dataset used to assess the reading comprehension abilities of agents
DROP (Dua et al., 2019) is a benchmark for evaluating reading comprehension
DROP is a dataset used for evaluating agents in the Reading Comprehension domain
DROP (Reading Comprehension) is a benchmark that assesses the ability to perform discrete reasoning and comprehend detailed information across multiple paragraphs
DROP is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4
Discrete Reasoning over Paragraphs (DROP) is a reading comprehension benchmark requiring models to resolve references in questions and perform discrete operations like sorting, counting, and addition.
DROP is a benchmark used to evaluate models on reading comprehension tasks
A dataset used for problems where a ground-truth answer value is given in exact match/span extraction problems</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50,10fda605f670bcfccfc13c2ca0dde959,24d7b89ae9522ae60d2317984951355b,81c504ffbcc5ed882e234802135295ba,84317ae35cc75d612287186d93461447,86f77e15d41cbd0cb33f635ccb2cb66b,bb87f82e6a9f1d4da6480ec78a0e3701,bc26e68b0b2783ba912b9e5606d9eb0b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="DUA ET AL. (2019)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Dua et al. in 2019 that discusses the DROP benchmark</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="MGSM">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MGSM is a benchmark for math tasks
MGSM (Math Generalization and Symbolic Manipulation) is a dataset used to evaluate the math abilities of agents
MGSM (Shi et al., 2023) is a benchmark for evaluating math capability under a multi-lingual setting
MGSM is a math dataset used to evaluate the performance of agents discovered by Meta Agent Search
MGSM (Multilingual Grade School Math Benchmark) evaluates mathematical problem-solving abilities across various languages
MGSM is the Math domain where the Verified Multimodal Agent was discovered</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,81c504ffbcc5ed882e234802135295ba,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="SHI ET AL. (2023)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Shi et al. in 2023 that discusses the MGSM benchmark</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="GSM8K">
      <data key="d0">BENCHMARK</data>
      <data key="d1">GSM8K is a benchmark for math tasks
GSM8K is a dataset used to evaluate the transferability of discovered agents on math tasks
GSM8K is a math dataset used to evaluate the performance of agents discovered by Meta Agent Search
GSM8K is a dataset used for evaluating the performance of models in mathematical problem-solving tasks
GSM8K is a benchmark used to evaluate the performance of language models
GSM8K is a benchmark used to evaluate the performance of AI models
GSM8K is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4
Grade School Math 8K (GSM8K) is a dataset of high-quality, diverse grade school math word problems requiring between 2 and 8 steps to solve.
GSM8K is a benchmark used to evaluate models on math problem-solving tasks
A dataset used for math-based questions in exact match/span extraction problems</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,103d98395c393552cc954c89d4e59f50,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,6fe27f9eb76cf2ddf712a2cee5783d1c,81c504ffbcc5ed882e234802135295ba,86f77e15d41cbd0cb33f635ccb2cb66b,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
      <data key="d3">BENCHMARK</data>
    </node>
    <node id="COBBE ET AL. (2021)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Cobbe et al. in 2021 that discusses the GSM8K benchmark</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="GSM-HARD">
      <data key="d0">BENCHMARK</data>
      <data key="d1">GSM-Hard is a benchmark for math tasks
GSM-Hard is a dataset used to evaluate the transferability of discovered agents on more challenging math tasks
GSM-Hard is a math dataset used to evaluate the performance of agents discovered by Meta Agent Search
GSM-Hard is a dataset used for evaluating the performance of models in more challenging mathematical problem-solving tasks</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">BENCHMARK</data>
    </node>
    <node id="GAO ET AL. (2023)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Gao et al. in 2023 that discusses the GSM-Hard benchmark</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="FERNANDO ET AL. (2024)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Fernando et al. in 2024 that discusses ADAS methods focusing on designing prompts</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="YANG ET AL. (2024)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Yang et al. in 2024 that discusses ADAS methods focusing on designing prompts</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="BOYER &amp; MOORE (1983)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Boyer &amp; Moore in 1983 that discusses Turing Completeness</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="LADHA (2024)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Ladha in 2024 that discusses Turing Completeness</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="FMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Foundation Models (FMs) are large-scale models proficient in coding, used as meta agents in ADAS
Foundation Models used in the Meta Agent Search process
Foundation Models (FMs) are large-scale models that possess knowledge to solve questions in various domains
</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,81c504ffbcc5ed882e234802135295ba,bc26e68b0b2783ba912b9e5606d9eb0b,dc55f071b95dec721a9820d39cdb3ccd</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="LU ET AL. (2024C)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Lu et al. in 2024 that discusses open-endedness algorithms leveraging human notions of interestingness</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="ZHANG ET AL. (2024A)">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Zhang et al. in 2024 that discusses open-endedness algorithms leveraging human notions of interestingness</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="AGENTIC SYSTEMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Agentic systems are systems designed to perform tasks autonomously
Agentic systems are systems that involve Foundation Models as modules to solve tasks by planning, using tools, and carrying out multiple, iterative steps of processing
Agentic systems are systems optimized to improve performance in various domains, particularly effective in mitigating errors like hallucinations or calculation mistakes
Agentic systems are machine learning systems that operate primarily over natural language and are interpretable to humans
Agentic Systems refer to systems designed to perform tasks autonomously</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,449db721e37968e073e3579b59e023b2,4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479,81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="ARC">
      <data key="d0">BENCHMARK</data>
      <data key="d1">ARC is a benchmark for logic puzzle tasks aimed at testing the general intelligence of AI systems
ARC (Abstraction and Reasoning Corpus) is a challenging logic puzzle task used in experiments to evaluate the performance of discovered agents
ARC is a dataset used to evaluate the performance of agents discovered by Meta Agent Search
Abstraction and Reasoning Corpus, a challenge involving learning transformation rules from input-output grid examples
ARC (Abstraction and Reasoning Corpus) is a dataset used to evaluate the performance of agents
ARC is a benchmark used for experiments in the paper, specifically mentioned in Section 4.1
A dataset used for search and evaluation experiments
ARC is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4
The AI2 Reasoning Challenge (ARC) is a benchmark developed by AllenAI to measure the reasoning, commonsense knowledge, and deep comprehension abilities of language models.</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,449db721e37968e073e3579b59e023b2,4b43decac6833d1515992f8869ecada7,81c504ffbcc5ed882e234802135295ba,86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="READING COMPREHENSION">
      <data key="d0">BENCHMARK</data>
      <data key="d1">Reading comprehension benchmarks test the ability of AI systems to understand and interpret text
Reading comprehension is a domain where agentic systems can be applied and evaluated
A domain tested by Meta Agent Search using the DROP benchmark
Reading Comprehension is a domain where FMs possess adequate knowledge to solve questions
Reading Comprehension is a task where models are evaluated on their ability to understand and interpret written text
Reading comprehension involves understanding, processing, and interpreting written text, which is necessary for learning and encompasses decoding, fluency, and vocabulary knowledge.
Reading comprehension is a critical skill involving processing and understanding text, necessary for learning and encompassing decoding, fluency, and vocabulary knowledge. It enables scenarios like question answering, search, and grounded reasoning.
Reading comprehension is a crucial capability for LLMs, especially for Small Language Models (SLMs), as they are better suited as reasoning engines than mere retrieval systems.</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,0c212c1467564ad33330b1f655a8e27e,2901d5e2711fa4f32d39cd8eea36cd71,4884e8429ca1e567dadf5e22b4b68274,81c504ffbcc5ed882e234802135295ba,86f77e15d41cbd0cb33f635ccb2cb66b,bc26e68b0b2783ba912b9e5606d9eb0b,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SCIENCE QUESTIONS">
      <data key="d0">BENCHMARK</data>
      <data key="d1">Science questions benchmarks test the ability of AI systems to answer questions related to science</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="MULTI-TASK PROBLEM SOLVING">
      <data key="d0">BENCHMARK</data>
      <data key="d1">Multi-task problem solving benchmarks test the ability of AI systems to solve a variety of tasks
A domain tested by Meta Agent Search using the MMLU benchmark</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="TRANSFERABILITY">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Transferability refers to the ability of AI systems to apply learned knowledge to different domains
Transferability refers to the ability of agents discovered by Meta Agent Search to perform well across different models and domains</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="OPEN-ENDEDNESS ALGORITHMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Open-endedness algorithms encourage the exploration of novel and interesting solutions</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="INTERESTINGNESS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Interestingness refers to the quality of being novel or worthwhile, often used in the context of open-endedness algorithms</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="F1 SCORES">
      <data key="d0">METRIC</data>
      <data key="d1">F1 scores are a measure of a model's accuracy, considering both precision and recall</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="ACCURACY RATES">
      <data key="d0">METRIC</data>
      <data key="d1">Accuracy rates measure the proportion of correct predictions made by a model</data>
      <data key="d2">81c504ffbcc5ed882e234802135295ba</data>
    </node>
    <node id="SEARCH SPACE">
      <data key="d0">COMPONENT</data>
      <data key="d1">The search space defines which agentic systems can be represented and thus discovered in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">COMPONENT</data>
    </node>
    <node id="EVALUATION FUNCTION">
      <data key="d0">COMPONENT</data>
      <data key="d1">The evaluation function defines how to evaluate a candidate agent on target objectives such as performance
A function used to evaluate the performance of discovered agents, which can be optimized to reduce costs</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,ef75d2c866bee783577ed9f65707cf13</data>
      <data key="d3">COMPONENT</data>
    </node>
    <node id="PROMPTBREEDER">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">PromptBreeder is a tool that mutates only the text prompts of an agent, but their other components, such as control flow, remain the same
PromptBreeder adopts FMs to automate prompt engineering for agents, focusing on the phrasing of instructions in the prompt to enhance reasoning capability</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,dc55f071b95dec721a9820d39cdb3ccd</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="CHASE">
      <data key="d0">PERSON</data>
      <data key="d1">Chase is an author referenced in the context of defining agentic systems involving Foundation Models</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NG">
      <data key="d0">PERSON</data>
      <data key="d1">Ng is an author referenced in the context of defining agentic systems involving Foundation Models</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CLUNE">
      <data key="d0">PERSON</data>
      <data key="d1">Clune is an author referenced in the context of research areas in AI-GAs
Clune is an author referenced in the context of discussions beyond the scope of the paper</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HUTTER">
      <data key="d0">PERSON</data>
      <data key="d1">Hutter is an author referenced in the context of research areas in AutoML</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FERNANDO">
      <data key="d0">PERSON</data>
      <data key="d1">Fernando is an author referenced in the context of PromptBreeder and other ADAS-related works</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHUGE">
      <data key="d0">PERSON</data>
      <data key="d1">Zhuge is an author referenced in the context of search spaces such as graph structures and reinforcement learning in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Liu is an author referenced in the context of search spaces such as feed-forward networks in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SUTTON">
      <data key="d0">PERSON</data>
      <data key="d1">Sutton is an author referenced in the context of the exploration-exploitation trade-off in search algorithms</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BARTO">
      <data key="d0">PERSON</data>
      <data key="d1">Barto is an author referenced in the context of the exploration-exploitation trade-off in search algorithms</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AI-GAS">
      <data key="d0">RESEARCH AREA</data>
      <data key="d1">AI-GAs (Artificial Intelligence-Generative Algorithms) is a research area related to ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="FEED-FORWARD NETWORKS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Feed-forward networks are a type of search space explored in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="GRAPH STRUCTURES">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Graph structures are a type of search space explored in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="EXPLORATION-EXPLOITATION TRADE-OFF">
      <data key="d0">CONCEPT</data>
      <data key="d1">The exploration-exploitation trade-off is a concept in search algorithms that balances discovering high-performance agentic systems and avoiding local optima</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="ACCURACY RATE">
      <data key="d0">METRIC</data>
      <data key="d1">Accuracy rate is a metric used in the evaluation function to assess an agent's performance on validation data</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="COST">
      <data key="d0">METRIC</data>
      <data key="d1">Cost is a metric used in the evaluation function to assess the expense of running an agent
Cost is an objective that can be considered in multi-objective ADAS
Cost refers to the resource-intensive nature of generating synthetic data with multiple agents using LLMs and tools</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="LATENCY">
      <data key="d0">METRIC</data>
      <data key="d1">Latency is a metric used in the evaluation function to assess the response time of an agent
Latency is an objective that can be considered in multi-objective ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479</data>
    </node>
    <node id="SAFETY">
      <data key="d0">METRIC</data>
      <data key="d1">Safety is a metric used in the evaluation function to assess the risk associated with an agent</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="MATHEMATICS">
      <data key="d0">DOMAIN</data>
      <data key="d1">Mathematics is a domain where agentic systems can be applied and evaluated</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="VALIDATION DATA">
      <data key="d0">DATA</data>
      <data key="d1">Validation data is used to assess the performance of an agent on unseen future data</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="CONTROL FLOW">
      <data key="d0">COMPONENT</data>
      <data key="d1">Control flow is a component of agentic systems that dictates the sequence of operations or steps an agent follows</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="TEXT PROMPTS">
      <data key="d0">COMPONENT</data>
      <data key="d1">Text prompts are components of agentic systems that can be mutated to create new agents</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="PROGRAMMING LANGUAGES">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Programming languages are used as a search space in ADAS to define and discover new agentic systems</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="OPEN-SOURCE AGENT FRAMEWORKS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Open-source agent frameworks like LangChain are used to build upon existing building blocks in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
    </node>
    <node id="AI SAFETY">
      <data key="d0">CONCEPT</data>
      <data key="d1">AI safety is a concept that involves ensuring the safe operation of AI systems, which can be enhanced by using readable program code in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="DEBUGGING">
      <data key="d0">PROCESS</data>
      <data key="d1">Debugging is a process that involves identifying and fixing issues in program code, which is made easier by using readable code in ADAS
Debugging is the process of analyzing running logs to improve agentic systems</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="UNSEEN FUTURE DATA">
      <data key="d0">DATA</data>
      <data key="d1">Unseen future data is data that an agent has not encountered before, used to evaluate its performance</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">DATA</data>
    </node>
    <node id="EXISTING HUMAN EFFORTS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Existing human efforts refer to the prior work and knowledge that can be built upon in ADAS, especially when using programming languages as the search space</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="EXISTING BUILDING BLOCKS">
      <data key="d0">COMPONENT</data>
      <data key="d1">Existing building blocks are components like RAG and search engine tools that can be used in open-source agent frameworks in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274</data>
      <data key="d3">COMPONENT</data>
    </node>
    <node id="SEARCH ENGINE TOOLS">
      <data key="d0">TOOL/COMPONENT</data>
      <data key="d1">Search engine tools are components that can be used in open-source agent frameworks like LangChain in ADAS
Search engine tools are existing human efforts that can be used as building blocks in ADAS</data>
      <data key="d2">4884e8429ca1e567dadf5e22b4b68274,6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TOOL/COMPONENT</data>
    </node>
    <node id="FM">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">FMs (Foundation Models) are used as meta agents in the Meta Agent Search algorithm to program new agents
FM stands for Foundation Models, which are used in ADAS to program agents
FM is a model used in various methods like COT and COT-SC</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b,6bdf681c0bd9e401ac72344a6a0ae479,97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="FUNSEARCH">
      <data key="d0">ALGORITHM</data>
      <data key="d1">FunSearch is an algorithm mentioned as a practice similar to Meta Agent Search, where a "forward" function is programmed to define a new agentic system
FunSearch is a tool where Foundation Models write code to discover better optimization algorithms</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="LLM DEBATE">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">LLM Debate is a technique used in the Meta Agent Search algorithm to enhance refinement through multiple critics
LLM Debate (Du et al., 2023) is a state-of-the-art hand-designed agent used as a baseline in Meta Agent Search
LLM Debate is a manually designed agent method used for comparison in the evaluation of Meta Agent Search
LLM Debate is a method used for improving the performance of models through debate and discussion
LLM Debate is a manually designed agent used for various tasks such as Math, Reading Comprehension, Multi-task, and Science</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="QUALITY-DIVERSITY">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Quality-Diversity is a technique used in the Meta Agent Search algorithm to explore interestingly new agents based on an ever-growing archive of previous discoveries
Quality-Diversity is a simplified version of Intelligent Go-Explore, which produces and ensembles diverse answers to better explore potential solutions
Quality-Diversity (Lu et al., 2024c) is a state-of-the-art hand-designed agent used as a baseline in Meta Agent Search
Quality-Diversity is a manually designed agent method used for comparison in the evaluation of Meta Agent Search
Quality-Diversity is a method used for improving the performance of models through diversity and quality control
Quality-Diversity is a manually designed agent used for various tasks such as Math, Reading Comprehension, Multi-task, and Science
Quality-Diversity is a concept that can be incorporated into the design of search algorithms for ADAS
Quality-Diversity is a simplified version of Intelligent Go-Explore and a state-of-the-art hand-designed agent baseline for experiments on ARC
Quality-Diversity is a method where three iterations are conducted to collect diverse answers based on previously proposed ones</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,6bdf681c0bd9e401ac72344a6a0ae479,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="ROMERA-PAREDES">
      <data key="d0">PERSON</data>
      <data key="d1">Romera-Paredes is an author mentioned in relation to the FunSearch algorithm</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="LU">
      <data key="d0">PERSON</data>
      <data key="d1">Lu is an author mentioned in relation to open-endedness algorithms that leverage human notions of interestingness</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhang is an author mentioned in relation to open-endedness algorithms that leverage human notions of interestingness</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="MADAAN">
      <data key="d0">PERSON</data>
      <data key="d1">Madaan is an author mentioned in relation to self-reflection iterations in the meta agent</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="SHINN">
      <data key="d0">PERSON</data>
      <data key="d1">Shinn is an author mentioned in relation to self-reflection iterations in the meta agent</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="CHOLLET">
      <data key="d0">PERSON</data>
      <data key="d1">Chollet is an author mentioned in relation to the ARC logic puzzle task</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="DUA">
      <data key="d0">PERSON</data>
      <data key="d1">Dua is an author mentioned in relation to the DROP dataset</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="SHI">
      <data key="d0">PERSON</data>
      <data key="d1">Shi is an author mentioned in relation to the MGSM dataset</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="COBBE">
      <data key="d0">PERSON</data>
      <data key="d1">Cobbe is an author mentioned in relation to the GSM8K dataset</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="GAO">
      <data key="d0">PERSON</data>
      <data key="d1">Gao is an author mentioned in relation to the GSM-Hard dataset</data>
      <data key="d2">24d7b89ae9522ae60d2317984951355b</data>
    </node>
    <node id="ARC CHALLENGE">
      <data key="d0">TASK/CHALLENGE</data>
      <data key="d1">The Abstraction and Reasoning Corpus (ARC) challenge aims to evaluate the general intelligence of AI systems through their ability to efficiently acquire new skills
A challenge involving learning transformation rules from input-output grid examples to predict the output grid for a test example</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="CHAIN-OF-THOUGHT (COT)">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Chain-of-Thought (COT) instructs the agent to output the reasoning before answering to improve complex problem-solving through intermediate steps
Chain-of-Thought (COT) is a state-of-the-art hand-designed agent baseline for experiments on ARC</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT (COT-SC)">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Self-Consistency with Chain-of-Thought (COT-SC) ensembles multiple parallel answers from COT to produce a more accurate answer
Self-Consistency with Chain-of-Thought (COT-SC) is a state-of-the-art hand-designed agent baseline for experiments on ARC</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="LLM-DEBATE">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">LLM-Debate enables different LLMs to debate with each other, leveraging diverse perspectives to find better answers
LLM-Debate is a state-of-the-art hand-designed agent baseline for experiments on ARC
LLM-Debate is a method where each debate module is assigned a unique role and the debate lasts for two rounds</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="ABSTRACTION AND REASONING CORPUS (ARC)">
      <data key="d0">DATASET</data>
      <data key="d1">The dataset used in the ARC challenge, consisting of visual input-output grid patterns</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="GREENBLATT, 2024">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to a common practice in the field, requiring the agent to write code for the transformation rule instead of answering directly</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="MADAAN ET AL., 2024">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to the authors who introduced the Self-Refine technique
A publication that discusses the Self-Refine method
A publication by Madaan et al. in 2024 related to Self-Refine
Madaan et al. (2024) is a reference cited in the context of self-reflection and improving the generated agent.
Madaan et al., 2024 is a publication referenced for the Self-Refine method</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,282313a8340c6792e8c35f53ed157cd0,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="DU ET AL., 2023">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to the authors who introduced the LLM-Debate technique
A publication that discusses the LLM Debate method
A publication by Du et al. in 2023 related to LLM Debate
Du et al., 2023 is a publication referenced for the LLM-Debate method</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="LU ET AL., 2024C">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to the authors who introduced the Quality-Diversity technique
A publication that discusses the Quality-Diversity method
A publication by Lu et al. in 2024 related to Quality-Diversity
Lu et al., 2024c is a publication referenced for the Quality-Diversity method</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="FALDOR ET AL., 2024">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to prior works on open-endedness and AI-GAs
A publication by Faldor et al. in 2024 related to OMNI-EPIC
A paper discussing OMNI-EPIC and its application in enabling FMs to create robotics learning environments by programming in code</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="LEHMAN &amp; STANLEY, 2011">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to prior works on open-endedness and AI-GAs</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="WANG ET AL., 2019">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to prior works on open-endedness and AI-GAs
A publication by Wang et al. in 2019 related to POET</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="WANG ET AL., 2020">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to prior works on open-endedness and AI-GAs
A publication by Wang et al. in 2020 related to POET</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="ZHANG ET AL., 2024A">
      <data key="d0">REFERENCE</data>
      <data key="d1">A reference to prior works on open-endedness and AI-GAs</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="APPENDIX C">
      <data key="d0">DOCUMENT SECTION</data>
      <data key="d1">A section in the document providing detailed implementation of the best agent discovered by Meta Agent Search</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="APPENDIX E">
      <data key="d0">DOCUMENT SECTION</data>
      <data key="d1">A section in the document providing more details about the baselines used in the study
The section of the document where more details about the baselines can be found</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="FIGURE 3">
      <data key="d0">VISUALIZATION</data>
      <data key="d1">A figure in the document showing the results of Meta Agent Search on the ARC challenge</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="PUBLIC TRAINING SET (EASY)">
      <data key="d0">DATASET</data>
      <data key="d1">A subset of the ARC dataset with grid dimensions &#8804;5&#215;5 used for training agents</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="VALIDATION SET">
      <data key="d0">DATASET</data>
      <data key="d1">A set of 20 questions sampled from the ARC dataset used for validating agents</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="TEST SET">
      <data key="d0">DATASET</data>
      <data key="d1">A set of 60 questions sampled from the ARC dataset used for testing agents</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="DYNAMIC MEMORY">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Dynamic memory is introduced for doing more refinements in the Meta Agent Search process</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="MULTIPLE CRITICS">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Multiple critics are introduced for enhanced refinement in the best agent discovered by Meta Agent Search</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="META-AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">The meta-agent in Meta Agent Search uses GPT-4 to discover novel agentic systems</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="CRITIC">
      <data key="d0">AGENT</data>
      <data key="d1">Critics are used to provide feedback for refining answers in the Meta Agent Search process</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="EFFICIENCY EXPERT">
      <data key="d0">AGENT</data>
      <data key="d1">An expert that evaluates the efficiency of answers in the Meta Agent Search process
Efficiency Expert is an expert role assigned to provide feedback on the efficiency of the code</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="READABILITY EXPERT">
      <data key="d0">AGENT</data>
      <data key="d1">An expert that evaluates the readability of answers in the Meta Agent Search process
Readability Expert is an expert role assigned to provide feedback on the readability of the code</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="SIMPLICITY EXPERT">
      <data key="d0">AGENT</data>
      <data key="d1">An expert that evaluates the simplicity of answers in the Meta Agent Search process
Simplicity Expert is an expert role assigned to provide feedback on the simplicity of the code</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="ENSEMBLE">
      <data key="d0">PROCESS</data>
      <data key="d1">Ensembling is used to combine the best answers in the Meta Agent Search process</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="HUMAN-LIKE FEEDBACK">
      <data key="d0">PROCESS</data>
      <data key="d1">Human-like feedback is simulated to refine answers in the Meta Agent Search process</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="TRANSFORMATION RULE">
      <data key="d0">CONCEPT</data>
      <data key="d1">The rule that the AI system learns to transform input grid patterns to output grid patterns in the ARC challenge
A rule learned from input-output grid examples to predict the output grid for a test example</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="NUMBER COUNTING">
      <data key="d0">SKILL</data>
      <data key="d1">A capability required by AI systems to efficiently learn from few-shot examples in the ARC challenge</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="GEOMETRY">
      <data key="d0">SKILL</data>
      <data key="d1">A capability required by AI systems to efficiently learn from few-shot examples in the ARC challenge</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="TOPOLOGY">
      <data key="d0">SKILL</data>
      <data key="d1">A capability required by AI systems to efficiently learn from few-shot examples in the ARC challenge</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="TOOL FUNCTIONS">
      <data key="d0">TOOL</data>
      <data key="d1">Functions provided in the framework to evaluate the generated transformation code in the ARC challenge</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="STOCHASTIC SAMPLING OF FMS">
      <data key="d0">PROCESS</data>
      <data key="d1">A process used to reduce variance in the validation and test accuracy of agents in the ARC challenge</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="ITERATION 3">
      <data key="d0">EVENT</data>
      <data key="d1">An iteration in Meta Agent Search where multiple COTs are used to generate possible answers, refine them, and ensemble the best answers</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7</data>
    </node>
    <node id="ITERATION 5">
      <data key="d0">EVENT</data>
      <data key="d1">An iteration in Meta Agent Search where the idea of incorporating diverse feedback emerged
The iteration where the idea of incorporating diverse feedback emerged</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="ITERATION 11">
      <data key="d0">EVENT</data>
      <data key="d1">An iteration in Meta Agent Search where the idea of evaluating for various specific traits via experts emerged
The iteration where the idea of evaluating for various specific traits such as efficiency and simplicity emerged</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="ITERATION 12">
      <data key="d0">EVENT</data>
      <data key="d1">An iteration in Meta Agent Search where the idea of simulating human-like feedback emerged
The iteration where the idea of simulating human-like feedback emerged</data>
      <data key="d2">1a6353c9d196dc2debad7c27c902bcd7,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="MMLU">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MMLU (Hendrycks et al., 2021) is a benchmark for evaluating multi-task problem solving
MMLU (Massive Multitask Language Understanding) benchmark assesses a model&#8217;s ability to answer questions across a wide range of subjects and difficulty levels
MMLU is a benchmark used to evaluate the performance of language models
MMLU is a benchmark used to evaluate the performance of AI models
MMLU is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4
Massive Multitask Language Understanding (MMLU) is a benchmark that measures a model&#8217;s multitask understanding across 57 academic subjects with approximately 16000 multiple-choice questions.
MMLU is a benchmark used to evaluate models on various mathematical tasks, including abstract algebra and college mathematics
MMLU is one of the datasets included in the MIRAGE collection</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,ab04427ae0415a1c812a35cf8d3ee1a2,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bc26e68b0b2783ba912b9e5606d9eb0b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="GPQA">
      <data key="d0">BENCHMARK</data>
      <data key="d1">GPQA (Rein et al., 2023) is a benchmark for evaluating the capability of solving hard (graduate-level) questions in science
GPQA is a dataset used for evaluating agents in the Science domain
GPQA (Graduate-Level Google-Proof Q&amp;A Benchmark) consists of challenging multiple-choice questions across biology, physics, and chemistry
GPQA is the Reading Comprehension domain where the Multi-Step Peer Review Agent and Divide and Conquer Agent were discovered
GPQA is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4
Graduate-level Google-Proof Q&amp;A (GPQA) is a challenging benchmark of 448 high-quality, difficult multiple-choice questions created by domain experts in biology, chemistry, and physics.
GPQA: A graduate-level Google-proof Q&amp;A benchmark is a paper published in 2023</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,3d1f6634f93f8a4c296dc8df7e59859e,84317ae35cc75d612287186d93461447,86f77e15d41cbd0cb33f635ccb2cb66b,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="STEP-BACK ABSTRACTION">
      <data key="d0">AGENT</data>
      <data key="d1">Step-back Abstraction (Zheng et al., 2023) is a state-of-the-art hand-designed agent used as a baseline in Meta Agent Search
Step-back Abstraction is a method used for improving the performance of models through abstraction and simplification
Step-back Abstraction is a manually designed agent used for various tasks such as Math, Reading Comprehension, Multi-task, and Science
Step-back Abstraction is a state-of-the-art hand-designed agent baseline for experiments on Reasoning and Problem-Solving domains
Step-back Abstraction is a method used in experiments on Reasoning and Problem-Solving domains</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,10fda605f670bcfccfc13c2ca0dde959,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="ROLE ASSIGNMENT">
      <data key="d0">AGENT</data>
      <data key="d1">Role Assignment (Xu et al., 2023) is a state-of-the-art hand-designed agent used as a baseline in Meta Agent Search
Role Assignment is a method used for assigning different roles to modules in an agentic system and enabling them to collaborate
Role Assignment is a manually designed agent used for various tasks such as Math, Reading Comprehension, Multi-task, and Science
Role Assignment is a state-of-the-art hand-designed agent baseline for experiments on Reasoning and Problem-Solving domains
Role Assignment is a method used in experiments on Reasoning and Problem-Solving domains</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,10fda605f670bcfccfc13c2ca0dde959,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="DUA ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Dua et al. are the authors of the DROP benchmark for evaluating reading comprehension</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="SHI ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Shi et al. are the authors of the MGSM benchmark for evaluating math capability under a multi-lingual setting</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="HENDRYCKS ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Hendrycks et al. are the authors of the MMLU benchmark for evaluating multi-task problem solving</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="REIN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Rein et al. are the authors of the GPQA benchmark for evaluating the capability of solving hard (graduate-level) questions in science</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="ZHENG ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Zheng et al. are the authors of the Step-back Abstraction agent
Zheng et al. are the authors of the Step-back Abstraction method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="LU ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Lu et al. are the authors of the Quality-Diversity agent
Lu et al. are the authors of the Quality-Diversity method
Lu et al. are authors referenced in the context of higher-order ADAS and subjective answer evaluations</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,6bdf681c0bd9e401ac72344a6a0ae479,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="XU ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Xu et al. are the authors of the Role Assignment agent
Xu et al. are the authors of the Role Assignment method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="MECHANISM">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A sophisticated feedback mechanism that refines answers more effectively by incorporating diverse feedback, evaluating for various specific traits, and simulating human-like feedback</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="MEYERSON ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Meyerson et al. are the authors who discussed the concept of crossover in evolution via LLMs</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="EXPERTS">
      <data key="d0">PERSON</data>
      <data key="d1">Individuals who evaluate various specific traits such as efficiency and simplicity</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="F1 SCORE">
      <data key="d0">METRIC</data>
      <data key="d1">A performance metric used to evaluate agents in the Reading Comprehension and Math domains</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="ACCURACY">
      <data key="d0">METRIC</data>
      <data key="d1">A performance metric used to evaluate agents in the Reading Comprehension and Math domains
Accuracy refers to the potential inaccuracies in synthetic data due to its inability to perfectly replicate real-world data</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="BOOTSTRAP CONFIDENCE INTERVAL">
      <data key="d0">METRIC</data>
      <data key="d1">A statistical measure reported in the performance comparison of Meta Agent Search and state-of-the-art hand-designed agents
Bootstrap confidence interval is a statistical method used to report the test accuracy of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="EXPERIMENT SETTINGS">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">Details about the datasets and experiment settings used in Meta Agent Search, found in Appendix D</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="BASELINES">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">The state-of-the-art hand-designed agents used for comparison in Meta Agent Search
Baselines are standard agents used for comparison in evaluations</data>
      <data key="d2">84317ae35cc75d612287186d93461447,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="APPENDIX D">
      <data key="d0">DOCUMENT</data>
      <data key="d1">The section of the document where more details about datasets and experiment settings can be found</data>
      <data key="d2">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </node>
    <node id="MULTI-TASK">
      <data key="d0">DOMAIN</data>
      <data key="d1">Multi-task is a domain where the knowledge in FMs is not sufficient to solve challenging questions
Multi-task is a task where models are evaluated on their ability to perform multiple different tasks simultaneously</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">DOMAIN</data>
    </node>
    <node id="CLAUDE-HAIKU">
      <data key="d0">MODEL</data>
      <data key="d1">Claude-Haiku is a model from Anthropic used to evaluate the performance of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="CLAUDE-SONNET">
      <data key="d0">MODEL</data>
      <data key="d1">Claude-Sonnet is a model from Anthropic used to evaluate the performance of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="STRUCTURED FEEDBACK AND ENSEMBLE AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">Structured Feedback and Ensemble Agent is one of the top agents discovered by Meta Agent Search
An agent that uses structured feedback and ensemble methods to solve tasks</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,449db721e37968e073e3579b59e023b2</data>
      <data key="d3">AGENT</data>
    </node>
    <node id="HIERARCHICAL COMMITTEE REINFORCEMENT AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">Hierarchical Committee Reinforcement Agent is one of the top agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">AGENT</data>
    </node>
    <node id="DYNAMIC MEMORY AND REFINEMENT AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">Dynamic Memory and Refinement Agent is one of the top agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">AGENT</data>
    </node>
    <node id="SVAMP">
      <data key="d0">DATASET</data>
      <data key="d1">SVAMP is a math dataset used to evaluate the performance of agents discovered by Meta Agent Search
SVAMP is a dataset used for evaluating the performance of models in mathematical problem-solving tasks</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">DATASET</data>
    </node>
    <node id="ASDIV">
      <data key="d0">DATASET</data>
      <data key="d1">ASDiv is a math dataset used to evaluate the performance of agents discovered by Meta Agent Search
ASDiv is a dataset used for evaluating the performance of models in mathematical problem-solving tasks</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">DATASET</data>
    </node>
    <node id="PATEL ET AL., 2021">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication that discusses the SVAMP dataset</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="MIAO ET AL., 2020">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication that discusses the ASDiv dataset</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="MULTI-TASK AND SCIENCE DOMAINS">
      <data key="d0">DOMAIN</data>
      <data key="d1">Multi-task and Science domains are areas where Meta Agent Search outperforms baselines, but the gap is smaller due to the challenging nature of the questions</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="HUMAN EFFORTS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Human efforts refer to the manual work that can be saved by using Meta Agent Search to develop better task-specific agents
Human efforts refer to existing work and tools that can be leveraged in ADAS</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71,6bdf681c0bd9e401ac72344a6a0ae479</data>
    </node>
    <node id="TASK-SPECIFIC AGENTS">
      <data key="d0">AGENT</data>
      <data key="d1">Task-specific agents are agents tailored to perform specific tasks effectively, discovered through Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="GENERALIZATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Generalization refers to the ability of agents discovered by Meta Agent Search to apply learned knowledge to new, unseen tasks or domains</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="MGSM (MATH)">
      <data key="d0">DOMAIN</data>
      <data key="d1">MGSM (Math) is a domain used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="GSM8K (COBBE ET AL., 2021)">
      <data key="d0">DOMAIN</data>
      <data key="d1">GSM8K (Cobbe et al., 2021) is a math domain used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="GSM-HARD (GAO ET AL., 2023)">
      <data key="d0">DOMAIN</data>
      <data key="d1">GSM-Hard (Gao et al., 2023) is a math domain used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="SVAMP (PATEL ET AL., 2021)">
      <data key="d0">DOMAIN</data>
      <data key="d1">SVAMP (Patel et al., 2021) is a math domain used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="ASDIV (MIAO ET AL., 2020)">
      <data key="d0">DOMAIN</data>
      <data key="d1">ASDiv (Miao et al., 2020) is a math domain used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </node>
    <node id="MGSM (MATH) DOMAIN">
      <data key="d0">DOMAIN</data>
      <data key="d1">MGSM (Math) domain is used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">DOMAIN</data>
    </node>
    <node id="FOUR POPULAR MATH DOMAINS">
      <data key="d0">DOMAIN</data>
      <data key="d1">Four popular math domains are used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">DOMAIN</data>
    </node>
    <node id="DOMAINS BEYOND MATH">
      <data key="d0">DOMAIN</data>
      <data key="d1">Domains beyond math are used to test the transferability and generalizability of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">DOMAIN</data>
    </node>
    <node id="TOP 3 AGENTS">
      <data key="d0">AGENT</data>
      <data key="d1">Top 3 agents are the best-performing agents discovered by Meta Agent Search, evaluated with GPT-3.5 on ARC and transferred to other models</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">AGENT</data>
    </node>
    <node id="INVENTED BUILDING BLOCKS AND DESIGN PATTERNS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Invented building blocks and design patterns are the components and strategies discovered by Meta Agent Search that contribute to the effectiveness of the agents</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="PERFORMANCE ON ARC">
      <data key="d0">CONCEPT</data>
      <data key="d1">Performance on ARC refers to the evaluation results of agents discovered by Meta Agent Search on the ARC dataset</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="TEST ACCURACY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Test accuracy is a measure of how well agents discovered by Meta Agent Search perform on specific datasets</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="95% BOOTSTRAP CONFIDENCE INTERVAL">
      <data key="d0">METHOD</data>
      <data key="d1">95% bootstrap confidence interval is a statistical method used to report the test accuracy of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">METHOD</data>
    </node>
    <node id="CLAUDE-SONNET (ANTHROPIC, 2024B)">
      <data key="d0">MODEL</data>
      <data key="d1">Claude-Sonnet (Anthropic, 2024b) is a model from Anthropic used to evaluate the performance of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="CLAUDE-HAIKU (ANTHROPIC, 2024A)">
      <data key="d0">MODEL</data>
      <data key="d1">Claude-Haiku (Anthropic, 2024a) is a model from Anthropic used to evaluate the performance of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="GPT-4 (OPENAI, 2024)">
      <data key="d0">MODEL</data>
      <data key="d1">GPT-4 (OpenAI, 2024) is a version of OpenAI's language model used to evaluate the performance of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="GPT-3.5 (OPENAI, 2022)">
      <data key="d0">MODEL</data>
      <data key="d1">GPT-3.5 (OpenAI, 2022) is a version of OpenAI's language model used to evaluate the performance of agents discovered by Meta Agent Search</data>
      <data key="d2">2901d5e2711fa4f32d39cd8eea36cd71</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="DYNAMIC ROLE-PLAYING ARCHITECTURE">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">Dynamic Role-Playing Architecture is a method used for improving the performance of models through dynamic role-playing and interaction
Dynamic Role-Playing Architecture is a top agent discovered in the Math domain and transferred to non-math domains</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="STRUCTURED MULTIMODAL FEEDBACK LOOP">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">Structured Multimodal Feedback Loop is a method used for improving the performance of models through structured feedback and interaction
Structured Multimodal Feedback Loop is a top agent discovered in the Math domain and transferred to non-math domains</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="INTERACTIVE MULTIMODAL FEEDBACK LOOP">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">Interactive Multimodal Feedback Loop is a method used for improving the performance of models through interactive feedback and interaction
Interactive Multimodal Feedback Loop is a top agent discovered in the Math domain and transferred to non-math domains</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="FM MODULES">
      <data key="d0">COMPONENT</data>
      <data key="d1">FM Modules are components in an agentic system that are assigned different roles and enabled to collaborate</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="AI-GENERATING ALGORITHMS">
      <data key="d0">METHOD/TECHNIQUE</data>
      <data key="d1">AI-Generating Algorithms are methods used for generating AI models and systems automatically
AI-Generating Algorithms (AI-GAs) are a field of research focused on learning components in AI systems to replace handcrafted ones</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="PATEL ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Patel et al. are the authors of the SVAMP dataset</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="MIAO ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Miao et al. are the authors of the ASDiv dataset</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="CHEN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Chen et al. are the authors of the Prompting Techniques method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="SCHULHOFF ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Schulhoff et al. are the authors of the Prompting Techniques method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="VEMPRALA ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Vemprala et al. are the authors of the method for developing new skills for embodied agents in code</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="NAKANO ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Nakano et al. are the authors of the Tool Use method</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="HONG ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Hong et al. are the authors of the method for assigning FM modules in the agentic system with different roles and enabling them to collaborate
Hong et al. are authors referenced in the context of incorporating organizational structure in agents</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64,6bdf681c0bd9e401ac72344a6a0ae479</data>
    </node>
    <node id="QIAN ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Qian et al. are the authors of the method for assigning FM modules in the agentic system with different roles and enabling them to collaborate</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="RICHARDS">
      <data key="d0">PERSON</data>
      <data key="d1">Richards is the author of the method for enabling the agent to instruct itself for the next action</data>
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="CHAIN-OF-THOUGHT-BASED PLANNING AND REASONING METHODS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="DEVELOPING NEW SKILLS FOR EMBODIED AGENTS IN CODE">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="EXTERNAL MEMORY AND RAG">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES AND ENABLING THEM TO COLLABORATE">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="ENABLING THE AGENT TO INSTRUCT ITSELF FOR THE NEXT ACTION">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0b6b4880e77d40e284702da16be4ef64</data>
    </node>
    <node id="MAML">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">MAML (Model-Agnostic Meta-Learning) is a technique that allows "learning to learn" for better sample efficiency and generalizability</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="META-RL">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Meta-RL (Meta-Reinforcement Learning) is a technique that allows "learning to learn" for continuous learning of multiple tasks</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="POET">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">POET (Paired Open-Ended Trailblazer) is a technique aimed at generating learning environments in an open-ended manner</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="FOUNDATION MODELS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Foundation Models (FMs) are large-scale models used to write code for various applications, including optimization algorithms and reinforcement learning</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="EOH">
      <data key="d0">TOOL</data>
      <data key="d1">EoH is a tool where Foundation Models write code to discover better optimization algorithms</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="DISCOPOP">
      <data key="d0">TOOL</data>
      <data key="d1">DiscoPOP is a tool where Foundation Models program the loss function for preference learning in FM alignment training</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="EUREKA">
      <data key="d0">TOOL</data>
      <data key="d1">Eureka is a tool that enables Foundation Models to write reward functions for reinforcement learning in robotics
Eureka is a tool that enables FMs to write reward functions for reinforcement learning in robotics
Eureka is a tool for human-level reward design via coding large language models</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="LANGUAGE-TO-REWARD">
      <data key="d0">TOOL</data>
      <data key="d1">Language-to-Reward is a tool that enables Foundation Models to write reward functions for reinforcement learning in robotics
Language-to-reward is a tool that enables FMs to write reward functions for reinforcement learning in robotics</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="CLUNE, 2019">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Clune in 2019 related to AI-Generating Algorithms
A paper discussing the pursuit of AGI and AI-GA</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="HUTTER ET AL., 2019">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Hutter et al. in 2019 related to AutoML</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="ELSKEN ET AL., 2019">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Elsken et al. in 2019 related to Neural Architecture Search</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="HU ET AL., 2021">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Hu et al. in 2021 related to Neural Architecture Search</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="LU ET AL., 2019">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Lu et al. in 2019 related to Neural Architecture Search</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="FINN ET AL., 2017">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Finn et al. in 2017 related to MAML</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="DUAN ET AL., 2017">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Duan et al. in 2017 related to Meta-RL</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="NORMAN &amp; CLUNE, 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Norman &amp; Clune in 2023 related to Meta-RL</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="WANG ET AL., 2016">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Wang et al. in 2016 related to Meta-RL</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="ZINTGRAF ET AL., 2021A">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Zintgraf et al. in 2021 related to Meta-RL</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="ZINTGRAF ET AL., 2021B">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Zintgraf et al. in 2021 related to Meta-RL</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="DHARNA ET AL., 2020">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Dharna et al. in 2020 related to POET</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="ROMERA-PAREDES ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Romera-Paredes et al. in 2024 related to FunSearch</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="LU ET AL., 2024A">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Lu et al. in 2024 related to DiscoPOP</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="RAFAILOV ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Rafailov et al. in 2024 related to FM alignment training</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb</data>
    </node>
    <node id="MA ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Ma et al. in 2023 related to Eureka
A paper discussing Eureka and its application in enabling FMs to write reward functions for reinforcement learning in robotics</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="YU ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Yu et al. in 2023 related to language-to-reward
A paper discussing language-to-reward and its application in enabling FMs to write reward functions for reinforcement learning in robotics</data>
      <data key="d2">7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ZHENG ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Zheng et al. in 2023 related to Step-back Abstraction
Zheng et al., 2023 is a publication referenced for the Step-back Abstraction method
A publication by Zheng et al. in 2023 that discusses Step-back Abstraction</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="XU ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Xu et al. in 2023 related to Role Assignment
A paper discussing the benefits of assigning personas or roles to agents
Xu et al., 2023 is a publication referenced for the Role Assignment method
A publication by Xu et al. in 2023 that discusses Role Assignment</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="OPRO">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">OPRO adopts FMs to automate prompt engineering for agents, focusing on the phrasing of instructions in the prompt to enhance reasoning capability</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="SELF-DISCOVER">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">Self-Discover adopts FMs to automate prompt engineering for agents, focusing on the phrasing of instructions in the prompt to enhance reasoning capability</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="EVOAGENT">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">EvoAgent optimizes role definition in the prompt, assigning personas or roles to agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="AGENTVERSE">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">AgentVerse optimizes role definition in the prompt, assigning personas or roles to agents
Agentverse is a paper titled "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors" presented at The Twelfth International Conference on Learning Representations, 2023</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="DYLAN">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">DyLAN uses FMs to score the response quality of nodes in each layer to prune the connections in a fully connected feed-forward network</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="DSPY">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">DSPy generates a set of possible nodes and optimizes across the Cartesian product of these nodes while optimizing the few-shot examples for nodes</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="GPT-SWARM">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">GPT-Swarm represents an agentic system in a graph with a predefined set of nodes and uses a Reinforcement Learning algorithm to optimize the possible connections between nodes</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="AGENTOPTIMIZER">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">AgentOptimizer learns the tools used in agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="AGENT SYMBOLIC LEARNING">
      <data key="d0">TOOL/TECHNOLOGY</data>
      <data key="d1">Agent Symbolic Learning learns prompts, tools, and control flow together in agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="AGI">
      <data key="d0">CONCEPT</data>
      <data key="d1">Artificial General Intelligence (AGI) is a concept in AI research that involves creating highly autonomous systems with general intelligence</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="AI-GA">
      <data key="d0">CONCEPT</data>
      <data key="d1">AI-GA refers to AI Generative Algorithms, which could potentially contribute to creating AGI faster than the current manual approach</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="BENGIO ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing the pursuit of AGI and AI-GA</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="BOSTROM, 2002">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing the pursuit of AGI and AI-GA</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ECOFFET ET AL., 2020">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing the pursuit of AGI and AI-GA</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="YUDKOWSKY ET AL., 2008">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing the pursuit of AGI and AI-GA</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ROKON ET AL., 2020">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing safety concerns when executing untrusted model-generated code</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="YEE ET AL., 2010">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing the use of sandbox environments to safely run untrusted model-generated code</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="YANG ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing OPRO and its application in automating prompt engineering for agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="FERNANDO ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing PromptBreeder and its application in automating prompt engineering for agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ZHOU ET AL., 2024A">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing Self-Discover and its application in automating prompt engineering for agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="YUAN ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing EvoAgent and its application in optimizing role definition in the prompt</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="KHATTAB ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing DSPy and its application in generating a set of possible nodes and optimizing across the Cartesian product of these nodes</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ZHUGE ET AL., 2024">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing GPT-Swarm and its application in representing an agentic system in a graph with a predefined set of nodes</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ZHANG ET AL., 2024B">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing AgentOptimizer and its application in learning the tools used in agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="ZHOU ET AL., 2024B">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A paper discussing Agent Symbolic Learning and its application in learning prompts, tools, and control flow together in agents</data>
      <data key="d2">dc55f071b95dec721a9820d39cdb3ccd</data>
    </node>
    <node id="BOSTROM">
      <data key="d0">PERSON</data>
      <data key="d1">Bostrom is an author referenced in the context of discussions beyond the scope of the paper</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ECOFFET ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Ecoffet et al. are authors referenced in the context of discussions beyond the scope of the paper</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUDKOWSKY ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Yudkowsky et al. are authors referenced in the context of discussions beyond the scope of the paper</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="API">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">API access to powerful FMs is used to program ADAS algorithms</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="SAFE-ADAS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Safe-ADAS refers to algorithms that conduct ADAS safely, avoiding harmful code and creating honest, helpful agents</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="CONSTITUTIONAL AI">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Constitutional AI is an idea that can be incorporated into Meta Agent Search to ensure the creation of safe agents</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="HIGHER-ORDER ADAS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Higher-order ADAS refers to the concept of improving the meta agent through ADAS, allowing for meta-learning of the meta agent and beyond</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="MULTI-OBJECTIVE ADAS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Multi-objective ADAS refers to integrating multiple objectives like cost, latency, and robustness into ADAS algorithms</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="NOVELTY SEARCH ALGORITHMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Novelty search algorithms focus on exploring new designs and can be incorporated into Meta Agent Search</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="AI-GENERATING">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">AI-generating refers to algorithms that can generate AI, a concept that can be incorporated into ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="OPEN-ENDED ALGORITHMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Open-ended algorithms are designed to explore a wide range of possibilities and can be incorporated into ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="EVALUATION FUNCTIONS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Evaluation functions are used to assess the performance of discovered agents in ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="MULTI-MODAL CAPABILITIES">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Multi-modal capabilities refer to the ability to handle different types of data, such as vision, in FMs</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="META">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Meta is an organization mentioned in the context of open-source research for safe-ADAS
Meta is the organization that published the news article "Open source ai is the path forward"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">ORGANIZATION</data>
    </node>
    <node id="CALDWELL">
      <data key="d0">PERSON</data>
      <data key="d1">Caldwell is an author referenced in the context of open-source research for safe-ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BAI ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Bai et al. are authors referenced in the context of Constitutional AI</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LANGCHAINAI">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">LangChainAI is the organization behind the LangChain framework
LangChainAI is the organization behind the Langchain project</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">ORGANIZATION</data>
    </node>
    <node id="HU ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Hu et al. are authors referenced in the context of multi-objective ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DEB ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Deb et al. are authors referenced in the context of multi-objective search algorithms</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CULLY &amp; DEMIRIS">
      <data key="d0">PERSON</data>
      <data key="d1">Cully &amp; Demiris are authors referenced in the context of Quality-Diversity</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MOURET &amp; CLUNE">
      <data key="d0">PERSON</data>
      <data key="d1">Mouret &amp; Clune are authors referenced in the context of Quality-Diversity</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FALDOR ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Faldor et al. are authors referenced in the context of Open-ended Algorithms</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="STANLEY &amp; LEHMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Stanley &amp; Lehman are authors referenced in the context of Open-ended Algorithms</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="STANLEY ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Stanley et al. are authors referenced in the context of Open-ended Algorithms</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SUTTON &amp; BARTO">
      <data key="d0">PERSON</data>
      <data key="d1">Sutton &amp; Barto are authors referenced in the context of balancing exploration and exploitation</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHOU ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Zhou et al. are authors referenced in the context of intelligent evaluation functions</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHIANG ET AL.">
      <data key="d0">PERSON</data>
      <data key="d1">Chiang et al. are authors referenced in the context of subjective answer evaluations</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NATURAL LANGUAGE">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Natural language is the representation used by agentic systems and humans in constructing organizations and society</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="HUMAN ORGANIZATIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Human organizations are structures that can be incorporated into agentic systems to improve their design</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="SOCIETY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Society is the broader context in which human organizations and agentic systems operate</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="COMPLEX DOMAINS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Complex domains refer to real-world applications involving multi-step interaction with complex environments</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="SINGLE-STEP QA TASKS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Single-step QA tasks are the type of tasks on which Meta Agent Search is evaluated in the paper</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="MULTIPLE DOMAINS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Multiple domains refer to the capability of ADAS algorithms to design agents that perform well across various fields</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="META-META AGENT">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Meta-meta agent is a higher-order agent that can be learned through meta-learning in ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="DATA PRIVACY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Data privacy is a priority that can influence the choice of FMs in agentic systems</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="ROBUSTNESS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Robustness is an objective that can be considered in multi-objective ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="EXPLORATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Exploration is a strategy in search algorithms to discover new designs</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="EXPLOITATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Exploitation is a strategy in search algorithms to utilize known good designs</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="SUBJECTIVE ANSWER EVALUATIONS">
      <data key="d0">PROCESS</data>
      <data key="d1">Subjective answer evaluations are tasks that do not have ground-truth answers and require novel evaluation functions in ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="GENERALIST AGENTS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Generalist agents are agents capable of performing well across multiple domains</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="INTERPRETABLE">
      <data key="d0">ATTRIBUTE</data>
      <data key="d1">Interpretable refers to the quality of being understandable to humans, as in the case of natural language used in agentic systems</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">ATTRIBUTE</data>
    </node>
    <node id="ORGANIZATIONAL STRUCTURE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Organizational structure refers to the arrangement of roles and responsibilities within human companies, which can be incorporated into agents</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="HUMAN COMPANIES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Human companies are organizations that can provide a model for the design of agentic systems</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="COMPLEXITY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Complexity refers to the intricate and interconnected nature of systems, which can emerge from human organizations and agentic systems
Complexity is one of the attributes that the Instruction Refinement Flow aims to enhance in the generated instructions.</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479,f7eb89a70f544664546a510e46d5febd</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="META-LEARNING">
      <data key="d0">PROCESS</data>
      <data key="d1">Meta-learning is the process of learning how to learn, applied to the meta agent and beyond in ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="TRAINING">
      <data key="d0">PROCESS</data>
      <data key="d1">Training is the process of teaching the meta agent to be safe and create helpful, harmless, honest agents</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="HONEST AGENTS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Honest agents are agents that are designed to be truthful and reliable</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="HELPFUL AGENTS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Helpful agents are agents that are designed to assist and provide value</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="HARMLESS AGENTS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Harmless agents are agents that are designed to avoid causing harm</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="NUMERICAL PERFORMANCE RESULTS">
      <data key="d0">DATA</data>
      <data key="d1">Numerical performance results are used to evaluate discovered agents in ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">DATA</data>
    </node>
    <node id="RUNNING LOGS">
      <data key="d0">DATA</data>
      <data key="d1">Running logs contain detailed information on the performance of agents during evaluation</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">DATA</data>
    </node>
    <node id="FAILURE MODES">
      <data key="d0">DATA</data>
      <data key="d1">Failure modes are patterns of errors or issues that occur during the operation of agentic systems</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">DATA</data>
    </node>
    <node id="SUCCESS MODES">
      <data key="d0">DATA</data>
      <data key="d1">Success modes are patterns of correct or optimal performance during the operation of agentic systems</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">DATA</data>
    </node>
    <node id="QA TASKS">
      <data key="d0">TASK</data>
      <data key="d1">QA tasks refer to question-answering tasks used to evaluate Meta Agent Search</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TASK</data>
    </node>
    <node id="REAL-WORLD APPLICATIONS">
      <data key="d0">TASK</data>
      <data key="d1">Real-world applications involve multi-step interactions with complex environments</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TASK</data>
    </node>
    <node id="ENVIRONMENTS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Environments are the settings or contexts in which agents operate and interact</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="INSTRUCTION">
      <data key="d0">DATA</data>
      <data key="d1">Instruction refers to the commands or tasks given to the meta agent in ADAS
Instructions are the tasks or guidelines that agents follow to perform their roles in the agentic flows.</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479,f7eb89a70f544664546a510e46d5febd</data>
      <data key="d3">DATA</data>
    </node>
    <node id="DOMAIN">
      <data key="d0">CONCEPT</data>
      <data key="d1">Domain refers to the specific area or field in which ADAS algorithms operate</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="META AGENT SEARCH ALGORITHM">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Meta Agent Search Algorithm is the specific algorithm used in Meta Agent Search toMeta Agent Search Algorithm is the specific algorithm used in Meta Agent Search to explore new designs</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="QUALITY-DIVERSITY ALGORITHMS">
      <data key="d0">TECHNOLOGY</data>
      <data key="d1">Quality-Diversity algorithms are used to explore a wide range of possibilities in ADAS</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">TECHNOLOGY</data>
    </node>
    <node id="BALANCING EXPLORATION AND EXPLOITATION">
      <data key="d0">PROCESS</data>
      <data key="d1">Balancing exploration and exploitation is a strategy in search algorithms to discover new designs while utilizing known good designs</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="DETAILED RUNNING LOGS">
      <data key="d0">DATA</data>
      <data key="d1">Detailed running logs contain rich information on the performance of agents during evaluation</data>
      <data key="d2">6bdf681c0bd9e401ac72344a6a0ae479</data>
      <data key="d3">DATA</data>
    </node>
    <node id="AS">
      <data key="d0">CONCEPT</data>
      <data key="d1">AS refers to the agentic system, a machine learning system that operates primarily over natural language and is interpretable to humans</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
    </node>
    <node id="HUMAN ORGANIZATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Human organization refers to the structured arrangement of individuals and groups in society</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="HUMAN SOCIETY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Human society refers to the collective of human beings and their social structures and institutions</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">CONCEPT</data>
    </node>
    <node id="AGENTIC SYSTEM">
      <data key="d0">SYSTEM</data>
      <data key="d1">A machine learning system that operates primarily over natural language and is interpretable to humans</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">SYSTEM</data>
    </node>
    <node id="HONG ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A work that incorporates the organizational structure for human companies in agents</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="PARK ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A work that simulates a human town with agents</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="CANADA CIFAR AI CHAIRS PROGRAM">
      <data key="d0">PROGRAM</data>
      <data key="d1">A program that supported the work on Automated Design of Agentic Systems</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PROGRAM</data>
    </node>
    <node id="SCHMIDT FUTURES">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">An organization that provided grants for the work on Automated Design of Agentic Systems</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">ORGANIZATION</data>
    </node>
    <node id="OPEN PHILANTHROPY">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">An organization that provided grants for the work on Automated Design of Agentic Systems</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">ORGANIZATION</data>
    </node>
    <node id="NSERC DISCOVERY GRANT">
      <data key="d0">GRANT</data>
      <data key="d1">A grant that supported the work on Automated Design of Agentic Systems</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">GRANT</data>
    </node>
    <node id="RAFAEL COSMAN">
      <data key="d0">PERSON</data>
      <data key="d1">A person who made a generous donation to support the work on Automated Design of Agentic Systems</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JENNY ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">A person acknowledged for insightful discussions and feedback on the work
Jenny Zhang is an author of the paper "Omni-epic: Open-endedness via models of human notions of interestingness with environments programmed in code"
Jenny Zhang is an author of the paper "OMNI: Open-endedness via models of human notions of interestingness"
Jenny Zhang is an author of the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,2600a1ed94ad2d3675ea80575c39cbd1,7de66b94cf868b37b1df51dc545c415f,cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RACH PRADHAN">
      <data key="d0">PERSON</data>
      <data key="d1">A person acknowledged for insightful discussions and feedback on the work</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RUIYU GOU">
      <data key="d0">PERSON</data>
      <data key="d1">A person acknowledged for insightful discussions and feedback on the work</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NICHOLAS IOANNIDIS">
      <data key="d0">PERSON</data>
      <data key="d1">A person acknowledged for insightful discussions and feedback on the work</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="EUNJEONG HWANG">
      <data key="d0">PERSON</data>
      <data key="d1">A person acknowledged for insightful discussions and feedback on the work</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUNTAO BAI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAURAV KADAVATH">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"
Saurav Kadavath is an author of the paper "Measuring mathematical problem solving with the math dataset"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SANDIPAN KUNDU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JACKSON KERNION">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANDY JONES">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANNA CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANNA GOLDIE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AZALIA MIRHOSEINI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CAMERON MCKINNON">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Constitutional AI: Harmlessness from AI Feedback"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GEOFFREY HINTON">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANDREW YAO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAWN SONG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"
Dawn Song is an author of the paper "Measuring massive multitask language understanding"
Dawn Song is an author of the paper "Measuring mathematical problem solving with the math dataset"Dawn Song is an author of the paper "The false promise of imitating proprietary llms"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,7de66b94cf868b37b1df51dc545c415f,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TREVOR DARRELL">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUVAL NOAH HARARI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YA-QIN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LAN XUE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHAI SHALEV-SHWARTZ">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="N BOSTROM">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ROBERT S BOYER">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "A Mechanical Proof of the Turing Completeness of Pure LISP"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="J STROTHER MOORE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "A Mechanical Proof of the Turing Completeness of Pure LISP"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TRACEY CALDWELL">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Ethical Hackers: Putting on the White Hat"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HARRISON CHASE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the blog post "What is an Agent?"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BANGHAO CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHAOFENG ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NICOLAS LANGREN&#201;">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHENGXIN ZHU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HENRIQUE PONDE DE OLIVEIRA PINTO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Evaluating Large Language Models Trained on Code"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HARRI EDWARDS">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Evaluating Large Language Models Trained on Code"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YURI BURDA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Evaluating Large Language Models Trained on Code"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEIZE CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"
Weize Chen is an author of the paper "Communicative agents for software development"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUSHENG SU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"
Yusheng Su is an author of the paper "Communicative agents for software development"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JINGWEI ZUO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHENG YANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"
Cheng Yang is an author of the paper "Communicative agents for software development"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHENFEI YUAN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHI-MIN CHAN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HEYANG YU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YI-HSIN HUNG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHEN QIAN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"
Chen Qian is an author of the paper "Communicative agents for software development"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEI-LIN CHIANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"
Wei-Lin Chiang is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIANMIN ZHENG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"
Lianmin Zheng is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YING SHENG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"
Ying Sheng is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANASTASIOS NIKOLAS ANGEL">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the paper "Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors"</data>
      <data key="d2">7de66b94cf868b37b1df51dc545c415f</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHEN, YUSHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Chen, Yusheng is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SU, JINGWEI">
      <data key="d0">PERSON</data>
      <data key="d1">Su, Jingwei is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZUO, CHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Zuo, Cheng is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YANG, CHENFEI">
      <data key="d0">PERSON</data>
      <data key="d1">Yang, Chenfei is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUAN, CHI-MIN">
      <data key="d0">PERSON</data>
      <data key="d1">Yuan, Chi-Min is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHAN, HEYANG">
      <data key="d0">PERSON</data>
      <data key="d1">Chan, Heyang is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YU, YAXI">
      <data key="d0">PERSON</data>
      <data key="d1">Yu, Yaxi is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HUNG, YI-HSIN">
      <data key="d0">PERSON</data>
      <data key="d1">Hung, Yi-Hsin is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QIAN, CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Qian, Chen is an author of the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THE TWELFTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors" was presented in 2023
The conference where the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines" was presented
The conference where the paper "Eureka: Human-level reward design via coding large language models" was presented
The Twelfth International Conference on Learning Representations is where the papers "Large language models as optimizers" and "OMNI: Open-endedness via models of human notions of interestingness" were presented</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,1b1399c76420a477c0c97893d258ae69,2600a1ed94ad2d3675ea80575c39cbd1,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="ANASTASIOS NIKOLAS ANGELOPOULOS">
      <data key="d0">PERSON</data>
      <data key="d1">Anastasios Nikolas Angelopoulos is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TIANLE LI">
      <data key="d0">PERSON</data>
      <data key="d1">Tianle Li is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DACHENG LI">
      <data key="d0">PERSON</data>
      <data key="d1">Dacheng Li is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HAO ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Hao Zhang is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BANGHUA ZHU">
      <data key="d0">PERSON</data>
      <data key="d1">Banghua Zhu is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MICHAEL JORDAN">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Jordan is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOSEPH E. GONZALEZ">
      <data key="d0">PERSON</data>
      <data key="d1">Joseph E. Gonzalez is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ION STOICA">
      <data key="d0">PERSON</data>
      <data key="d1">Ion Stoica is an author of the paper "Chatbot arena: An open platform for evaluating llms by human preference"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHATBOT ARENA">
      <data key="d0">PAPER</data>
      <data key="d1">Chatbot arena is a paper titled "Chatbot arena: An open platform for evaluating llms by human preference" published in 2024</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="FRAN&#199;OIS CHOLLET">
      <data key="d0">PERSON</data>
      <data key="d1">Fran&#231;ois Chollet is the author of the paper "On the measure of intelligence"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ON THE MEASURE OF INTELLIGENCE">
      <data key="d0">PAPER</data>
      <data key="d1">On the measure of intelligence is a paper authored by Fran&#231;ois Chollet, published as an arXiv preprint in 2019</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="AI-GAS: AI-GENERATING ALGORITHMS">
      <data key="d0">PAPER</data>
      <data key="d1">Ai-gas: Ai-generating algorithms is a paper authored by Jeff Clune, published as an arXiv preprint in 2019</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="TRAINING VERIFIERS TO SOLVE MATH WORD PROBLEMS">
      <data key="d0">PAPER</data>
      <data key="d1">Training verifiers to solve math word problems is a paper authored by Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, and Reiichiro Nakano, published as an arXiv preprint in 2021
A paper discussing the training of verifiers to solve math word problems, published in 2021</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="ANTOINE CULLY">
      <data key="d0">PERSON</data>
      <data key="d1">Antoine Cully is an author of the paper "Quality and diversity optimization: A unifying modular framework"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIANNIS DEMIRIS">
      <data key="d0">PERSON</data>
      <data key="d1">Yiannis Demiris is an author of the paper "Quality and diversity optimization: A unifying modular framework"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QUALITY AND DIVERSITY OPTIMIZATION">
      <data key="d0">PAPER</data>
      <data key="d1">Quality and diversity optimization is a paper authored by Antoine Cully and Yiannis Demiris, published in IEEE Transactions on Evolutionary Computation in 2017</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="N. DALAL">
      <data key="d0">PERSON</data>
      <data key="d1">N. Dalal is an author of the paper "Histograms of oriented gradients for human detection"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="B. TRIGGS">
      <data key="d0">PERSON</data>
      <data key="d1">B. Triggs is an author of the paper "Histograms of oriented gradients for human detection"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HISTOGRAMS OF ORIENTED GRADIENTS FOR HUMAN DETECTION">
      <data key="d0">PAPER</data>
      <data key="d1">Histograms of oriented gradients for human detection is a paper authored by N. Dalal and B. Triggs, presented at the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&#8217;05)</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="KALYANMOY DEB">
      <data key="d0">PERSON</data>
      <data key="d1">Kalyanmoy Deb is an author of the paper "A fast and elitist multiobjective genetic algorithm: Nsga-ii"
Kalyanmoy Deb is an author of the paper "Revisiting residual networks for adversarial robustness"
Kalyanmoy Deb is an author of the paper "Nsga-net: neural architecture search using multi-objective genetic algorithm"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,1b1399c76420a477c0c97893d258ae69,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AMRIT PRATAP">
      <data key="d0">PERSON</data>
      <data key="d1">Amrit Pratap is an author of the paper "A fast and elitist multiobjective genetic algorithm: Nsga-ii"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAMEER AGARWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Sameer Agarwal is an author of the paper "A fast and elitist multiobjective genetic algorithm: Nsga-ii"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TAMT MEYARIVAN">
      <data key="d0">PERSON</data>
      <data key="d1">TAMT Meyarivan is an author of the paper "A fast and elitist multiobjective genetic algorithm: Nsga-ii"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="A FAST AND ELITIST MULTIOBJECTIVE GENETIC ALGORITHM: NSGA-II">
      <data key="d0">PAPER</data>
      <data key="d1">A fast and elitist multiobjective genetic algorithm: Nsga-ii is a paper authored by Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan, published in IEEE Transactions on Evolutionary Computation in 2002</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="AARON DHARNA">
      <data key="d0">PERSON</data>
      <data key="d1">Aaron Dharna is an author of the paper "Co-generation of game levels and game-playing agents"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JULIAN TOGELIUS">
      <data key="d0">PERSON</data>
      <data key="d1">Julian Togelius is an author of the paper "Co-generation of game levels and game-playing agents"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LISA B SOROS">
      <data key="d0">PERSON</data>
      <data key="d1">Lisa B Soros is an author of the paper "Co-generation of game levels and game-playing agents"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CO-GENERATION OF GAME LEVELS AND GAME-PLAYING AGENTS">
      <data key="d0">PAPER</data>
      <data key="d1">Co-generation of game levels and game-playing agents is a paper authored by Aaron Dharna, Julian Togelius, and Lisa B Soros, presented at the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment in 2020</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="YILUN DU">
      <data key="d0">PERSON</data>
      <data key="d1">Yilun Du is an author of the paper "Improving factuality and reasoning in language models through multiagent debate"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHUANG LI">
      <data key="d0">PERSON</data>
      <data key="d1">Shuang Li is an author of the paper "Improving factuality and reasoning in language models through multiagent debate"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANTONIO TORRALBA">
      <data key="d0">PERSON</data>
      <data key="d1">Antonio Torralba is an author of the paper "Improving factuality and reasoning in language models through multiagent debate"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IMPROVING FACTUALITY AND REASONING IN LANGUAGE MODELS THROUGH MULTIAGENT DEBATE">
      <data key="d0">PAPER</data>
      <data key="d1">Improving factuality and reasoning in language models through multiagent debate is a paper authored by Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch, published as an arXiv preprint in 2023</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="DHEERU DUA">
      <data key="d0">PERSON</data>
      <data key="d1">Dheeru Dua is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"
Dheeru Dua is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIZHONG WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yizhong Wang is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"
Yizhong Wang is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"Yizhong Wang is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PRADEEP DASIGI">
      <data key="d0">PERSON</data>
      <data key="d1">Pradeep Dasigi is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"
Pradeep Dasigi is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GABRIEL STANOVSKY">
      <data key="d0">PERSON</data>
      <data key="d1">Gabriel Stanovsky is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"
Gabriel Stanovsky is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAMEER SINGH">
      <data key="d0">PERSON</data>
      <data key="d1">Sameer Singh is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"
Sameer Singh is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MATT GARDNER">
      <data key="d0">PERSON</data>
      <data key="d1">Matt Gardner is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"
Matt Gardner is an author of the paper "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DROP: A READING COMPREHENSION BENCHMARK REQUIRING DISCRETE REASONING OVER PARAGRAPHS">
      <data key="d0">PAPER</data>
      <data key="d1">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs is a paper authored by Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner, presented at the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
A paper discussing the DROP benchmark for reading comprehension, published in 2019</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,cc20c99cad8edecc66b82ac751ff7172</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="YAN DUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Yan Duan is an author of the paper "RL^2: Fast reinforcement learning via slow reinforcement learning"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XI CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Xi Chen is an author of the paper "RL^2: Fast reinforcement learning via slow reinforcement learning"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PETER L. BARTLETT">
      <data key="d0">PERSON</data>
      <data key="d1">Peter L. Bartlett is an author of the paper "RL^2: Fast reinforcement learning via slow reinforcement learning"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RL^2: FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING">
      <data key="d0">PAPER</data>
      <data key="d1">RL^2: Fast reinforcement learning via slow reinforcement learning is a paper authored by Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel, presented at the International Conference on Learning Representations in 2017</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="JOEL LEHMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Joel Lehman is an author of the paper "Open questions in creating safe open-ended AI: Tensions between control and creativity"
Joel Lehman is an author of the paper "Abandoning objectives: Evolution through the search for novelty alone"
Joel Lehman is an author of the paper "Language model crossover: Variation through few-shot prompting"
Joel Lehman is an author of the paper "Designing neural networks through neuroevolution"Joel Lehman is an author of the book "Why greatness cannot be planned: The myth of the objective"
Joel Lehman is an author of the papers "Poet: open-ended coevolution of environments and their optimized solutions" and "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions"
Joel Lehman is an author of the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,1b1399c76420a477c0c97893d258ae69,2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e,cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="OPEN QUESTIONS IN CREATING SAFE OPEN-ENDED AI: TENSIONS BETWEEN CONTROL AND CREATIVITY">
      <data key="d0">PAPER</data>
      <data key="d1">Open questions in creating safe open-ended AI: Tensions between control and creativity is a paper authored by Adrien Ecoffet, Jeff Clune, and Joel Lehman, presented at the Conference on Artificial Life in 2020</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="THOMAS ELSKEN">
      <data key="d0">PERSON</data>
      <data key="d1">Thomas Elsken is an author of the paper "Neural architecture search: A survey"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAN HENDRIK METZEN">
      <data key="d0">PERSON</data>
      <data key="d1">Jan Hendrik Metzen is an author of the paper "Neural architecture search: A survey"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FRANK HUTTER">
      <data key="d0">PERSON</data>
      <data key="d1">Frank Hutter is an author of the paper "Neural architecture search: A survey"
Frank Hutter is an author of the book "Automated machine learning: methods, systems, challenges"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NEURAL ARCHITECTURE SEARCH: A SURVEY">
      <data key="d0">PAPER</data>
      <data key="d1">Neural architecture search: A survey is a paper authored by Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter, published in the Journal of Machine Learning Research in 2019</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="MAXENCE FALDOR">
      <data key="d0">PERSON</data>
      <data key="d1">Maxence Faldor is an author of the paper "Omni-epic: Open-endedness via models of human notions of interestingness with environments programmed in code"</data>
      <data key="d2">022e7927d281e80e188f29ea343cc115</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PAL">
      <data key="d0">TOOL/FRAMEWORK</data>
      <data key="d1">Pal is a program-aided language model discussed in the paper by Yiming Yang, Jamie Callan, and Graham Neubig</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="INTERNATIONAL CONFERENCE ON MACHINE LEARNING">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "Pal: Program-aided language models" was presented
The International Conference on Machine Learning is where the paper "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions" was presented</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="RYAN GREENBLATT">
      <data key="d0">PERSON</data>
      <data key="d1">Ryan Greenblatt is the author of the technical report "Getting 50% sota on arc-agi with gpt-4"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="DAN HENDRYCKS">
      <data key="d0">PERSON</data>
      <data key="d1">Dan Hendrycks is an author of the paper "Measuring massive multitask language understanding"
Dan Hendrycks is an author of the paper "Measuring mathematical problem solving with the math dataset"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="COLLIN BURNS">
      <data key="d0">PERSON</data>
      <data key="d1">Collin Burns is an author of the paper "Measuring massive multitask language understanding"
Collin Burns is an author of the paper "Measuring mathematical problem solving with the math dataset"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="STEVEN BASART">
      <data key="d0">PERSON</data>
      <data key="d1">Steven Basart is an author of the paper "Measuring massive multitask language understanding"
Steven Basart is an author of the paper "Measuring mathematical problem solving with the math dataset"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ANDY ZOU">
      <data key="d0">PERSON</data>
      <data key="d1">Andy Zou is an author of the paper "Measuring massive multitask language understanding"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="MANTAS MAZEIKA">
      <data key="d0">PERSON</data>
      <data key="d1">Mantas Mazeika is an author of the paper "Measuring massive multitask language understanding"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="JACOB STEINHARDT">
      <data key="d0">PERSON</data>
      <data key="d1">Jacob Steinhardt is an author of the paper "Measuring massive multitask language understanding"
Jacob Steinhardt is an author of the paper "Measuring mathematical problem solving with the math dataset"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "Measuring massive multitask language understanding" was presented
The conference where the paper "Language models are multilingual chain-of-thought reasoners" was presented
The conference where the paper "OMNI: Open-endedness via models of human notions of interestingness" was presented</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e,cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </node>
    <node id="SIRUI HONG">
      <data key="d0">PERSON</data>
      <data key="d1">Sirui Hong is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="XIAWU ZHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Xiawu Zheng is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JONATHAN CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Jonathan Chen is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUHENG CHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Yuheng Cheng is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JINLIN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jinlin Wang is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CEYAO ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Ceyao Zhang is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZILI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Zili Wang is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="STEVEN KA SHING YAU">
      <data key="d0">PERSON</data>
      <data key="d1">Steven Ka Shing Yau is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZIJUAN LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Zijuan Lin is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIYANG ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Liyang Zhou is an author of the paper "Metagpt: Meta programming for multi-agent collaborative framework"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="METAGPT">
      <data key="d0">TOOL/FRAMEWORK</data>
      <data key="d1">Metagpt is a meta programming framework for multi-agent collaboration discussed in the paper by Sirui Hong et al.</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">TOOL/FRAMEWORK</data>
    </node>
    <node id="RAN CHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Ran Cheng is an author of the paper "Accelerating multi-objective neural architecture search by random-weight evaluation"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHENG HE">
      <data key="d0">PERSON</data>
      <data key="d1">Cheng He is an author of the paper "Accelerating multi-objective neural architecture search by random-weight evaluation"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHICHAO LU">
      <data key="d0">PERSON</data>
      <data key="d1">Zhichao Lu is an author of the paper "Accelerating multi-objective neural architecture search by random-weight evaluation"Zhichao Lu is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model"
Zhichao Lu is an author of the paper "Nsga-net: neural architecture search using multi-objective genetic algorithm"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JING WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jing Wang is an author of the paper "Accelerating multi-objective neural architecture search by random-weight evaluation"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MIAO ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Miao Zhang is an author of the paper "Accelerating multi-objective neural architecture search by random-weight evaluation"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="COMPLEX &amp; INTELLIGENT SYSTEMS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Accelerating multi-objective neural architecture search by random-weight evaluation" was published</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="SHIHUA HUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Shihua Huang is an author of the paper "Revisiting residual networks for adversarial robustness"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VISHNU NARESH BODDETI">
      <data key="d0">PERSON</data>
      <data key="d1">Vishnu Naresh Boddeti is an author of the paper "Revisiting residual networks for adversarial robustness"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "Revisiting residual networks for adversarial robustness" was presented
The conference where the paper "Deepmad: Mathematical architecture design for deep convolutional neural network" was presented</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="LARS KOTTHOFF">
      <data key="d0">PERSON</data>
      <data key="d1">Lars Kotthoff is an author of the book "Automated machine learning: methods, systems, challenges"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOAQUIN VANSCHOREN">
      <data key="d0">PERSON</data>
      <data key="d1">Joaquin Vanschoren is an author of the book "Automated machine learning: methods, systems, challenges"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SPRINGER NATURE">
      <data key="d0">PUBLISHER</data>
      <data key="d1">The publisher of the book "Automated machine learning: methods, systems, challenges"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PUBLISHER</data>
    </node>
    <node id="OMAR KHATTAB">
      <data key="d0">PERSON</data>
      <data key="d1">Omar Khattab is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"
Omar Khattab is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ARNAV SINGHVI">
      <data key="d0">PERSON</data>
      <data key="d1">Arnav Singhvi is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PARIDHI MAHESHWARI">
      <data key="d0">PERSON</data>
      <data key="d1">Paridhi Maheshwari is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHIYUAN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhiyuan Zhang is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KESHAV SANTHANAM">
      <data key="d0">PERSON</data>
      <data key="d1">Keshav Santhanam is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAIFUL HAQ">
      <data key="d0">PERSON</data>
      <data key="d1">Saiful Haq is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ASHUTOSH SHARMA">
      <data key="d0">PERSON</data>
      <data key="d1">Ashutosh Sharma is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THOMAS T JOSHI">
      <data key="d0">PERSON</data>
      <data key="d1">Thomas T Joshi is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HANNA MOAZAM">
      <data key="d0">PERSON</data>
      <data key="d1">Hanna Moazam is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HEATHER MILLER">
      <data key="d0">PERSON</data>
      <data key="d1">Heather Miller is an author of the paper "Dspy: Compiling declarative language model calls into state-of-the-art pipelines"
Heather Miller is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALEX KRIZHEVSKY">
      <data key="d0">PERSON</data>
      <data key="d1">Alex Krizhevsky is an author of the paper "Imagenet classification with deep convolutional neural networks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GEOFFREY E HINTON">
      <data key="d0">PERSON</data>
      <data key="d1">Geoffrey E Hinton is an author of the paper "Imagenet classification with deep convolutional neural networks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ABRAHIM LADHA">
      <data key="d0">PERSON</data>
      <data key="d1">Abrahim Ladha is the author of the lecture "Lecture 11: Turing-completeness"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CS 4510 AUTOMATA AND COMPLEXITY">
      <data key="d0">COURSE</data>
      <data key="d1">The course for which the lecture "Lecture 11: Turing-completeness" was given</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">COURSE</data>
    </node>
    <node id="RISHABH SINGHAL">
      <data key="d0">PERSON</data>
      <data key="d1">Rishabh Singhal is the scribe for the lecture "Lecture 11: Turing-completeness"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KENNETH O STANLEY">
      <data key="d0">PERSON</data>
      <data key="d1">Kenneth O Stanley is an author of the paper "Abandoning objectives: Evolution through the search for novelty alone"
Kenneth O Stanley is an author of the paper "Designing neural networks through neuroevolution"Kenneth O Stanley is an author of the book "Why greatness cannot be planned: The myth of the objective"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="EVOLUTIONARY COMPUTATION">
      <data key="d0">PUBLICATION</data>
      <data key="d1">The journal where the paper "Abandoning objectives: Evolution through the search for novelty alone" was published</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="PATRICK LEWIS">
      <data key="d0">PERSON</data>
      <data key="d1">Patrick Lewis is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ETHAN PEREZ">
      <data key="d0">PERSON</data>
      <data key="d1">Ethan Perez is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALEKSANDRA PIKTUS">
      <data key="d0">PERSON</data>
      <data key="d1">Aleksandra Piktus is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FABIO PETRONI">
      <data key="d0">PERSON</data>
      <data key="d1">Fabio Petroni is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VLADIMIR KARPUKHIN">
      <data key="d0">PERSON</data>
      <data key="d1">Vladimir Karpukhin is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HEINRICH K&#220;TTLER">
      <data key="d0">PERSON</data>
      <data key="d1">Heinrich K&#252;ttler is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MIKE LEWIS">
      <data key="d0">PERSON</data>
      <data key="d1">Mike Lewis is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEN-TAU YIH">
      <data key="d0">PERSON</data>
      <data key="d1">Wen-tau Yih is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TIM ROCKT&#196;SCHEL">
      <data key="d0">PERSON</data>
      <data key="d1">Tim Rockt&#228;schel is an author of the paper "Retrieval-augmented generation for knowledge-intensive NLP tasks"
Tim Rockt&#228;schel is the author of the book "Artificial Intelligence: 10 Things You Should Know"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FEI LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Fei Liu is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model"
Fei Liu is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TONG XIALIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Tong Xialiang is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MINGXUAN YUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Mingxuan Yuan is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XI LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Xi Lin is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FU LUO">
      <data key="d0">PERSON</data>
      <data key="d1">Fu Luo is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHENKUN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhenkun Wang is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QINGFU ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Qingfu Zhang is an author of the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FORTY-FIRST INTERNATIONAL CONFERENCE ON MACHINE LEARNING">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "Evolution of heuristics: Towards efficient automatic algorithm design using large language model" was presented
The conference where the paper "Offline training of language model agents with functions as learnable weights" was presented</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e,cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">CONFERENCE</data>
    </node>
    <node id="ZIJUN LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Zijun Liu is an author of the paper "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YANZHE ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yanzhe Zhang is an author of the paper "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PENG LI">
      <data key="d0">PERSON</data>
      <data key="d1">Peng Li is an author of the paper "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YANG LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Yang Liu is an author of the paper "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DIYI YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Diyi Yang is an author of the paper "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHRIS LU">
      <data key="d0">PERSON</data>
      <data key="d1">Chris Lu is an author of the paper "The AI Scientist: Towards fully automated open-ended scientific discovery"Chris Lu is an author of the paper "Arbitrary order meta-learning with simple population-based evolution"Chris Lu is an author of the paper "Discovering preference optimization algorithms with and for large language models"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SEBASTIAN TOWERS">
      <data key="d0">PERSON</data>
      <data key="d1">Sebastian Towers is an author of the paper "Arbitrary order meta-learning with simple population-based evolution"Sebastian Towers is an author</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAKOB FOERSTER">
      <data key="d0">PERSON</data>
      <data key="d1">Jakob Foerster is an author of the paper "Arbitrary order meta-learning with simple population-based evolution"Jakob Foerster is an author of the paper "Discovering preference optimization algorithms with and for large language models"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALIFE 2023: GHOST IN THE MACHINE: PROCEEDINGS OF THE 2023 ARTIFICIAL LIFE CONFERENCE">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "Arbitrary order meta-learning with simple population-based evolution" was presented</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="MIT PRESS">
      <data key="d0">PUBLISHER</data>
      <data key="d1">The publisher of the proceedings "ALIFE 2023: Ghost in the Machine: Proceedings of the 2023 Artificial Life Conference"
MIT Press is the publisher of the book "Reinforcement learning: An introduction"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="SAMUEL HOLT">
      <data key="d0">PERSON</data>
      <data key="d1">Samuel Holt is an author of the paper "Discovering preference optimization algorithms with and for large language models"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="CLAUDIO FANCONI">
      <data key="d0">PERSON</data>
      <data key="d1">Claudio Fanconi is an author of the paper "Discovering preference optimization algorithms with and for large language models"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="ALEX J CHAN">
      <data key="d0">PERSON</data>
      <data key="d1">Alex J Chan is an author of the paper "Discovering preference optimization algorithms with and for large language models"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="MIHAELA VAN DER SCHAAR">
      <data key="d0">PERSON</data>
      <data key="d1">Mihaela van der Schaar is an author of the paper "Discovering preference optimization algorithms with and for large language models"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
    </node>
    <node id="ROBERT TJARKO LANGE">
      <data key="d0">PERSON</data>
      <data key="d1">Robert Tjarko Lange is an author of the paper "Discovering preference optimization algorithms with and for large language models"Robert Tjarko Lange is an author of the paper "The AI Scientist: Towards fully automated open-ended scientific discovery"</data>
      <data key="d2">6109537356a2ce2339f77c827aa3668e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THE AI SCIENTIST">
      <data key="d0">PAPER</data>
      <data key="d1">The AI Scientist: Towards fully automated open-ended scientific discovery is a paper published as an arXiv preprint in 2024</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PAPER</data>
    </node>
    <node id="IAN WHALEN">
      <data key="d0">PERSON</data>
      <data key="d1">Ian Whalen is an author of the paper "Nsga-net: neural architecture search using multi-objective genetic algorithm"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VISHNU BODDETI">
      <data key="d0">PERSON</data>
      <data key="d1">Vishnu Boddeti is an author of the paper "Nsga-net: neural architecture search using multi-objective genetic algorithm"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YASHESH DHEBAR">
      <data key="d0">PERSON</data>
      <data key="d1">Yashesh Dhebar is an author of the paper "Nsga-net: neural architecture search using multi-objective genetic algorithm"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ERIK GOODMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Erik Goodman is an author of the paper "Nsga-net: neural architecture search using multi-objective genetic algorithm"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WOLFGANG BANZHAF">
      <data key="d0">PERSON</data>
      <data key="d1">Wolfgang Banzhaf is an author of the paper "Nsga-net: neural architecture search using multi-objective genetic algorithm"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NSGA-NET">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Nsga-net is a neural architecture search tool using a multi-objective genetic algorithm</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE">
      <data key="d0">EVENT</data>
      <data key="d1">The conference where the paper "Nsga-net: neural architecture search using multi-objective genetic algorithm" was presented
The Genetic and Evolutionary Computation Conference is where the paper "Poet: open-ended coevolution of environments and their optimized solutions" was presented</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">EVENT</data>
    </node>
    <node id="YECHENG JASON MA">
      <data key="d0">PERSON</data>
      <data key="d1">Yecheng Jason Ma is an author of the paper "Eureka: Human-level reward design via coding large language models"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WILLIAM LIANG">
      <data key="d0">PERSON</data>
      <data key="d1">William Liang is an author of the paper "Eureka: Human-level reward design via coding large language models"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="OSBERT BASTANI">
      <data key="d0">PERSON</data>
      <data key="d1">Osbert Bastani is an author of the paper "Eureka: Human-level reward design via coding large language models"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DINESH JAYARAMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Dinesh Jayaraman is an author of the paper "Eureka: Human-level reward design via coding large language models"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ELLIOT MEYERSON">
      <data key="d0">PERSON</data>
      <data key="d1">Elliot Meyerson is an author of the paper "Language model crossover: Variation through few-shot prompting"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MARK J NELSON">
      <data key="d0">PERSON</data>
      <data key="d1">Mark J Nelson is an author of the paper "Language model crossover: Variation through few-shot prompting"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HERBIE BRADLEY">
      <data key="d0">PERSON</data>
      <data key="d1">Herbie Bradley is an author of the paper "Language model crossover: Variation through few-shot prompting"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ADAM GAIER">
      <data key="d0">PERSON</data>
      <data key="d1">Adam Gaier is an author of the paper "Language model crossover: Variation through few-shot prompting"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ARASH MORADI">
      <data key="d0">PERSON</data>
      <data key="d1">Arash Moradi is an author of the paper "Language model crossover: Variation through few-shot prompting"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AMY K HOOVER">
      <data key="d0">PERSON</data>
      <data key="d1">Amy K Hoover is an author of the paper "Language model crossover: Variation through few-shot prompting"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LANGUAGE MODEL CROSSOVER">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Language model crossover is a tool for variation through few-shot prompting</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="SHEN-YUN MIAO">
      <data key="d0">PERSON</data>
      <data key="d1">Shen-yun Miao is an author of the paper "A diverse corpus for evaluating and developing english math word problem solvers"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHAO-CHUN LIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Chao-Chun Liang is an author of the paper "A diverse corpus for evaluating and developing english math word problem solvers"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KEH-YIH SU">
      <data key="d0">PERSON</data>
      <data key="d1">Keh-Yih Su is an author of the paper "A diverse corpus for evaluating and developing english math word problem solvers"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="A DIVERSE CORPUS FOR EVALUATING AND DEVELOPING ENGLISH MATH WORD PROBLEM SOLVERS">
      <data key="d0">TOOL/DATASET</data>
      <data key="d1">A diverse corpus for evaluating and developing english math word problem solvers is a dataset for evaluating and developing English math word problem solvers</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/DATASET</data>
    </node>
    <node id="THE 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS">
      <data key="d0">EVENT</data>
      <data key="d1">The conference where the paper "A diverse corpus for evaluating and developing english math word problem solvers" was presented</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">EVENT</data>
    </node>
    <node id="JEAN-BAPTISTE MOURET">
      <data key="d0">PERSON</data>
      <data key="d1">Jean-Baptiste Mouret is an author of the paper "Illuminating search spaces by mapping elites"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ILLUMINATING SEARCH SPACES BY MAPPING ELITES">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Illuminating search spaces by mapping elites is a tool for illuminating search spaces</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="JEFF WU">
      <data key="d0">PERSON</data>
      <data key="d1">Jeff Wu is an author of the paper "Webgpt: Browser-assisted question-answering with human feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LONG OUYANG">
      <data key="d0">PERSON</data>
      <data key="d1">Long Ouyang is an author of the paper "Webgpt: Browser-assisted question-answering with human feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHRISTINA KIM">
      <data key="d0">PERSON</data>
      <data key="d1">Christina Kim is an author of the paper "Webgpt: Browser-assisted question-answering with human feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WILLIAM SAUNDERS">
      <data key="d0">PERSON</data>
      <data key="d1">William Saunders is an author of the paper "Webgpt: Browser-assisted question-answering with human feedback"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEBGPT">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Webgpt is a tool for browser-assisted question-answering with human feedback</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="ANDREW NG">
      <data key="d0">PERSON</data>
      <data key="d1">Andrew Ng is the author of the newsletter issue "Issue 253"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BEN NORMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Ben Norman is an author of the paper "First-explore, then exploit: Meta-learning intelligent exploration"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FIRST-EXPLORE, THEN EXPLOIT">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">First-explore, then exploit is a tool for meta-learning intelligent exploration</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="CHATGPT">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">ChatGPT is a conversational AI model introduced by OpenAI
ChatGPT is a baseline model evaluated on the Orca-Bench dataset</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69,bd4eb9459bc29b4c2da4658914fd4635</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="SIMPLE EVALS">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Simple Evals is a tool by OpenAI for evaluating AI models</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="JOON SUNG PARK">
      <data key="d0">PERSON</data>
      <data key="d1">Joon Sung Park is an author of the paper "Generative agents: Interactive simulacra of human behavior"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOSEPH O&#8217;BRIEN">
      <data key="d0">PERSON</data>
      <data key="d1">Joseph O&#8217;Brien is an author of the paper "Generative agents: Interactive simulacra of human behavior"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CARRIE JUN CAI">
      <data key="d0">PERSON</data>
      <data key="d1">Carrie Jun Cai is an author of the paper "Generative agents: Interactive simulacra of human behavior"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MEREDITH RINGEL MORRIS">
      <data key="d0">PERSON</data>
      <data key="d1">Meredith Ringel Morris is an author of the paper "Generative agents: Interactive simulacra of human behavior"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MICHAEL S BERNSTEIN">
      <data key="d0">PERSON</data>
      <data key="d1">Michael S Bernstein is an author of the paper "Generative agents: Interactive simulacra of human behavior"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GENERATIVE AGENTS">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Generative agents are interactive simulacra of human behavior</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="THE 36TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY">
      <data key="d0">EVENT</data>
      <data key="d1">The conference where the paper "Generative agents: Interactive simulacra of human behavior" was presented</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">EVENT</data>
    </node>
    <node id="ARKIL PATEL">
      <data key="d0">PERSON</data>
      <data key="d1">Arkil Patel is an author of the paper "Are NLP models really able to solve simple math word problems?"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SATWIK BHATTAMISHRA">
      <data key="d0">PERSON</data>
      <data key="d1">Satwik Bhattamishra is an author of the paper "Are NLP models really able to solve simple math word problems?"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NAVIN GOYAL">
      <data key="d0">PERSON</data>
      <data key="d1">Navin Goyal is an author of the paper "Are NLP models really able to solve simple math word problems?"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THE 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES">
      <data key="d0">EVENT</data>
      <data key="d1">The conference where the paper "Are NLP models really able to solve simple math word problems?" was presented</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">EVENT</data>
    </node>
    <node id="JUYUAN XU">
      <data key="d0">PERSON</data>
      <data key="d1">Juyuan Xu is an author of the paper "Communicative agents for software development"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="COMMUNICATIVE AGENTS">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Communicative agents are tools for software development</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">TOOL/ALGORITHM</data>
    </node>
    <node id="ZIHAO XIE">
      <data key="d0">PERSON</data>
      <data key="d1">Zihao Xie is an author of the paper "Scaling large-language-model-based multi-agent collaboration"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIFEI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yifei Wang is an author of the paper "Scaling large-language-model-based multi-agent collaboration"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEI LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Wei Liu is an author of the paper "Scaling large-language-model-based multi-agent collaboration"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUFAN DANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yufan Dang is an author of the paper "Scaling large-language-model-based multi-agent collaboration"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHUOYUN DU">
      <data key="d0">PERSON</data>
      <data key="d1">Zhuoyun Du is an author of the paper "Scaling large-language-model-based multi-agent collaboration"</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SCALING LARGE-LANGUAGE-MODEL-BASED MULTI-AGENT COLLABORATION">
      <data key="d0">TOOL/ALGORITHM</data>
      <data key="d1">Scaling large-language-model-based multi-agent collaboration is a tool for multi-agent collaboration</data>
      <data key="d2">1b1399c76420a477c0c97893d258ae69</data>
    </node>
    <node id="QIANG WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Qiang Wang is an author of the paper "Tool learning with large language models: A survey"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="DAWEI YIN">
      <data key="d0">PERSON</data>
      <data key="d1">Dawei Yin is an author of the paper "Tool learning with large language models: A survey"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JUN XU">
      <data key="d0">PERSON</data>
      <data key="d1">Jun Xu is an author of the paper "Tool learning with large language models: A survey"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JI-RONG WEN">
      <data key="d0">PERSON</data>
      <data key="d1">Ji-Rong Wen is an author of the paper "Tool learning with large language models: A survey"
Ji-Rong Wen is an author of the paper "A survey on the memory mechanism of large language model based agents"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </node>
    <node id="RAFAEL RAFAILOV">
      <data key="d0">PERSON</data>
      <data key="d1">Rafael Rafailov is an author of the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ARCHIT SHARMA">
      <data key="d0">PERSON</data>
      <data key="d1">Archit Sharma is an author of the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ERIC MITCHELL">
      <data key="d0">PERSON</data>
      <data key="d1">Eric Mitchell is an author of the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="STEFANO ERMON">
      <data key="d0">PERSON</data>
      <data key="d1">Stefano Ermon is an author of the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="DAVID REIN">
      <data key="d0">PERSON</data>
      <data key="d1">David Rein is an author of the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"
David Rein is an author of the paper "GPQA: A graduate-level Google-proof Q&amp;A benchmark"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="BETTY LI HOU">
      <data key="d0">PERSON</data>
      <data key="d1">Betty Li Hou is an author of the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"
Betty Li Hou is an author of the paper "GPQA: A graduate-level Google-proof Q&amp;A benchmark"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ASA COOPER STICKLAND">
      <data key="d0">PERSON</data>
      <data key="d1">Asa Cooper Stickland is an author of the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"
Asa Cooper Stickland is an author of the paper "GPQA: A graduate-level Google-proof Q&amp;A benchmark"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="JACKSON PETTY">
      <data key="d0">PERSON</data>
      <data key="d1">Jackson Petty is an author of the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"
Jackson Petty is an author of the paper "GPQA: A graduate-level Google-proof Q&amp;A benchmark"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="RICHARD YUANZHE PANG">
      <data key="d0">PERSON</data>
      <data key="d1">Richard Yuanzhe Pang is an author of the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"
Richard Yuanzhe Pang is an author of the paper "GPQA: A graduate-level Google-proof Q&amp;A benchmark"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="JULIEN DIRANI">
      <data key="d0">PERSON</data>
      <data key="d1">Julien Dirani is an author of the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"
Julien Dirani is an author of the paper "GPQA: A graduate-level Google-proof Q&amp;A benchmark"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="JULIAN MICHAEL">
      <data key="d0">PERSON</data>
      <data key="d1">Julian Michael is an author of the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"
Julian Michael is an author of the paper "GPQA: A graduate-level Google-proof Q&amp;A benchmark"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SAMUEL R. BOWMAN">
      <data key="d0">PERSON</data>
      <data key="d1">Samuel R. Bowman is an author of the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"
Samuel R. Bowman is an author of the paper "GPQA: A graduate-level Google-proof Q&amp;A benchmark"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="TORAN BRUCE RICHARDS">
      <data key="d0">PERSON</data>
      <data key="d1">Toran Bruce Richards is the creator of the AutoGPT project on GitHub</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="AUTOGPT">
      <data key="d0">TOOL/PROJECT</data>
      <data key="d1">AutoGPT is a project hosted on GitHub by Toran Bruce Richards</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SEVEN DIALS">
      <data key="d0">PUBLISHER</data>
      <data key="d1">Seven Dials is the publisher of the book "Artificial Intelligence: 10 Things You Should Know"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MD OMAR FARUK ROKON">
      <data key="d0">PERSON</data>
      <data key="d1">Md Omar Faruk Rokon is an author of the paper "SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="RISUL ISLAM">
      <data key="d0">PERSON</data>
      <data key="d1">Risul Islam is an author of the paper "SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="AHMAD DARKI">
      <data key="d0">PERSON</data>
      <data key="d1">Ahmad Darki is an author of the paper "SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="EVANGELOS E PAPALEXAKIS">
      <data key="d0">PERSON</data>
      <data key="d1">Evangelos E Papalexakis is an author of the paper "SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MICHALIS FALOUTSOS">
      <data key="d0">PERSON</data>
      <data key="d1">Michalis Faloutsos is an author of the paper "SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="INTERNATIONAL SYMPOSIUM ON RESEARCH IN ATTACKS, INTRUSIONS AND DEFENSES">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub" was presented</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="BERNARDINO ROMERA-PAREDES">
      <data key="d0">PERSON</data>
      <data key="d1">Bernardino Romera-Paredes is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MOHAMMADAMIN BAREKATAIN">
      <data key="d0">PERSON</data>
      <data key="d1">Mohammadamin Barekatain is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ALEXANDER NOVIKOV">
      <data key="d0">PERSON</data>
      <data key="d1">Alexander Novikov is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MATEJ BALOG">
      <data key="d0">PERSON</data>
      <data key="d1">Matej Balog is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="M PAWAN KUMAR">
      <data key="d0">PERSON</data>
      <data key="d1">M Pawan Kumar is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="EMILIEN DUPONT">
      <data key="d0">PERSON</data>
      <data key="d1">Emilien Dupont is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="FRANCISCO JR RUIZ">
      <data key="d0">PERSON</data>
      <data key="d1">Francisco JR Ruiz is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JORDAN S ELLENBERG">
      <data key="d0">PERSON</data>
      <data key="d1">Jordan S Ellenberg is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="PENGMING WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Pengming Wang is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="OMAR FAWZI">
      <data key="d0">PERSON</data>
      <data key="d1">Omar Fawzi is an author of the paper "Mathematical discoveries from program search with large language models"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ERIC HAMBRO">
      <data key="d0">PERSON</data>
      <data key="d1">Eric Hambro is an author of the paper "Toolformer: Language models can teach themselves to use tools"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="NEURAL INFORMATION PROCESSING SYSTEMS">
      <data key="d0">CONFERENCE</data>
      <data key="d1">The conference where the paper "Toolformer: Language models can teach themselves to use tools" was presented</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SANDER SCHULHOFF">
      <data key="d0">PERSON</data>
      <data key="d1">Sander Schulhoff is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MICHAEL ILIE">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Ilie is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="NISHANT BALEPUR">
      <data key="d0">PERSON</data>
      <data key="d1">Nishant Balepur is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="KONSTANTINE KAHADZE">
      <data key="d0">PERSON</data>
      <data key="d1">Konstantine Kahadze is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="AMANDA LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Amanda Liu is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="CHENGLEI SI">
      <data key="d0">PERSON</data>
      <data key="d1">Chenglei Si is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="YINHENG LI">
      <data key="d0">PERSON</data>
      <data key="d1">Yinheng Li is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="AAYUSH GUPTA">
      <data key="d0">PERSON</data>
      <data key="d1">Aayush Gupta is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="HYOJUNG HAN">
      <data key="d0">PERSON</data>
      <data key="d1">HyoJung Han is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SEVIEN SCHULHOFF">
      <data key="d0">PERSON</data>
      <data key="d1">Sevien Schulhoff is an author of the paper "The prompt report: A systematic survey of prompting techniques"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="XUAN SHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Xuan Shen is an author of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="YAOHUA WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yaohua Wang is an author of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MING LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Ming Lin is an author of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="YILUN HUANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yilun Huang is an author of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="HAO TANG">
      <data key="d0">PERSON</data>
      <data key="d1">Hao Tang is an author of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="XIUYU SUN">
      <data key="d0">PERSON</data>
      <data key="d1">Xiuyu Sun is an author of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="YANZHI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yanzhi Wang is an author of the paper "Deepmad: Mathematical architecture design for deep convolutional neural network"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="FREDA SHI">
      <data key="d0">PERSON</data>
      <data key="d1">Freda Shi is an author of the paper "Language models are multilingual chain-of-thought reasoners"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MIRAC SUZGUN">
      <data key="d0">PERSON</data>
      <data key="d1">Mirac Suzgun is an author of the paper "Language models are multilingual chain-of-thought reasoners"
Mirac Suzgun is an author of the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="MARKUS FREITAG">
      <data key="d0">PERSON</data>
      <data key="d1">Markus Freitag is an author of the paper "Language models are multilingual chain-of-thought reasoners"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SURAJ SRIVATS">
      <data key="d0">PERSON</data>
      <data key="d1">Suraj Srivats is an author of the paper "Language models are multilingual chain-of-thought reasoners"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SOROUSH VOSOUGHI">
      <data key="d0">PERSON</data>
      <data key="d1">Soroush Vosoughi is an author of the paper "Language models are multilingual chain-of-thought reasoners"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SEBASTIAN RUDER">
      <data key="d0">PERSON</data>
      <data key="d1">Sebastian Ruder is an author of the paper "Language models are multilingual chain-of-thought reasoners"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="DIPANJAN DAS">
      <data key="d0">PERSON</data>
      <data key="d1">Dipanjan Das is an author of the paper "Language models are multilingual chain-of-thought reasoners"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="RISTO MIIKKULAINEN">
      <data key="d0">PERSON</data>
      <data key="d1">Risto Miikkulainen is an author of the paper "Designing neural networks through neuroevolution"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="NATURE MACHINE INTELLIGENCE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Nature Machine Intelligence is the journal where the paper "Designing neural networks through neuroevolution" was published</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="RICHARD S SUTTON">
      <data key="d0">PERSON</data>
      <data key="d1">Richard S Sutton is an author of the book "Reinforcement learning: An introduction"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ANDREW G BARTO">
      <data key="d0">PERSON</data>
      <data key="d1">Andrew G Barto is an author of the book "Reinforcement learning: An introduction"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="SAI VEMPRALA">
      <data key="d0">PERSON</data>
      <data key="d1">Sai Vemprala is an author of the paper "ChatGPT for robotics: Design principles and model abilities"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ROGERIO BONATTI">
      <data key="d0">PERSON</data>
      <data key="d1">Rogerio Bonatti is an author of the paper "ChatGPT for robotics: Design principles and model abilities"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ARTHUR BUCKER">
      <data key="d0">PERSON</data>
      <data key="d1">Arthur Bucker is an author of the paper "ChatGPT for robotics: Design principles and model abilities"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ASHISH KAPOOR">
      <data key="d0">PERSON</data>
      <data key="d1">Ashish Kapoor is an author of the paper "ChatGPT for robotics: Design principles and model abilities"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JANE X WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jane X Wang is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ZEB KURTH-NELSON">
      <data key="d0">PERSON</data>
      <data key="d1">Zeb Kurth-Nelson is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="DHRUVA TIRUMALA">
      <data key="d0">PERSON</data>
      <data key="d1">Dhruva Tirumala is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="HUBERT SOYER">
      <data key="d0">PERSON</data>
      <data key="d1">Hubert Soyer is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JOEL Z LEIBO">
      <data key="d0">PERSON</data>
      <data key="d1">Joel Z Leibo is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="REMI MUNOS">
      <data key="d0">PERSON</data>
      <data key="d1">Remi Munos is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="CHARLES BLUNDELL">
      <data key="d0">PERSON</data>
      <data key="d1">Charles Blundell is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="DHARSHAN KUMARAN">
      <data key="d0">PERSON</data>
      <data key="d1">Dharshan Kumaran is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="MATT BOTVINICK">
      <data key="d0">PERSON</data>
      <data key="d1">Matt Botvinick is an author of the paper "Learning to reinforcement learn"
Matt Botvinick is an author of the paper "Learning to reinforcement learn"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="LEI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Lei Wang is an author of the paper "A survey on large language model based autonomous agents"
Lei Wang is an author of the paper "A survey on large language model based autonomous agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="CHEN MA">
      <data key="d0">PERSON</data>
      <data key="d1">Chen Ma is an author of the paper "A survey on large language model based autonomous agents"
Chen Ma is an author of the paper "A survey on large language model based autonomous agents"
Chen Ma is an author of the paper "A survey on the memory mechanism of large language model based agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </node>
    <node id="XUEYANG FENG">
      <data key="d0">PERSON</data>
      <data key="d1">Xueyang Feng is an author of the paper "A survey on large language model based autonomous agents"
Xueyang Feng is an author of the paper "A survey on large language model based autonomous agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ZEYU ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Zeyu Zhang is an author of the paper "A survey on large language model based autonomous agents"
Zeyu Zhang is an author of the paper "A survey on large language model based autonomous agents"
Zeyu Zhang is an author of the paper "A survey on the memory mechanism of large language model based agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </node>
    <node id="HAO YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Hao Yang is an author of the paper "A survey on large language model based autonomous agents"
Hao Yang is an author of the paper "A survey on large language model based autonomous agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JINGSEN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jingsen Zhang is an author of the paper "A survey on large language model based autonomous agents"
Jingsen Zhang is an author of the paper "A survey on large language model based autonomous agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="ZHIYUAN CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Zhiyuan Chen is an author of the paper "A survey on large language model based autonomous agents"
Zhiyuan Chen is an author of the paper "A survey on large language model based autonomous agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="JIAKAI TANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jiakai Tang is an author of the paper "A survey on large language model based autonomous agents"
Jiakai Tang is an author of the paper "A survey on large language model based autonomous agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="XU CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Xu Chen is an author of the paper "A survey on large language model based autonomous agents"
Xu Chen is an author of the paper "A survey on large language model based autonomous agents"
Xu Chen is an author of the paper "A survey on the memory mechanism of large language model based agents"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3,cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </node>
    <node id="FRONTIERS OF COMPUTER SCIENCE">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Frontiers of Computer Science is the journal where the paper "A survey on large language model based autonomous agents" was published
Frontiers of Computer Science is the journal where the paper "A survey on large language model based autonomous agents" was published</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,34d0bb2211fc795fe1096442e086a2b3</data>
    </node>
    <node id="RUI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Rui Wang is an author of the papers "Poet: open-ended coevolution of environments and their optimized solutions" and "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="KENNETH O. STANLEY">
      <data key="d0">PERSON</data>
      <data key="d1">Kenneth O. Stanley is an author of the papers "Poet: open-ended coevolution of environments and their optimized solutions" and "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="ASSOCIATION FOR COMPUTING MACHINERY">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">The Association for Computing Machinery is the organization that hosted the Genetic and Evolutionary Computation Conference</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="ADITYA RAWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Aditya Rawal is an author of the paper "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="JIALE ZHI">
      <data key="d0">PERSON</data>
      <data key="d1">Jiale Zhi is an author of the paper "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="YULUN LI">
      <data key="d0">PERSON</data>
      <data key="d1">Yulun Li is an author of the paper "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="QUOC V LE">
      <data key="d0">PERSON</data>
      <data key="d1">Quoc V Le is an author of the papers "Self-consistency improves chain of thought reasoning in language models" and "Chain-of-thought prompting elicits reasoning in large language models"Quoc V Le is an author of the paper "Self-consistency improves chain of thought reasoning in language models"
Quoc V Le is an author of the paper "Take a step back: Evoking reasoning via abstraction in large language models"
Quoc V Le is an author of the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ED H. CHI">
      <data key="d0">PERSON</data>
      <data key="d1">Ed H. Chi is an author of the paper "Self-consistency improves chain of thought reasoning in language models"Ed H. Chi is an author of the papers "Self-consistency improves chain of thought reasoning in language models" and "Chain-of-thought prompting elicits reasoning in large language models"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THE ELEVENTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS">
      <data key="d0">EVENT</data>
      <data key="d1">The Eleventh International Conference on Learning Representations is where the paper "Self-consistency improves chain of thought reasoning in language models" was presentedThe Eleventh International Conference on Learning Representations is where the papers "Self-consistency improves chain of thought reasoning in language models" and "React: Synergizing reasoning and acting in language models" were presented</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">EVENT</data>
    </node>
    <node id="QINGYUN WU">
      <data key="d0">PERSON</data>
      <data key="d1">Qingyun Wu is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation framework"
Qingyun Wu is an author of the paper "Offline training of language model agents with functions as learnable weights"
Qingyun Wu is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GAGAN BANSAL">
      <data key="d0">PERSON</data>
      <data key="d1">Gagan Bansal is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation framework"
Gagan Bansal is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIEYU ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jieyu Zhang is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation framework"
Jieyu Zhang is an author of the paper "Offline training of language model agents with functions as learnable weights"
Jieyu Zhang is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIRAN WU">
      <data key="d0">PERSON</data>
      <data key="d1">Yiran Wu is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation framework"
Yiran Wu is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHAOKUN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Shaokun Zhang is an author of the paper "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms"Shaokun Zhang is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation framework"Shaokun Zhang is an author of the papers "Autogen: Enabling next-gen llm applications via multi-agent conversation framework" and "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms"
Shaokun Zhang is an author of the paper "Offline training of language model agents with functions as learnable weights"
Shaokun Zhang is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ERKANG ZHU">
      <data key="d0">PERSON</data>
      <data key="d1">Erkang Zhu is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation framework"
Erkang Zhu is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BEIBIN LI">
      <data key="d0">PERSON</data>
      <data key="d1">Beibin Li is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation framework"
Beibin Li is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LI JIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Li Jiang is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation framework"
Li Jiang is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIAOYUN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Xiaoyun Zhang is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation framework"
Xiaoyun Zhang is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Chi Wang is an author of the papers "Autogen: Enabling next-gen llm applications via multi-agent conversation framework" and "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms"Chi Wang is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation framework"
Chi Wang is an author of the paper "Offline training of language model agents with functions as learnable weights"
Chi Wang is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BENFENG XU">
      <data key="d0">PERSON</data>
      <data key="d1">Benfeng Xu is an author of the paper "Expertprompting: Instructing large language models to be distinguished experts"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AN YANG">
      <data key="d0">PERSON</data>
      <data key="d1">An Yang is an author of the paper "Expertprompting: Instructing large language models to be distinguished experts"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JUNYANG LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Junyang Lin is an author of the paper "Expertprompting: Instructing large language models to be distinguished experts"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QUAN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Quan Wang is an author of the paper "Expertprompting: Instructing large language models to be distinguished experts"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHANG ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Chang Zhou is an author of the paper "Expertprompting: Instructing large language models to be distinguished experts"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YONGDONG ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yongdong Zhang is an author of the paper "Expertprompting: Instructing large language models to be distinguished experts"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHENDONG MAO">
      <data key="d0">PERSON</data>
      <data key="d1">Zhendong Mao is an author of the paper "Expertprompting: Instructing large language models to be distinguished experts"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHENGRUN YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Chengrun Yang is an author of the paper "Large language models as optimizers"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIFENG LU">
      <data key="d0">PERSON</data>
      <data key="d1">Yifeng Lu is an author of the paper "Large language models as optimizers"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HANXIAO LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Hanxiao Liu is an author of the paper "Large language models as optimizers"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BENNET YEE">
      <data key="d0">PERSON</data>
      <data key="d1">Bennet Yee is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAVID SEHR">
      <data key="d0">PERSON</data>
      <data key="d1">David Sehr is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GREGORY DARDYK">
      <data key="d0">PERSON</data>
      <data key="d1">Gregory Dardyk is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="J BRADLEY CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">J Bradley Chen is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ROBERT MUTH">
      <data key="d0">PERSON</data>
      <data key="d1">Robert Muth is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TAVIS ORMANDY">
      <data key="d0">PERSON</data>
      <data key="d1">Tavis Ormandy is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHIKI OKASAKA">
      <data key="d0">PERSON</data>
      <data key="d1">Shiki Okasaka is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NEHA NARULA">
      <data key="d0">PERSON</data>
      <data key="d1">Neha Narula is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NICHOLAS FULLAGAR">
      <data key="d0">PERSON</data>
      <data key="d1">Nicholas Fullagar is an author of the paper "Native client: A sandbox for portable, untrusted x86 native code"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="COMMUNICATIONS OF THE ACM">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Communications of the ACM is the journal where the paper "Native client: A sandbox for portable, untrusted x86 native code" was published</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="WENHAO YU">
      <data key="d0">PERSON</data>
      <data key="d1">Wenhao Yu is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NIMROD GILEADI">
      <data key="d0">PERSON</data>
      <data key="d1">Nimrod Gileadi is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SEAN KIRMANI">
      <data key="d0">PERSON</data>
      <data key="d1">Sean Kirmani is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MONTSERRAT GONZALEZ ARENAS">
      <data key="d0">PERSON</data>
      <data key="d1">Montserrat Gonzalez Arenas is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HAO-TIEN LEWIS CHIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Hao-Tien Lewis Chiang is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TOM EREZ">
      <data key="d0">PERSON</data>
      <data key="d1">Tom Erez is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LEONARD HASENCLEVER">
      <data key="d0">PERSON</data>
      <data key="d1">Leonard Hasenclever is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAN HUMPLIK">
      <data key="d0">PERSON</data>
      <data key="d1">Jan Humplik is an author of the paper "Language to rewards for robotic skill synthesis"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CONFERENCE ON ROBOT LEARNING">
      <data key="d0">EVENT</data>
      <data key="d1">The Conference on Robot Learning is where the paper "Language to rewards for robotic skill synthesis" was presented</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="SIYU YUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Siyu Yuan is an author of the paper "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIANGJIE CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Jiangjie Chen is an author of the paper "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="DEQING YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Deqing Yang is an author of the paper "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="ELIEZER YUDKOWSKY">
      <data key="d0">PERSON</data>
      <data key="d1">Eliezer Yudkowsky is an author of the paper "Artificial Intelligence as a positive and negative factor in global risk"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="GLOBAL CATASTROPHIC RISKS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Global Catastrophic Risks is the journal where the paper "Artificial Intelligence as a positive and negative factor in global risk" was published</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="MATEI ZAHARIA">
      <data key="d0">PERSON</data>
      <data key="d1">Matei Zaharia is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="LINGJIAO CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Lingjiao Chen is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="JARED QUINCY DAVIS">
      <data key="d0">PERSON</data>
      <data key="d1">Jared Quincy Davis is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="CHRIS POTTS">
      <data key="d0">PERSON</data>
      <data key="d1">Chris Potts is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="JAMES ZOU">
      <data key="d0">PERSON</data>
      <data key="d1">James Zou is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="MICHAEL CARBIN">
      <data key="d0">PERSON</data>
      <data key="d1">Michael Carbin is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="JONATHAN FRANKLE">
      <data key="d0">PERSON</data>
      <data key="d1">Jonathan Frankle is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="NAVEEN RAO">
      <data key="d0">PERSON</data>
      <data key="d1">Naveen Rao is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="ALI GHODSI">
      <data key="d0">PERSON</data>
      <data key="d1">Ali Ghodsi is an author of the paper "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="BAIR">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">BAIR is the organization that published the blog post "The shift from models to compound ai systems"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1</data>
    </node>
    <node id="KENNETH STANLEY">
      <data key="d0">PERSON</data>
      <data key="d1">Kenneth Stanley is an author of the paper "OMNI: Open-endedness via models of human notions of interestingness"
Kenneth Stanley is an author of the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d2">2600a1ed94ad2d3675ea80575c39cbd1,cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </node>
    <node id="JIALE LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Jiale Liu is an author of the paper "Offline training of language model agents with functions as learnable weights"
Jiale Liu is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LINXIN SONG">
      <data key="d0">PERSON</data>
      <data key="d1">Linxin Song is an author of the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RANJAY KRISHNA">
      <data key="d0">PERSON</data>
      <data key="d1">Ranjay Krishna is an author of the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIAOHE BO">
      <data key="d0">PERSON</data>
      <data key="d1">Xiaohe Bo is an author of the paper "A survey on the memory mechanism of large language model based agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RUI LI">
      <data key="d0">PERSON</data>
      <data key="d1">Rui Li is an author of the paper "A survey on the memory mechanism of large language model based agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QUANYU DAI">
      <data key="d0">PERSON</data>
      <data key="d1">Quanyu Dai is an author of the paper "A survey on the memory mechanism of large language model based agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIEMING ZHU">
      <data key="d0">PERSON</data>
      <data key="d1">Jieming Zhu is an author of the paper "A survey on the memory mechanism of large language model based agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZHENHUA DONG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhenhua Dong is an author of the paper "A survey on the memory mechanism of large language model based agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HENG-TZE CHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Heng-Tze Cheng is an author of the paper "Take a step back: Evoking reasoning via abstraction in large language models"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ED H CHI">
      <data key="d0">PERSON</data>
      <data key="d1">Ed H Chi is an author of the paper "Take a step back: Evoking reasoning via abstraction in large language models"
Ed H Chi is an author of the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PEI ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Pei Zhou is an author of the paper "Self-discover: Large language models self-compose reasoning structures"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAY PUJARA">
      <data key="d0">PERSON</data>
      <data key="d1">Jay Pujara is an author of the paper "Self-discover: Large language models self-compose reasoning structures"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIANG REN">
      <data key="d0">PERSON</data>
      <data key="d1">Xiang Ren is an author of the paper "Self-discover: Large language models self-compose reasoning structures"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WANGCHUNSHU ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Wangchunshu Zhou is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIXIN OU">
      <data key="d0">PERSON</data>
      <data key="d1">Yixin Ou is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHENGWEI DING">
      <data key="d0">PERSON</data>
      <data key="d1">Shengwei Ding is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LONG LI">
      <data key="d0">PERSON</data>
      <data key="d1">Long Li is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIALONG WU">
      <data key="d0">PERSON</data>
      <data key="d1">Jialong Wu is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="TIANNAN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Tiannan Wang is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIAMIN CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Jiamin Chen is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHUAI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Shuai Wang is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIAOHUA XU">
      <data key="d0">PERSON</data>
      <data key="d1">Xiaohua Xu is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NINGYU ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Ningyu Zhang is an author of the paper "Symbolic learning enables self-evolving agents"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MINGCHEN ZHUGE">
      <data key="d0">PERSON</data>
      <data key="d1">Mingchen Zhuge is an author of the paper "GPTSwarm: Language agents as optimizable graphs"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WENYI WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Wenyi Wang is an author of the paper "GPTSwarm: Language agents as optimizable graphs"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LOUIS KIRSCH">
      <data key="d0">PERSON</data>
      <data key="d1">Louis Kirsch is an author of the paper "GPTSwarm: Language agents as optimizable graphs"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FRANCESCO FACCIO">
      <data key="d0">PERSON</data>
      <data key="d1">Francesco Faccio is an author of the paper "GPTSwarm: Language agents as optimizable graphs"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DMITRII KHIZBULLIN">
      <data key="d0">PERSON</data>
      <data key="d1">Dmitrii Khizbullin is an author of the paper "GPTSwarm: Language agents as optimizable graphs"
Dmitrii Khizbullin is an author of the paper "Camel: Communicative agents for 'mind' exploration of large language model society"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="J&#220;RGEN SCHMIDHUBER">
      <data key="d0">PERSON</data>
      <data key="d1">J&#252;rgen Schmidhuber is an author of the paper "GPTSwarm: Language agents as optimizable graphs"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LUISA ZINTGRAF">
      <data key="d0">PERSON</data>
      <data key="d1">Luisa Zintgraf is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SEBASTIAN SCHULZE">
      <data key="d0">PERSON</data>
      <data key="d1">Sebastian Schulze is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LEO FENG">
      <data key="d0">PERSON</data>
      <data key="d1">Leo Feng is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MAXIMILIAN IGL">
      <data key="d0">PERSON</data>
      <data key="d1">Maximilian Igl is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KYRIACOS SHIARLIS">
      <data key="d0">PERSON</data>
      <data key="d1">Kyriacos Shiarlis is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YARIN GAL">
      <data key="d0">PERSON</data>
      <data key="d1">Yarin Gal is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"
Yarin Gal is an author of the paper "The curse of recursion: Training on generated data makes models forget"
Yarin Gal is an author of the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc802d9b841fde55e9c0c2ba0ef7869d,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KATJA HOFMANN">
      <data key="d0">PERSON</data>
      <data key="d1">Katja Hofmann is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHIMON WHITESON">
      <data key="d0">PERSON</data>
      <data key="d1">Shimon Whiteson is an author of the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LUISA M ZINTGRAF">
      <data key="d0">PERSON</data>
      <data key="d1">Luisa M Zintgraf is an author of the paper "Exploration in approximate hyper-state space for meta reinforcement learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="KRISTIAN HARTIKAINEN">
      <data key="d0">PERSON</data>
      <data key="d1">Kristian Hartikainen is an author of the paper "Exploration in approximate hyper-state space for meta reinforcement learning"</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOURNAL OF MACHINE LEARNING RESEARCH">
      <data key="d0">JOURNAL</data>
      <data key="d1">The journal where the paper "Varibad: Variational bayes-adaptive deep RL via meta-learning" was published</data>
      <data key="d2">cc802d9b841fde55e9c0c2ba0ef7869d</data>
      <data key="d3">JOURNAL</data>
    </node>
    <node id="NAME">
      <data key="d0">SECTION/KEY</data>
      <data key="d1">The "name" section corresponds to the name of the next agent architecture proposed by the meta agent.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="CODE">
      <data key="d0">SECTION/KEY</data>
      <data key="d1">The "code" section contains the exact Python code for the "forward()" function that the meta agent proposes to implement.
Code is the solution generated by the FM_Module based on the initial instruction and task information
Code refers to the programming code being evaluated and refined throughout the process</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0,449db721e37968e073e3579b59e023b2,84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="IMPLEMENTATION MISTAKES">
      <data key="d0">ISSUE/ERROR</data>
      <data key="d1">Implementation mistakes are errors or issues in the code that the meta agent needs to identify and correct during the self-reflection process.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="IMPROVEMENT">
      <data key="d0">PROCESS</data>
      <data key="d1">Improvement involves refining and optimizing the existing implementation to increase performance or effectiveness without altering the overall design framework.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="RUNTIME ERROR">
      <data key="d0">ISSUE/ERROR</data>
      <data key="d1">A runtime error is an error encountered during the execution of the generated code, prompting the meta agent to debug and correct the code.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="FRAMEWORK">
      <data key="d0">SYSTEM/TOOL</data>
      <data key="d1">The framework is a simple system provided to the meta agent to implement basic functions, such as querying Foundation Models and formatting prompts.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="NAMEDTUPLE INFO OBJECT">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">The namedtuple Info object is used in the framework to encapsulate and combine different types of information, facilitating communication between modules.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="INFO OBJECT">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">The Info object is a namedtuple used to encapsulate various pieces of information, such as FM responses and task descriptions, within the framework.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
      <data key="d3">DATA STRUCTURE</data>
    </node>
    <node id="APPENDIX B">
      <data key="d0">SECTION/DOCUMENT</data>
      <data key="d1">Appendix B contains the framework code used by the meta agent.
Appendix B specifies the types of tasks/benchmarks and the corresponding methods used to extract answers and generate metrics.
Appendix B contains the prompts utilized in the evaluations for summarization abilities</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0,86f77e15d41cbd0cb33f635ccb2cb66b,8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="APPENDICES C AND D">
      <data key="d0">SECTION/DOCUMENT</data>
      <data key="d1">Appendices C and D contain additional information relevant to the meta agent's operation and evaluation.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="OUTPUT INSTRUCTION AND EXAMPLE">
      <data key="d0">SECTION/DOCUMENT</data>
      <data key="d1">This section provides instructions and examples for the meta agent's output format, including the "thought," "name," and "code" keys.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="ARCHIVE">
      <data key="d0">DATASET/REPOSITORY</data>
      <data key="d1">The archive contains existing methods and architectures that the meta agent compares its proposed architecture against during self-reflection.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="WRONG IMPLEMENTATION EXAMPLES">
      <data key="d0">SECTION/DOCUMENT</data>
      <data key="d1">This section provides examples of potential mistakes the meta agent may make in implementation, used for self-reflection and debugging.</data>
      <data key="d2">282313a8340c6792e8c35f53ed157cd0</data>
    </node>
    <node id="FM MODULE">
      <data key="d0">TOOL/COMPONENT</data>
      <data key="d1">A module that constructs prompts by concatenating input Info objects into a structured format and generates responses using a specified model</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="INFO">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">A named tuple used for holding task information, including name, author, content, and iteration index
Info is a data structure used to provide structured feedback during the refinement process
An object used to encapsulate information about sub-problems and their solutions</data>
      <data key="d2">84317ae35cc75d612287186d93461447,d66dc9ce4a9545b44f7486ea057b5937,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="FORMAT_INST">
      <data key="d0">FUNCTION</data>
      <data key="d1">A lambda function that formats instructions for FM responses in a specific JSON format</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="ROLE_DESC">
      <data key="d0">FUNCTION</data>
      <data key="d1">A lambda function that describes the role of the FM Module</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="GET_JSON_RESPONSE_FROM_GPT">
      <data key="d0">FUNCTION</data>
      <data key="d1">A function to get JSON responses from a GPT model, handling rate limit errors using backoff</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="FM_MODULE">
      <data key="d0">CLASS</data>
      <data key="d1">A base class for an FM module, containing attributes like output fields, name, role, model, temperature, and ID
FM_Module is a module used to generate initial candidate solutions by thinking and writing code
FM_Module is a module used for various tasks such as providing human-like feedback, expert feedback, and refinement of code solutions
A module used in various stages of problem-solving, including decomposition, specialization, and integration</data>
      <data key="d2">449db721e37968e073e3579b59e023b2,84317ae35cc75d612287186d93461447,d66dc9ce4a9545b44f7486ea057b5937,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="AGENT SYSTEM">
      <data key="d0">SYSTEM</data>
      <data key="d1">A system that processes task information and returns either a namedtuple Info or a string as the final answer</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="META-AGENT SEARCH">
      <data key="d0">SYSTEM</data>
      <data key="d1">A system that uses a simple framework for agentic tasks, including modules like FM_Module</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="CODE 1">
      <data key="d0">CODE SNIPPET</data>
      <data key="d1">A code snippet showing the simple framework used in Meta-Agent Search</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="CODE 2">
      <data key="d0">CODE SNIPPET</data>
      <data key="d1">A code snippet showing an example of implementing self-reflection using the framework</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="TASK DESCRIPTIONS">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">Descriptions of tasks that are used to facilitate communication between different modules</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="FM RESPONSES">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">Responses generated by the FM Module based on the constructed prompts</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="RESULTS FROM TOOL FUNCTION CALLS">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">Results obtained from calling functions within the tool</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="SYSTEM PROMPT">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">Part of the prompt generated by the FM Module, providing system-level instructions</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="USER PROMPT">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">Part of the prompt generated by the FM Module, providing user-level instructions</data>
      <data key="d2">d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="COT_INITIAL_INSTRUCTION">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">Instruction for initial reasoning in the self-reflection process
An initial instruction used by the cot_module to start the task-solving process</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7,d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="COT_REFLECT_INSTRUCTION">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">Instruction for reflecting on previous attempts and feedback to improve task-solving
An instruction to reflect on previous attempts and feedback to improve the task solution</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7,d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="CRITIC_INSTRUCTION">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">Instruction for providing feedback and correcting the answer
An instruction to review and criticize the answer, or confirm its correctness</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7,d66dc9ce4a9545b44f7486ea057b5937</data>
    </node>
    <node id="COT_MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">A module named FM_Module used for 'Chain-of-Thought' processing, involving thinking and answering</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="CRITIC_MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">A module named FM_Module used for providing feedback and correcting answers</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="N_MAX">
      <data key="d0">PARAMETER</data>
      <data key="d1">The maximum number of attempts allowed, set to 5</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="TASKINFO">
      <data key="d0">DATA</data>
      <data key="d1">The initial input data for the task
TaskInfo is the data input provided to the agent for solving the task
TaskInfo is a data structure used to provide initial instructions and context for the code evaluation process
The input information or task that needs to be processed and solved</data>
      <data key="d2">449db721e37968e073e3579b59e023b2,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="THINKING">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of thinking involved in solving the task
Thinking refers to the thought process or reasoning applied during code evaluation and feedback generation</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="GPT-4O-2024-05-13">
      <data key="d0">MODEL</data>
      <data key="d1">A version of OpenAI's language model used by the meta agent
GPT-4o-2024-05-13 is a version of OpenAI's language model used by the meta agent
GPT-4o-2024-05-13 is a version of OpenAI's language model used by the meta agent</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="GPT-3.5-TURBO-0125">
      <data key="d0">MODEL</data>
      <data key="d1">A version of OpenAI's language model used to evaluate discovered agents and baselines
GPT-3.5-turbo-0125 is a version of OpenAI's language model used by discovered agents and baselines
GPT-3.5-turbo-0125 is a version of OpenAI's language model used to evaluate discovered agents and baselines to reduce compute cost
A version of OpenAI's language model used during the evaluation of discovered agents</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="EXACT MATCH">
      <data key="d0">METRIC</data>
      <data key="d1">A metric used to calculate the accuracy rate by comparing the reference solution and the predicted answer</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="INPUT GRID">
      <data key="d0">DATA</data>
      <data key="d1">A rectangular matrix of integers representing colors, used as input in the ARC challenge</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="OUTPUT GRID">
      <data key="d0">DATA</data>
      <data key="d1">A rectangular matrix of integers representing colors, used as output in the ARC challenge</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="EXPERIMENT DETAILS">
      <data key="d0">SECTION</data>
      <data key="d1">Details about the experiments conducted for the ARC challenge</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="DEMONSTRATION EXAMPLES">
      <data key="d0">DATA</data>
      <data key="d1">Examples provided in the ARC challenge to demonstrate the transformation rule</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="TEST EXAMPLE">
      <data key="d0">DATA</data>
      <data key="d1">An example in the ARC challenge where the output grid needs to be predicted</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="EXAMPLE 0">
      <data key="d0">DATA</data>
      <data key="d1">An example task from the ARC challenge involving specific input and output grids</data>
      <data key="d2">4b43decac6833d1515992f8869ecada7</data>
    </node>
    <node id="BEST AGENT">
      <data key="d0">AGENT</data>
      <data key="d1">The best agent on ARC discovered by Meta Agent Search</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">AGENT</data>
    </node>
    <node id="INITIAL INSTRUCTION">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">The initial instruction given to the FM_Module to generate candidate solutions</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">INSTRUCTION</data>
    </node>
    <node id="INITIAL SOLUTIONS">
      <data key="d0">SOLUTIONS</data>
      <data key="d1">Initial solutions are the candidate solutions generated by the FM_Module</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">SOLUTIONS</data>
    </node>
    <node id="THOUGHTS">
      <data key="d0">DATA</data>
      <data key="d1">Thoughts are the intermediate outputs from the FM_Module, including thinking and code</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">DATA</data>
    </node>
    <node id="CORRECT EXAMPLES">
      <data key="d0">DATA</data>
      <data key="d1">Correct examples are the examples where the generated code produced the correct output</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">DATA</data>
    </node>
    <node id="WRONG EXAMPLES">
      <data key="d0">DATA</data>
      <data key="d1">Wrong examples are the examples where the generated code produced the incorrect output</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">DATA</data>
    </node>
    <node id="NUM_CANDIDATES">
      <data key="d0">PARAMETER</data>
      <data key="d1">Num_candidates is the number of initial candidate solutions to be generated</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">PARAMETER</data>
    </node>
    <node id="TEMPERATURE">
      <data key="d0">PARAMETER</data>
      <data key="d1">Temperature is a parameter used in the FM_Module to control the randomness of the generated solutions</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
      <data key="d3">PARAMETER</data>
    </node>
    <node id="EXPERIMENT">
      <data key="d0">PROCESS</data>
      <data key="d1">The experiment conducted to discover the best agent on ARC using Meta Agent Search</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
    </node>
    <node id="REPOSITORY">
      <data key="d0">PLATFORM</data>
      <data key="d1">The repository on GitHub containing all agents from the experiment</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
    </node>
    <node id="ENSEMBLE METHODS">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">Ensemble methods are techniques used to combine multiple models to improve performance</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
    </node>
    <node id="INITIAL CANDIDATE SOLUTIONS">
      <data key="d0">SOLUTIONS</data>
      <data key="d1">Initial candidate solutions are the first set of solutions generated by the FM_Module</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
    </node>
    <node id="CORRECT_COUNT">
      <data key="d0">DATA</data>
      <data key="d1">Correct_count is the number of correct examples produced by the generated code</data>
      <data key="d2">449db721e37968e073e3579b59e023b2</data>
    </node>
    <node id="INITIAL_INSTRUCTION">
      <data key="d0">DATA</data>
      <data key="d1">Initial instruction is the starting guideline or command given to the system for processing</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="CORRECT_EXAMPLES">
      <data key="d0">DATA</data>
      <data key="d1">Correct examples are instances where the code has produced the correct output</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="WRONG_EXAMPLES">
      <data key="d0">DATA</data>
      <data key="d1">Wrong examples are instances where the code has produced incorrect output</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="INITIAL_SOLUTIONS">
      <data key="d0">DATA</data>
      <data key="d1">Initial solutions are the first set of code solutions generated and evaluated based on initial instructions and feedback</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="HUMAN_LIKE_FEEDBACK_MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">Human-like Feedback Module is a module designed to simulate human-like feedback for code evaluation</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="HUMAN_FEEDBACK_INSTRUCTION">
      <data key="d0">DATA</data>
      <data key="d1">Human feedback instruction is the guideline provided to the Human-like Feedback Module to generate feedback</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="HUMAN_FEEDBACK">
      <data key="d0">DATA</data>
      <data key="d1">Human feedback is the feedback generated by the Human-like Feedback Module simulating human evaluation</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="EXPERT_ROLES">
      <data key="d0">DATA</data>
      <data key="d1">Expert roles are specific roles assigned to expert advisors to provide targeted feedback on code</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="EXPERT_ADVISORS">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">Expert advisors are modules assigned specific roles to evaluate and provide targeted feedback on code</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="EXPERT_INSTRUCTION">
      <data key="d0">DATA</data>
      <data key="d1">Expert instruction is the guideline provided to expert advisors to evaluate code and provide feedback</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="EXPERT_FEEDBACK">
      <data key="d0">DATA</data>
      <data key="d1">Expert feedback is the targeted feedback provided by expert advisors based on their specific roles</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="REFINEMENT_MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">Refinement Module is a module used to iteratively refine code solutions based on structured feedback</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="MAX_REFINEMENT_ITERATIONS">
      <data key="d0">DATA</data>
      <data key="d1">Max refinement iterations is the maximum number of iterations allowed for refining code solutions</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="REFINEMENT_INSTRUCTION">
      <data key="d0">DATA</data>
      <data key="d1">Refinement instruction is the guideline provided to the Refinement Module to refine code solutions</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="REFINEMENT_THINKING">
      <data key="d0">PROCESS</data>
      <data key="d1">Refinement thinking refers to the thought process applied during the refinement of code solutions</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="REFINED_CODE">
      <data key="d0">DATA</data>
      <data key="d1">Refined code is the improved version of the code after undergoing refinement iterations</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="REFINED_SOLUTIONS">
      <data key="d0">DATA</data>
      <data key="d1">Refined solutions are the set of code solutions that have been improved through refinement iterations</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="SORTED_SOLUTIONS">
      <data key="d0">DATA</data>
      <data key="d1">Sorted solutions are the refined solutions sorted based on their performance</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="TOP_SOLUTIONS">
      <data key="d0">DATA</data>
      <data key="d1">Top solutions are the best-performing solutions selected from the sorted solutions</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="FINAL_DECISION_INSTRUCTION">
      <data key="d0">DATA</data>
      <data key="d1">Final decision instruction is the guideline provided to make a final decision on the best code solution</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="FINAL_DECISION_MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">Final Decision Module is a module used to make the final decision on the best code solution</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="FINAL_THOUGHTS">
      <data key="d0">PROCESS</data>
      <data key="d1">Final thoughts refer to the thought process applied during the final decision-making on the best code solution</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="FINAL_CODE">
      <data key="d0">DATA</data>
      <data key="d1">Final code is the final version of the code selected as the best solution</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="VALIDATION_SET">
      <data key="d0">DATA</data>
      <data key="d1">Validation set is a subset of data used to validate the performance of agents</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="TEST_SET">
      <data key="d0">DATA</data>
      <data key="d1">Test set is a subset of data used to test the performance of agents</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="ZERO-SHOT STYLE QUESTIONS">
      <data key="d0">DATA</data>
      <data key="d1">Zero-shot style questions are questions that the agents have not seen during training</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="ONE-SHOT STYLE QUESTIONS">
      <data key="d0">DATA</data>
      <data key="d1">One-shot style questions are questions that the agents have seen once during training</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="DISCOVERED AGENTS">
      <data key="d0">AGENT</data>
      <data key="d1">Discovered agents are agents evaluated using the "gpt-3.5-turbo-0125" model</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="META_AGENT">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="DISCOVERED_AGENTS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="AUTOMATED DESIGN OF AGENTIC SYSTEMS">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Automated Design of Agentic Systems is a document detailing the experiment and evaluation process for reasoning and problem-solving domains</data>
      <data key="d2">84317ae35cc75d612287186d93461447</data>
    </node>
    <node id="REASONING AND PROBLEM-SOLVING DOMAINS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="BAHRAIN">
      <data key="d0">LOCATION</data>
      <data key="d1">Bahrain is a country where non-nationals make up more than half of the population, with a significant number of immigrants from South and Southeast Asia</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="INDIANS">
      <data key="d0">NATIONALITY</data>
      <data key="d1">Indians are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="BANGLADESHIS">
      <data key="d0">NATIONALITY</data>
      <data key="d1">Bangladeshis are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="PAKISTANIS">
      <data key="d0">NATIONALITY</data>
      <data key="d1">Pakistanis are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="FILIPINOS">
      <data key="d0">NATIONALITY</data>
      <data key="d1">Filipinos are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="INDONESIANS">
      <data key="d0">NATIONALITY</data>
      <data key="d1">Indonesians are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="BIOLOGY">
      <data key="d0">SUBJECT</data>
      <data key="d1">Biology is one of the domains included in the GPQA benchmark</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="PHYSICS">
      <data key="d0">SUBJECT</data>
      <data key="d1">Physics is one of the domains included in the GPQA benchmark</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="CHEMISTRY">
      <data key="d0">SUBJECT</data>
      <data key="d1">Chemistry is one of the domains included in the GPQA benchmark</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="STEM">
      <data key="d0">SUBJECT</data>
      <data key="d1">STEM is one of the subject areas included in the MMLU benchmark</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="SOCIAL SCIENCES">
      <data key="d0">SUBJECT</data>
      <data key="d1">Social Sciences is one of the subject areas included in the MMLU benchmark</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="HUMANITIES">
      <data key="d0">SUBJECT</data>
      <data key="d1">Humanities is one of the subject areas included in the MMLU benchmark</data>
      <data key="d2">10fda605f670bcfccfc13c2ca0dde959</data>
    </node>
    <node id="SHI ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Shi et al. in 2023 that discusses the Verified Multimodal Agent</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="REIN ET AL., 2023">
      <data key="d0">PUBLICATION</data>
      <data key="d1">A publication by Rein et al. in 2023 that discusses the Multi-Step Peer Review Agent and Divide and Conquer Agent</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="PROBLEM-SOLVING">
      <data key="d0">DOMAIN</data>
      <data key="d1">Problem-Solving is one of the domains where experiments were conducted</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="EXPERIMENTS">
      <data key="d0">PROCESS</data>
      <data key="d1">Experiments were conducted on Reasoning and Problem-Solving domains</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="PHYSICS EXPERT">
      <data key="d0">ROLE</data>
      <data key="d1">Physics Expert is a role assigned in the LLM-Debate method
A specialized expert module focused on solving sub-problems related to physics</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="CHEMISTRY EXPERT">
      <data key="d0">ROLE</data>
      <data key="d1">Chemistry Expert is a role assigned in the LLM-Debate method
A specialized expert module focused on solving sub-problems related to chemistry</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="PHYSICS CRITIC">
      <data key="d0">ROLE</data>
      <data key="d1">Physics Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="CHEMISTRY CRITIC">
      <data key="d0">ROLE</data>
      <data key="d1">Chemistry Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="BIOLOGY CRITIC">
      <data key="d0">ROLE</data>
      <data key="d1">Biology Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="GENERAL CRITIC">
      <data key="d0">ROLE</data>
      <data key="d1">General Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="BIOLOGY EXPERT">
      <data key="d0">ROLE</data>
      <data key="d1">Biology Expert is a role assigned in the LLM-Debate method
A specialized expert module focused on solving sub-problems related to biology</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="SCIENCE GENERALIST">
      <data key="d0">ROLE</data>
      <data key="d1">Science Generalist is a role assigned in the LLM-Debate method</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="FINAL DECISION">
      <data key="d0">ROLE</data>
      <data key="d1">Final Decision is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b</data>
    </node>
    <node id="DECOMPOSITION MODULE">
      <data key="d0">ROLE</data>
      <data key="d1">Decomposition Module is a role in the Divide and Conquer Agent
A module named FM_Module used for decomposing a problem into sub-problems</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="SPECIALIZED EXPERT">
      <data key="d0">ROLE</data>
      <data key="d1">Specialized Expert is a role in the Divide and Conquer Agent
A module named FM_Module used for solving specific sub-problems, with roles such as Physics Expert, Chemistry Expert, Biology Expert, and General Expert</data>
      <data key="d2">97457e990eb6e3c88c11c862f9e3265b,ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="INTEGRATION MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">A module named FM_Module used for integrating solutions to sub-problems into a final answer</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="SUB_PROBLEMS">
      <data key="d0">DATA/OUTPUT</data>
      <data key="d1">The decomposed parts of the main problem generated by the Decomposition Module</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="SUB_SOLUTIONS">
      <data key="d0">DATA/OUTPUT</data>
      <data key="d1">The solutions to the sub-problems generated by the Specialized Experts</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="VISUAL REPRESENTATION MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">A module named FM_Module used for generating visual representations of problems</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="VERIFICATION MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">A module named FM_Module used for verifying the accuracy and relevance of visual representations</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="CHAIN-OF-THOUGHT MODULE">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">A module named FM_Module used for solving problems using verified visual aids</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="GPT-4O-MINI">
      <data key="d0">MODEL</data>
      <data key="d1">A newer version of OpenAI's language model that is less expensive and offers better performance than GPT-3.5-Turbo-0125</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="ADAS ALGORITHMS">
      <data key="d0">ALGORITHM</data>
      <data key="d1">Algorithms used in the Automated Design of Agentic Systems</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="GENERAL EXPERT">
      <data key="d0">TOOL/MODULE</data>
      <data key="d1">A specialized expert module focused on solving general sub-problems</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="DECOMPOSITION INSTRUCTION">
      <data key="d0">DATA/INPUT</data>
      <data key="d1">The instruction provided to the Decomposition Module to decompose the main task</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="SUB_PROBLEM INSTRUCTION">
      <data key="d0">DATA/INPUT</data>
      <data key="d1">The instruction provided to specialized experts to solve sub-problems step by step</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="INTEGRATION INSTRUCTION">
      <data key="d0">DATA/INPUT</data>
      <data key="d1">The instruction provided to the Integration Module to integrate sub-solutions into a final answer</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="VISUAL INSTRUCTION">
      <data key="d0">DATA/INPUT</data>
      <data key="d1">The instruction provided to the Visual Representation Module to create a visual representation of the problem</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="VERIFICATION INSTRUCTION">
      <data key="d0">DATA/INPUT</data>
      <data key="d1">The instruction provided to the Verification Module to verify the accuracy and relevance of the visual representation</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="COT INSTRUCTION">
      <data key="d0">DATA/INPUT</data>
      <data key="d1">The instruction provided to the Chain-of-Thought Module to solve the problem using the verified visual aid</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="OPENAI API">
      <data key="d0">SERVICE</data>
      <data key="d1">The API service used for querying models like GPT-3.5-Turbo-0125 and GPT-4o-Mini</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="EXPERIMENT COST">
      <data key="d0">DATA/OUTPUT</data>
      <data key="d1">The cost associated with running search and evaluation experiments using the OpenAI API</data>
      <data key="d2">ef75d2c866bee783577ed9f65707cf13</data>
    </node>
    <node id="AGENTINSTRUCT">
      <data key="d0">TOOL/FRAMEWORK</data>
      <data key="d1">AgentInstruct is an extensible agentic framework for automatically creating large amounts of diverse and high-quality synthetic data
AgentInstruct is an agentic solution for Generative Teaching that focuses on creating demonstration and feedback data using raw documents as input
AgentInstruct defines three different flows: Content Transformation Flow, Seed Instruction Generation Flow, and Instruction Refinement Flow to automate the generation process and ensure diversity and complexity in generated data.
A dataset used to create approximately 22 million instructions aimed at teaching various skills
AgentInstruct aims to synthesize a large and diverse corpus of data with varying degrees of difficulty to evaluate baseline models
AgentInstruct is a targeted training method used to improve reading comprehension capabilities in language models like Mistral.
AgentInstruct is a process used to enhance the proficiency of models like Mistral across various difficulties in math
AgentInstruct is an approach that achieved a reduction in hallucinations and maintained quality levels comparable to GPT-4
AgentInstruct is a technique that reduces human expertise required for data generation and enables the creation of high-quality synthetic data at scale
A method for generating large amounts of diverse and high-quality data for model post-training using agentic flowsAgentInstruct is a method for generating large amounts of diverse and high-quality data for model post-training using agentic flows</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a,6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635,dd9a46950237e49ef9b1c7ef08e08d42,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="GENERATIVE TEACHING">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">Generative Teaching refers to the process of using synthetic data created by powerful models to teach a new skill or behavior to another model
Generative Teaching is a methodology for generating abundant amounts of diverse, challenging, and high-quality data to teach a particular skill to an AI model
Generative Teaching is a concept aimed at teaching skills rather than generating data to meet specific benchmarks</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="ARINDAM MITRA">
      <data key="d0">PERSON</data>
      <data key="d1">Arindam Mitra is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"
An author of the Phi-3 technical reportArindam Mitra is an author of the Phi-3 technical report
Arindam Mitra is an author of the paper "Orca-math: Unlocking the potential of SLMs in grade school math"Arindam Mitra is an author of the paper "Orca: Progressive learning from complex explanation traces of GPT-4"Arindam Mitra is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="LUCIANO DEL CORRO">
      <data key="d0">PERSON</data>
      <data key="d1">Luciano Del Corro is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"
Luciano Del Corro is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="GUOQING ZHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Guoqing Zheng is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"
Guoqing Zheng is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="SHWETI MAHAJAN">
      <data key="d0">PERSON</data>
      <data key="d1">Shweti Mahajan is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"
Shweti Mahajan is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="DANY ROUHANA">
      <data key="d0">PERSON</data>
      <data key="d1">Dany Rouhana is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="ANDRES CODAS">
      <data key="d0">PERSON</data>
      <data key="d1">Andres Codas is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"
Andres Codas is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="YADONG LU">
      <data key="d0">PERSON</data>
      <data key="d1">Yadong Lu is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="WEI-GE CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Wei-ge Chen is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="OLGA VROUSGOS">
      <data key="d0">PERSON</data>
      <data key="d1">Olga Vrousgos is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="CORBY ROSSET">
      <data key="d0">PERSON</data>
      <data key="d1">Corby Rosset is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"
An author of the Phi-3 technical reportCorby Rosset is an author of the Phi-3 technical report
Corby Rosset is an author of the paper "Direct Nash optimization: Teaching language models to self-improve with general preferences"Corby Rosset is an author of the paper "Orca 2: Teaching small language models how to reason"Corby Rosset is an author of the paper "Orca-math: Unlocking the potential of SLMs in grade school math"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="FILLIPE SILVA">
      <data key="d0">PERSON</data>
      <data key="d1">Fillipe Silva is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="HAMED KHANPOUR">
      <data key="d0">PERSON</data>
      <data key="d1">Hamed Khanpour is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"
Hamed Khanpour is an author of the paper "Orca-math: Unlocking the potential of SLMs in grade school math"Hamed Khanpour is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="YASH LARA">
      <data key="d0">PERSON</data>
      <data key="d1">Yash Lara is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="AHMED AWADALLAH">
      <data key="d0">PERSON</data>
      <data key="d1">Ahmed Awadallah is one of the authors of the paper "AgentInstruct: Toward Generative Teaching with Agentic Flows"
An author of the Phi-3 technical reportAhmed Awadallah is an author of the Phi-3 technical report
Ahmed Awadallah is an author of the paper "Orca-math: Unlocking the potential of SLMs in grade school math"Ahmed Awadallah is an author of the paper "Orca: Progressive learning from complex explanation traces of GPT-4"Ahmed Awadallah is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="SYNTHETIC DATA">
      <data key="d0">CONCEPT/DATA</data>
      <data key="d1">Synthetic data refers to data that is artificially generated rather than obtained by direct measurement
Data generated artificially, used in model training to address concerns like lack of diversity and need for human curationSynthetic data is data generated artificially, used in model training to address concerns like lack of diversity and need for human curation</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="MISTRAL-7B">
      <data key="d0">MODEL</data>
      <data key="d1">Mistral-7b is a base language model that was post-trained using data generated by AgentInstruct
Mistral-7B is a base model that was fine-tuned using the synthetic dataset created by AgentInstruct</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="ORCA-3">
      <data key="d0">MODEL</data>
      <data key="d1">Orca-3 is the resulting model after post-training Mistral-7b with data generated by AgentInstruct
Orca-3 is the fine-tuned version of the Mistral-7B model, showing significant improvements over other instruction-tuned models
A model trained using a dataset of approximately 25.8 million paired instructions, finetuned on Mistral-7b-v0.1
Orca-3 is a model evaluated on the Orca-Bench dataset, showing notable enhancement in capabilities during post-training
Orca-3 is a language model evaluated on various benchmarks in a zero-shot setting.
Orca-3 is a 7B model that has shown significant improvements in various benchmarks, including reading comprehension, math, and format following
Orca-3 is a model that showed substantial improvement after being post-trained with a dataset generated by AgentInstructA model that showed substantial improvement after being post-trained with a dataset generated by AgentInstruct</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a,6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="AGIEVAL">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AGIEval is a benchmark used to evaluate the performance of language models
AGIEval is a benchmark used to evaluate the performance of AI models
AGIEval is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4
AGIEval is a human-centric benchmark that evaluates a model&#8217;s abilities in tasks pertinent to human cognition and problem-solving, including standardized exams like SAT and LSAT.
AGIEval is a benchmark used to evaluate models on various tasks, including reading comprehension and math</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="BBH">
      <data key="d0">BENCHMARK</data>
      <data key="d1">BBH is a benchmark used to evaluate the performance of language models
BBH is a benchmark used to evaluate the performance of AI models
BBH is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4
Big Bench Hard (BBH) is a set of 23 tasks from the broader Big-Bench benchmark requiring complex, multi-step reasoning across various academic subjects.</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,b88745a13b69cecbc0ee9c3af41389bf,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="ALPACAEVAL">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AlpacaEval is a benchmark used to evaluate the performance of language models
AlpacaEval is a benchmark used to evaluate the performance of AI models
AlpacaEval is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4
AlpacaEval is a benchmark designed for chat-based language models to assess their abilities in instruction-following tasks, consisting of 805 instructions.
Alpacaeval: An automatic evaluator of instruction-following models is a project published in 2023
AlpacaEval is a benchmark that measures win-rates by comparing the outputs of the evaluated model to a reference answer, using GPT-4-turbo for evaluation.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba,3d1f6634f93f8a4c296dc8df7e59859e,6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,b88745a13b69cecbc0ee9c3af41389bf,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="LLAMA-8B-INSTRUCT">
      <data key="d0">MODEL</data>
      <data key="d1">LLAMA-8B-instruct is a language model that was outperformed by Orca-3
LLAMA-8B-instruct is another AI model that Orca-3 outperforms on multiple benchmarks</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="GPT-3.5-TURBO">
      <data key="d0">MODEL</data>
      <data key="d1">GPT-3.5-turbo is a language model that was outperformed by Orca-3
GPT-3.5-turbo is a baseline model evaluated on various benchmarks
GPT-3.5-turbo is a version of OpenAI's language model, with scores for GSM8K taken from a specific reference.
GPT-3.5-turbo is a model used as a benchmark for evaluating the performance of other models like Orca-3-7B
GPT-3.5-turbo is a language model evaluated on various benchmarks for summarization and hallucination rates
GPT-3.5-turbo is a version of OpenAI's language model used in the evaluation of MIRAGE datasets</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="LLMS">
      <data key="d0">CONCEPT</data>
      <data key="d1">LLMs, or Large Language Models, are advanced language models that have been significantly developed using synthetic data
</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="SLMS">
      <data key="d0">CONCEPT</data>
      <data key="d1">SLMs, or Small Language Models, are smaller language models that also benefit from synthetic data in their training
</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="RLHF">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">RLHF, or Reinforcement Learning from Human Feedback, is a process used in the training of language models</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="AGENTIC WORKFLOWS">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">Agentic workflows involve using agents to generate high-quality data through reflection, iteration, and tool usage</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="MULTI-AGENT WORKFLOWS">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">Multi-agent workflows involve multiple agents working together to generate new prompts and responses, simulating scenarios and automating data generation</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="MISTRAL-7B-INSTRUCT">
      <data key="d0">MODEL</data>
      <data key="d1">Mistral-7b-Instruct is a version of the Mistral-7b model used for comparison with Orca-3
Mistral-7b-Instruct is a language model used as a baseline for comparison in various benchmarks.
Mistral-7B-Instruct is a model used as a baseline for comparison with Orca-3Mistral-7B-Instruct is a model used as a baseline for comparison with Orca-3-7B
Mistral-7B-Instruct is a language model evaluated on various benchmarks for summarization and hallucination rates</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,8ee9617c145e19fa95f1f9349bfbe69b,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="FOFO">
      <data key="d0">BENCHMARK</data>
      <data key="d1">FOFO is a benchmark used to evaluate the performance of language models
FOFO is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4
Format Following (FoFo) is a benchmark that evaluates a model&#8217;s ability to follow complex, domain-specific formats across various real-world domains like Healthcare, Finance, and Marketing.
FoFo is a benchmark used to evaluate models on their ability to follow formatting guidelines
FOFO is a benchmark evaluated using GPT-4 to score the format correctness of model responses, with scores ranging from 0 to 1.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba,6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="MIRAGE-RAG">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MIRAGE-RAG is a benchmark used to evaluate the performance of language models</data>
      <data key="d2">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </node>
    <node id="SYNTHETIC-DATA-GENERATION-AS-A-SERVICE">
      <data key="d0">SERVICE</data>
      <data key="d1">Synthetic-Data-Generation-As-A-Service is a proposed service for generating data for post-training and fine-tuning of AI models using raw materials</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
      <data key="d3">SERVICE</data>
    </node>
    <node id="CONTENT TRANSFORMATION AGENTS">
      <data key="d0">AGENTS</data>
      <data key="d1">Content Transformation Agents are used in the AgentInstruct methodology to transform raw seeds into diverse instructions</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
      <data key="d3">AGENTS</data>
    </node>
    <node id="REFINEMENT AGENTS">
      <data key="d0">AGENTS</data>
      <data key="d1">Refinement Agents are used in the AgentInstruct methodology to iteratively refine the complexity and quality of seed instructions</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
      <data key="d3">AGENTS</data>
    </node>
    <node id="RAW SEEDS">
      <data key="d0">DATA</data>
      <data key="d1">Raw seeds are unstructured text documents or source code used as input for AgentInstruct to generate diverse data</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
      <data key="d3">DATA</data>
    </node>
    <node id="CONTENT TRANSFORMATION FLOW">
      <data key="d0">PROCESS</data>
      <data key="d1">Content Transformation Flow is a process in the AgentInstruct methodology where raw seeds are transformed by Content Transformation Agents
Content Transformation Flow converts the raw seed into an intermediate representation that simplifies the creation of instructions tailored to specific objectives.
Content Transformation Flow is a process that transforms arbitrary articles into well-crafted pieces conducive to the formulation of a wide array of reading comprehension question types.
The Content Transformation Flow synthesizes a list of APIs from a random seed, which can be source code snippets or an API description</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,427e98b00e49b6a8f8649054122dd45b,b88745a13b69cecbc0ee9c3af41389bf,f7eb89a70f544664546a510e46d5febd</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="SEED INSTRUCTION CREATION FLOW">
      <data key="d0">PROCESS</data>
      <data key="d1">Seed Instruction Creation Flow is a process in the AgentInstruct methodology where a diverse set of instructions is created from transformed seeds
A process that consumes a list of APIs and employs various agents to create tasks of different types, including those requiring single or multiple APIs with varying parameter completeness.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,b88745a13b69cecbc0ee9c3af41389bf</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="REFINEMENT FLOW">
      <data key="d0">PROCESS</data>
      <data key="d1">Refinement Flow is a process in the AgentInstruct methodology where the complexity and quality of seed instructions are iteratively refined
A process aimed at increasing the complexity of tasks by suggesting refinements to increase the number of steps required to solve the task.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,b88745a13b69cecbc0ee9c3af41389bf</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="CREATIVE WRITING">
      <data key="d0">SKILL</data>
      <data key="d1">Creative writing is one of the skills covered by the synthetic post-training dataset created by AgentInstruct</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="DATA FILTERING">
      <data key="d0">PROCESS</data>
      <data key="d1">Data filtering is a process applied by AgentInstruct to ensure the quality of generated data</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="VERIFICATION">
      <data key="d0">PROCESS</data>
      <data key="d1">Verification is a process applied by AgentInstruct to ensure the quality of generated data</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="DEMONSTRATION DATA">
      <data key="d0">DATA</data>
      <data key="d1">Demonstration data is created by AgentInstruct to teach AI models specific skills</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="FEEDBACK DATA">
      <data key="d0">DATA</data>
      <data key="d1">Feedback data is created by AgentInstruct to teach AI models specific skills</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="POST-TRAINING">
      <data key="d0">PROCESS</data>
      <data key="d1">Post-training is a process where AI models are further trained using synthetic datasets created by AgentInstruct</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="CONTINUAL LEARNING">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">Continual learning is the ongoing process of training AI models to improve their performance over time</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="SELF-IMPROVEMENT">
      <data key="d0">CONCEPT/PROCESS</data>
      <data key="d1">Self-improvement is the process where AI models enhance their own capabilities using generated prompts and responses</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="WEB DATA">
      <data key="d0">DATA</data>
      <data key="d1">Web data is a type of raw material used as seeds for generating synthetic datasets</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="DOMAIN SPECIFIC DATA">
      <data key="d0">DATA</data>
      <data key="d1">Domain specific data is used as seeds to improve AI models in certain specializations</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="TEXTBOOK CHAPTERS">
      <data key="d0">DATA</data>
      <data key="d1">Textbook chapters are a type of raw seed used in the AgentInstruct methodology</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="WEB ARTICLES">
      <data key="d0">DATA</data>
      <data key="d1">Web articles are a type of raw seed used in the AgentInstruct methodology</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="CODE SNIPPETS">
      <data key="d0">DATA</data>
      <data key="d1">Code snippets are a type of raw seed used in the AgentInstruct methodology</data>
      <data key="d2">b88745a13b69cecbc0ee9c3af41389bf</data>
    </node>
    <node id="AGENTIC FLOWS">
      <data key="d0">PROCESS/TECHNIQUE</data>
      <data key="d1">Agentic flows are used to automate the generation process and leverage raw articles as seeds to foster diversity and ensure that problems generated in different iterations are distinct and of broad coverage.
A technique used by AgentInstruct for synthetic data generation, facilitating model training and customizationAgentic flows are a technique used by AgentInstruct for synthetic data generation, facilitating model training and customization</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SEED INSTRUCTION GENERATION FLOW">
      <data key="d0">PROCESS/TECHNIQUE</data>
      <data key="d1">Seed Instruction Generation Flow takes transformed content and generates a set of diverse instructions following a comprehensive taxonomy.
The Seed Instruction Generation Flow is a process that compiles a collection of reading comprehension question types and uses multiple agents to generate questions based on predefined types from a given text.
A flow that involves generating seed instructions using various text modification methods</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,5819b66e04fd77fa705574edc49395bb,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="INSTRUCTION REFINEMENT FLOW">
      <data key="d0">PROCESS/TECHNIQUE</data>
      <data key="d1">Instruction Refinement Flow iteratively enhances the complexity and quality of instructions generated by the Seed Instruction Flow.
The Instruction Refinement Flow is a process involving suggester-editor agents that modify passage-question pairs to create more complex or unanswerable questions, or to alter the answers.
The Instruction Refinement Flow involves a Suggester-Editor pair that increases the complexity of generated instructions</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,427e98b00e49b6a8f8649054122dd45b,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SUGGESTER AGENT">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Suggester agents propose various approaches to increase the intricacy of the initial instructions, making them more complex, unsolvable, or tricky.
The Suggester Agent is a tool that provides suggestions to modify passage-question pairs, such as introducing hypothetical studies or adding complexity to questions.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="EDITOR AGENT">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Editor agents modify the instructions in accordance with the suggestions made by the Suggester agents.
The Editor Agent is a tool that implements modifications suggested by the Suggester Agent to refine passage-question pairs.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="TEXT MODIFICATION">
      <data key="d0">SKILL</data>
      <data key="d1">Text modification involves changing existing text to improve its quality, modify its tone, or fit a specific context or audience.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="OPEN DOMAIN QUESTION ANSWERING">
      <data key="d0">SKILL</data>
      <data key="d1">Open domain question answering involves generating responses to questions over a wide range of topics, without being restricted to a specific domain.
Open Domain Question Answering is a process used to generate math problems for evaluating AI models</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="WEB AGENT">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">A web agent is a software program that autonomously performs tasks on the web, such as where to click and how much to scroll.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="BRAIN TEASER">
      <data key="d0">SKILL</data>
      <data key="d1">A brain teaser is a problem or puzzle that typically requires thought to solve, often for amusement but also used for training logical thinking and problem-solving skills.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="ANALYTICAL REASONING">
      <data key="d0">SKILL</data>
      <data key="d1">Analytical reasoning involves the ability to look at information, be it qualitative or quantitative, and discern patterns within the information to draw logical conclusions.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="MULTIPLE CHOICE QUESTIONS">
      <data key="d0">SKILL</data>
      <data key="d1">Multiple choice questions are a form of assessment where respondents select the best possible answer from a list of choices.
Models are evaluated in an open-ended generation setting with an empty system message, and GPT-4 is used for extraction of the option selected by the model from the model&#8217;s response</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="DATA TO TEXT">
      <data key="d0">SKILL</data>
      <data key="d1">Data-to-text refers to generating human-readable textual summaries from source data, used for reports, explanations, or narratives from structured data.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="FERMI PROBLEMS">
      <data key="d0">SKILL</data>
      <data key="d1">Fermi problems are estimation problems that seek quick, rough estimates of quantities that can be difficult to measure, often requiring justified guesses or assumptions.
Fermi problems are quick, rough estimates of quantities that are difficult to measure, named after physicist Enrico Fermi. They often require making justified guesses or assumptions to reach a solution.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="CODING">
      <data key="d0">SKILL</data>
      <data key="d1">Coding involves writing code following instructions, understanding code, debugging code, and writing test cases.
Coding involves writing code following instructions, understanding code, debugging code, and tracing or writing test cases.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="TEXT EXTRACTION">
      <data key="d0">SKILL</data>
      <data key="d1">Text extraction is the process of retrieving relevant information from a larger text document, including tasks like named entity recognition, keyword extraction, or extracting specific data fields from unstructured text.
Text extraction is the process of retrieving relevant information from a larger text document, including tasks like named entity recognition, keyword extraction, or extracting specific data fields from unstructured text.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="RAW ARTICLES">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Raw articles are used as seeds in agentic flows to foster diversity and ensure broad coverage of generated problems.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SEED INSTRUCTIONS">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Seed instructions are generated from transformed content and are iteratively refined to boost quality, diversity, and complexity.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SEARCH API">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">A tool that agents can use to perform search operations as part of their tasks.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="CODE INTERPRETER">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">A tool that agents can use to interpret and execute code as part of their tasks.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="CALCULATOR">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">A tool that agents can use to perform calculations as part of their tasks.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="READING COMPREHENSION TESTS">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Tests that present text passages of varying lengths and subjects, followed by questions that assess the reader&#8217;s understanding.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="TABLE 1">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">A table that provides a full list of the 17 different skills implemented in the agentic flows, each having multiple subcategories.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="CASE STUDIES">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Case studies explain how the workflows work for generating data for specific skills such as Reading Comprehension, Text Modification, and Tool Use.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SKILLS">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">The 17 different skills implemented in the agentic flows, including reading comprehension, question answering, coding, retrieval augmented generation, creative writing, tool/API use, and Web control.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="SUGGESTIONS">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Suggestions are proposed by Suggester agents to increase the intricacy of the initial instructions.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="EDITING">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Editing is the process performed by Editor agents to modify instructions based on the suggestions from Suggester agents.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="TASKS">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Tasks are the specific activities or problems that agents work on within the agentic flows.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="OBJECTIVES">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Objectives are the goals or targets that the instructions are tailored to achieve in the Content Transformation Flow.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="INTERMEDIATE REPRESENTATION">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">An intermediate representation is created from raw seeds to simplify the creation of instructions tailored to specific objectives.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="TAXONOMY">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">A pre-defined, but extensible, taxonomy is used in the Seed Instruction Generation Flow to introduce diversity in the generated instructions.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="QUALITY">
      <data key="d0">COMPONENT/ACTOR</data>
      <data key="d1">Quality is one of the attributes that the Instruction Refinement Flow aims to boost in the generated instructions.</data>
      <data key="d2">f7eb89a70f544664546a510e46d5febd</data>
    </node>
    <node id="TEXT CLASSIFICATION">
      <data key="d0">PROCESS</data>
      <data key="d1">Text classification is a machine learning task where text documents are automatically classified into predefined categories, used for spam detection, sentiment analysis, and topic labeling among others.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="RETRIEVAL AUGMENTED GENERATION">
      <data key="d0">METHOD</data>
      <data key="d1">Retrieval Augmented Generation (RAG) is a method in natural language processing that combines retrieval-based and generative models to generate responses by first retrieving relevant documents and then using these documents to generate a response.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="CREATIVE CONTENT GENERATION">
      <data key="d0">ACTIVITY</data>
      <data key="d1">Creative content generation involves the creation of original content, often involving elements of novelty, value, and surprise. In AI, this could refer to generating text, music, or images that are new, meaningful, and interesting.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="FEW SHOT REASONING">
      <data key="d0">CONCEPT</data>
      <data key="d1">Few-shot reasoning refers to the ability of a machine learning model to understand new concepts, patterns, or tasks with minimal examples or guidance, mimicking the human ability to learn quickly from few examples.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="CONVERSATION">
      <data key="d0">ACTIVITY</data>
      <data key="d1">Conversation refers to conversational agents or chatbots that interact with humans in a natural, human-like manner.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="AGENTINSTRUCT FLOW">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">AgentInstruct Flow is a process implemented for various capabilities, including reading comprehension, to facilitate learning and understanding through structured tasks and activities.
The AgentInstruct Flow is a process for text modification that involves editing and refining written content to enhance its quality and effectiveness or alter its attributes.
The AgentInstruct Flow involves enabling models to interact with external tools or services via APIs to extend their functionality</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d,427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="ARGUMENT PASSAGE GENERATOR">
      <data key="d0">TOOL</data>
      <data key="d1">Argument Passage Generator is a tool within the Content Transformation Flow that generates argument passages from seed articles to facilitate the creation of reading comprehension materials.
An agent adept at creating passages that articulate arguments, which may occasionally contain logical inconsistencies</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="URIC ACID">
      <data key="d0">SUBSTANCE</data>
      <data key="d1">Uric acid is a substance produced naturally by the breakdown of purine, a type of dietary protein. Excessive amounts can lead to health complications such as hyperuricemia, which may increase the risk of cardiovascular disease.
Uric acid is a chemical found in red meat and seafood, and its levels in the body can be influenced by lifestyle choices such as alcohol consumption and physical inactivity. High levels are associated with an increased risk of cardiovascular disease, while low levels can indicate underlying kidney or liver issues.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="HYPERURICEMIA">
      <data key="d0">CONDITION</data>
      <data key="d1">Hyperuricemia is a condition characterized by high levels of uric acid in the blood, typically defined as levels above 6 mg/dL in women and 7 mg/dL in men. It can result from increased production of uric acid or insufficient elimination through urine.
Hyperuricemia is a condition characterized by high levels of uric acid in the blood, which is associated with an increased risk of cardiovascular disease.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="HYPOURICEMIA">
      <data key="d0">CONDITION</data>
      <data key="d1">Hypouricemia is a condition characterized by low levels of uric acid, which is less common and usually does not present symptoms but can indicate underlying kidney or liver issues.
Hypouricemia is a condition characterized by low levels of uric acid in the blood, which is less common and usually asymptomatic but can indicate underlying kidney or liver issues.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="ENRICO FERMI">
      <data key="d0" />
      <data key="d1">Enrico Fermi was a physicist after whom Fermi problems are named.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LSAT LOGICAL REASONING TEST">
      <data key="d0">TEST</data>
      <data key="d1">The LSAT Logical Reasoning test features specialized question categories, including assumption, strengthening/weakening, flaw, and inference questions.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="CARDIOVASCULAR DISEASE">
      <data key="d0">CONDITION</data>
      <data key="d1">Cardiovascular disease is a health condition that may be associated with high levels of uric acid in the blood.
Cardiovascular disease is a class of diseases that involve the heart or blood vessels, and high levels of uric acid are associated with an increased risk of developing these diseases.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="KIDNEY DISEASES">
      <data key="d0">CONDITION</data>
      <data key="d1">Kidney diseases are health conditions that can be caused by the lack or excess of uric acid in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="LEUKEMIA">
      <data key="d0">CONDITION</data>
      <data key="d1">Leukemia is a disease that can cause an imbalance of uric acid in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="OBESITY">
      <data key="d0">CONDITION</data>
      <data key="d1">Obesity is a health condition that can cause an imbalance of uric acid in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="ANEMIA">
      <data key="d0">CONDITION</data>
      <data key="d1">Anemia is a health condition that can cause an imbalance of uric acid in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="ALCOHOL">
      <data key="d0">SUBSTANCE</data>
      <data key="d1">Alcohol consumption is a lifestyle factor that can contribute to high levels of uric acid in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="PROCESSED FOODS">
      <data key="d0">SUBSTANCE</data>
      <data key="d1">Processed foods are a lifestyle factor that can contribute to high levels of uric acid in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="RED MEAT">
      <data key="d0">SUBSTANCE</data>
      <data key="d1">Red meat is a dietary source of purines, which can contribute to high levels of uric acid in the body.
Red meat is a type of food that contains uric acid, which can influence its levels in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="SEAFOOD">
      <data key="d0">SUBSTANCE</data>
      <data key="d1">Seafood is a dietary source of purines, which can contribute to high levels of uric acid in the body.
Seafood is a type of food that contains uric acid, which can influence its levels in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="LABORATORY BLOOD TESTS">
      <data key="d0">TEST</data>
      <data key="d1">Laboratory blood tests are used to diagnose conditions related to uric acid levels in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="URINE TESTS">
      <data key="d0">TEST</data>
      <data key="d1">Urine tests are used to diagnose conditions related to uric acid levels in the body.</data>
      <data key="d2">0c212c1467564ad33330b1f655a8e27e</data>
    </node>
    <node id="LABORATORY TESTS">
      <data key="d0">PROCESS</data>
      <data key="d1">Laboratory tests are medical tests conducted on blood and urine samples to diagnose conditions such as hyperuricemia and hypouricemia.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="CONTENT TRANSFORMATION AGENT">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">The Content Transformation Agent is a tool that determines which subset of agents to engage in the Seed Instruction Generation Flow based on the content.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="PASSAGE-QUESTION PAIRS">
      <data key="d0">OUTPUT</data>
      <data key="d1">Passage-question pairs are the output of the Seed Instruction Generation Flow, consisting of a passage of text and a corresponding question generated by the agents.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="TEXT MODIFICATION TASKS">
      <data key="d0">TASK</data>
      <data key="d1">Text modification tasks are specific tasks such as paraphrasing, expansion, simplification, and redacting content, which are performed by agents in the AgentInstruct Flow.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="PARAPHRASING AGENT">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">The Paraphrasing Agent is a tool that takes a piece of text and creates several paraphrased versions of it as part of the text modification tasks.
The Paraphrasing Agent creates text modification tasks based on a given input text and instruction</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="ALCOHOL CONSUMPTION">
      <data key="d0">LIFESTYLE CHOICE</data>
      <data key="d1">Alcohol consumption is a lifestyle choice that can influence uric acid levels in the body.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="PHYSICAL INACTIVITY">
      <data key="d0">LIFESTYLE CHOICE</data>
      <data key="d1">Physical inactivity is a lifestyle choice that can influence uric acid levels in the body.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="KIDNEY ISSUES">
      <data key="d0">CONDITION</data>
      <data key="d1">Kidney issues can be indicated by low levels of uric acid in the blood, known as hypouricemia.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="LIVER ISSUES">
      <data key="d0">CONDITION</data>
      <data key="d1">Liver issues can be indicated by low levels of uric acid in the blood, known as hypouricemia.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="LITERAL COMPREHENSION QUESTIONS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Literal comprehension questions are a type of reading comprehension question that focuses on understanding the explicit content of a text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="CRITICAL COMPREHENSION QUESTIONS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Critical comprehension questions are a type of reading comprehension question that focuses on evaluating and analyzing the content of a text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="EVALUATIVE COMPREHENSION QUESTIONS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Evaluative comprehension questions are a type of reading comprehension question that focuses on making judgments about the content of a text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="REASONING QUESTIONS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Reasoning questions are a type of reading comprehension question that focuses on logical thinking and drawing conclusions from the content of a text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="IDENTIFYING ASSUMPTIONS QUESTIONS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Identifying assumptions questions are a type of reading comprehension question that focuses on recognizing underlying assumptions in the content of a text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="IDENTIFYING INFORMATION THAT STRENGTHENS/WEAKENS AN ARGUMENT QUESTIONS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">These questions focus on identifying information that either strengthens or weakens an argument presented in the text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="ORDERING EVENTS QUESTIONS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Ordering events questions are a type of reading comprehension question that focuses on arranging events in the correct sequence based on the content of a text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="APPENDIX A">
      <data key="d0">DOCUMENT SECTION</data>
      <data key="d1">Appendix A is a section in a document that lists various types of reading comprehension questions and text modification tasks.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="STRENGTHEN TYPE QUESTION">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">A strengthen type question is a reading comprehension question that asks which information most strengthens an argument presented in the text.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="HYPOTHETICAL STUDY">
      <data key="d0">STUDY</data>
      <data key="d1">A hypothetical study is a suggested study that could potentially strengthen an argument, requiring the test-taker to infer its impact on the relationship between uric acid levels and cardiovascular disease.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="GENETIC PREDISPOSITION">
      <data key="d0">CONDITION</data>
      <data key="d1">Genetic predisposition refers to the likelihood of developing a condition based on one's genetic makeup, such as hyperuricemia and its correlation with increased cardiovascular events.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="DISTRACTOR OPTION">
      <data key="d0">QUESTION ELEMENT</data>
      <data key="d1">A distractor option is a misleading answer choice in a multiple-choice question that seems correct but does not directly relate to the causal relationship being tested.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="PARAPHRASING">
      <data key="d0">TEXT MODIFICATION TASK</data>
      <data key="d1">Paraphrasing is a text modification task that involves rephrasing a piece of text while retaining its original meaning.
Rewriting text using different words and sentence structures while maintaining the original meaning</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="SIMPLIFICATION">
      <data key="d0">TEXT MODIFICATION TASK</data>
      <data key="d1">Simplification is a text modification task that involves making a piece of text easier to understand by reducing its complexity.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="REDACTING">
      <data key="d0">TEXT MODIFICATION TASK</data>
      <data key="d1">Redacting is a text modification task that involves removing or obscuring parts of a text for confidentiality or clarity.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="STYLING">
      <data key="d0">TEXT MODIFICATION TASK</data>
      <data key="d1">Styling is a text modification task that involves changing the appearance or format of a text to improve its presentation.</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d</data>
    </node>
    <node id="CODE SWITCHING">
      <data key="d0">TEXT MODIFICATION TASK</data>
      <data key="d1">Code switching is a text modification task that involves alternating between different languages or dialects within a text.
Alternating between languages or dialects within a text, often to reflect bilingual speakers&#8217; patterns or for creative writing</data>
      <data key="d2">1d8835c0ce90e56be22873bcf2740a5d,5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="NATASCHA VAN DER ZWAN">
      <data key="d0">PERSON</data>
      <data key="d1">Natascha van der Zwan is a researcher who identifies three distinct research streams that have approached financialization</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="FINANCIALIZATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Financialization is a broad concept that encompasses the increasing social impact and interconnection of financial discourses, markets, actors, and institutions</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="AMERICAN ANTHROPOLOGICAL ASSOCIATION (AAA)">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">The American Anthropological Association is an organization that hosts the SEA 2017 Annual Meeting and provides a platform for submitting abstracts</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="SEA 2017 ANNUAL MEETING">
      <data key="d0">EVENT</data>
      <data key="d1">The SEA 2017 Annual Meeting is an event held from April 6-8, 2017 at the University of Iowa, Iowa City, USA, with an abstract submission deadline of December 1, 2016</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="SUGGESTER-EDITOR PAIR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">The Suggester-Editor pair is a duo that increases the complexity of generated instructions by providing suggestions and edits</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="API RETRIEVAL AGENT">
      <data key="d0">TOOL/AGENT</data>
      <data key="d1">The API Retrieval Agent iteratively searches for similar code to expand the API list during the Content Transformation Flow</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="VIEW ALL FOOD ITEMS">
      <data key="d0">API</data>
      <data key="d1">The "View All Food Items" API allows clients to obtain a detailed list of food items, complete with nutritional profiles</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="SEARCH FOOD ITEMS">
      <data key="d0">API</data>
      <data key="d1">The "Search Food Items" API allows clients to search for food items by name and retrieve a list of matching items
Allows clients to search for food items by name and retrieve a list of matching items. It requires a query parameter and optionally a limit parameter to restrict the number of results.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="UNIVERSITY OF IOWA">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">The University of Iowa is the location where the SEA 2017 Annual Meeting was held</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="APRIL 6-8, 2017">
      <data key="d0">DATE</data>
      <data key="d1">The dates when the SEA 2017 Annual Meeting took place</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="DECEMBER 1, 2016">
      <data key="d0">DATE</data>
      <data key="d1">The deadline for abstract submissions for the SEA 2017 Annual Meeting</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="FINANCE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Finance is a field that deals with the study of investments, financial systems, and the management of money</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="RANDOM SEED">
      <data key="d0">CONCEPT</data>
      <data key="d1">A random seed is used to create a seed instruction for generating text modification tasks</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="SUGGESTION 1">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">Incorporate a fictional narrative. Use a conversational style with colloquial language and include a humorous element</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="SUGGESTION 2">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">Translate the event details into a poetic format. Maintain accurate information while using rhyming couplets and ensure the tone remains light and engaging</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="SUGGESTION 3">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">Frame the event details as a social media post. Use internet slang and emojis. Keep the message within 280 characters</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="MODIFIED INSTRUCTION 1">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">Rewrite the event details (date, location, abstract deadline) as if you&#8217;re telling a funny story to a friend using casual and colloquial language, while incorporating a fictional narrative that still conveys the necessary information</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="MODIFIED INSTRUCTION 2">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">Transform the event details (date, location, abstract deadline) into a light-hearted poem with rhyming couplets, ensuring that the essential information is accurately conveyed in a poetic format</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="MODIFIED INSTRUCTION 3">
      <data key="d0">INSTRUCTION</data>
      <data key="d1">Craft a social media post that includes the event details (date, location, abstract deadline) using internet slang, emojis, and a casual tone, while keeping the message concise and within 280 characters</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="LIBRARY RECONSTRUCTION">
      <data key="d0">PROCESS</data>
      <data key="d1">Library Reconstruction is a scenario in the Content Transformation Flow where a list of APIs is synthesized from a random seed</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="FOOD ITEMS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Food items are products that can be consumed and are often listed with nutritional profiles such as calorie count, protein, and fat</data>
      <data key="d2">427e98b00e49b6a8f8649054122dd45b</data>
    </node>
    <node id="GET FOOD ITEM DETAILS">
      <data key="d0">API</data>
      <data key="d1">Provides detailed information about a specific food item. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="CREATE MEAL PLAN">
      <data key="d0">API</data>
      <data key="d1">Enables the creation of a meal plan based on specified dietary preferences, caloric goals, and the number of meals per day. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="UPDATE FOOD ITEM">
      <data key="d0">API</data>
      <data key="d1">Allows updating the details of an existing food item. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="TRACK USER MEAL">
      <data key="d0">API</data>
      <data key="d1">Enables tracking of user meals. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="GET DIETARY RECOMMENDATIONS">
      <data key="d0">API</data>
      <data key="d1">Provides dietary recommendations based on user preferences and nutritional needs. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="ADD NEW FOOD ITEM">
      <data key="d0">API</data>
      <data key="d1">Allows adding a new food item to the database. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="DELETE FOOD ITEM">
      <data key="d0">API</data>
      <data key="d1">Enables the deletion of a food item from the database. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="GET USER NUTRITIONAL STATS">
      <data key="d0">API</data>
      <data key="d1">Provides nutritional statistics for a user. The parameters required for this API are not specified in the text.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="AGENT-INSTRUCT FLOW">
      <data key="d0">PROCESS</data>
      <data key="d1">A process that creates multi-turn conversations and instructions for an AI assistant to follow, including making API calls and concluding processes based on the availability of required parameters and APIs.</data>
      <data key="d2">0922646b93a124514ce2a267d961d229</data>
    </node>
    <node id="USER">
      <data key="d0">PERSON</data>
      <data key="d1">The individual requesting the creation of a meal plan, tracking of daily meals, new food recommendations, a nutritional summary, and updates to the food database.
The individual requesting the creation of a vegetarian meal plan and providing feedback on the meal plan
A participant in the multi-turn interaction in Orca-Bench</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="ASSISTANT">
      <data key="d0">AI</data>
      <data key="d1">The AI assistant responsible for creating a meal plan, tracking meals, providing food recommendations, generating a nutritional summary, and updating the food database based on the user's requests.
The entity responsible for creating the meal plan and providing an overview of it
A participant in the multi-turn interaction in Orca-Bench, providing responses</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="QUINOA SALAD">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A recipe that the user wants to add to the database.
A food item that the assistant is asked to add to the database, requiring nutritional information</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="CHANA MASALA">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A food item whose calorie count the user believes is incorrect and wants to update.
A food item that the assistant is asked to update in the database, requiring its unique identifier</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="BUTTER CHICKEN">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A food item that the user wants to remove from the database.
A food item that the assistant is asked to remove from the database, requiring its unique identifier</data>
      <data key="d2">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="API_CALL">
      <data key="d0">ACTION</data>
      <data key="d1">An action to create a meal plan with specific dietary preferences, caloric goal, and number of meals</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="KNOWLEDGEPILE">
      <data key="d0">DATA SOURCE</data>
      <data key="d1">A source of unstructured text and code files used in the creation of the AgentInstruct dataset</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="AUTOMATHTEXT">
      <data key="d0">DATA SOURCE</data>
      <data key="d1">A source of unstructured text and code files used in the creation of the AgentInstruct dataset</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="OPENSTAX">
      <data key="d0">DATA SOURCE</data>
      <data key="d1">A source of unstructured text and code files used in the creation of the AgentInstruct dataset</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="APACHE-2.0 LICENSED SOURCE CODE">
      <data key="d0">DATA SOURCE</data>
      <data key="d1">A source of unstructured text and code files used in the creation of the AgentInstruct dataset</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="ORCA-2.5-DATASET">
      <data key="d0">DATASET</data>
      <data key="d1">A dataset consisting of approximately 3.8 million paired instructions sourced from various Orca versions and other publicly available sources</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="MISTRAL-7B-V0.1">
      <data key="d0">MODEL</data>
      <data key="d1">The base model used for finetuning with the AgentInstruct dataset to create Orca-3</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="NVIDIA A100">
      <data key="d0">HARDWARE</data>
      <data key="d1">The hardware used for training the Orca-3 model, consisting of 152 GPUs</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="ADAMW OPTIMIZER">
      <data key="d0">OPTIMIZER</data>
      <data key="d1">The optimizer used for training the Orca-3 model with an initial learning rate of 8e-6</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="ORCA-BENCH">
      <data key="d0">DATASET</data>
      <data key="d1">A held-out test set consisting of 100 samples from each of the 17 skills curated using AgentInstruct, used for evaluating the performance of models
Orca-Bench is a dataset used to evaluate the performance of various baseline models, scored relative to GPT-4 on a scale from 0 to 10</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="VEGETARIAN MEAL PLAN">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="ORCA-1">
      <data key="d0" />
      <data key="d1">A previous version of the Orca model, which contributed to the Orca-2.5-dataset</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="ORCA-2">
      <data key="d0" />
      <data key="d1">A previous version of the Orca model, which contributed to the Orca-2.5-dataset</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="ORCA-MATH">
      <data key="d0" />
      <data key="d1">A version of the Orca model focused on mathematical instructions, which contributed to the Orca-2.5-dataset
Orca-math: Unlocking the potential of SLMs in grade school math is a paper published in 2024</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a,3d1f6634f93f8a4c296dc8df7e59859e</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="DAY 2">
      <data key="d0">DAY</data>
      <data key="d1">The second day of the vegetarian meal plan, including breakfast, lunch, and dinner with specific food items and total calories</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="OATMEAL WITH FRUITS">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A breakfast food item included in the vegetarian meal plan for Day 1</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="ALMOND MILK">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A breakfast food item included in the vegetarian meal plan for Day 1</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="CHICKPEA SALAD">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A lunch food item included in the vegetarian meal plan for Day 1</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="WHOLE WHEAT BREAD">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A lunch food item included in the vegetarian meal plan for Day 1</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="MIXED VEGETABLE STIR FRY">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A dinner food item included in the vegetarian meal plan for Day 1</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="BROWN RICE">
      <data key="d0">FOOD ITEM</data>
      <data key="d1">A dinner food item included in the vegetarian meal plan for Day 1</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="OPEN DOMAIN QUESTION ANSWERING (ODQA)">
      <data key="d0">SKILL</data>
      <data key="d1">A skill category in the Orca-Bench dataset, consisting of 100 questions from the initial seed instruction phase</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="COMPLEX ODQA">
      <data key="d0">SKILL</data>
      <data key="d1">A skill category in the Orca-Bench dataset, consisting of more intricate questions developed during the refinement phase
Complex ODQA is a subset of questions developed during the refinement phase, including more intricate questions</data>
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="DAY 1">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">09cb89de3b77d765983cff25b7d74a1a</data>
    </node>
    <node id="ORCA-2.5">
      <data key="d0">MODEL</data>
      <data key="d1">Orca-2.5 is a baseline model evaluated on the Orca-Bench dataset
Orca-2.5 is a previous version of the Orca language model used for comparison in reading comprehension evaluations.
Orca-2.5 is a 7B model used as a baseline for comparison with Orca-3</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="MISTRAL-INSTRUCT-7B">
      <data key="d0">MODEL</data>
      <data key="d1">Mistral-Instruct-7B is a baseline model evaluated on the Orca-Bench dataset</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="LLAMA3-8B-INSTRUCT">
      <data key="d0">MODEL</data>
      <data key="d1">LLAMA3-8B-Instruct is a baseline model evaluated on various benchmarks
LLAMA3-8B-Instruct is a language model evaluated on various benchmarks for summarization and hallucination rates</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="IFEVAL">
      <data key="d0">BENCHMARK</data>
      <data key="d1">IFEval is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, GPT-3.5-turbo, and GPT-4
Instruction-Following Evaluation (IFEval) is a benchmark measuring a model&#8217;s ability to follow natural language instructions using a set of 500 prompts covering 25 types of verifiable instructions.
IFEval is a benchmark that checks if the model response follows verifiable instructions given in the prompt, using code provided by the authors.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba,86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="INFOBENCH">
      <data key="d0">BENCHMARK</data>
      <data key="d1">InfoBench is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, GPT-3.5-turbo, and GPT-4
InFoBench is a benchmark that evaluates models' instruction-following capability using the Decomposed Requirements Following Ratio (DRFR) metric.
InfoBench: Evaluating instruction following ability in large language models is a paper published in 2024
InfoBench is a benchmark evaluated using GPT-4 to determine if the model response follows decomposed instructions.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba,3d1f6634f93f8a4c296dc8df7e59859e,86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="EQBENCH">
      <data key="d0">BENCHMARK</data>
      <data key="d1">EQBench is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4
A benchmark used to generate emotion scores from conversations, evaluated using GPT-4 to extract and calibrate the scores</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="SYSTEM MESSAGE">
      <data key="d0">MESSAGE</data>
      <data key="d1">A part of the multi-turn interaction in Orca-Bench, crafted by GPT-4
A predefined message used to guide the GPT-4 model in extracting student responses</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="TEACHER">
      <data key="d0">ROLE</data>
      <data key="d1">GPT-4 acting as the teacher in the multi-turn interaction in Orca-Bench</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="STUDENT">
      <data key="d0">ROLE</data>
      <data key="d1">The model being evaluated in the multi-turn interaction in Orca-Bench</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="ORCA-3 CHECKPOINT EPOCH 1">
      <data key="d0">MODEL</data>
      <data key="d1">A specific checkpoint of the Orca-3 model evaluated on the Orca-Bench dataset</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="ORCA-3 CHECKPOINT EPOCH 2">
      <data key="d0">MODEL</data>
      <data key="d1">A specific checkpoint of the Orca-3 model evaluated on the Orca-Bench dataset</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="TABLE 2">
      <data key="d0">DATA REPRESENTATION</data>
      <data key="d1">A table encapsulating the average (macro) scores across all assessed dimensions for different models</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="FIGURE 4">
      <data key="d0">DATA REPRESENTATION</data>
      <data key="d1">A figure illustrating the performance comparison between baseline models and Orca-3 checkpoints</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="BENCHMARK RESULTS">
      <data key="d0">RESULTS</data>
      <data key="d1">The section evaluating Orca-3 against 5 baseline models on various benchmarks</data>
      <data key="d2">bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="METRIC-V2">
      <data key="d0">BENCHMARK</data>
      <data key="d1">Metric-v2 is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4
</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="METRIC-V1">
      <data key="d0">BENCHMARK</data>
      <data key="d1">Metric-v1 is a benchmark used to evaluate the performance of models including Orca-3, Orca-2.5, Mistral-7B-Instruct, LLAMA3-8B-Instruct, GPT-3.5-turbo, and GPT-4
Metric-v1 is a performance metric used to evaluate models, showing a 28% improvement in one of the benchmarks.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </node>
    <node id="LSAT">
      <data key="d0">EXAM</data>
      <data key="d1">The Law School Admission Test (LSAT) is a standardized test used for law school admissions, known for its difficulty in reading comprehension sections.
The Law School Admission Test (LSAT) is a standardized test used for law school admissions, known for its difficulty</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="SAT">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="BIG-BENCH">
      <data key="d0" />
      <data key="d1">Big-Bench is a broader benchmark from which the Big Bench Hard (BBH) tasks are selected.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
      <data key="d3">TOOL/BENCHMARK</data>
    </node>
    <node id="DOMAIN EXPERTS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="GRADE SCHOOL MATH">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="DOMAIN-SPECIFIC FORMATS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="INSTRUCTION-FOLLOWING">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="ALPACA WEB DEMO">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="DRFR">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="MISTRAL">
      <data key="d0" />
      <data key="d1">Mistral is a language model that showed substantial improvement in reading comprehension capabilities through targeted training with AgentInstruct.
Mistral is a model family used as a base for fine-tuning with AgentInstruct data</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="ALLENAI">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">AllenAI is the organization that developed the AI2 Reasoning Challenge (ARC) benchmark.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="AI-HUMAN COLLABORATION">
      <data key="d0">PROCESS</data>
      <data key="d1">AI-Human collaboration is the process used to create the diverse range of real-world formats and instructions for the FoFo benchmark.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="ZERO-SHOT SETTING">
      <data key="d0">PROCESS</data>
      <data key="d1">Zero-shot setting is the evaluation method used for Orca-3 and other baseline models unless mentioned otherwise.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="LAW SCHOOL ADMISSION TESTS (LSATS)">
      <data key="d0">EXAM</data>
      <data key="d1">The Law School Admission Tests (LSATs) are standardized tests used for law school admissions, known for their difficulty in reading comprehension sections.</data>
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="BENCHMARKS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </node>
    <node id="GEMINI PRO">
      <data key="d0">MODEL</data>
      <data key="d1">Gemini Pro is a model used as a benchmark for evaluating the format-following capabilities of other models like Orca-3-7BGemini Pro is a model used as a benchmark for evaluating the format-following capabilities of other models like Orca-3
Gemini Pro is a model whose scores are referenced from the original paper</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b,bb87f82e6a9f1d4da6480ec78a0e3701</data>
      <data key="d3">MODEL</data>
    </node>
    <node id="MULTIPLE-CHOICE QUESTIONS FLOWS">
      <data key="d0">PROCESS</data>
      <data key="d1">Multiple-Choice Questions Flows is a process used to generate math problems for evaluating AI models</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="PHI3">
      <data key="d0">PUBLICATION</data>
      <data key="d1">Phi3 is a paper that reported the accuracy scores for GPT-3.5-turbo on the GSM8K benchmark</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
      <data key="d3">PUBLICATION</data>
    </node>
    <node id="ORCA-3-7B">
      <data key="d0">MODEL</data>
      <data key="d1">Orca-3-7B is a model that has shown significant improvements in various benchmarks, including reading comprehension, math, and format following
Orca-3-7B is a language model evaluated on various benchmarks for summarization and hallucination rates
Orca-3-7B is a model fine-tuned with AgentInstruct data based on the Mistral model family, used in the evaluation of MIRAGE datasets</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="ORCA-2.5-7B">
      <data key="d0">MODEL</data>
      <data key="d1">Orca-2.5-7B is a model used as a baseline for comparison with Orca-3-7B
Orca-2.5-7B is a language model evaluated on various benchmarks for summarization and hallucination rates
Orca-2.5-7B is a model used in the evaluation of MIRAGE datasets</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="AGIEVAL LSAT-RC">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AGIEval LSAT-RC is a benchmark used to evaluate models on the reading comprehension sections of the LSAT</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="AGIEVAL SAT-EN">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AGIEval SAT-EN is a benchmark used to evaluate models on the English sections of the SAT</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="AGIEVAL GAOKAO-ENGLISH">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AGIEval Gaokao-English is a benchmark used to evaluate models on the English sections of the Gaokao</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="AGIEVAL LSAT-LR">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AGIEval LSAT-LR is a benchmark used to evaluate models on the logical reasoning sections of the LSAT</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="AGIEVAL MATH">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AGIEval Math is a benchmark used to evaluate models on math problem-solving tasks</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="AGIEVAL SAT-MATH">
      <data key="d0">BENCHMARK</data>
      <data key="d1">AGIEval SAT-Math is a benchmark used to evaluate models on the math sections of the SAT</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="BBH MULTISTEP-ARITHMETIC-TWO">
      <data key="d0">BENCHMARK</data>
      <data key="d1">BBH Multistep-Arithmetic-Two is a benchmark used to evaluate models on multi-step arithmetic problems</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="MMLU ABSTRACT ALGEBRA">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MMLU Abstract Algebra is a benchmark used to evaluate models on abstract algebra tasks</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="MMLU COLLEGE MATHEMATICS">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MMLU College Mathematics is a benchmark used to evaluate models on college-level mathematics tasks</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="MMLU HIGH-SCHOOL MATHEMATICS">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MMLU High-School Mathematics is a benchmark used to evaluate models on high school-level mathematics tasks</data>
      <data key="d2">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </node>
    <node id="FOFO BENCHMARK">
      <data key="d0">BENCHMARK</data>
      <data key="d1">FoFo benchmark is used to evaluate the performance of language models, including Orca-3-7B</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="ACI-BENCH">
      <data key="d0">BENCHMARK</data>
      <data key="d1">ACI-Bench is a dataset designed for benchmarking automatic report generation from doctor-patient conversations</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="INSTRUSUM">
      <data key="d0">BENCHMARK</data>
      <data key="d1">InstruSum is a dataset for evaluating the generation capabilities of language models for instruction-controllable summarization</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="ORCA-SUM">
      <data key="d0">BENCHMARK</data>
      <data key="d1">Orca-Sum is a benchmark created to evaluate language models' ability to follow summarization and grounded data transformation instructions</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="MIRAGE">
      <data key="d0">BENCHMARK</data>
      <data key="d1">MIRAGE is a benchmark focusing on answering medical questions by referring to information retrieved from a medical corpus
MIRAGE is a collection of datasets used for evaluating the performance of various models on different tasks</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="MMLU-MED">
      <data key="d0">DATASET</data>
      <data key="d1">MMLU-Med is a dataset used in the MIRAGE benchmark for evaluating medical question answering</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="MEDQA-US">
      <data key="d0">DATASET</data>
      <data key="d1">MedQA-US is a dataset used in the MIRAGE benchmark for evaluating medical question answering</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="MEDMCQA">
      <data key="d0">DATASET</data>
      <data key="d1">MedMCQA is a dataset used in the MIRAGE benchmark for evaluating medical question answering</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="PUBMEDQA">
      <data key="d0">DATASET</data>
      <data key="d1">PubMedQA is a dataset used in the MIRAGE benchmark for evaluating medical question answering
PubMedQA is one of the datasets included in the MIRAGE collection and is considered an effective testbed for assessing models' ability to do RAG</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="BIOASQ">
      <data key="d0">DATASET</data>
      <data key="d1">BioASQ is a dataset used in the MIRAGE benchmark for evaluating medical question answering
BioASQ is one of the datasets included in the MIRAGE collection</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="HUGGING FACE">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Hugging Face is an organization that hosts various datasets used for constructing the Orca-Sum benchmark</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="CO-T">
      <data key="d0">SKILL</data>
      <data key="d1">CoT (Chain of Thought) is a skill used by GPT-4 for medical question answering in the MIRAGE benchmark</data>
      <data key="d2">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </node>
    <node id="MEDMEDQA">
      <data key="d0">DATASET</data>
      <data key="d1">MedMedQA is one of the datasets included in the MIRAGE collection</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="USMEDMCQA">
      <data key="d0">DATASET</data>
      <data key="d1">USMedMCQA is one of the datasets included in the MIRAGE collection</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="MISTRAL-7B-INSTRUCT-V0.1">
      <data key="d0">MODEL</data>
      <data key="d1">Mistral-7B-Instruct-v0.1 is a model used in the evaluation of MIRAGE datasets</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="MEDRAG">
      <data key="d0">TOOL</data>
      <data key="d1">MedRAG is the retrieval mechanism used across all models on MIRAGE, involving the same retrieval function and number of retrieved documents</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
      <data key="d3">TOOL</data>
    </node>
    <node id="AZURE">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Azure is recommended for reviewing transparency notes related to large language models
Azure is a cloud computing service provided by Microsoft, mentioned in the context of transparency notes</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">ORGANIZATION</data>
    </node>
    <node id="TABLE 8">
      <data key="d0">DOCUMENT SECTION</data>
      <data key="d1">Table 8 shows the evaluation results of RAG skill on MIRAGE datasets with and without leveraging RAG</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="AGENTINSTRUCT RAG FLOW">
      <data key="d0">TECHNIQUE</data>
      <data key="d1">AgentInstruct RAG flow is a technique used to train Orca-3, resulting in substantial performance improvement</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="LIMITATIONS">
      <data key="d0">DOCUMENT SECTION</data>
      <data key="d1">The section discussing the limitations of AgentInstruct and synthetic data generation</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="DATA BIASES">
      <data key="d0">ISSUE</data>
      <data key="d1">Data Biases refer to the biases present in the source data that can be carried over to large language models</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="LACK OF TRANSPARENCY">
      <data key="d0">ISSUE</data>
      <data key="d1">Lack of Transparency refers to the difficulty in understanding the rationale behind specific outputs or decisions of large language models</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="CONTENT HARMS">
      <data key="d0">ISSUE</data>
      <data key="d1">Content Harms refer to various types of harmful content that large language models can generate
Content harms refer to various types of harmful content that large language models can generate, necessitating awareness and preventive actions</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="VALIDATION">
      <data key="d0">ISSUE</data>
      <data key="d1">Validation refers to the difficulty in ensuring synthetic data accurately represents the desired scenarios</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="DEPENDENCY ON SEED DATA">
      <data key="d0">ISSUE</data>
      <data key="d1">Dependency on Seed Data refers to the quality of synthetic data being dependent on the quality of the real data used as seeds</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="EXTENSIBILITY">
      <data key="d0">ISSUE</data>
      <data key="d1">Extensibility refers to the human effort required to create agentic flows for different skills</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="BIAS">
      <data key="d0">ISSUE</data>
      <data key="d1">Bias refers to the potential for synthetic data to reflect and amplify biases present in the original seed data</data>
      <data key="d2">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </node>
    <node id="TRANSPARENCY NOTES">
      <data key="d0">DOCUMENTATION</data>
      <data key="d1">Transparency notes from Azure provide information about the rationale behind specific outputs or decisions</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="CONTENT MODERATION SERVICES">
      <data key="d0">SERVICE</data>
      <data key="d1">Services provided by different companies and institutions to moderate and prevent harmful content generated by language models</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="GOVERNMENT AND TECHNOLOGY LEADERS">
      <data key="d0">GROUP</data>
      <data key="d1">Entities hoped to provide better regulations and standards around content harms for AI technologies in the future</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="RESEARCH AND OPEN SOURCE COMMUNITY">
      <data key="d0">GROUP</data>
      <data key="d1">Communities valued for their role in addressing content harms and improving AI technologies</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="HALLUCINATION">
      <data key="d0">CONCEPT/ISSUE</data>
      <data key="d1">The phenomenon where language models fabricate content, making it unreliable for critical decisions or information</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="UNSTRUCTURED DATA SOURCES">
      <data key="d0">DATA TYPE</data>
      <data key="d1">Sources of data that are not organized in a pre-defined manner, used by AgentInstruct for generating synthetic data</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="25M PAIR DATASET">
      <data key="d0">DATASET</data>
      <data key="d1">A dataset comprising 25 million pairs of prompts and responses generated by AgentInstruct for post-training the Orca-3 model</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">DATASET</data>
    </node>
    <node id="MODEL TRAINING">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of training machine learning models, which can benefit from synthetic data generated by AgentInstructModel training is the process of training machine learning models, which can benefit from synthetic data generated by AgentInstruct</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="DOMAIN/TASK SPECIALIZATION">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of customizing models for specific domains or tasks using synthetic dataDomain/task specialization is the process of customizing models for specific domains or tasks using synthetic data</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PROCESS</data>
    </node>
    <node id="MARAH ABDIN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMarah Abdin is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAM ADE JACOBS">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportSam Ade Jacobs is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AMMAR AHMAD AWAN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAmmar Ahmad Awan is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JYOTI ANEJA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJyoti Aneja is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HANY AWADALLA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportHany Awadalla is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NGUYEN BACH">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportNguyen Bach is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AMIT BAHREE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAmit Bahree is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ARASH BAKHTIARI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportArash Bakhtiari is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIANMIN BAO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJianmin Bao is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HARKIRAT BEHL">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportHarkirat Behl is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALON BENHAIM">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAlon Benhaim is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MISHA BILENKO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMisha Bilenko is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JOHAN BJORCK">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJohan Bjorck is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="S&#201;BASTIEN BUBECK">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportS&#233;bastien Bubeck is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="QIN CAI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportQin Cai is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MARTIN CAI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMartin Cai is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CAIO C&#201;SAR TEODORO MENDES">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportCaio C&#233;sar Teodoro Mendes is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VISHRAV CHAUDHARY">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportVishrav Chaudhary is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DONG CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportDong Chen is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DONGDONG CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportDongdong Chen is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YEN-CHUN CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportYen-Chun Chen is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YI-LING CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportYi-Ling Chen is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PARUL CHOPRA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportParul Chopra is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIYANG DAI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportXiyang Dai is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ALLIE DEL GIORNO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAllie Del Giorno is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GUSTAVO DE ROSA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportGustavo de Rosa is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MATTHEW DIXON">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMatthew Dixon is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RONEN ELDAN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportRonen Eldan is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="VICTOR FRAGOSO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportVictor Fragoso is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DAN ITER">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportDan Iter is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MEI GAO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMei Gao is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MIN GAO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMin Gao is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIANFENG GAO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJianfeng Gao is an author of the Phi-3 technical report
Jianfeng Gao is an author of the paper "Instruction tuning with GPT-4"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AMIT GARG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAmit Garg is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ABHISHEK GOSWAMI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAbhishek Goswami is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SURIYA GUNASEKAR">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportSuriya Gunasekar is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="EMMAN HAIDER">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportEmman Haider is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JUNHENG HAO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJunheng Hao is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RUSSELL J. HEWETT">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportRussell J. Hewett is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAMIE HUYNH">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJamie Huynh is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MOJAN JAVAHERIPI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMojan Javaheripi is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIN JIN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportXin Jin is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PIERO KAUFFMANN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportPiero Kauffmann is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NIKOS KARAMPATZIAKIS">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportNikos Karampatziakis is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DONGWOO KIM">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportDongwoo Kim is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MAHOUD KHADEMI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMahoud Khademi is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LEV KURILENKO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportLev Kurilenko is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JAMES R. LEE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJames R. Lee is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIN TAT LEE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportYin Tat Lee is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUANZHI LI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportYuanzhi Li is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YUNSHENG LI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportYunsheng Li is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHEN LIANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportChen Liang is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LARS LIDEN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportLars Liden is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CE LIU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportCe Liu is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MENGCHEN LIU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMengchen Liu is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEISHUNG LIU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportWeishung Liu is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ERIC LIN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportEric Lin is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHONG LUO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportChong Luo is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PIYUSH MADAN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportPiyush Madan is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MATT MAZZOLA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMatt Mazzola is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HARDIK MODI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportHardik Modi is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BRANDON NORICK">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportBrandon Norick is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BARUN PATRA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportBarun Patra is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DANIEL PEREZ-BECKER">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportDaniel Perez-Becker is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="THOMAS PORTET">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportThomas Portet is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="REID PRYZANT">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportReid Pryzant is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HEYANG QIN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportHeyang Qin is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MARKO RADMILAC">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMarko Radmilac is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SAMBUDHA ROY">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportSambudha Roy is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="OLATUNJI RUWASE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportOlatunji Ruwase is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="OLLI SAARIKIVI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportOlli Saarikivi is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="AMIN SAIED">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAmin Saied is an author of the Phi-3 technical report
Amin Saied is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ADIL SALIM">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAdil Salim is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MICHAEL SANTACROCE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMichael Santacroce is an author of the Phi-3 technical report
Michael Santacroce is an author of the paper "Direct Nash optimization: Teaching language models to self-improve with general preferences"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SHITAL SHAH">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportShital Shah is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="NING SHANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportNing Shang is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HITESHI SHARMA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportHiteshi Sharma is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SWADHEEN SHUKLA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportSwadheen Shukla is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIA SONG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportXia Song is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MASAHIRO TANAKA">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMasahiro Tanaka is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ANDREA TUPINI">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportAndrea Tupini is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XIN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportXin Wang is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LIJUAN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportLijuan Wang is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHUNYU WANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportChunyu Wang is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YU WANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportYu Wang is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="RACHEL WARD">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportRachel Ward is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="GUANHUA WANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportGuanhua Wang is an author of the Phi-3 technical report</data>
      <data key="d2">dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="PHILIPP WITTE">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportPhilipp Witte is an author of the Phi-3 technical report
Philipp Witte is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="HAIPING WU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportHaiping Wu is an author of the Phi-3 technical report
Haiping Wu is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="MICHAEL WYATT">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportMichael Wyatt is an author of the Phi-3 technical report
Michael Wyatt is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="BIN XIAO">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportBin Xiao is an author of the Phi-3 technical report
Bin Xiao is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CAN XU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportCan Xu is an author of the Phi-3 technical report
Can Xu is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"
Can Xu is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42,f4e98ee0b7fb42428f3312f29cb444dd</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIAHANG XU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJiahang Xu is an author of the Phi-3 technical report
Jiahang Xu is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="WEIJIAN XU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportWeijian Xu is an author of the Phi-3 technical report
Weijian Xu is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="SONALI YADAV">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportSonali Yadav is an author of the Phi-3 technical report
Sonali Yadav is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="FAN YANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportFan Yang is an author of the Phi-3 technical report
Fan Yang is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIANWEI YANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJianwei Yang is an author of the Phi-3 technical report
Jianwei Yang is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ZIYI YANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportZiyi Yang is an author of the Phi-3 technical report
Ziyi Yang is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YIFAN YANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportYifan Yang is an author of the Phi-3 technical report
Yifan Yang is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="DONGHAN YU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportDonghan Yu is an author of the Phi-3 technical report
Donghan Yu is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LU YUAN">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportLu Yuan is an author of the Phi-3 technical report
Lu Yuan is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CHENGRUIDONG ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportChengruidong Zhang is an author of the Phi-3 technical report
Chengruidong Zhang is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="CYRIL ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportCyril Zhang is an author of the Phi-3 technical report
Cyril Zhang is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="JIANWEN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportJianwen Zhang is an author of the Phi-3 technical report
Jianwen Zhang is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="LI LYNA ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical reportLi Lyna Zhang is an author of the Phi-3 technical report
Li Lyna Zhang is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="YI ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical report
Yi Zhang is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="YUE ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical report
Yue Zhang is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="YUNAN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical report
Yunan Zhang is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="XIREN ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">An author of the Phi-3 technical report
Xiren Zhou is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="PHI-3 TECHNICAL REPORT">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A technical report authored by multiple individuals, including Marah Abdin and Sam Ade Jacobs
Phi-3 technical report is a document describing a highly capable language model that can run locally on a phone, published in 2024</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172,dd9a46950237e49ef9b1c7ef08e08d42</data>
    </node>
    <node id="WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Wang is an author of the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ISAAC COWHEY">
      <data key="d0">PERSON</data>
      <data key="d1">Isaac Cowhey is an author of the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="OREN ETZIONI">
      <data key="d0">PERSON</data>
      <data key="d1">Oren Etzioni is an author of the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="TUSHAR KHOT">
      <data key="d0">PERSON</data>
      <data key="d1">Tushar Khot is an author of the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ASHISH SABHARWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Ashish Sabharwal is an author of the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CARISSA SCHOENICK">
      <data key="d0">PERSON</data>
      <data key="d1">Carissa Schoenick is an author of the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="OYVIND TAFJORD">
      <data key="d0">PERSON</data>
      <data key="d1">Oyvind Tafjord is an author of the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="THINK YOU HAVE SOLVED QUESTION ANSWERING? TRY ARC, THE AI2 REASONING CHALLENGE">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the AI2 Reasoning Challenge (ARC) for question answering, published in 2018</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CODEPARROT">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">CodeParrot is the organization behind the Github-code clean dataset</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="GITHUB-CODE CLEAN DATASET">
      <data key="d0">DATASET</data>
      <data key="d1">A dataset provided by CodeParrot, accessed in 2022</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="NING DING">
      <data key="d0">PERSON</data>
      <data key="d1">Ning Ding is an author of the paper "Enhancing chat language models by scaling high-quality instructional conversations"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="YULIN CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Yulin Chen is an author of the paper "Enhancing chat language models by scaling high-quality instructional conversations"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="BOKAI XU">
      <data key="d0">PERSON</data>
      <data key="d1">Bokai Xu is an author of the paper "Enhancing chat language models by scaling high-quality instructional conversations"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ZHI ZHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhi Zheng is an author of the paper "Enhancing chat language models by scaling high-quality instructional conversations"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="SHENGDING HU">
      <data key="d0">PERSON</data>
      <data key="d1">Shengding Hu is an author of the paper "Enhancing chat language models by scaling high-quality instructional conversations"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ENHANCING CHAT LANGUAGE MODELS BY SCALING HIGH-QUALITY INSTRUCTIONAL CONVERSATIONS">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the enhancement of chat language models by scaling high-quality instructional conversations, published in 2023</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ZHAOYE FEI">
      <data key="d0">PERSON</data>
      <data key="d1">Zhaoye Fei is an author of the paper "Query of cc: Unearthing large scale domain-specific knowledge from public corpora"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="YUNFAN SHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Yunfan Shao is an author of the paper "Query of cc: Unearthing large scale domain-specific knowledge from public corpora"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="LINYANG LI">
      <data key="d0">PERSON</data>
      <data key="d1">Linyang Li is an author of the paper "Query of cc: Unearthing large scale domain-specific knowledge from public corpora"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ZHIYUAN ZENG">
      <data key="d0">PERSON</data>
      <data key="d1">Zhiyuan Zeng is an author of the paper "Query of cc: Unearthing large scale domain-specific knowledge from public corpora"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HANG YAN">
      <data key="d0">PERSON</data>
      <data key="d1">Hang Yan is an author of the paper "Query of cc: Unearthing large scale domain-specific knowledge from public corpora"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="XIPENG QIU">
      <data key="d0">PERSON</data>
      <data key="d1">Xipeng Qiu is an author of the paper "Query of cc: Unearthing large scale domain-specific knowledge from public corpora"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="DAHUA LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Dahua Lin is an author of the paper "Query of cc: Unearthing large scale domain-specific knowledge from public corpora"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="QUERY OF CC: UNEARTHING LARGE SCALE DOMAIN-SPECIFIC KNOWLEDGE FROM PUBLIC CORPORA">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the extraction of domain-specific knowledge from public corpora, published in 2024</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ARNAV GUDIBANDE">
      <data key="d0">PERSON</data>
      <data key="d1">Arnav Gudibande is an author of the paper "The false promise of imitating proprietary llms"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ERIC WALLACE">
      <data key="d0">PERSON</data>
      <data key="d1">Eric Wallace is an author of the paper "The false promise of imitating proprietary llms"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CHARLIE SNELL">
      <data key="d0">PERSON</data>
      <data key="d1">Charlie Snell is an author of the paper "The false promise of imitating proprietary llms"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="XINYANG GENG">
      <data key="d0">PERSON</data>
      <data key="d1">Xinyang Geng is an author of the paper "The false promise of imitating proprietary llms"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HAO LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Hao Liu is an author of the paper "The false promise of imitating proprietary llms"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="THE FALSE PROMISE OF IMITATING PROPRIETARY LLMS">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the limitations of imitating proprietary large language models, published in 2023</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="AKUL ARORA">
      <data key="d0">PERSON</data>
      <data key="d1">Akul Arora is an author of the paper "Measuring mathematical problem solving with the math dataset"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ERIC TANG">
      <data key="d0">PERSON</data>
      <data key="d1">Eric Tang is an author of the paper "Measuring mathematical problem solving with the math dataset"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="MEASURING MATHEMATICAL PROBLEM SOLVING WITH THE MATH DATASET">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the measurement of mathematical problem solving using the math dataset, published in 2021</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HAMISH IVISON">
      <data key="d0">PERSON</data>
      <data key="d1">Hamish Ivison is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="VALENTINA PYATKIN">
      <data key="d0">PERSON</data>
      <data key="d1">Valentina Pyatkin is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="NATHAN LAMBERT">
      <data key="d0">PERSON</data>
      <data key="d1">Nathan Lambert is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="MATTHEW PETERS">
      <data key="d0">PERSON</data>
      <data key="d1">Matthew Peters is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="NOAH A. SMITH">
      <data key="d0">PERSON</data>
      <data key="d1">Noah A. Smith is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="IZ BELTAGY">
      <data key="d0">PERSON</data>
      <data key="d1">Iz Beltagy is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HANNANEH HAJISHIRZI">
      <data key="d0">PERSON</data>
      <data key="d1">Hannaneh Hajishirzi is an author of the paper "Camels in a changing climate: Enhancing lm adaptation with tulu 2"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CAMELS IN A CHANGING CLIMATE: ENHANCING LM ADAPTATION WITH TULU 2">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the enhancement of language model adaptation with Tulu 2, published in 2023</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ALBERT Q. JIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Albert Q. Jiang is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ALEXANDRE SABLAYROLLES">
      <data key="d0">PERSON</data>
      <data key="d1">Alexandre Sablayrolles is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ARTHUR MENSCH">
      <data key="d0">PERSON</data>
      <data key="d1">Arthur Mensch is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CHRIS BAMFORD">
      <data key="d0">PERSON</data>
      <data key="d1">Chris Bamford is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="DEVENDRA SINGH CHAPLOT">
      <data key="d0">PERSON</data>
      <data key="d1">Devendra Singh Chaplot is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="DIEGO DE LAS CASAS">
      <data key="d0">PERSON</data>
      <data key="d1">Diego de las Casas is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="FLORIAN BRESSAND">
      <data key="d0">PERSON</data>
      <data key="d1">Florian Bressand is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="GIANNA LENGYEL">
      <data key="d0">PERSON</data>
      <data key="d1">Gianna Lengyel is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="GUILLAUME LAMPLE">
      <data key="d0">PERSON</data>
      <data key="d1">Guillaume Lample is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="LUCILE SAULNIER">
      <data key="d0">PERSON</data>
      <data key="d1">Lucile Saulnier is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="L&#201;LIO RENARD LAVAUD">
      <data key="d0">PERSON</data>
      <data key="d1">L&#233;lio Renard Lavaud is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="PIERRE STOCK">
      <data key="d0">PERSON</data>
      <data key="d1">Pierre Stock is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="TEVEN LE SAO">
      <data key="d0">PERSON</data>
      <data key="d1">Teven Le Sao is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="THOMAS WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Thomas Wang is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="TIMOTH&#201;E LACROIX">
      <data key="d0">PERSON</data>
      <data key="d1">Timoth&#233;e Lacroix is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="WILLIAM EL SAYED">
      <data key="d0">PERSON</data>
      <data key="d1">William El Sayed is an author of the paper "Mistral 7b"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="MISTRAL 7B">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the Mistral 7b model, published in 2023</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HARRISON LEE">
      <data key="d0">PERSON</data>
      <data key="d1">Harrison Lee is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="SAMRAT PHATALE">
      <data key="d0">PERSON</data>
      <data key="d1">Samrat Phatale is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HASSAN MANSOOR">
      <data key="d0">PERSON</data>
      <data key="d1">Hassan Mansoor is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="THOMAS MESNARD">
      <data key="d0">PERSON</data>
      <data key="d1">Thomas Mesnard is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="JOHAN FERRET">
      <data key="d0">PERSON</data>
      <data key="d1">Johan Ferret is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="KELLIE LU">
      <data key="d0">PERSON</data>
      <data key="d1">Kellie Lu is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="COLTON BISHOP">
      <data key="d0">PERSON</data>
      <data key="d1">Colton Bishop is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ETHAN HALL">
      <data key="d0">PERSON</data>
      <data key="d1">Ethan Hall is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="VICTOR CARBUNE">
      <data key="d0">PERSON</data>
      <data key="d1">Victor Carbune is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ABHINAV RASTOGI">
      <data key="d0">PERSON</data>
      <data key="d1">Abhinav Rastogi is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="SUSHANT PRAKASH">
      <data key="d0">PERSON</data>
      <data key="d1">Sushant Prakash is an author of the paper "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="RLAIF: SCALING REINFORCEMENT LEARNING FROM HUMAN FEEDBACK WITH AI FEEDBACK">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the scaling of reinforcement learning from human feedback with AI feedback, published in 2023</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="GUOHAO LI">
      <data key="d0">PERSON</data>
      <data key="d1">Guohao Li is an author of the paper "Camel: Communicative agents for 'mind' exploration of large language model society"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HASAN ABED AL KADER HAMMOUD">
      <data key="d0">PERSON</data>
      <data key="d1">Hasan Abed Al Kader Hammoud is an author of the paper "Camel: Communicative agents for 'mind' exploration of large language model society"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="HANI ITANI">
      <data key="d0">PERSON</data>
      <data key="d1">Hani Itani is an author of the paper "Camel: Communicative agents for 'mind' exploration of large language model society"</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="BERNARD GHANEM">
      <data key="d0">PERSON</data>
      <data key="d1">Bernard Ghanem is an author of the paper "Camel: Communicative agents for 'mind' exploration of large language model society"
Bernard Ghanem is an author of the paper "Camel: Communicative agents for 'mind' exploration of large language model society"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CAMEL: COMMUNICATIVE AGENTS FOR 'MIND' EXPLORATION OF LARGE LANGUAGE MODEL SOCIETY">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing communicative agents for exploring the 'mind' of large language model society, published in 2023</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="XUECHEN LI">
      <data key="d0">PERSON</data>
      <data key="d1">Xuechen Li is an author of the paper "Alpaca"
Xuechen Li is an author of the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="TIANYI ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Tianyi Zhang is an author of the paper "Alpaca"
Tianyi Zhang is an author of the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="YANN DUBOIS">
      <data key="d0">PERSON</data>
      <data key="d1">Yann Dubois is an author of the paper "Alpaca"
Yann Dubois is an author of the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ROHAN TAORI">
      <data key="d0">PERSON</data>
      <data key="d1">Rohan Taori is an author of the paper "Alpaca"
Rohan Taori is an author of the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ISHAAN GULRAJANI">
      <data key="d0">PERSON</data>
      <data key="d1">Ishaan Gulrajani is an author of the paper "Alpaca"
Ishaan Gulrajani is an author of the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="CARLOS GUESTRIN">
      <data key="d0">PERSON</data>
      <data key="d1">Carlos Guestrin is an author of the paper "Alpaca"
Carlos Guestrin is an author of the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="TATSUNORI B. HASHIMOTO">
      <data key="d0">PERSON</data>
      <data key="d1">Tatsunori B. Hashimoto is an author of the paper "Alpaca"
Tatsunori B. Hashimoto is an author of the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="ALPACA">
      <data key="d0">DOCUMENT</data>
      <data key="d1">A paper discussing the Alpaca model, published in 2023</data>
      <data key="d2">cc20c99cad8edecc66b82ac751ff7172</data>
    </node>
    <node id="RII KHIZBULLIN">
      <data key="d0">PERSON</data>
      <data key="d1">Rii Khizbullin is an author of the paper "Camel: Communicative agents for 'mind' exploration of large language model society"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="CAMEL">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">Camel: Communicative agents for 'mind' exploration of large language model society is a paper published in 2023</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="YIXIN LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Yixin Liu is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ALEXANDER R. FABBRI">
      <data key="d0">PERSON</data>
      <data key="d1">Alexander R. Fabbri is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="JIAWEN CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Jiawen Chen is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="YILUN ZHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Yilun Zhao is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SIMENG HAN">
      <data key="d0">PERSON</data>
      <data key="d1">Simeng Han is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SHAFIQ JOTY">
      <data key="d0">PERSON</data>
      <data key="d1">Shafiq Joty is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="DRAGOMIR RADEV">
      <data key="d0">PERSON</data>
      <data key="d1">Dragomir Radev is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="CHIEN-SHENG WU">
      <data key="d0">PERSON</data>
      <data key="d1">Chien-Sheng Wu is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ARMAN COHAN">
      <data key="d0">PERSON</data>
      <data key="d1">Arman Cohan is an author of the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="BENCHMARKING GENERATION AND EVALUATION CAPABILITIES OF LARGE LANGUAGE MODELS FOR INSTRUCTION CONTROLLABLE SUMMARIZATION">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">A paper published in 2023 that benchmarks the generation and evaluation capabilities of large language models for instruction controllable summarization</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="LM-SYS">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Lm-sys is the organization behind the Mt-Bench project</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="DANIEL VAN STRIEN">
      <data key="d0">PERSON</data>
      <data key="d1">Daniel van Strien is an author of the paper "Cosmopedia: how to create large-scale synthetic data for pre-training"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="LOUBNA BEN ALLAL">
      <data key="d0">PERSON</data>
      <data key="d1">Loubna Ben Allal is an author of the paper "Cosmopedia: how to create large-scale synthetic data for pre-training"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ANTON LOZHKOV">
      <data key="d0">PERSON</data>
      <data key="d1">Anton Lozhkov is an author of the paper "Cosmopedia: how to create large-scale synthetic data for pre-training"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="COSMOPEDIA">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">Cosmopedia: how to create large-scale synthetic data for pre-training is a paper published in 2024</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="CLARISSE SIMOES">
      <data key="d0">PERSON</data>
      <data key="d1">Clarisse Simoes is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SAHAJ AGARWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Sahaj Agarwal is an author of the paper "Orca: Progressive learning from complex explanation traces of GPT-4"Sahaj Agarwal is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XUXI CHEN">
      <data key="d0">PERSON</data>
      <data key="d1">Xuxi Chen is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ANASTASIA RAZDAIBIEDINA">
      <data key="d0">PERSON</data>
      <data key="d1">Anastasia Razdaibiedina is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ERIK JONES">
      <data key="d0">PERSON</data>
      <data key="d1">Erik Jones is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="KRITI AGGARWAL">
      <data key="d0">PERSON</data>
      <data key="d1">Kriti Aggarwal is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="HAMID PALANGI">
      <data key="d0">PERSON</data>
      <data key="d1">Hamid Palangi is an author of the paper "Orca: Progressive learning from complex explanation traces of GPT-4"Hamid Palangi is an author of the paper "Orca 2: Teaching small language models how to reason"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ORCA 2">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">Orca 2: Teaching small language models how to reason is a paper published in 2023</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SUBHABRATA MUKHERJEE">
      <data key="d0">PERSON</data>
      <data key="d1">Subhabrata Mukherjee is an author of the paper "Orca: Progressive learning from complex explanation traces of GPT-4"Subhabrata Mukherjee is an author of the paper "Xtremedistil: Multi-stage distillation for massive multilingual models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="XTREMEDISTIL">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">Xtremedistil: Multi-stage distillation for massive multilingual models is a paper published in 2020</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="GANESH JAWAHAR">
      <data key="d0">PERSON</data>
      <data key="d1">Ganesh Jawahar is an author of the paper "Orca: Progressive learning from complex explanation traces of GPT-4"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
      <data key="d3">PERSON</data>
    </node>
    <node id="ORCA">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">Orca: Progressive learning from complex explanation traces of GPT-4 is a paper published in 2023</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SAMUEL J. PAECH">
      <data key="d0">PERSON</data>
      <data key="d1">Samuel J. Paech is an author of the paper "EQ-Bench: An emotional intelligence benchmark for large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="EQ-BENCH">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">EQ-Bench: An emotional intelligence benchmark for large language models is a paper published in 2024</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="BAOLIN PENG">
      <data key="d0">PERSON</data>
      <data key="d1">Baolin Peng is an author of the paper "Instruction tuning with GPT-4"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="CHUNYUAN LI">
      <data key="d0">PERSON</data>
      <data key="d1">Chunyuan Li is an author of the paper "Instruction tuning with GPT-4"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="PENGCHENG HE">
      <data key="d0">PERSON</data>
      <data key="d1">Pengcheng He is an author of the paper "Instruction tuning with GPT-4"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="MICHEL GALLEY">
      <data key="d0">PERSON</data>
      <data key="d1">Michel Galley is an author of the paper "Instruction tuning with GPT-4"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="INSTRUCTION TUNING WITH GPT-4">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">Instruction tuning with GPT-4 is a paper published in 2023</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="YIWEI QIN">
      <data key="d0">PERSON</data>
      <data key="d1">Yiwei Qin is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="KAIQIANG SONG">
      <data key="d0">PERSON</data>
      <data key="d1">Kaiqiang Song is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="YEBO WEN HU">
      <data key="d0">PERSON</data>
      <data key="d1">Yebo Wen Hu is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="WENLIN YAO">
      <data key="d0">PERSON</data>
      <data key="d1">Wenlin Yao is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="SANGWOO CHO">
      <data key="d0">PERSON</data>
      <data key="d1">Sangwoo Cho is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="XIAOYANG WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Xiaoyang Wang is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="XUANSHENG WU">
      <data key="d0">PERSON</data>
      <data key="d1">Xuansheng Wu is an author of the paper "InfoBench: Evaluating instruction following ability in large language models"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="TOOLLLM">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">ToolLLM: Facilitating large language models to master 16000+ real-world APIs is a paper published in 2023</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="MOHAMMED LATIF SIDDIQ">
      <data key="d0">PERSON</data>
      <data key="d1">Mohammed Latif Siddiq is an author of the paper "The curse of recursion: Training on generated data makes models forget"
Mohammed Latif Siddiq is an author of the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JIAHAO ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Jiahao Zhang is an author of the paper "The curse of recursion: Training on generated data makes models forget"
Jiahao Zhang is an author of the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="LINDSAY RONEY">
      <data key="d0">PERSON</data>
      <data key="d1">Lindsay Roney is an author of the paper "The curse of recursion: Training on generated data makes models forget"
Lindsay Roney is an author of the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JOANNA C. S.">
      <data key="d0">PERSON</data>
      <data key="d1">Joanna C. S. is an author of the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="THE CURSE OF RECURSION">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">The curse of recursion: Training on generated data makes models forget is a paper published in 2024</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="ILIA SHUMAILOV">
      <data key="d0">PERSON</data>
      <data key="d1">Ilia Shumailov is an author of the paper "The curse of recursion: Training on generated data makes models forget"
Ilia Shumailov is an author of the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ZAKHAR SHUMAYLOV">
      <data key="d0">PERSON</data>
      <data key="d1">Zakhar Shumaylov is an author of the paper "The curse of recursion: Training on generated data makes models forget"
Zakhar Shumaylov is an author of the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YIREN ZHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Yiren Zhao is an author of the paper "The curse of recursion: Training on generated data makes models forget"
Yiren Zhao is an author of the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="NICOLAS PAPERNOT">
      <data key="d0">PERSON</data>
      <data key="d1">Nicolas Papernot is an author of the paper "The curse of recursion: Training on generated data makes models forget"
Nicolas Papernot is an author of the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ROSS ANDERSON">
      <data key="d0">PERSON</data>
      <data key="d1">Ross Anderson is an author of the paper "The curse of recursion: Training on generated data makes models forget"
Ross Anderson is an author of the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="CHING-AN CHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Ching-An Cheng is an author of the paper "Direct Nash optimization: Teaching language models to self-improve with general preferences"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="TENGYANG XIE">
      <data key="d0">PERSON</data>
      <data key="d1">Tengyang Xie is an author of the paper "Direct Nash optimization: Teaching language models to self-improve with general preferences"</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="DIRECT NASH OPTIMIZATION">
      <data key="d0">PROJECT/PAPER</data>
      <data key="d1">Direct Nash optimization: Teaching language models to self-improve with general preferences is a paper published in 2024</data>
      <data key="d2">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </node>
    <node id="JOANNA C. S. SANTOS">
      <data key="d0">PERSON</data>
      <data key="d1">Joanna C. S. Santos is an author of the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="NATHANAEL SCH&#196;RLI">
      <data key="d0">PERSON</data>
      <data key="d1">Nathanael Sch&#228;rli is an author of the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="WEN WAI YIM">
      <data key="d0">PERSON</data>
      <data key="d1">Wen wai Yim is an author of the paper "Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YUJUAN FU">
      <data key="d0">PERSON</data>
      <data key="d1">Yujuan Fu is an author of the paper "Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ASMA BEN ABACHA">
      <data key="d0">PERSON</data>
      <data key="d1">Asma Ben Abacha is an author of the paper "Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="NEAL SNIDER">
      <data key="d0">PERSON</data>
      <data key="d1">Neal Snider is an author of the paper "Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="THOMAS LIN">
      <data key="d0">PERSON</data>
      <data key="d1">Thomas Lin is an author of the paper "Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="MELIHA YETISGEN">
      <data key="d0">PERSON</data>
      <data key="d1">Meliha Yetisgen is an author of the paper "Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="AHMED HASSAN AWADALLAH">
      <data key="d0">PERSON</data>
      <data key="d1">Ahmed Hassan Awadallah is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="RYEN W WHITE">
      <data key="d0">PERSON</data>
      <data key="d1">Ryen W White is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="DOUG BURGER">
      <data key="d0">PERSON</data>
      <data key="d1">Doug Burger is an author of the paper "Autogen: Enabling next-gen llm applications via multi-agent conversation"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="CONGYING XIA">
      <data key="d0">PERSON</data>
      <data key="d1">Congying Xia is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="CHEN XING">
      <data key="d0">PERSON</data>
      <data key="d1">Chen Xing is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JIANGSHU DU">
      <data key="d0">PERSON</data>
      <data key="d1">Jiangshu Du is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="XINYI YANG">
      <data key="d0">PERSON</data>
      <data key="d1">Xinyi Yang is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YIHAO FENG">
      <data key="d0">PERSON</data>
      <data key="d1">Yihao Feng is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="RAN XU">
      <data key="d0">PERSON</data>
      <data key="d1">Ran Xu is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="WENPENG YIN">
      <data key="d0">PERSON</data>
      <data key="d1">Wenpeng Yin is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="CAIMING XIONG">
      <data key="d0">PERSON</data>
      <data key="d1">Caiming Xiong is an author of the paper "Fofo: A benchmark to evaluate llms&#8217; format-following capability"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="GUANGZHI XIONG">
      <data key="d0">PERSON</data>
      <data key="d1">Guangzhi Xiong is an author of the paper "Benchmarking retrieval-augmented generation for medicine"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="QIAO JIN">
      <data key="d0">PERSON</data>
      <data key="d1">Qiao Jin is an author of the paper "Benchmarking retrieval-augmented generation for medicine"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ZHIYONG LU">
      <data key="d0">PERSON</data>
      <data key="d1">Zhiyong Lu is an author of the paper "Benchmarking retrieval-augmented generation for medicine"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="AIDONG ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Aidong Zhang is an author of the paper "Benchmarking retrieval-augmented generation for medicine"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="QINGFENG SUN">
      <data key="d0">PERSON</data>
      <data key="d1">Qingfeng Sun is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="KAI ZHENG">
      <data key="d0">PERSON</data>
      <data key="d1">Kai Zheng is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="XIUBO GENG">
      <data key="d0">PERSON</data>
      <data key="d1">Xiubo Geng is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="PU ZHAO">
      <data key="d0">PERSON</data>
      <data key="d1">Pu Zhao is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JIAZHAN FENG">
      <data key="d0">PERSON</data>
      <data key="d1">Jiazhan Feng is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="CHONGYANG TAO">
      <data key="d0">PERSON</data>
      <data key="d1">Chongyang Tao is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="DAXIN JIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Daxin Jiang is an author of the paper "Wizardlm: Empowering large language models to follow complex instructions"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="LONGHUI YU">
      <data key="d0">PERSON</data>
      <data key="d1">Longhui Yu is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="WEISEN JIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Weisen Jiang is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="HAN SHI">
      <data key="d0">PERSON</data>
      <data key="d1">Han Shi is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JINCHENG YU">
      <data key="d0">PERSON</data>
      <data key="d1">Jincheng Yu is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ZHENGYING LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Zhengying Liu is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YU ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yu Zhang is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JAMES T KWOK">
      <data key="d0">PERSON</data>
      <data key="d1">James T Kwok is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ZHENGUO LI">
      <data key="d0">PERSON</data>
      <data key="d1">Zhenguo Li is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ADRIAN WELLER">
      <data key="d0">PERSON</data>
      <data key="d1">Adrian Weller is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="WEIYANG LIU">
      <data key="d0">PERSON</data>
      <data key="d1">Weiyang Liu is an author of the paper "Metamath: Bootstrap your own mathematical questions for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YIFAN ZHANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yifan Zhang is an author of the paper "Automathtext: Autonomous data selection with language models for mathematical texts"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YIFAN LUO">
      <data key="d0">PERSON</data>
      <data key="d1">Yifan Luo is an author of the paper "Automathtext: Autonomous data selection with language models for mathematical texts"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YANG YUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Yang Yuan is an author of the paper "Automathtext: Autonomous data selection with language models for mathematical texts"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="ANDREW CHI-CHIH YAO">
      <data key="d0">PERSON</data>
      <data key="d1">Andrew Chi-Chih Yao is an author of the paper "Automathtext: Autonomous data selection with language models for mathematical texts"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="WANJUN ZHONG">
      <data key="d0">PERSON</data>
      <data key="d1">Wanjun Zhong is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="RUIXIANG CUI">
      <data key="d0">PERSON</data>
      <data key="d1">Ruixiang Cui is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YIDUO GUO">
      <data key="d0">PERSON</data>
      <data key="d1">Yiduo Guo is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YAOBO LIANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yaobo Liang is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="SHUAI LU">
      <data key="d0">PERSON</data>
      <data key="d1">Shuai Lu is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YANLIN WANG">
      <data key="d0">PERSON</data>
      <data key="d1">Yanlin Wang is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="NAN DUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Nan Duan is an author of the paper "Agieval: A human-centric benchmark for evaluating foundation models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="JEFFREY ZHOU">
      <data key="d0">PERSON</data>
      <data key="d1">Jeffrey Zhou is an author of the paper "Instruction-following evaluation for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="TIANJIAN LU">
      <data key="d0">PERSON</data>
      <data key="d1">Tianjian Lu is an author of the paper "Instruction-following evaluation for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="SIDDHARTHA BRAHMA">
      <data key="d0">PERSON</data>
      <data key="d1">Siddhartha Brahma is an author of the paper "Instruction-following evaluation for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="SUJOY BASU">
      <data key="d0">PERSON</data>
      <data key="d1">Sujoy Basu is an author of the paper "Instruction-following evaluation for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="YI LUAN">
      <data key="d0">PERSON</data>
      <data key="d1">Yi Luan is an author of the paper "Instruction-following evaluation for large language models"</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="DEBATE PASSAGE GENERATOR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An agent that specializes in crafting passages that mimic the structure and content of debate transcripts</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="CONVERSATION PASSAGE GENERATOR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An agent that generates passages that depict dialogues</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="MEETING TRANSCRIPT GENERATOR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An agent designed to produce meeting transcripts</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="POEM GENERATOR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An agent that generates poems</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="SATIRICAL PASSAGE GENERATOR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An agent that creates texts infused with satirical wit</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="INSTRUCTIONAL PASSAGE GENERATOR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An agent that generates passages resembling instructional manuals</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="LONG TEXT GENERATOR">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">An agent that extends the original text by incorporating additional information, thereby increasing its length</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="IDENTITY AGENT">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A straightforward agent that replicates the input text verbatim</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="LITERAL COMPREHENSION QUESTION">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A question that asks for a specific detail(s) or fact(s) clearly stated in the text</data>
      <data key="d2">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="NUMERICAL DISCRETE REASONING">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">Questions that require the reader to use numerical reasoning over many facts from the text
Questions that require the reader to use numerical reasoning over many facts from the text</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="CRITICAL COMPREHENSION QUESTION">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">Construct two statements about the purpose or point of view that the reader must assess as true or false, with one being true and the other false
Construct two statements about the purpose or point of view that the reader must assess as true or false, with one being true and the other false</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="EVALUATIVE COMPREHENSION QUESTION">
      <data key="d0">TOOL/PROCESS</data>
      <data key="d1">A type of question that requires an essay response to evaluate comprehension
An open-ended question that prompts an in-depth analysis of the text&#8217;s theme or the effectiveness of an argument</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb,f4e98ee0b7fb42428f3312f29cb444dd</data>
    </node>
    <node id="VOCABULARY AND LANGUAGE USE">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">A fill-in-the-blank question that tests understanding of a particular word or phrase used in the text</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="RELATIONSHIP COMPREHENSION QUESTION">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">A matching question where respondents pair items based on a specific criterion</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="SEQUENCING EVENTS">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">A series of events from the text arranged in the correct chronological order</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="STRENGTHEN">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Identify information that would make the argument&#8217;s conclusion more likely to be true</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="WEAKEN">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Find evidence or an argument that would make the conclusion less likely to be true</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="ASSUMPTION">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Determine what must be true for the argument to hold</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="FLAW">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Point out a mistake in the argument&#8217;s reasoning</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="INFERENCE">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Choose an option that logically follows from the information provided</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="PRINCIPLE">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Recognize the general rule or principle that underlies the argument</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="METHOD OF REASONING">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Describe how the argument is constructed logically</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="RESOLVE THE PARADOX">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Offer an explanation that reconciles seemingly contradictory information</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT SIMPLIFICATION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Making text easier to read and understand by using simpler words and sentence structures, often for children or language learners</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT EXPANSION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Adding more information or detail to make text more comprehensive or to meet a certain word count</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT TRANSLATION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Converting text from one language to another while attempting to preserve the original meaning as closely as possible</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT FORMATTING">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Altering the appearance of text to improve readability or for stylistic purposes</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="SENTIMENT MODIFICATION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Changing the tone of the text to alter its emotional impact, such as making a sentence sound more positive or negative</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT ANNOTATION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Adding notes, comments, or explanations to a text, often for the purpose of analysis or to provide additional context</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="KEYWORD REPLACEMENT">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Substituting specific words or phrases with synonyms or related terms</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT REMOVING">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Redacting or removing content from text</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT CAPITALIZATION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Adjusting the case of letters in text, such as converting to uppercase, lowercase, title case, or sentence case, starting every sentence with a particular letter, word</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT STYLING">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Applying styles like bold, italics, underline, etc., to emphasize certain parts of the text or for aesthetic purposes</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="CONTENT REWRITING">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Extensively modifying a text to produce a new version, which could involve changing the perspective, style, or target audience</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="DATA NORMALIZATION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Standardizing text to ensure consistency, such as converting dates and times to a standard format or unifying the spelling of words</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="PLAGIARISM REWORDING">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Altering text to avoid plagiarism, ensuring that the content is original</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT OBFUSCATION">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Intentionally making text vague or harder to understand, sometimes for security purposes like masking personal data</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXTUAL ENTAILMENT">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Modifying a sentence or phrase to either entail or contradict another sentence, often used in natural language processing tasks</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="REWRITING WITH VOCABULARY LIMITATIONS">
      <data key="d0">TEXT MODIFICATION</data>
      <data key="d1">Rewriting the entire text or a piece of it while using a limited vocabulary, such as all words starting with a specific letter</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="EVALUATOR ASSISTANT">
      <data key="d0">ROLE</data>
      <data key="d1">An unbiased evaluator that parses student responses and returns the alphabet ID of the answer selected by the student</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="SPECIFIC DETAIL(S) OR FACT(S)">
      <data key="d0">QUESTION TYPE</data>
      <data key="d1">Questions that require the reader to identify specific details or facts clearly stated in the text</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="TEXT MODIFICATION FLOW">
      <data key="d0">PROCESS</data>
      <data key="d1">A flow that includes various methods for modifying text, such as paraphrasing, text simplification, and text expansion</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="INSTRUCTION TAXONOMY">
      <data key="d0">PROCESS</data>
      <data key="d1">A classification system used for generating seed instructions, including methods like paraphrasing and text simplification</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="EVALUATION DETAILS">
      <data key="d0">PROCESS</data>
      <data key="d1">Details about the types of tasks/benchmarks and the methods used to extract answers and generate metrics</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="OPEN-ENDED GENERATION SETTING">
      <data key="d0">EVALUATION METHOD</data>
      <data key="d1">A setting in which models are evaluated by generating open-ended responses</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="REGEX BASED EXTRACTION">
      <data key="d0">EVALUATION METHOD</data>
      <data key="d1">A method previously used for extracting options selected by models in multiple choice questions</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="STUDENT RESPONSE">
      <data key="d0">EVALUATION METHOD</data>
      <data key="d1">The response given by a student, which is parsed to extract the selected answer
The answer or response provided by the student to the given question</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50,5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="ANSWER OPTIONS">
      <data key="d0">EVALUATION METHOD</data>
      <data key="d1">The set of possible answers provided to the student for a multiple choice question</data>
      <data key="d2">5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="PARSED STUDENT ANSWER">
      <data key="d0">EVALUATION METHOD</data>
      <data key="d1">The final answer extracted from the student&#8217;s response, represented by the alphabet ID of the selected option
The final answer extracted from the student's response, represented by the alphabets corresponding to the chosen options</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50,5819b66e04fd77fa705574edc49395bb</data>
    </node>
    <node id="EXACT MATCH/SPAN EXTRACTION PROBLEMS">
      <data key="d0">TASK TYPE</data>
      <data key="d1">Tasks involving math-based questions or problems where a ground-truth answer value is given, requiring the model to generate and match the answer with the provided ground-truth</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="MATHS GPT-4 EXTRACTION SYSTEM MESSAGE">
      <data key="d0">SYSTEM MESSAGE</data>
      <data key="d1">A specific system message used for evaluating student answers to math word problems, ensuring the final answer matches the problem setter's answer</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="GENERAL EXTRACTION SYSTEM MESSAGE">
      <data key="d0">SYSTEM MESSAGE</data>
      <data key="d1">A system message used for parsing student responses and matching them with the correct answer in exact match/span extraction problems</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="EQBENCH GPT-4 EXTRACTION SYSTEM MESSAGE">
      <data key="d0">SYSTEM MESSAGE</data>
      <data key="d1">A system message used for extracting emotion scores from evaluated model responses in EQBench tasks</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="OPTIONS">
      <data key="d0">TASK TYPE</data>
      <data key="d1">The set of possible answers provided for the student to choose from</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="FINAL ANSWER">
      <data key="d0">OUTPUT FORMAT</data>
      <data key="d1">The correct answer to the question, typically provided by the problem setter</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="ERROR ANALYSIS">
      <data key="d0">PROCESS</data>
      <data key="d1">The process of comparing the student's answer with the correct answer to determine if they match</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="FINAL VERDICT">
      <data key="d0">OUTPUT FORMAT</data>
      <data key="d1">The result of the error analysis, indicating whether the student's answer is 'Correct' or 'Incorrect'</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="EMOTION SCORES">
      <data key="d0">OUTPUT FORMAT</data>
      <data key="d1">Scores assigned to different emotions based on the student's response in EQBench tasks</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="CRITIQUE">
      <data key="d0">PROCESS</data>
      <data key="d1">A step-by-step analysis of the student's response, used to revise emotion scores in EQBench tasks</data>
      <data key="d2">103d98395c393552cc954c89d4e59f50</data>
    </node>
    <node id="ALEX">
      <data key="d0">PERSON</data>
      <data key="d1">Alex is a person who is already in a relationship and has been confessed to by Elliot, putting Alex in an awkward position.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="RESIGNED">
      <data key="d0">EMOTION</data>
      <data key="d1">Resigned is an emotion felt by Elliot, scored at 7, indicating a strong sense of acceptance of the situation.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="ANGRY">
      <data key="d0">EMOTION</data>
      <data key="d1">Angry is an emotion felt by Elliot, scored at 3, indicating a mild sense of frustration or self-directed anger.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="HOPEFUL">
      <data key="d0">EMOTION</data>
      <data key="d1">Hopeful is an emotion felt by Elliot, scored at 5, indicating a moderate sense of optimism that Alex might reciprocate his feelings.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="EMBARRASSED">
      <data key="d0">EMOTION</data>
      <data key="d1">Embarrassed is an emotion felt by Elliot, scored at 8, indicating a strong sense of discomfort for putting Alex in an awkward position.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="OPEN-ENDED GENERATION">
      <data key="d0">TASK</data>
      <data key="d1">Open-Ended Generation tasks involve prompting a model to generate an answer to an open-ended question without a ground-truth answer for comparison.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="QUALITY JUDGE">
      <data key="d0">PROCESS</data>
      <data key="d1">Quality Judge is a process where a judge evaluates the quality of a response provided by an AI assistant, assessing criteria like instruction adherence, content grounding, and overall quality.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="ELLIOT">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="REVISED SCORES">
      <data key="d0">DATA</data>
      <data key="d1">Revised scores are the numerical values assigned to each of Elliot's emotions: Resigned (7), Angry (3), Hopeful (5), and Embarrassed (8).</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="PROMPT TEMPLATE">
      <data key="d0">TOOL</data>
      <data key="d1">Prompt template is a predefined format used for evaluating hallucination detection and summarization quality in AI-generated responses.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="TEXT SUMMARIZATION">
      <data key="d0">TASK</data>
      <data key="d1">Text Summarization is the task of generating a concise and accurate summary of a given text, evaluated for quality and hallucination.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="HALLUCINATION DETECTION">
      <data key="d0">PROCESS</data>
      <data key="d1">Hallucination Detection is the process of identifying and evaluating hallucinated content in AI-generated summaries.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="SUMMARIZATION QUALITY">
      <data key="d0">PROCESS</data>
      <data key="d1">Summarization Quality is the process of evaluating the quality of AI-generated summaries based on criteria like instruction adherence, content grounding, and overall quality.</data>
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <node id="EMOTIONS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </node>
    <edge source="DARREN EDGE" target="HA TRINH">
      <data key="d4">16.0</data>
      <data key="d5">Darren Edge and Ha Trinh co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="DARREN EDGE" target="NEWMAN CHENG">
      <data key="d4">16.0</data>
      <data key="d5">Darren Edge and Newman Cheng co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="DARREN EDGE" target="JOSHUA BRADLEY">
      <data key="d4">16.0</data>
      <data key="d5">Darren Edge and Joshua Bradley co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="DARREN EDGE" target="ALEX CHAO">
      <data key="d4">16.0</data>
      <data key="d5">Darren Edge and Alex Chao co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="DARREN EDGE" target="APURVA MODY">
      <data key="d4">16.0</data>
      <data key="d5">Darren Edge and Apurva Mody co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="DARREN EDGE" target="STEVEN TRUITT">
      <data key="d4">16.0</data>
      <data key="d5">Darren Edge and Steven Truitt co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="DARREN EDGE" target="JONATHAN LARSON">
      <data key="d4">16.0</data>
      <data key="d5">Darren Edge and Jonathan Larson co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="HA TRINH" target="NEWMAN CHENG">
      <data key="d4">16.0</data>
      <data key="d5">Ha Trinh and Newman Cheng co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="HA TRINH" target="JOSHUA BRADLEY">
      <data key="d4">16.0</data>
      <data key="d5">Ha Trinh and Joshua Bradley co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="HA TRINH" target="ALEX CHAO">
      <data key="d4">16.0</data>
      <data key="d5">Ha Trinh and Alex Chao co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="HA TRINH" target="APURVA MODY">
      <data key="d4">16.0</data>
      <data key="d5">Ha Trinh and Apurva Mody co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="HA TRINH" target="STEVEN TRUITT">
      <data key="d4">16.0</data>
      <data key="d5">Ha Trinh and Steven Truitt co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="HA TRINH" target="JONATHAN LARSON">
      <data key="d4">16.0</data>
      <data key="d5">Ha Trinh and Jonathan Larson co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="NEWMAN CHENG" target="JOSHUA BRADLEY">
      <data key="d4">16.0</data>
      <data key="d5">Newman Cheng and Joshua Bradley co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="NEWMAN CHENG" target="ALEX CHAO">
      <data key="d4">16.0</data>
      <data key="d5">Newman Cheng and Alex Chao co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="NEWMAN CHENG" target="APURVA MODY">
      <data key="d4">16.0</data>
      <data key="d5">Newman Cheng and Apurva Mody co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="NEWMAN CHENG" target="STEVEN TRUITT">
      <data key="d4">16.0</data>
      <data key="d5">Newman Cheng and Steven Truitt co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="NEWMAN CHENG" target="JONATHAN LARSON">
      <data key="d4">16.0</data>
      <data key="d5">Newman Cheng and Jonathan Larson co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="JOSHUA BRADLEY" target="ALEX CHAO">
      <data key="d4">16.0</data>
      <data key="d5">Joshua Bradley and Alex Chao co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="JOSHUA BRADLEY" target="APURVA MODY">
      <data key="d4">16.0</data>
      <data key="d5">Joshua Bradley and Apurva Mody co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="JOSHUA BRADLEY" target="STEVEN TRUITT">
      <data key="d4">16.0</data>
      <data key="d5">Joshua Bradley and Steven Truitt co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="JOSHUA BRADLEY" target="JONATHAN LARSON">
      <data key="d4">16.0</data>
      <data key="d5">Joshua Bradley and Jonathan Larson co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="ALEX CHAO" target="APURVA MODY">
      <data key="d4">16.0</data>
      <data key="d5">Alex Chao and Apurva Mody co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="ALEX CHAO" target="STEVEN TRUITT">
      <data key="d4">16.0</data>
      <data key="d5">Alex Chao and Steven Truitt co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="ALEX CHAO" target="JONATHAN LARSON">
      <data key="d4">16.0</data>
      <data key="d5">Alex Chao and Jonathan Larson co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="APURVA MODY" target="STEVEN TRUITT">
      <data key="d4">16.0</data>
      <data key="d5">Apurva Mody and Steven Truitt co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="APURVA MODY" target="JONATHAN LARSON">
      <data key="d4">16.0</data>
      <data key="d5">Apurva Mody and Jonathan Larson co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="STEVEN TRUITT" target="JONATHAN LARSON">
      <data key="d4">16.0</data>
      <data key="d5">Steven Truitt and Jonathan Larson co-authored the paper "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES">
      <data key="d4">14.0</data>
      <data key="d5">Microsoft Research and Microsoft Strategic Missions and Technologies are both part of Microsoft and collaborated on the paper</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="MICROSOFT OFFICE OF THE CTO">
      <data key="d4">14.0</data>
      <data key="d5">Microsoft Research and Microsoft Office of the CTO are both part of Microsoft and collaborated on the paper</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="ARINDAM MITRA">
      <data key="d4">8.0</data>
      <data key="d5">Arindam Mitra is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="LUCIANO DEL CORRO">
      <data key="d4">8.0</data>
      <data key="d5">Luciano Del Corro is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="GUOQING ZHENG">
      <data key="d4">8.0</data>
      <data key="d5">Guoqing Zheng is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="SHWETI MAHAJAN">
      <data key="d4">8.0</data>
      <data key="d5">Shweti Mahajan is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="DANY ROUHANA">
      <data key="d4">8.0</data>
      <data key="d5">Dany Rouhana is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="ANDRES CODAS">
      <data key="d4">8.0</data>
      <data key="d5">Andres Codas is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="YADONG LU">
      <data key="d4">8.0</data>
      <data key="d5">Yadong Lu is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="WEI-GE CHEN">
      <data key="d4">8.0</data>
      <data key="d5">Wei-ge Chen is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="OLGA VROUSGOS">
      <data key="d4">8.0</data>
      <data key="d5">Olga Vrousgos is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="CORBY ROSSET">
      <data key="d4">8.0</data>
      <data key="d5">Corby Rosset is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="FILLIPE SILVA">
      <data key="d4">8.0</data>
      <data key="d5">Fillipe Silva is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="HAMED KHANPOUR">
      <data key="d4">8.0</data>
      <data key="d5">Hamed Khanpour is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="YASH LARA">
      <data key="d4">8.0</data>
      <data key="d5">Yash Lara is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT RESEARCH" target="AHMED AWADALLAH">
      <data key="d4">1.0</data>
      <data key="d5">Ahmed Awadallah is affiliated with Microsoft Research</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES" target="MICROSOFT OFFICE OF THE CTO">
      <data key="d4">2.0</data>
      <data key="d5">Microsoft Strategic Missions and Technologies and Microsoft Office of the CTO are both part of Microsoft and collaborated on the paper</data>
      <data key="d6">0c932f7def033fa2b1bf210fbb771e7d</data>
    </edge>
    <edge source="GRAPH RAG" target="GLOBAL SUMMARIZATION">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG is an approach based on global summarization of an LLM-derived knowledge graph</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GRAPH RAG" target="KNOWLEDGE GRAPH">
      <data key="d4">17.0</data>
      <data key="d5">A Knowledge Graph is used in the Graph RAG approach
Graph RAG can create and reason over knowledge graphs</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7,edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="COMMUNITY DESCRIPTIONS">
      <data key="d4">8.0</data>
      <data key="d5">Community Descriptions provide complete coverage of the underlying graph index</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GRAPH RAG" target="SOURCE TEXTS">
      <data key="d4">16.0</data>
      <data key="d5">Source Texts are the original documents used for summarization in the Graph RAG approach
Source texts are used for summarization in the Graph RAG method</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="GRAPH RAG PIPELINE">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG Pipeline is the implementation of the Graph RAG approach</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GRAPH RAG" target="LLM PROMPTS">
      <data key="d4">8.0</data>
      <data key="d5">LLM Prompts are used to extract elements of a graph index from text chunks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GRAPH RAG" target="DATASET">
      <data key="d4">15.0</data>
      <data key="d5">Graph RAG uses dataset summaries to answer user queries
Dataset refers to the collection of data used for evaluating Graph RAG</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582,ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="CONTEXT WINDOW">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG uses a context window to generate answers</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="GRAPH RAG" target="PUBLIC FIGURES">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG identifies public figures in entertainment articles</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="GRAPH RAG" target="GRAPH COMMUNITIES">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG uses different levels of graph communities to answer user queries</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="GRAPH RAG" target="PUBLIC FIGURES IN CONTROVERSY">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG includes public figures involved in controversies</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="GRAPH RAG" target="TAYLOR SWIFT">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG mentions Taylor Swift</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="GRAPH RAG" target="TRAVIS KELCE">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG mentions Travis Kelce</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="GRAPH RAG" target="BRITNEY SPEARS">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG mentions Britney Spears</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="GRAPH RAG" target="JUSTIN TIMBERLAKE">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG mentions Justin Timberlake</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="GRAPH RAG" target="LLM">
      <data key="d4">24.0</data>
      <data key="d5">LLM assesses Graph RAGGraph RAG is assessed by the LLM for its comprehensiveness, diversity, empowerment, and directness
Graph RAG uses LLMs to process and generate text based on retrieved information</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b,edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="ENTERTAINMENT ARTICLES">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG provides a comprehensive list of public figures mentioned in entertainment articles</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="GRAPH RAG" target="PODCAST DATASET">
      <data key="d4">8.0</data>
      <data key="d5">The Podcast dataset is used to evaluate the performance of the Graph RAG method</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="NEWS DATASET">
      <data key="d4">8.0</data>
      <data key="d5">The News dataset is used to evaluate the performance of the Graph RAG method</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="COMPREHENSIVENESS">
      <data key="d4">16.0</data>
      <data key="d5">Graph RAG achieves high comprehensiveness win rates
Graph RAG has a high win rate in comprehensiveness</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="DIVERSITY">
      <data key="d4">16.0</data>
      <data key="d5">Graph RAG achieves high diversity win rates
Graph RAG has a high win rate in diversity</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="EMPOWERMENT">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG performs comparably on empowerment</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="COMMUNITY SUMMARIES">
      <data key="d4">23.0</data>
      <data key="d5">Community summaries are used in the Graph RAG method
Graph RAG uses community summaries as a type of self-memory for generation-augmented retrieval
Community summaries are summaries of root-level communities in the entity-based graph index used in Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8,edab4014b8f55e5b25bd7f396314be1f,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="TS">
      <data key="d4">1.0</data>
      <data key="d5">TS represents global text summarization without a graph index in the Graph RAG method</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GRAPH RAG" target="NA&#207;VE RAG">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG is an advanced method that improves upon Na&#239;ve RAG by using a self-generated graph index</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="GENERATION-AUGMENTED RETRIEVAL (GAR)">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG incorporates concepts from generation-augmented retrieval (GAR)</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="MODULAR RAG">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG includes patterns from Modular RAG for iterative and dynamic cycles of interleaved retrieval and generation</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="SELF-MEMORY (SELFMEM)">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG uses the concept of self-memory for generation-augmented retrieval</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="ITERATIVE RETRIEVAL-GENERATION (ITER-RETGEN)">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG uses iterative retrieval-generation strategies</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="FEDERATED RETRIEVAL-GENERATION (FEB4RAG)">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG uses federated retrieval-generation strategies</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="MULTI-DOCUMENT SUMMARIZATION">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG can be used for multi-document summarization</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="MULTI-HOP QUESTION ANSWERING">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG can be used for multi-hop question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="HIERARCHICAL INDEX">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG uses a hierarchical index to organize text chunks</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="SCALABILITY">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG demonstrates scalability by reducing context token requirements</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="TUNING ELEMENT EXTRACTION PROMPTS">
      <data key="d4">7.0</data>
      <data key="d5">Tuning element extraction prompts can improve the retention of specific details in the Graph RAG index</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="RAPTOR">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's hierarchical index is similar to RAPTOR's method of clustering text chunk vectors</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="TREE OF CLARIFICATIONS">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's hierarchical approach is similar to generating a tree of clarifications</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="KAPING">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's use of a knowledge graph index is similar to KAPING</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="G-RETRIEVER">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's use of graph structures is similar to G-Retriever</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="GRAPH-TOOLFORMER">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's use of graph metrics is similar to Graph-ToolFormer</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="SURGE">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's narrative grounding is similar to SURGE</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="FABULA">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's use of narrative templates is similar to FABULA</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="NALLM">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's ability to create and reason over knowledge graphs is similar to NaLLM</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="GRAPHRAG">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's ability to create and reason over knowledge graphs is similar to GraphRAG</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="CAIRE-COVID">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's multi-document summarization is similar to CAiRE-COVID</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="ITRG">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's multi-hop question answering is similar to ITRG</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="IR-COT">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's multi-hop question answering is similar to IR-CoT</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="DSP">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG's multi-hop question answering is similar to DSP</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="PODCAST INTERMEDIATE-LEVEL SUMMARIES">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG generates podcast intermediate-level summaries</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="NEWS LOW-LEVEL COMMUNITY SUMMARIES">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG generates news low-level community summaries</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="TABLE 3">
      <data key="d4">8.0</data>
      <data key="d5">Table 3 illustrates the scalability advantages of Graph RAG</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="ROOT-LEVEL COMMUNITY SUMMARIES">
      <data key="d4">7.0</data>
      <data key="d5">Graph RAG generates root-level community summaries</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="ITERATIVE QUESTION ANSWERING">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG is used for iterative question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH RAG" target="GRAPH INDEX">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG uses a graph index to partition data for global summarization</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="RAG">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG combines knowledge graph generation with Retrieval-Augmented Generation (RAG)</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="QFS">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG uses Query-Focused Summarization (QFS) to support human sensemaking</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="MAP-REDUCE">
      <data key="d4">8.0</data>
      <data key="d5">Graph RAG is compared to a graph-free approach using map-reduce for global summarization</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="SELF-CHECKGPT">
      <data key="d4">7.0</data>
      <data key="d5">SelfCheckGPT is suggested for comparing fabrication rates in the analysis of Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="ALONSO GUEVARA FERN&#193;NDEZ">
      <data key="d4">8.0</data>
      <data key="d5">Alonso Guevara Fern&#225;ndez contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="AMBER HOAK">
      <data key="d4">8.0</data>
      <data key="d5">Amber Hoak contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="ANDR&#201;S MORALES ESQUIVEL">
      <data key="d4">8.0</data>
      <data key="d5">Andr&#233;s Morales Esquivel contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="BEN CUTLER">
      <data key="d4">8.0</data>
      <data key="d5">Ben Cutler contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="BILLIE RINALDI">
      <data key="d4">8.0</data>
      <data key="d5">Billie Rinaldi contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="CHRIS SANCHEZ">
      <data key="d4">8.0</data>
      <data key="d5">Chris Sanchez contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="CHRIS TREVINO">
      <data key="d4">8.0</data>
      <data key="d5">Chris Trevino contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="CHRISTINE CAGGIANO">
      <data key="d4">8.0</data>
      <data key="d5">Christine Caggiano contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="DAVID TITTSWORTH">
      <data key="d4">8.0</data>
      <data key="d5">David Tittsworth contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="DAYENNE DE SOUZA">
      <data key="d4">8.0</data>
      <data key="d5">Dayenne de Souza contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="DOUGLAS ORBAKER">
      <data key="d4">8.0</data>
      <data key="d5">Douglas Orbaker contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="ED CLARK">
      <data key="d4">8.0</data>
      <data key="d5">Ed Clark contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="GABRIEL NIEVES-PONCE">
      <data key="d4">8.0</data>
      <data key="d5">Gabriel Nieves-Ponce contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="GAUDY BLANCO MENESES">
      <data key="d4">8.0</data>
      <data key="d5">Gaudy Blanco Meneses contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="KATE LYTVYNETS">
      <data key="d4">8.0</data>
      <data key="d5">Kate Lytvynets contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="KATY SMITH">
      <data key="d4">8.0</data>
      <data key="d5">Katy Smith contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="M&#211;NICA CARVAJAL">
      <data key="d4">8.0</data>
      <data key="d5">M&#243;nica Carvajal contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="NATHAN EVANS">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Evans contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="RICHARD ORTEGA">
      <data key="d4">8.0</data>
      <data key="d5">Richard Ortega contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="RODRIGO RACANICCI">
      <data key="d4">8.0</data>
      <data key="d5">Rodrigo Racanicci contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="SARAH SMITH">
      <data key="d4">8.0</data>
      <data key="d5">Sarah Smith contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="SHANE SOLOMON">
      <data key="d4">1.0</data>
      <data key="d5">Shane Solomon contributed to the work on Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="SENSEMAKING QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">Sensemaking questions are used to evaluate the performance of Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="CORPORA">
      <data key="d4">7.0</data>
      <data key="d5">Corpora are collections of text data used in the evaluation of Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="USER QUERIES">
      <data key="d4">7.0</data>
      <data key="d5">User queries are used to retrieve information from the graph index in Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="COMMUNITY HIERARCHY">
      <data key="d4">7.0</data>
      <data key="d5">Community hierarchy refers to the hierarchical structure of communities in the graph index used in Graph RAG</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="MAP-REDUCE SUMMARIZATION">
      <data key="d4">7.0</data>
      <data key="d5">Map-Reduce summarization is a method compared to Graph RAG for global summarization</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GRAPH RAG" target="PYTHON">
      <data key="d4">8.0</data>
      <data key="d5">Python is the programming language used for the implementation of Graph RAG approaches</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="LOUVAIN">
      <data key="d4">9.0</data>
      <data key="d5">Louvain is a community detection algorithm used to partition graphs into modular communities</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="LEIDEN">
      <data key="d4">17.0</data>
      <data key="d5">Leiden is a community detection algorithm used to partition graphs into modular communities
Leiden is used for community detection in large-scale graphs</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7,e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="GRAPH COMMUNITIES">
      <data key="d4">8.0</data>
      <data key="d5">Community detection results in the formation of graph communities</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="FORTUNATO, 2010">
      <data key="d4">6.0</data>
      <data key="d5">Fortunato's 2010 survey is referenced in the context of community detection algorithms</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="JIN ET AL., 2021">
      <data key="d4">6.0</data>
      <data key="d5">Jin et al.'s 2021 survey is referenced in the context of community detection algorithms</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LEIDEN" target="TRAAG ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Traag et al. are referenced for their work on the Leiden algorithm in 2019</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="LEIDEN" target="MULTIHOP-RAG">
      <data key="d4">7.0</data>
      <data key="d5">Leiden is used to detect communities in the MultiHop-RAG dataset</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LEIDEN" target="TRAAG ET AL., 2019">
      <data key="d4">7.0</data>
      <data key="d5">Traag et al.'s 2019 work discusses the Leiden algorithm</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="MICROSOFT" target="KEVIN SCOTT">
      <data key="d4">8.0</data>
      <data key="d5">Kevin Scott is the CTO of Microsoft.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="LEWIS ET AL." target="MEMORY STRUCTURES">
      <data key="d4">14.0</data>
      <data key="d5">Lewis et al. contributed to the research on memory structures in agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="LEWIS ET AL." target="EXTERNAL MEMORY AND RAG">
      <data key="d4">8.0</data>
      <data key="d5">Lewis et al. are the authors of the External Memory and RAG methods</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="LASKAR ET AL." target="SUMMARIZATION TASKS">
      <data key="d4">8.0</data>
      <data key="d5">Laskar et al. are referenced for their work on summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="YAO ET AL." target="DENG ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Yao et al. and Deng et al. are co-referenced in the context of language models performing in complex environments such as web navigation.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="YAO ET AL." target="SCHICK ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Yao et al. and Schick et al. are co-referenced in the context of language models performing in complex environments such as tool-use.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="YAO ET AL." target="FAN ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Yao et al. and Fan et al. are co-referenced in the context of language models performing in complex environments such as open-ended games.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="YAO ET AL." target="CHAIN-OF-THOUGHT">
      <data key="d4">14.0</data>
      <data key="d5">Yao et al. contributed to the research on chain-of-thought planning and reasoning</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="YAO ET AL." target="CHAIN-OF-THOUGHT-BASED PLANNING AND REASONING METHODS">
      <data key="d4">8.0</data>
      <data key="d5">Yao et al. are the authors of the Chain-of-Thought-based planning and reasoning methods</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="GOODWIN ET AL." target="SUMMARIZATION TASKS">
      <data key="d4">8.0</data>
      <data key="d5">Goodwin et al. are referenced for their work on summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="LIU AND LAPATA" target="SUMMARIZATION TASKS">
      <data key="d4">8.0</data>
      <data key="d5">Liu and Lapata are referenced for their work on summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GPT" target="SUMMARIZATION TASKS">
      <data key="d4">9.0</data>
      <data key="d5">GPT models are referenced for their ability to perform summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GPT" target="ACHIAM ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Achiam et al. are referenced for their work on GPT in 2023</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GPT" target="BROWN ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Brown et al. are referenced for their work on GPT in 2020</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GPT" target="FOUNDATION MODELS (FMS)">
      <data key="d4">16.0</data>
      <data key="d5">GPT is an example of a Foundation Model used in agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="GPT" target="OPENAI">
      <data key="d4">16.0</data>
      <data key="d5">OpenAI is the organization that developed the GPT Foundation Model</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="LLAMA" target="SUMMARIZATION TASKS">
      <data key="d4">9.0</data>
      <data key="d5">Llama models are referenced for their ability to perform summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="LLAMA" target="TOUVRON ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Touvron et al. are referenced for their work on Llama in 2023</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GEMINI" target="SUMMARIZATION TASKS">
      <data key="d4">9.0</data>
      <data key="d5">Gemini models are referenced for their ability to perform summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GEMINI" target="ANIL ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Anil et al. are referenced for their work on Gemini in 2023</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GEMINI" target="R. ANIL">
      <data key="d4">8.0</data>
      <data key="d5">R. Anil is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="S. BORGEAUD">
      <data key="d4">8.0</data>
      <data key="d5">S. Borgeaud is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="Y. WU">
      <data key="d4">8.0</data>
      <data key="d5">Y. Wu is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="J.-B. ALAYRAC">
      <data key="d4">8.0</data>
      <data key="d5">J.-B. Alayrac is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="J. YU">
      <data key="d4">8.0</data>
      <data key="d5">J. Yu is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="R. SORICUT">
      <data key="d4">8.0</data>
      <data key="d5">R. Soricut is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="J. SCHALKWYK">
      <data key="d4">8.0</data>
      <data key="d5">J. Schalkwyk is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="A. M. DAI">
      <data key="d4">8.0</data>
      <data key="d5">A. M. Dai is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GEMINI" target="A. HAUTH">
      <data key="d4">8.0</data>
      <data key="d5">A. Hauth is an author of the Gemini paper</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="BROWN ET AL." target="CHOWDHERY ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Brown et al. and Chowdhery et al. are co-referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="BROWN ET AL." target="TOUVRON ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Brown et al. and Touvron et al. are co-referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="BROWN ET AL." target="OPENAI">
      <data key="d4">7.0</data>
      <data key="d5">Brown et al. and OpenAI are co-referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="BROWN ET AL." target="IN-CONTEXT LEARNING">
      <data key="d4">16.0</data>
      <data key="d5">Brown et al. contributed to the understanding of in-context learning in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TOUVRON ET AL." target="CHOWDHERY ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Chowdhery et al. and Touvron et al. are co-referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="TOUVRON ET AL." target="OPENAI">
      <data key="d4">7.0</data>
      <data key="d5">Touvron et al. and OpenAI are co-referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="KURATOV ET AL." target="LLM CONTEXT WINDOWS">
      <data key="d4">8.0</data>
      <data key="d5">Kuratov et al. are referenced for their work on LLM context windows in 2024</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="LIU ET AL." target="LLM CONTEXT WINDOWS">
      <data key="d4">8.0</data>
      <data key="d5">Liu et al. are referenced for their work on LLM context windows in 2023</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="LIU ET AL." target="SEARCH ALGORITHMS">
      <data key="d4">16.0</data>
      <data key="d5">Liu et al. explored combining search algorithms with language model agents</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="RAG" target="QUERY-FOCUSED SUMMARIZATION">
      <data key="d4">7.0</data>
      <data key="d5">RAG is mentioned as inadequate for query-focused summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="RAG" target="SEMANTIC SEARCH (SS)">
      <data key="d4">7.0</data>
      <data key="d5">RAG is used in the semantic search approach to retrieve and add text chunks</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="RAG" target="LANGCHAIN">
      <data key="d4">12.0</data>
      <data key="d5">RAG is a building block used in the LangChain framework</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="RAG" target="META AGENT SEARCH">
      <data key="d4">6.0</data>
      <data key="d5">RAG is used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="RAG" target="AGENTINSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct generates data covering the skill of RAG</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="RAG" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B's RAG skill is evaluated to generate informed, contextually precise responses</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="RAG" target="MIRAGE">
      <data key="d4">17.0</data>
      <data key="d5">MIRAGE benchmark is used to evaluate the RAG skill of language models
RAG is used to enhance model performance on MIRAGE datasets</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="NEWMAN" target="GRAPH MODULARITY">
      <data key="d4">8.0</data>
      <data key="d5">Newman is referenced for their work on the modularity of graphs in 2006</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="LOUVAIN" target="BLONDEL ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Blondel et al. are referenced for their work on the Louvain algorithm in 2008</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="HOTPOTQA" target="ENTITY EXTRACTION">
      <data key="d4">8.0</data>
      <data key="d5">HotPotQA is a dataset used to evaluate entity extraction with varying chunk sizes</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="HOTPOTQA" target="YANG ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Yang et al. are referenced for their work on the HotPotQA dataset in 2018
Yang et al. is referenced in the context of the HotPotQA benchmark used in the empirical evaluation of LATS.</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7,93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="HOTPOTQA" target="RAG SYSTEMS">
      <data key="d4">7.0</data>
      <data key="d5">HotPotQA is used to evaluate RAG systems for open-domain question answering.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="HOTPOTQA" target="LANGUAGE AGENT TREE SEARCH (LATS)">
      <data key="d4">8.0</data>
      <data key="d5">LATS was evaluated on the HotPotQA benchmark to demonstrate its effectiveness in decision-making and reasoning.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="HOTPOTQA" target="LATS">
      <data key="d4">65.0</data>
      <data key="d5">LATS was evaluated on the HotPotQA dataset
LATS is evaluated on the HotPotQA benchmark to demonstrate its reasoning and acting capabilities.
LATS achieves high performance on the decision-making setting of HotPotQA
LATS is tested on HotPotQA to demonstrate the effect of each component
LATS is evaluated on the HotPotQA dataset for performance and cost
LATS is evaluated using the HotPotQA dataset
LATS is evaluated on the HotPotQA dataset using metrics such as Exact Match (EM).</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HOTPOTQA" target="YANG ET AL., 2018">
      <data key="d4">8.0</data>
      <data key="d5">Yang et al., 2018 is the reference for the HotPotQA dataset</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="HOTPOTQA" target="GPT-3.5">
      <data key="d4">23.0</data>
      <data key="d5">GPT-3.5 is used to evaluate reasoning-based prompting results on HotpotQA
GPT-3.5 is used to solve question answering tasks in HotPotQA</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf,b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="HOTPOTQA" target="YAO ET AL., 2023B">
      <data key="d4">7.0</data>
      <data key="d5">The setup for HotPotQA follows the method described in the paper by Yao et al. (2023b).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HOTPOTQA" target="SHINN ET AL., 2023">
      <data key="d4">7.0</data>
      <data key="d5">The setup for HotPotQA follows the method described in the paper by Shinn et al. (2023).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HOTPOTQA" target="HAO ET AL., 2023">
      <data key="d4">7.0</data>
      <data key="d5">The setup for HotPotQA follows the method described in the paper by Hao et al. (2023).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HOTPOTQA" target="WEI ET AL., 2022">
      <data key="d4">7.0</data>
      <data key="d5">The setup for HotPotQA follows the method described in the paper by Wei et al. (2022).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HOTPOTQA" target="WANG ET AL., 2022">
      <data key="d4">7.0</data>
      <data key="d5">The setup for HotPotQA follows the method described in the paper by Wang et al. (2022).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HOTPOTQA" target="YAO ET AL., 2023A">
      <data key="d4">1.0</data>
      <data key="d5">The setup for HotPotQA follows the method described in the paper by Yao et al. (2023a).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HOTPOTQA" target="COT">
      <data key="d4">6.0</data>
      <data key="d5">CoT slightly enhances performance on questions requiring reasoning in HotPotQA</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HOTPOTQA" target="REACT">
      <data key="d4">6.0</data>
      <data key="d5">ReAct is used as a prompting method in HotPotQA</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HOTPOTQA" target="TOT">
      <data key="d4">7.0</data>
      <data key="d5">ToT shows larger gains in performance on HotPotQA</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HOTPOTQA" target="RAP">
      <data key="d4">7.0</data>
      <data key="d5">RAP shows larger gains in performance on HotPotQA</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HOTPOTQA" target="REFLEXION">
      <data key="d4">7.0</data>
      <data key="d5">Reflexion is competitive with other methods on HotPotQA</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HOTPOTQA" target="TAB. 2">
      <data key="d4">8.0</data>
      <data key="d5">Tab. 2 shows the performance of internal reasoning and external retrieval strategies on HotPotQA</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HOTPOTQA" target="TAB. 3">
      <data key="d4">8.0</data>
      <data key="d5">Tab. 3 shows the performance of various methods on HotPotQA in different settings</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HOTPOTQA" target="TAB. 10">
      <data key="d4">7.0</data>
      <data key="d5">Tab. 10 shows the cost of different methods on HotPotQA</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="HOTPOTQA" target="CROWDWORKERS">
      <data key="d4">8.0</data>
      <data key="d5">Crowdworkers crafted the question-answer pairs in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="WIKIPEDIA">
      <data key="d4">8.0</data>
      <data key="d5">Wikipedia is the source of the documents used in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="FIG. 4">
      <data key="d4">7.0</data>
      <data key="d5">Figure 4 illustrates an example task of HotPotQA</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="TAB. 8">
      <data key="d4">7.0</data>
      <data key="d5">Table 8 shows the results for HotPotQA</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="TRAJECTORIES">
      <data key="d4">8.0</data>
      <data key="d5">Trajectories refer to the number of paths sampled in the HotPotQA experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="SUPPORTING FACTS">
      <data key="d4">8.0</data>
      <data key="d5">Supporting facts are provided by crowdworkers in the HotPotQA dataset to justify answers</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="ENTITY">
      <data key="d4">8.0</data>
      <data key="d5">Entity is a type of question in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="LOCATION">
      <data key="d4">8.0</data>
      <data key="d5">Location is a type of question in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="DATE">
      <data key="d4">8.0</data>
      <data key="d5">Date is a type of question in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="COMPARISON">
      <data key="d4">8.0</data>
      <data key="d5">Comparison is a type of question in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="SHARED PROPERTIES">
      <data key="d4">8.0</data>
      <data key="d5">Shared properties are compared between two entities in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="SUPPORTING DOCUMENTS">
      <data key="d4">8.0</data>
      <data key="d5">Supporting documents are used to answer questions in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="QUESTION-ANSWER PAIRS">
      <data key="d4">8.0</data>
      <data key="d5">Question-answer pairs are crafted by crowdworkers in the HotPotQA dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="WIKIPEDIA PARAGRAPHS">
      <data key="d4">8.0</data>
      <data key="d5">Wikipedia paragraphs are used in the HotPotQA benchmark setting</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="SUBSET OF 100 QUESTIONS">
      <data key="d4">8.0</data>
      <data key="d5">A randomly selected subset of 100 questions is used in the HotPotQA experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HOTPOTQA" target="MAXIMUM DEPTH LIMIT">
      <data key="d4">8.0</data>
      <data key="d5">Maximum depth limit is set to 6 in the HotPotQA experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="GPT-4-TURBO" target="ENTITY EXTRACTION">
      <data key="d4">9.0</data>
      <data key="d5">GPT-4-Turbo is used for entity extraction in the HotPotQA dataset</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GPT-4-TURBO" target="CONTEXT WINDOW SIZE">
      <data key="d4">8.0</data>
      <data key="d5">Context window size is tested on GPT-4-Turbo</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="MAP-REDUCE" target="QUERY-FOCUSED SUMMARIZATION">
      <data key="d4">8.0</data>
      <data key="d5">Map-Reduce is a technique used for query-focused summarization of an entire corpus</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="MAP-REDUCE" target="TEXT SUMMARIZATION (TS)">
      <data key="d4">8.0</data>
      <data key="d5">Map-Reduce is used in text summarization to shuffle and chunk source texts</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="PODCAST TRANSCRIPTS" target="SENSE-MAKING QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">Podcast transcripts are used to generate activity-centered sense-making questions</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="PODCAST TRANSCRIPTS" target="TECH JOURNALIST">
      <data key="d4">7.0</data>
      <data key="d5">A tech journalist uses podcast transcripts to gain insights and trends in the tech industry.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="PODCAST TRANSCRIPTS" target="KEVIN SCOTT">
      <data key="d4">8.0</data>
      <data key="d5">Kevin Scott is a participant in the podcast conversations included in the podcast transcripts dataset.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="PODCAST TRANSCRIPTS" target="TECH LEADERS">
      <data key="d4">8.0</data>
      <data key="d5">Tech leaders are participants in the podcast conversations included in the podcast transcripts dataset.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NEWS ARTICLES" target="SENSE-MAKING QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">News articles are used to generate activity-centered sense-making questions</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="NEWS ARTICLES" target="EDUCATOR">
      <data key="d4">7.0</data>
      <data key="d5">An educator uses news articles to teach about health and wellness.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NEWS ARTICLES" target="MULTIHOP-RAG">
      <data key="d4">8.0</data>
      <data key="d5">MultiHop-RAG includes news articles as part of its benchmark dataset.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NEWS ARTICLES" target="HEALTH EDUCATION">
      <data key="d4">8.0</data>
      <data key="d5">News articles are used by educators to integrate current topics into health education curricula.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NEWS ARTICLES" target="PREVENTIVE MEDICINE">
      <data key="d4">7.0</data>
      <data key="d5">News articles address the concept of preventive medicine, relevant to health education.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NEWS ARTICLES" target="WELLNESS">
      <data key="d4">7.0</data>
      <data key="d5">News articles address the concept of wellness, relevant to health education.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NEWS ARTICLES" target="PUBLIC HEALTH">
      <data key="d4">7.0</data>
      <data key="d5">News articles provide insights into public health priorities, relevant to health education.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NEWS ARTICLES" target="HEALTH LITERACY">
      <data key="d4">7.0</data>
      <data key="d5">News articles highlight the importance of health literacy, relevant to health education.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="NAIVE RAG" target="GLOBAL SUMMARIZATION">
      <data key="d4">7.0</data>
      <data key="d5">Naive RAG is a baseline technique compared against global summarization approaches</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="NAIVE RAG" target="DIRECTNESS">
      <data key="d4">7.0</data>
      <data key="d5">Naive RAG produces the most direct responses</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GLOBAL MAP-REDUCE" target="GLOBAL SUMMARIZATION">
      <data key="d4">8.0</data>
      <data key="d5">Global Map-Reduce is a summarization technique compared against Naive RAG</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="GLOBAL SUMMARIZATION">
      <data key="d4">9.0</data>
      <data key="d5">Graph RAG Approach is a high-level data flow and pipeline for global summarization</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="TEXT CHUNKS">
      <data key="d4">8.0</data>
      <data key="d5">Text Chunks are segments of source documents used for processing in the Graph RAG approach</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="ELEMENT INSTANCES">
      <data key="d4">8.0</data>
      <data key="d5">Element Instances are graph nodes and edges extracted from text chunks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="ELEMENT INSTANCES" target="ELEMENT SUMMARIES">
      <data key="d4">8.0</data>
      <data key="d5">Element instances are further summarized into element summaries</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="NAMED ENTITIES" target="ENTITY EXTRACTION">
      <data key="d4">2.0</data>
      <data key="d5">Named Entities are broad classes of entities like people, places, and organizations extracted from text</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="NAMED ENTITIES" target="LLM">
      <data key="d4">8.0</data>
      <data key="d5">The LLM is used to extract named entities from text</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="SUMMARIZATION TASKS" target="LLM">
      <data key="d4">9.0</data>
      <data key="d5">LLMs are referenced for their ability to perform summarization tasks</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="GRAPH INDEXING">
      <data key="d4">8.0</data>
      <data key="d5">Entity extraction is used in the graph indexing process</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="ACTIVITY-CENTERED SENSE-MAKING QUESTIONS" target="EVALUATION">
      <data key="d4">8.0</data>
      <data key="d5">Activity-Centered Sense-Making Questions are generated from real-world datasets for evaluation</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="COMPREHENSIVENESS" target="EVALUATION">
      <data key="d4">8.0</data>
      <data key="d5">Comprehensiveness is a target quality for evaluating summarization approaches</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="COMPREHENSIVENESS" target="LLM EVALUATOR">
      <data key="d4">9.0</data>
      <data key="d5">LLM Evaluator assesses answers based on comprehensiveness</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="DIVERSITY" target="EVALUATION">
      <data key="d4">8.0</data>
      <data key="d5">Diversity is a target quality for evaluating summarization approaches</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="DIVERSITY" target="LLM EVALUATOR">
      <data key="d4">9.0</data>
      <data key="d5">LLM Evaluator assesses answers based on diversity</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="DIVERSITY" target="INSTRUCTION REFINEMENT FLOW">
      <data key="d4">8.0</data>
      <data key="d5">Instruction Refinement Flow aims to ensure diversity in the generated instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="EMPOWERMENT" target="EVALUATION">
      <data key="d4">8.0</data>
      <data key="d5">Empowerment is a target quality for evaluating summarization approaches</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="EMPOWERMENT" target="LLM EVALUATOR">
      <data key="d4">9.0</data>
      <data key="d5">LLM Evaluator assesses answers based on empowerment</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="EMPOWERMENT" target="SENSEMAKING">
      <data key="d4">8.0</data>
      <data key="d5">Empowerment is key to helping users reach an informed understanding during sensemaking activities</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="HIERARCHICAL LEVEL" target="EVALUATION">
      <data key="d4">7.0</data>
      <data key="d5">Hierarchical Level is a variable in the evaluation of community summaries</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="TOKEN COSTS" target="EVALUATION">
      <data key="d4">7.0</data>
      <data key="d5">Token Costs are a metric used to evaluate the efficiency of summarization approaches</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="LLM PROMPTS" target="FEW-SHOT EXAMPLES">
      <data key="d4">8.0</data>
      <data key="d5">Few-Shot Examples are used in LLM prompts for in-context learning</data>
      <data key="d6">64476a39d7d8b87b399e3bd3cead79c7</data>
    </edge>
    <edge source="FEW-SHOT EXAMPLES" target="LLM">
      <data key="d4">7.0</data>
      <data key="d5">Few-shot examples are provided to the LLM to improve its performance in specialized domains</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LLM" target="COVARIATES">
      <data key="d4">7.0</data>
      <data key="d5">The LLM extracts covariates associated with detected entities</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LLM" target="LOGIT BIAS">
      <data key="d4">6.0</data>
      <data key="d5">Logit bias is used to guide the LLM in making yes/no decisions during entity extraction</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LLM" target="GLEANINGS">
      <data key="d4">7.0</data>
      <data key="d5">Gleanings involve multiple rounds of extraction by the LLM to detect additional entities</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LLM" target="CLAIMS">
      <data key="d4">7.0</data>
      <data key="d5">The LLM extracts claims linked to detected entities</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LLM" target="FIGURE 2">
      <data key="d4">6.0</data>
      <data key="d5">Figure 2 shows the impact of using larger chunk sizes without a drop in quality or the forced introduction of noise</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LLM" target="COMMUNITY SUMMARIES">
      <data key="d4">9.0</data>
      <data key="d5">The LLM is used to generate community summaries based on the hierarchical community structure.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="LLM" target="GLOBAL ANSWER">
      <data key="d4">9.0</data>
      <data key="d5">The LLM generates the global answer by processing community summaries and user queries.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="LLM" target="DATASET">
      <data key="d4">9.0</data>
      <data key="d5">The LLM generates questions based on the dataset</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="LLM" target="NA&#207;VE RAG">
      <data key="d4">9.0</data>
      <data key="d5">Na&#239;ve RAG is assessed by the LLM for its comprehensiveness, diversity, empowerment, and directnessLLM assesses Na&#239;ve RAG</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="LLM" target="AD-HOC LLM USE">
      <data key="d4">7.0</data>
      <data key="d5">Ad-hoc LLM use involves the spontaneous use of large language models</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LLM" target="RAG APPROACHES AND SYSTEMS">
      <data key="d4">8.0</data>
      <data key="d5">RAG approaches and systems involve using LLMs</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LLM" target="AGENT">
      <data key="d4">7.0</data>
      <data key="d5">Each agent is powered by an LLM and can optionally use tools such as search APIs, code interpreter, or a calculator.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="LLM" target="CONTENT TRANSFORMATION FLOW">
      <data key="d4">7.0</data>
      <data key="d5">An LLM is used in the Content Transformation Flow to hypothesize other APIs present in the library</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="EVALUATION" target="QUESTION GENERATION">
      <data key="d4">8.0</data>
      <data key="d5">The generated questions are evaluated for quality and effectiveness</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="EVALUATION" target="LATS">
      <data key="d4">8.0</data>
      <data key="d5">Evaluation is the third operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="EVALUATION" target="SCALAR VALUE">
      <data key="d4">8.0</data>
      <data key="d5">Scalar value is assigned to each new child node during the evaluation operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="EVALUATION" target="HEURISTIC">
      <data key="d4">8.0</data>
      <data key="d5">Heuristic is used to guide the search algorithm during the evaluation operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="EVALUATION" target="SELF-GENERATED LM SCORE">
      <data key="d4">1.0</data>
      <data key="d5">Self-generated LM score is a component of the value function used in the evaluation operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="EVALUATION" target="AGENT">
      <data key="d4">16.0</data>
      <data key="d5">The agent's progress in task completion is quantified during the evaluation process</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="EVALUATION" target="MATH">
      <data key="d4">1.0</data>
      <data key="d5">Math is another capability evaluated in language models</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="ELEMENT SUMMARIES" target="GRAPH ELEMENT">
      <data key="d4">8.0</data>
      <data key="d5">Element summaries are created for each graph element</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="GRAPH ELEMENT" target="ENTITY GRAPH">
      <data key="d4">8.0</data>
      <data key="d5">Graph elements are part of the entity graph</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="ENTITY GRAPH" target="KNOWLEDGE GRAPHS">
      <data key="d4">6.0</data>
      <data key="d5">Entity graphs are differentiated from typical knowledge graphs</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="GRAPH COMMUNITIES" target="COMMUNITY SUMMARIES">
      <data key="d4">8.0</data>
      <data key="d5">Community summaries are created for each graph community</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="GRAPH COMMUNITIES" target="HIERARCHICAL CLUSTERING">
      <data key="d4">7.0</data>
      <data key="d5">Hierarchical clustering reveals internal structure within graph communities</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="GRAPH COMMUNITIES" target="C0">
      <data key="d4">8.0</data>
      <data key="d5">C0 is the root-level community summaries in Graph RAG</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="GRAPH COMMUNITIES" target="C1">
      <data key="d4">8.0</data>
      <data key="d5">C1 is the high-level community summaries in Graph RAG</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="GRAPH COMMUNITIES" target="C2">
      <data key="d4">8.0</data>
      <data key="d5">C2 is the intermediate-level community summaries in Graph RAG</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="GRAPH COMMUNITIES" target="C3">
      <data key="d4">8.0</data>
      <data key="d5">C3 is the low-level community summaries in Graph RAG</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="LEAF-LEVEL COMMUNITIES">
      <data key="d4">8.0</data>
      <data key="d5">Community summaries are generated from the prioritized elements of leaf-level communities.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="HIGHER-LEVEL COMMUNITIES">
      <data key="d4">8.0</data>
      <data key="d5">Community summaries are generated from the ranked sub-communities of higher-level communities.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="GLOBAL ANSWER">
      <data key="d4">9.0</data>
      <data key="d5">Community summaries are used to generate the global answer to a user query.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="C0">
      <data key="d4">7.0</data>
      <data key="d5">C0 represents root-level community summaries</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="C1">
      <data key="d4">7.0</data>
      <data key="d5">C1 represents intermediate-level community summaries</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="C2">
      <data key="d4">7.0</data>
      <data key="d5">C2 represents intermediate-level community summaries</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="C3">
      <data key="d4">7.0</data>
      <data key="d5">C3 represents low-level community summaries</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="COMMUNITY ANSWERS">
      <data key="d4">7.0</data>
      <data key="d5">Community answers are generated from community summaries</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTIHOP-RAG" target="OPENORD">
      <data key="d4">6.0</data>
      <data key="d5">OpenORD is used for node layout in the visualization of the MultiHop-RAG dataset</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="MULTIHOP-RAG" target="FORCE ATLAS 2">
      <data key="d4">6.0</data>
      <data key="d5">Force Atlas 2 is used for node layout in the visualization of the MultiHop-RAG dataset</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="MULTIHOP-RAG" target="TANG AND YANG, 2024">
      <data key="d4">7.0</data>
      <data key="d5">Tang and Yang's 2024 work discusses the MultiHop-RAG dataset</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="MULTIHOP-RAG" target="TANG AND YANG">
      <data key="d4">1.0</data>
      <data key="d5">Tang and Yang (2024) conducted a study on the MultiHop-RAG benchmark dataset.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="OPENORD" target="MARTIN ET AL., 2011">
      <data key="d4">6.0</data>
      <data key="d5">Martin et al.'s 2011 work discusses the OpenORD tool</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="FORCE ATLAS 2" target="JACOMY ET AL., 2014">
      <data key="d4">1.0</data>
      <data key="d5">Jacomy et al.'s 2014 work discusses the Force Atlas 2 tool</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="HIERARCHICAL CLUSTERING" target="ROOT COMMUNITIES">
      <data key="d4">7.0</data>
      <data key="d5">Root communities are identified in the hierarchical clustering</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="HIERARCHICAL CLUSTERING" target="LEAF-LEVEL COMMUNITIES">
      <data key="d4">7.0</data>
      <data key="d5">Leaf-level communities are identified in the hierarchical clustering</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LEAF-LEVEL COMMUNITIES" target="TOKEN LIMIT">
      <data key="d4">1.0</data>
      <data key="d5">Leaf-level communities are prioritized and added to the LLM context window until the token limit is reached</data>
      <data key="d6">e66ed885a08f92cc69f4895302c33047</data>
    </edge>
    <edge source="LEAF-LEVEL COMMUNITIES" target="ROOT-LEVEL COMMUNITIES">
      <data key="d4">8.0</data>
      <data key="d5">Leaf-level communities are subdivisions within root-level communities, focusing on more granular elements.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="SCIENCE" target="META AGENT SEARCH">
      <data key="d4">27.0</data>
      <data key="d5">Meta Agent Search tests agents in the Science domain
Meta Agent Search is used in the Science domain
Meta Agent Search matches state-of-the-art baselines in Science</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="FIGURE 2" target="LATS">
      <data key="d4">7.0</data>
      <data key="d5">Figure 2 provides an overview of the six operations in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="ROOT-LEVEL COMMUNITIES" target="HIGHER-LEVEL COMMUNITIES">
      <data key="d4">8.0</data>
      <data key="d5">Higher-level communities are broader divisions within root-level communities, summarizing sub-communities.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="GLOBAL ANSWER" target="USER QUERY">
      <data key="d4">9.0</data>
      <data key="d5">The global answer is generated in response to a user query.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="MT-BENCH" target="RAG SYSTEMS">
      <data key="d4">7.0</data>
      <data key="d5">MT-Bench is used to evaluate RAG systems for open-domain question answering.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="MT-BENCH" target="GPT-4">
      <data key="d4">15.0</data>
      <data key="d5">MT-Bench uses GPT-4 as the evaluator for assessing chat assistants
MT-Bench uses GPT-4 to judge each turn's response and provide a score.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba,86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="MT-BENCH" target="OPEN-ENDED GENERATION">
      <data key="d4">7.0</data>
      <data key="d5">MT-Bench is a benchmark under the category of Open-Ended Generation tasks.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="DATA SENSEMAKING" target="KOESTEN ET AL.">
      <data key="d4">6.0</data>
      <data key="d5">Koesten et al. (2021) studied data sensemaking behaviors.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="DATA SENSEMAKING" target="XU AND LAPATA">
      <data key="d4">1.0</data>
      <data key="d5">Xu and Lapata (2021) studied methods for extracting latent summarization queries, relevant to data sensemaking.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="TECH LEADERS" target="PRIVACY LAWS">
      <data key="d4">7.0</data>
      <data key="d5">Tech leaders discuss the impact of privacy laws on technology development in the podcast transcripts.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="TECH LEADERS" target="POLICY AND REGULATION">
      <data key="d4">7.0</data>
      <data key="d5">Tech leaders discuss the role of policy and regulation in the tech industry in the podcast transcripts.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="TECH LEADERS" target="COLLABORATIONS">
      <data key="d4">7.0</data>
      <data key="d5">Tech leaders discuss collaborations between tech companies and governments in the podcast transcripts.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="INNOVATION" target="ETHICAL CONSIDERATIONS">
      <data key="d4">7.0</data>
      <data key="d5">Innovation and ethical considerations are topics discussed together by guests in the podcast transcripts.</data>
      <data key="d6">4930fce6da868f894757a9da465807ba</data>
    </edge>
    <edge source="DATASET" target="TEXT SUMMARIZATION (TS)">
      <data key="d4">7.0</data>
      <data key="d5">Text Summarization applies map-reduce approach to source texts in the dataset</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="DATASET" target="SEMANTIC SEARCH (SS)">
      <data key="d4">7.0</data>
      <data key="d5">Semantic Search retrieves text chunks from the dataset</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="TEXT SUMMARIZATION (TS)" target="CONTEXT WINDOW">
      <data key="d4">7.0</data>
      <data key="d5">Text Summarization uses a context window to generate answers</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="SEMANTIC SEARCH (SS)" target="CONTEXT WINDOW">
      <data key="d4">7.0</data>
      <data key="d5">Semantic Search uses a context window to generate answers</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="CONTEXT WINDOW" target="PODCAST DATASET">
      <data key="d4">6.0</data>
      <data key="d5">Podcast dataset is indexed with a context window size of 600 tokens</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="CONTEXT WINDOW" target="NEWS DATASET">
      <data key="d4">6.0</data>
      <data key="d5">News dataset is indexed with a context window size of 600 tokens</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="CONTEXT WINDOW" target="TOKEN">
      <data key="d4">7.0</data>
      <data key="d5">Tokens are units of text used in the context window</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="DIRECTNESS" target="LLM EVALUATOR">
      <data key="d4">9.0</data>
      <data key="d5">LLM Evaluator assesses answers based on directness</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="LLM EVALUATOR" target="QUESTION">
      <data key="d4">9.0</data>
      <data key="d5">The LLM evaluator is provided with the question to assess the quality of answers</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="LLM EVALUATOR" target="ANSWER">
      <data key="d4">9.0</data>
      <data key="d5">The LLM evaluator assesses the quality of the answer</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="LLM EVALUATOR" target="ASSESSMENT">
      <data key="d4">9.0</data>
      <data key="d5">The LLM evaluator performs the assessment of answers</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="LLM EVALUATOR" target="STOCHASTICITY">
      <data key="d4">7.0</data>
      <data key="d5">Stochasticity is accounted for in the LLM evaluator's assessments</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="PUBLIC FIGURES" target="ENTERTAINMENT INDUSTRY">
      <data key="d4">8.0</data>
      <data key="d5">Public figures are repeatedly mentioned in various entertainment articles</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="PUBLIC FIGURES" target="ACTORS AND DIRECTORS">
      <data key="d4">8.0</data>
      <data key="d5">Actors and directors are public figures in the entertainment industry</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="PUBLIC FIGURES" target="MUSICIANS AND EXECUTIVES">
      <data key="d4">8.0</data>
      <data key="d5">Musicians and executives are public figures in the entertainment industry</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="PUBLIC FIGURES" target="ATHLETES AND COACHES">
      <data key="d4">8.0</data>
      <data key="d5">Athletes and coaches are public figures in the entertainment industry</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="PUBLIC FIGURES" target="INFLUENCERS AND ENTREPRENEURS">
      <data key="d4">1.0</data>
      <data key="d5">Influencers and entrepreneurs are public figures in the entertainment industry</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="ACTIVITY-CENTERED APPROACH" target="QUESTION GENERATION">
      <data key="d4">9.0</data>
      <data key="d5">The activity-centered approach is used to automate the generation of questions</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="RELATIONSHIP EXTRACTION" target="GRAPH INDEXING">
      <data key="d4">8.0</data>
      <data key="d5">Relationship extraction is used in the graph indexing process</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="TOKEN" target="LM">
      <data key="d4">16.0</data>
      <data key="d5">Tokens are the basic elements of natural language used in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="QUESTION" target="THOUGHT">
      <data key="d4">14.0</data>
      <data key="d5">The thought process involves reasoning about how to answer the question</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="QUESTION" target="ARTHUR&#8217;S MAGAZINE">
      <data key="d4">18.0</data>
      <data key="d5">Arthur&#8217;s Magazine is part of the question "Which magazine was started first Arthur&#8217;s Magazine or First for Women?"</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="QUESTION" target="FIRST FOR WOMEN">
      <data key="d4">18.0</data>
      <data key="d5">First for Women is part of the question "Which magazine was started first Arthur&#8217;s Magazine or First for Women?"</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="QUESTION" target="STUDENT RESPONSE">
      <data key="d4">17.0</data>
      <data key="d5">The student response is given in response to the question
The student response is an answer to the given question</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50,5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="QUESTION" target="OPTIONS">
      <data key="d4">8.0</data>
      <data key="d5">Options are the set of possible answers provided for the question</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="ANSWER" target="COT_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The cot_module generates the answer</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ANSWER" target="CRITIC_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The critic_module reviews the answer</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ANSWER" target="FINAL_CODE">
      <data key="d4">9.0</data>
      <data key="d5">Answer is generated from the final code</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="ANSWER" target="INTEGRATION MODULE">
      <data key="d4">9.0</data>
      <data key="d5">The Integration Module combines sub-solutions to form the final answer</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="STOCHASTICITY" target="MEAN SCORES">
      <data key="d4">7.0</data>
      <data key="d5">Mean scores are used to account for the stochasticity of LLMs</data>
      <data key="d6">26b2dad01a219bc034ac7d6a32d07582</data>
    </edge>
    <edge source="PUBLIC FIGURES IN CONTROVERSY" target="TAYLOR SWIFT">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift is frequently mentioned in the media due to her high-profile status and public interest in her career and personal life</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="PUBLIC FIGURES IN CONTROVERSY" target="TRAVIS KELCE">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce is frequently mentioned in the media due to his high-profile status and public interest in his career and personal life</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="PUBLIC FIGURES IN CONTROVERSY" target="BRITNEY SPEARS">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears is frequently mentioned in the media due to her high-profile status and public interest in her career and personal life</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="PUBLIC FIGURES IN CONTROVERSY" target="JUSTIN TIMBERLAKE">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake is frequently mentioned in the media due to his high-profile status and public interest in his career and personal life</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="PUBLIC FIGURES IN CONTROVERSY" target="ENTERTAINMENT ARTICLES">
      <data key="d4">9.0</data>
      <data key="d5">Entertainment articles frequently mention public figures involved in controversies</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="PUBLIC FIGURES IN CONTROVERSY" target="MEDIA COVERAGE">
      <data key="d4">9.0</data>
      <data key="d5">Public figures in controversy receive significant media coverage</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="PUBLIC FIGURES IN CONTROVERSY" target="NA&#207;VE RAG">
      <data key="d4">8.0</data>
      <data key="d5">Na&#239;ve RAG includes public figures involved in controversies</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="MUSICIANS" target="TAYLOR SWIFT">
      <data key="d4">10.0</data>
      <data key="d5">Taylor Swift is a musician</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="MUSICIANS" target="BRITNEY SPEARS">
      <data key="d4">10.0</data>
      <data key="d5">Britney Spears is a musician</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="MUSICIANS" target="JUSTIN TIMBERLAKE">
      <data key="d4">1.0</data>
      <data key="d5">Justin Timberlake is a musician</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="ATHLETES" target="TRAVIS KELCE">
      <data key="d4">10.0</data>
      <data key="d5">Travis Kelce is an athlete</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="ENTERTAINMENT ARTICLES">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift is frequently mentioned in entertainment articles</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="MEDIA COVERAGE">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift receives significant media coverage</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="PROFESSIONAL ACHIEVEMENTS">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift is highlighted for her professional achievements</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="PERSONAL LIVES">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift's personal life is of public interest</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="CULTURAL IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift has a significant cultural impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="ECONOMIC IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift has a significant economic impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="MEDIA REACTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Taylor Swift's activities generate media reactions</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TAYLOR SWIFT" target="NA&#207;VE RAG">
      <data key="d4">8.0</data>
      <data key="d5">Na&#239;ve RAG mentions Taylor Swift</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="ENTERTAINMENT ARTICLES">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce is frequently mentioned in entertainment articles</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="MEDIA COVERAGE">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce receives significant media coverage</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="PROFESSIONAL ACHIEVEMENTS">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce is highlighted for his professional achievements</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="PERSONAL LIVES">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce's personal life is of public interest</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="CULTURAL IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce has a significant cultural impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="ECONOMIC IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce has a significant economic impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="MEDIA REACTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Travis Kelce's activities generate media reactions</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="TRAVIS KELCE" target="NA&#207;VE RAG">
      <data key="d4">8.0</data>
      <data key="d5">Na&#239;ve RAG mentions Travis Kelce</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="ENTERTAINMENT ARTICLES">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears is frequently mentioned in entertainment articles</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="MEDIA COVERAGE">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears receives significant media coverage</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="PROFESSIONAL ACHIEVEMENTS">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears is highlighted for her professional achievements</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="PERSONAL LIVES">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears's personal life is of public interest</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="CULTURAL IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears has a significant cultural impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="ECONOMIC IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears has a significant economic impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="MEDIA REACTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Britney Spears's activities generate media reactions</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="BRITNEY SPEARS" target="NA&#207;VE RAG">
      <data key="d4">8.0</data>
      <data key="d5">Na&#239;ve RAG mentions Britney Spears</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="ENTERTAINMENT ARTICLES">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake is frequently mentioned in entertainment articles</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="MEDIA COVERAGE">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake receives significant media coverage</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="PROFESSIONAL ACHIEVEMENTS">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake is highlighted for his professional achievements</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="PERSONAL LIVES">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake's personal life is of public interest</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="CULTURAL IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake has a significant cultural impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="ECONOMIC IMPACT">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake has a significant economic impact</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="MEDIA REACTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Justin Timberlake's activities generate media reactions</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="JUSTIN TIMBERLAKE" target="NA&#207;VE RAG">
      <data key="d4">8.0</data>
      <data key="d5">Na&#239;ve RAG mentions Justin Timberlake</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="NA&#207;VE RAG" target="ENTERTAINMENT ARTICLES">
      <data key="d4">8.0</data>
      <data key="d5">Na&#239;ve RAG provides a list of public figures mentioned in entertainment articles</data>
      <data key="d6">c8e8019de153e439d6a79dcf209b943b</data>
    </edge>
    <edge source="NA&#207;VE RAG" target="GAO ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Gao et al., 2023 is a study on Na&#239;ve RAG</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="CONTEXT WINDOW SIZE" target="KURATOV ET AL., 2024">
      <data key="d4">6.0</data>
      <data key="d5">Kuratov et al. discuss the potential for information to be lost in the middle of longer contexts</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="CONTEXT WINDOW SIZE" target="LIU ET AL., 2023">
      <data key="d4">6.0</data>
      <data key="d5">Liu et al. discuss the potential for information to be lost in the middle of longer contexts</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="LIU ET AL., 2023" target="DYLAN">
      <data key="d4">8.0</data>
      <data key="d5">Liu et al., 2023 discusses DyLAN and its application in using FMs to score the response quality of nodes in each layer to prune the connections</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="GPT-4" target="HALLUCINATION JUDGE">
      <data key="d4">15.0</data>
      <data key="d5">GPT-4 is used in the Hallucination Judge process to evaluate generated summaries
GPT-4 is used in the Hallucination Judge process to evaluate generated summaries.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GPT-4" target="LANGUAGE AGENT TREE SEARCH (LATS)">
      <data key="d4">9.0</data>
      <data key="d5">LATS uses GPT-4 in its experimental evaluation, achieving state-of-the-art pass@1 accuracy for programming on HumanEval.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="GPT-4" target="LATS">
      <data key="d4">18.0</data>
      <data key="d5">LATS was used with GPT-4 to achieve a 92.7 Pass@1 rate on HumanEval
LATS is used with GPT-4 to achieve high performance in various benchmarks, including HotPotQA and HumanEval.</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-4" target="REACT">
      <data key="d4">8.0</data>
      <data key="d5">ReAct is used with GPT-4 to enhance reasoning and acting capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-4" target="REFLEXION">
      <data key="d4">8.0</data>
      <data key="d5">Reflexion is used with GPT-4 to enhance reasoning capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-4" target="HUMANEVAL">
      <data key="d4">34.0</data>
      <data key="d5">GPT-4 is evaluated on the HumanEval benchmark to measure its Pass@1 accuracy.
GPT-4 is used to set the state of the art for HumanEval with LATS
GPT-4 is used to evaluate performance on the HumanEval dataset.</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc,fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-4" target="META AGENT SEARCH">
      <data key="d4">41.0</data>
      <data key="d5">Meta Agent Search transfers discovered agents from GPT-3.5 to GPT-4
Meta Agent Search uses GPT-4 as the meta agent
Meta Agent Search uses GPT-4 as the language model for the meta agent
Meta Agent Search uses GPT-4 to evaluate the performance of discovered agents</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="GPT-4" target="OPENAI">
      <data key="d4">1.0</data>
      <data key="d5">OpenAI is the organization behind the GPT-4 model</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="GPT-4" target="SYNTHETIC DATA">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4 is used to generate responses to prompts for creating synthetic data</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="GPT-4" target="AGENTINSTRUCT">
      <data key="d4">16.0</data>
      <data key="d5">AgentInstruct uses GPT-4 to generate high-quality data</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="GPT-4" target="ORCA-BENCH">
      <data key="d4">10.0</data>
      <data key="d5">GPT-4 is used as a baseline for scoring the performance of models on the Orca-Bench dataset
Orca-Bench scores are evaluated relative to GPT-4, which is used as the benchmark with a score of 10</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="ORCA-3">
      <data key="d4">15.0</data>
      <data key="d5">Orca-3 is evaluated against GPT-4 on various benchmarks
Orca-3's performance is compared to GPT-4 in various benchmarks</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="AGIEVAL">
      <data key="d4">7.0</data>
      <data key="d5">AGIEval is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="MMLU">
      <data key="d4">7.0</data>
      <data key="d5">MMLU is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="ARC">
      <data key="d4">7.0</data>
      <data key="d5">ARC is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="BBH">
      <data key="d4">7.0</data>
      <data key="d5">BBH is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="GPQA">
      <data key="d4">7.0</data>
      <data key="d5">GPQA is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="DROP">
      <data key="d4">7.0</data>
      <data key="d5">DROP is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="GSM8K">
      <data key="d4">7.0</data>
      <data key="d5">GSM8K is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="FOFO">
      <data key="d4">15.0</data>
      <data key="d5">FOFO is a benchmark used to evaluate the performance of GPT-4
FOFO uses GPT-4 to evaluate the format correctness of model responses.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="BENCHMARK RESULTS">
      <data key="d4">7.0</data>
      <data key="d5">Benchmark results section evaluates GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="METRIC-V2">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v2 is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="METRIC-V1">
      <data key="d4">1.0</data>
      <data key="d5">Metric-v1 is a benchmark used to evaluate the performance of GPT-4</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-4" target="ORCA-3-7B">
      <data key="d4">17.0</data>
      <data key="d5">Orca-3-7B's performance is compared to GPT-4 in various benchmarks
GPT-4 is used to evaluate the summarization and hallucination rates of Orca-3-7B</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="GPT-4" target="APPENDIX B">
      <data key="d4">6.0</data>
      <data key="d5">Appendix B contains the prompts used by GPT-4 for evaluating summarization abilities</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="GPT-4" target="CO-T">
      <data key="d4">1.0</data>
      <data key="d5">GPT-4 uses the CoT skill for medical question answering in the MIRAGE benchmark</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="GPT-4" target="MIRAGE">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4 is used in the evaluation of MIRAGE datasets</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="GPT-4" target="AZURE">
      <data key="d4">14.0</data>
      <data key="d5">Azure is recommended for reviewing transparency notes related to GPT-4</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="GPT-4" target="MULTIPLE CHOICE QUESTIONS">
      <data key="d4">9.0</data>
      <data key="d5">GPT-4 is used for extracting the option selected by the model from the model&#8217;s response in multiple choice questions</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="GPT-4" target="EVALUATOR ASSISTANT">
      <data key="d4">1.0</data>
      <data key="d5">GPT-4 assists the Evaluator Assistant in parsing student responses</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="GPT-4" target="SYSTEM MESSAGE">
      <data key="d4">9.0</data>
      <data key="d5">The system message is used to guide GPT-4 in extracting student responses</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="GPT-4" target="EXACT MATCH/SPAN EXTRACTION PROBLEMS">
      <data key="d4">9.0</data>
      <data key="d5">GPT-4 is used to extract exact answers and match them with ground-truth answers in exact match/span extraction problems</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="GPT-4" target="EQBENCH">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4 is used to extract and calibrate emotion scores in EQBench tasks</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="GPT-4" target="IFEVAL">
      <data key="d4">8.0</data>
      <data key="d5">IFEval uses GPT-4 to check if the model response follows verifiable instructions.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="GPT-4" target="ALPACAEVAL">
      <data key="d4">8.0</data>
      <data key="d5">AlpacaEval uses GPT-4-turbo to measure win-rates by comparing model outputs.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="GPT-4" target="INFOBENCH">
      <data key="d4">8.0</data>
      <data key="d5">InfoBench uses GPT-4 to determine if the model response follows decomposed instructions.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="GPT-4" target="QUALITY JUDGE">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4 is used in the Quality Judge process to evaluate the quality of AI-generated responses.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="GPT-4" target="HALLUCINATION DETECTION">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4 is used in the Hallucination Detection process to evaluate AI-generated summaries.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="GPT-4" target="SUMMARIZATION QUALITY">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4 is used in the Summarization Quality process to evaluate the quality of AI-generated summaries.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="HALLUCINATION JUDGE" target="PROMPT TEMPLATE">
      <data key="d4">8.0</data>
      <data key="d5">The Hallucination Judge process uses a specific prompt template to evaluate hallucination in AI-generated summaries.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="KOESTEN, L." target="GREGORY, K.">
      <data key="d4">24.0</data>
      <data key="d5">Koesten, L. and Gregory, K. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"
Koesten, L. and Gregory, K. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="KOESTEN, L." target="GROTH, P.">
      <data key="d4">16.0</data>
      <data key="d5">Koesten, L. and Groth, P. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"
Koesten, L. and Groth, P. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="KOESTEN, L." target="SIMPERL, E.">
      <data key="d4">16.0</data>
      <data key="d5">Koesten, L. and Simperl, E. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"
Koesten, L. and Simperl, E. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GREGORY, K." target="GROTH, P.">
      <data key="d4">16.0</data>
      <data key="d5">Gregory, K. and Groth, P. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"
Gregory, K. and Groth, P. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GREGORY, K." target="SIMPERL, E.">
      <data key="d4">16.0</data>
      <data key="d5">Gregory, K. and Simperl, E. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"
Gregory, K. and Simperl, E. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GROTH, P." target="SIMPERL, E.">
      <data key="d4">16.0</data>
      <data key="d5">Groth, P. and Simperl, E. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"
Groth, P. and Simperl, E. co-authored the paper "Talking datasets&#8211;understanding data sensemaking behaviours"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="KURATOV, Y." target="BULATOV, A.">
      <data key="d4">16.0</data>
      <data key="d5">Kuratov, Y. and Bulatov, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"
Kuratov, Y. and Bulatov, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="KURATOV, Y." target="ANOKHIN, P.">
      <data key="d4">16.0</data>
      <data key="d5">Kuratov, Y. and Anokhin, P. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"
Kuratov, Y. and Anokhin, P. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="KURATOV, Y." target="SOROKIN, D.">
      <data key="d4">16.0</data>
      <data key="d5">Kuratov, Y. and Sorokin, D. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"
Kuratov, Y. and Sorokin, D. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="KURATOV, Y." target="SOROKIN, A.">
      <data key="d4">16.0</data>
      <data key="d5">Kuratov, Y. and Sorokin, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"
Kuratov, Y. and Sorokin, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="KURATOV, Y." target="BURTSEV, M.">
      <data key="d4">16.0</data>
      <data key="d5">Kuratov, Y. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"
Kuratov, Y. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="BULATOV, A." target="ANOKHIN, P.">
      <data key="d4">16.0</data>
      <data key="d5">Bulatov, A. and Anokhin, P. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"
Bulatov, A. and Anokhin, P. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">df50c95dff7da074cbb2f68e88686f88,ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="BULATOV, A." target="SOROKIN, D.">
      <data key="d4">8.0</data>
      <data key="d5">Bulatov, A. and Sorokin, D. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="BULATOV, A." target="SOROKIN, A.">
      <data key="d4">8.0</data>
      <data key="d5">Bulatov, A. and Sorokin, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="BULATOV, A." target="BURTSEV, M.">
      <data key="d4">8.0</data>
      <data key="d5">Bulatov, A. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="ANOKHIN, P." target="SOROKIN, D.">
      <data key="d4">8.0</data>
      <data key="d5">Anokhin, P. and Sorokin, D. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="ANOKHIN, P." target="SOROKIN, A.">
      <data key="d4">8.0</data>
      <data key="d5">Anokhin, P. and Sorokin, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="ANOKHIN, P." target="BURTSEV, M.">
      <data key="d4">8.0</data>
      <data key="d5">Anokhin, P. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="SOROKIN, D." target="SOROKIN, A.">
      <data key="d4">8.0</data>
      <data key="d5">Sorokin, D. and Sorokin, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="SOROKIN, D." target="BURTSEV, M.">
      <data key="d4">8.0</data>
      <data key="d5">Sorokin, D. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="SOROKIN, A." target="BURTSEV, M.">
      <data key="d4">8.0</data>
      <data key="d5">Sorokin, A. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="LANGCHAIN" target="LLAMAINDEX">
      <data key="d4">7.0</data>
      <data key="d5">LangChain and LlamaIndex both support various graph databases and graph-based RAG applications</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LANGCHAIN" target="NEO4J">
      <data key="d4">7.0</data>
      <data key="d5">LangChain supports the Neo4J graph database</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LANGCHAIN" target="NEBULAGRAPH">
      <data key="d4">7.0</data>
      <data key="d5">LangChain supports the NebulaGraph graph database</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LANGCHAIN" target="GRAPH DATABASES">
      <data key="d4">7.0</data>
      <data key="d5">LangChain supports various graph databases</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LANGCHAIN" target="GRAPH-BASED RAG APPLICATIONS">
      <data key="d4">7.0</data>
      <data key="d5">LangChain supports graph-based RAG applications</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LANGCHAIN" target="SEARCH SPACE">
      <data key="d4">14.0</data>
      <data key="d5">LangChain is an open-source agent framework that can be used within the search space</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="LANGCHAIN" target="SEARCH ENGINE TOOLS">
      <data key="d4">1.0</data>
      <data key="d5">Search engine tools are components that can be used in open-source agent frameworks like LangChain in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="LASKAR, M. T. R." target="HOQUE, E.">
      <data key="d4">8.0</data>
      <data key="d5">Laskar, M. T. R. and Hoque, E. co-authored the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="LASKAR, M. T. R." target="HUANG, J.">
      <data key="d4">8.0</data>
      <data key="d5">Laskar, M. T. R. and Huang, J. co-authored the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="HOQUE, E." target="HUANG, J.">
      <data key="d4">1.0</data>
      <data key="d5">Hoque, E. and Huang, J. co-authored the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"</data>
      <data key="d6">ede7063998065122cf7a7152979c1909</data>
    </edge>
    <edge source="GENERATION-AUGMENTED RETRIEVAL (GAR)" target="MAO ET AL., 2020">
      <data key="d4">8.0</data>
      <data key="d5">Mao et al., 2020 is a study on generation-augmented retrieval</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="SELF-MEMORY (SELFMEM)" target="CHENG ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Cheng et al., 2024 is a study on self-memory</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="ITERATIVE RETRIEVAL-GENERATION (ITER-RETGEN)" target="SHAO ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Shao et al., 2023 is a study on iterative retrieval-generation</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="FEDERATED RETRIEVAL-GENERATION (FEB4RAG)" target="WANG ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Wang et al., 2024 is a study on federated retrieval-generation</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTI-DOCUMENT SUMMARIZATION" target="SU ET AL., 2020">
      <data key="d4">8.0</data>
      <data key="d5">Su et al., 2020 is a study on multi-document summarization</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTI-HOP QUESTION ANSWERING" target="MULTI-HOP QUESTION ANSWERING (ITRG)">
      <data key="d4">8.0</data>
      <data key="d5">ITRG is a system for multi-hop question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTI-HOP QUESTION ANSWERING" target="MULTI-HOP QUESTION ANSWERING (IR-COT)">
      <data key="d4">8.0</data>
      <data key="d5">IR-CoT is a system for multi-hop question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTI-HOP QUESTION ANSWERING" target="MULTI-HOP QUESTION ANSWERING (DSP)">
      <data key="d4">8.0</data>
      <data key="d5">DSP is a system for multi-hop question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTI-HOP QUESTION ANSWERING" target="FENG ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Feng et al., 2023 is a study on multi-hop question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTI-HOP QUESTION ANSWERING" target="TRIVEDI ET AL., 2022">
      <data key="d4">8.0</data>
      <data key="d5">Trivedi et al., 2022 is a study on multi-hop question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="MULTI-HOP QUESTION ANSWERING" target="KHATTAB ET AL., 2022">
      <data key="d4">8.0</data>
      <data key="d5">Khattab et al., 2022 is a study on multi-hop question answering</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="HIERARCHICAL INDEX" target="HIERARCHICAL INDEX OF TEXT CHUNKS">
      <data key="d4">8.0</data>
      <data key="d5">A hierarchical index of text chunks is a type of hierarchical index</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="HIERARCHICAL INDEX" target="SARTHI ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Sarthi et al., 2024 is a study on hierarchical index</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LLAMAINDEX" target="NEO4J">
      <data key="d4">7.0</data>
      <data key="d5">LlamaIndex supports the Neo4J graph database</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LLAMAINDEX" target="NEBULAGRAPH">
      <data key="d4">7.0</data>
      <data key="d5">LlamaIndex supports the NebulaGraph graph database</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LLAMAINDEX" target="GRAPH DATABASES">
      <data key="d4">7.0</data>
      <data key="d5">LlamaIndex supports various graph databases</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="LLAMAINDEX" target="GRAPH-BASED RAG APPLICATIONS">
      <data key="d4">7.0</data>
      <data key="d5">LlamaIndex supports graph-based RAG applications</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="NEO4J" target="GRAPH-BASED RAG APPLICATIONS">
      <data key="d4">8.0</data>
      <data key="d5">Neo4J is a format used in graph-based RAG applications</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="NEO4J" target="NALLM">
      <data key="d4">8.0</data>
      <data key="d5">NaLLM uses Neo4J format for creating and reasoning over knowledge graphs</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="SENSEMAKING" target="SENSEMAKING ACTIVITY">
      <data key="d4">8.0</data>
      <data key="d5">Sensemaking activity involves sensemaking</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="TREE OF CLARIFICATIONS" target="KIM ET AL., 2023">
      <data key="d4">1.0</data>
      <data key="d5">Kim et al., 2023 is a study on tree of clarifications</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPHRAG" target="NEBULA-GRAPH">
      <data key="d4">8.0</data>
      <data key="d5">GraphRAG uses Nebula-Graph format for creating and reasoning over knowledge graphs</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="TRAJANOSKA ET AL., 2023" target="YAO ET AL., 2023">
      <data key="d4">7.0</data>
      <data key="d5">Both studies focus on using LLMs for knowledge graph creation and completion</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="BAN ET AL., 2023" target="ZHANG ET AL., 2024">
      <data key="d4">7.0</data>
      <data key="d5">Both studies focus on using LLMs for the extraction of causal graphs</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="BAEK ET AL., 2023" target="HE ET AL., 2024">
      <data key="d4">7.0</data>
      <data key="d5">Both studies focus on advanced RAG methods</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="ZHANG, 2023" target="KANG ET AL., 2023">
      <data key="d4">7.0</data>
      <data key="d5">Both studies focus on advanced RAG methods</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="RANADE AND JOSHI, 2023" target="WANG ET AL., 2023B">
      <data key="d4">7.0</data>
      <data key="d5">Both studies focus on advanced RAG methods</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="WANG ET AL., 2023B" target="SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT (COT-SC)">
      <data key="d4">14.0</data>
      <data key="d5">Self-Consistency with Chain-of-Thought (COT-SC) was introduced by Wang et al., 2023b
Wang et al., 2023b is a publication referenced for the Self-Consistency with Chain-of-Thought (COT-SC) method</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="WANG ET AL., 2023B" target="COT-SC">
      <data key="d4">14.0</data>
      <data key="d5">Wang et al., 2023b discusses the COT-SC method
The publication by Wang et al. in 2023 is related to COT-SC</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="RAM ET AL., 2023" target="RAG APPROACHES AND SYSTEMS">
      <data key="d4">8.0</data>
      <data key="d5">Ram et al., 2023 is a study on RAG approaches and systems</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GAO ET AL., 2023" target="GSM-HARD">
      <data key="d4">8.0</data>
      <data key="d5">Gao et al., 2023 discusses the GSM-Hard dataset</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="WANG ET AL., 2024" target="META AGENT SEARCH">
      <data key="d4">16.0</data>
      <data key="d5">Wang et al., 2024 discusses the effectiveness of Meta Agent Search</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="AD-HOC LLM USE" target="QUOTES">
      <data key="d4">7.0</data>
      <data key="d5">Quotes are used in ad-hoc LLM use</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="AD-HOC LLM USE" target="CITATIONS">
      <data key="d4">7.0</data>
      <data key="d5">Citations are used in ad-hoc LLM use</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="AD-HOC LLM USE" target="EXAMPLES">
      <data key="d4">7.0</data>
      <data key="d5">Examples are used in ad-hoc LLM use</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="RAG APPROACHES AND SYSTEMS" target="PRE-RETRIEVAL STRATEGIES">
      <data key="d4">7.0</data>
      <data key="d5">Pre-retrieval strategies are part of RAG approaches and systems</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="RAG APPROACHES AND SYSTEMS" target="POST-RETRIEVAL STRATEGIES">
      <data key="d4">7.0</data>
      <data key="d5">Post-retrieval strategies are part of RAG approaches and systems</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="TEXT-RELATIONSHIP GRAPHS" target="GRAPH DATABASES">
      <data key="d4">7.0</data>
      <data key="d5">Text-relationship graphs are supported by graph databases</data>
      <data key="d6">edab4014b8f55e5b25bd7f396314be1f</data>
    </edge>
    <edge source="GRAPH-BASED RAG APPLICATIONS" target="NEBULA-GRAPH">
      <data key="d4">8.0</data>
      <data key="d5">Nebula-Graph is a format used in graph-based RAG applications</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="ARXIV">
      <data key="d4">8.0</data>
      <data key="d5">arXiv is the repository where the GPT-4 technical report is published</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="J. ACHIAM">
      <data key="d4">8.0</data>
      <data key="d5">J. Achiam is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="S. ADLER">
      <data key="d4">8.0</data>
      <data key="d5">S. Adler is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="S. AGARWAL">
      <data key="d4">8.0</data>
      <data key="d5">S. Agarwal is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="L. AHMAD">
      <data key="d4">8.0</data>
      <data key="d5">L. Ahmad is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="I. AKKAYA">
      <data key="d4">8.0</data>
      <data key="d5">I. Akkaya is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="F. L. ALEMAN">
      <data key="d4">8.0</data>
      <data key="d5">F. L. Aleman is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="D. ALMEIDA">
      <data key="d4">8.0</data>
      <data key="d5">D. Almeida is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="J. ALTENSCHMIDT">
      <data key="d4">8.0</data>
      <data key="d5">J. Altenschmidt is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="S. ALTMAN">
      <data key="d4">8.0</data>
      <data key="d5">S. Altman is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="GPT-4 TECHNICAL REPORT" target="S. ANADKAT">
      <data key="d4">8.0</data>
      <data key="d5">S. Anadkat is an author of the GPT-4 technical report</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="KNOWLEDGE-AUGMENTED LANGUAGE MODEL PROMPTING" target="J. BAEK">
      <data key="d4">8.0</data>
      <data key="d5">J. Baek is an author of the paper on knowledge-augmented language model prompting</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="KNOWLEDGE-AUGMENTED LANGUAGE MODEL PROMPTING" target="A. F. AJI">
      <data key="d4">8.0</data>
      <data key="d5">A. F. Aji is an author of the paper on knowledge-augmented language model prompting</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="KNOWLEDGE-AUGMENTED LANGUAGE MODEL PROMPTING" target="A. SAFFARI">
      <data key="d4">8.0</data>
      <data key="d5">A. Saffari is an author of the paper on knowledge-augmented language model prompting</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="QUERY TOOLS" target="T. BAN">
      <data key="d4">8.0</data>
      <data key="d5">T. Ban is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="QUERY TOOLS" target="L. CHEN">
      <data key="d4">8.0</data>
      <data key="d5">L. Chen is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="QUERY TOOLS" target="X. WANG">
      <data key="d4">8.0</data>
      <data key="d5">X. Wang is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="QUERY TOOLS" target="H. CHEN">
      <data key="d4">8.0</data>
      <data key="d5">H. Chen is an author of the paper on harnessing large language models for advanced causal discovery</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="QUERY FOCUSED ABSTRACTIVE SUMMARIZATION" target="T. BAUMEL">
      <data key="d4">8.0</data>
      <data key="d5">T. Baumel is an author of the paper on query focused abstractive summarization</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="QUERY FOCUSED ABSTRACTIVE SUMMARIZATION" target="M. EYAL">
      <data key="d4">8.0</data>
      <data key="d5">M. Eyal is an author of the paper on query focused abstractive summarization</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="QUERY FOCUSED ABSTRACTIVE SUMMARIZATION" target="M. ELHADAD">
      <data key="d4">8.0</data>
      <data key="d5">M. Elhadad is an author of the paper on query focused abstractive summarization</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="FAST UNFOLDING OF COMMUNITIES" target="V. D. BLONDEL">
      <data key="d4">8.0</data>
      <data key="d5">V. D. Blondel is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="FAST UNFOLDING OF COMMUNITIES" target="J.-L. GUILLAUME">
      <data key="d4">8.0</data>
      <data key="d5">J.-L. Guillaume is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="FAST UNFOLDING OF COMMUNITIES" target="R. LAMBIOTTE">
      <data key="d4">8.0</data>
      <data key="d5">R. Lambiotte is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="FAST UNFOLDING OF COMMUNITIES" target="E. LEFEBVRE">
      <data key="d4">1.0</data>
      <data key="d5">E. Lefebvre is an author of the paper on fast unfolding of communities in large networks</data>
      <data key="d6">ac21ebe9a9d70d691c717f961d3f10c8</data>
    </edge>
    <edge source="PYTHON" target="MBPP">
      <data key="d4">8.0</data>
      <data key="d5">Python is the programming language used in the MBPP dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="PYTHON" target="BOYER &amp; MOORE (1983)">
      <data key="d4">16.0</data>
      <data key="d5">Boyer &amp; Moore (1983) discusses Turing Completeness</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="PYTHON" target="LADHA (2024)">
      <data key="d4">16.0</data>
      <data key="d5">Ladha (2024) discusses Turing Completeness</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="ARXIV" target="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)">
      <data key="d4">16.0</data>
      <data key="d5">The paper on Automated Design of Agentic Systems is published on arXiv</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="CHEN, W." target="DUAN, N.">
      <data key="d4">16.0</data>
      <data key="d5">Duan, N. and Chen, W. co-authored the paper "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy"</data>
      <data key="d6">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
    </edge>
    <edge source="SU, D." target="XU, Y.">
      <data key="d4">16.0</data>
      <data key="d5">Su, D. and Xu, Y. co-authored the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management"</data>
      <data key="d6">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
    </edge>
    <edge source="SU, D." target="YU, T.">
      <data key="d4">16.0</data>
      <data key="d5">Su, D. and Yu, T. co-authored the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management"</data>
      <data key="d6">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
    </edge>
    <edge source="SU, D." target="SIDDIQUE, F. B.">
      <data key="d4">16.0</data>
      <data key="d5">Su, D. and Siddique, F. B. co-authored the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management"</data>
      <data key="d6">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
    </edge>
    <edge source="SU, D." target="BAREZI, E. J.">
      <data key="d4">16.0</data>
      <data key="d5">Su, D. and Barezi, E. J. co-authored the paper "Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management"</data>
      <data key="d6">d4c8ce26fd0f9a7bc6dad0efa1ce98e3</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="GPT-3.5">
      <data key="d4">9.0</data>
      <data key="d5">LATS uses GPT-3.5 in its experimental evaluation, demonstrating gradient-free performance for web navigation on WebShop.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="MONTE CARLO TREE SEARCH (MCTS)">
      <data key="d4">9.0</data>
      <data key="d5">LATS integrates Monte Carlo Tree Search (MCTS) to enhance decision-making capabilities of language models.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="REACT">
      <data key="d4">15.0</data>
      <data key="d5">LATS expands upon the ReAct technique to improve reasoning and decision-making.
Language Agent Tree Search (LATS) addresses the shortcomings of ReAct.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8,9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="HUMANEVAL">
      <data key="d4">8.0</data>
      <data key="d5">LATS was evaluated on the HumanEval benchmark, achieving state-of-the-art pass@1 accuracy for programming with GPT-4.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="WEBSHOP">
      <data key="d4">8.0</data>
      <data key="d5">LATS was evaluated on the WebShop benchmark, demonstrating gradient-free performance for web navigation with GPT-3.5.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="GITHUB">
      <data key="d4">8.0</data>
      <data key="d5">The code for LATS is available on GitHub.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="PROCEEDINGS OF THE 41ST INTERNATIONAL CONFERENCE ON MACHINE LEARNING">
      <data key="d4">8.0</data>
      <data key="d5">The paper on LATS was presented at the Proceedings of the 41st International Conference on Machine Learning.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="CHAIN-OF-THOUGHT (COT) PROMPTING">
      <data key="d4">7.0</data>
      <data key="d5">Language Agent Tree Search (LATS) addresses the shortcomings of Chain-of-thought (CoT) prompting.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH (LATS)" target="TREE-OF-THOUGHT (TOT) PROMPTING">
      <data key="d4">7.0</data>
      <data key="d5">Language Agent Tree Search (LATS) addresses the shortcomings of Tree-of-thought (ToT) prompting.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="ANDY ZHOU" target="UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN">
      <data key="d4">8.0</data>
      <data key="d5">Andy Zhou is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="ANDY ZHOU" target="LAPIS LABS">
      <data key="d4">8.0</data>
      <data key="d5">Andy Zhou is affiliated with Lapis Labs.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="KAI YAN" target="UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN">
      <data key="d4">8.0</data>
      <data key="d5">Kai Yan is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="MICHAL SHLAPENTOKH-ROTHMAN" target="UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN">
      <data key="d4">8.0</data>
      <data key="d5">Michal Shlapentokh-Rothman is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="HAOHAN WANG" target="UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN">
      <data key="d4">8.0</data>
      <data key="d5">Haohan Wang is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="YU-XIONG WANG" target="UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN">
      <data key="d4">8.0</data>
      <data key="d5">Yu-Xiong Wang is affiliated with the University of Illinois Urbana-Champaign.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="GPT-3.5" target="LATS">
      <data key="d4">17.0</data>
      <data key="d5">LATS was used with GPT-3.5 to achieve high performance on WebShop
LATS is used with GPT-3.5 to achieve high performance in various benchmarks, including HotPotQA and HumanEval.</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="REACT">
      <data key="d4">8.0</data>
      <data key="d5">ReAct is used with GPT-3.5 to enhance reasoning and acting capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="REFLEXION">
      <data key="d4">8.0</data>
      <data key="d5">Reflexion is used with GPT-3.5 to enhance reasoning capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="TOT">
      <data key="d4">8.0</data>
      <data key="d5">ToT is used with GPT-3.5 to enhance decision-making capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="RAP">
      <data key="d4">8.0</data>
      <data key="d5">RAP is used with GPT-3.5 to enhance decision-making capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="COT">
      <data key="d4">8.0</data>
      <data key="d5">CoT is used with GPT-3.5 to enhance reasoning capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="COT-SC">
      <data key="d4">8.0</data>
      <data key="d5">CoT-SC is used with GPT-3.5 to enhance reasoning capabilities in various benchmarks, including HotPotQA.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="HUMANEVAL">
      <data key="d4">25.0</data>
      <data key="d5">GPT-3.5 is evaluated on the HumanEval benchmark to measure its Pass@1 accuracy.
GPT-3.5 is used to evaluate performance on the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="GPT-3.5" target="MBPP">
      <data key="d4">8.0</data>
      <data key="d5">GPT-3.5 is used to evaluate the performance of various prompting methods on MBPP</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="GPT-3.5" target="WEBSHOP">
      <data key="d4">8.0</data>
      <data key="d5">GPT-3.5 is used in WebShop for acting-based prompting methods</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="GPT-3.5" target="GAME OF 24">
      <data key="d4">8.0</data>
      <data key="d5">GPT-3.5 is used to perform experiments in the Game of 24</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="GPT-3.5" target="META AGENT SEARCH">
      <data key="d4">41.0</data>
      <data key="d5">Meta Agent Search transfers discovered agents from GPT-3.5 to GPT-4
Discovered agents and baselines in Meta Agent Search are evaluated using GPT-3.5
Meta Agent Search uses GPT-3.5 to evaluate discovered agents and baselines
Meta Agent Search uses GPT-3.5 to evaluate the performance of discovered agents</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="GPT-3.5" target="OPENAI">
      <data key="d4">8.0</data>
      <data key="d5">OpenAI is the organization behind the GPT-3.5 model</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="GPT-3.5" target="NEURAL ARCHITECTURE SEARCH">
      <data key="d4">14.0</data>
      <data key="d5">Neural Architecture Search shows insights into Neural Networks, and GPT-3.5 was used to evaluate agentic systems</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="GPT-3.5" target="ORCA-3">
      <data key="d4">16.0</data>
      <data key="d5">Orca-3 outperforms GPT-3.5 on multiple benchmarks</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="LATS">
      <data key="d4">9.0</data>
      <data key="d5">LATS is based on Monte Carlo Tree Search to enhance language model performance</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="SILVER ET AL., 2017">
      <data key="d4">8.0</data>
      <data key="d5">Silver et al., 2017 is the reference for the success of Monte Carlo Tree Search in model-based reinforcement learning</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="TREE-OF-THOUGHT (TOT) PROMPTING">
      <data key="d4">8.0</data>
      <data key="d5">Tree-of-thought (ToT) prompting uses search algorithms like those in Monte Carlo Tree Search (MCTS) to explore multiple reasoning paths.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="UPPER CONFIDENCE BOUNDS APPLIED TO TREES (UCT)">
      <data key="d4">9.0</data>
      <data key="d5">Upper Confidence bounds applied to Trees (UCT) is a metric used in Monte Carlo Tree Search (MCTS) to select the best child state for expansion.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="ATARI">
      <data key="d4">8.0</data>
      <data key="d5">Monte Carlo Tree Search (MCTS) has been successfully applied in the Atari decision-making environment.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="GO">
      <data key="d4">8.0</data>
      <data key="d5">Monte Carlo Tree Search (MCTS) has been successfully applied in the Go decision-making environment.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="SILVER ET AL. (2016)">
      <data key="d4">8.0</data>
      <data key="d5">Silver et al. demonstrated the success of Monte Carlo Tree Search (MCTS) in the game of Go.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="MONTE CARLO TREE SEARCH (MCTS)" target="YE ET AL. (2021)">
      <data key="d4">1.0</data>
      <data key="d5">Ye et al. demonstrated the success of Monte Carlo Tree Search (MCTS) in the Atari environment.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REACT" target="LATS">
      <data key="d4">47.0</data>
      <data key="d5">LATS outperforms the ReAct method
LATS incorporates designs from ReAct for reasoning, acting, and planning
ReAct is a framework used in LATS where the action space consists of permissible actions and reasoning traces
LATS surpasses ReAct in performance by expanding more nodes with principled search
LATS is compared with ReAct in the HotPotQA experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8,99d90aededb61e04241516ed9ec656cc,c234cb83764b899335af0950677ad024,c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="REACT" target="YAO ET AL., 2023B">
      <data key="d4">23.0</data>
      <data key="d5">Yao et al., 2023b is the reference for the ReAct method
The paper by Yao et al. (2023b) discusses the ReAct method and its application in language models.
Yao et al., 2023b discusses the ReAct method and its performance on WebShop</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="REACT" target="COT">
      <data key="d4">14.0</data>
      <data key="d5">ReAct and CoT are both prompting techniques used in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="REACT" target="YA0 ET AL.">
      <data key="d4">18.0</data>
      <data key="d5">Yao et al. contributed to the development of the ReAct technique</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="REACT" target="CHAIN-OF-THOUGHT (COT) PROMPTING">
      <data key="d4">8.0</data>
      <data key="d5">ReAct builds on the principles of Chain-of-thought (CoT) prompting by adding interactions with an external environment to the reasoning process.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REACT" target="TREE-OF-THOUGHT (TOT) PROMPTING">
      <data key="d4">7.0</data>
      <data key="d5">Both Tree-of-thought (ToT) prompting and ReAct extend the basic principles of Chain-of-thought (CoT) prompting to improve reasoning and decision-making.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REACT" target="LANGUAGE MODEL (LM)">
      <data key="d4">8.0</data>
      <data key="d5">ReAct uses a language model (LM) to generate actions and reasoning traces, enhanced by interactions with an external environment.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REACT" target="OBSERVATION O">
      <data key="d4">8.0</data>
      <data key="d5">Observation o is used in the ReAct process to improve reasoning and acting.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REACT" target="ACTION A">
      <data key="d4">8.0</data>
      <data key="d5">Action a is generated in the ReAct process based on observations from the environment.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REACT" target="RAP (HAO ET AL., 2023)">
      <data key="d4">7.0</data>
      <data key="d5">RAP is a reasoning-based method similar to ReAct.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REACT" target="WEI ET AL., 2022">
      <data key="d4">9.0</data>
      <data key="d5">Wei et al., 2022 introduced the ReAct prompting method</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="REACT" target="WEBSHOP">
      <data key="d4">8.0</data>
      <data key="d5">ReAct is a prompting method used in WebShop with GPT-3.5</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="REACT" target="PERFORMANCE">
      <data key="d4">7.0</data>
      <data key="d5">ReAct is evaluated for performance in the study</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="REACT" target="TOKEN CONSUMPTION">
      <data key="d4">7.0</data>
      <data key="d5">ReAct is evaluated for token consumption in the study</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="REACT" target="FIG. 4">
      <data key="d4">7.0</data>
      <data key="d5">Figure 4 illustrates how ReAct works on an example task of HotPotQA</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HUMANEVAL" target="LATS">
      <data key="d4">26.0</data>
      <data key="d5">LATS set the state of the art on HumanEval with GPT-4
LATS sets the state of the art for HumanEval with GPT-4
LATS is evaluated using the HumanEval dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="HUMANEVAL" target="CHEN ET AL., 2021">
      <data key="d4">17.0</data>
      <data key="d5">Chen et al., 2021 is the reference for the HumanEval dataset
Chen et al., 2021 introduced the HumanEval dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="HUMANEVAL" target="TAB. 4">
      <data key="d4">8.0</data>
      <data key="d5">Tab. 4 shows the performance of various methods on programming tasks using the HumanEval dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HUMANEVAL" target="APPENDIX SEC. D">
      <data key="d4">7.0</data>
      <data key="d5">Appendix Sec. D provides additional details on the evaluation of LATS and other methods on programming tasks using the HumanEval dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HUMANEVAL" target="CHEN ET AL., 2023A">
      <data key="d4">8.0</data>
      <data key="d5">Chen et al., 2023a discusses the use of an LM to generate a synthetic test suite for evaluating programming tasks using the HumanEval dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="HUMANEVAL" target="REFLEXION">
      <data key="d4">7.0</data>
      <data key="d5">Reflexion is compared with LATS in the HumanEval experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HUMANEVAL" target="FIG. 3">
      <data key="d4">7.0</data>
      <data key="d5">Figure 3 shows the results of the HumanEval experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HUMANEVAL" target="SAMPLING SIZE">
      <data key="d4">8.0</data>
      <data key="d5">Sampling size is a parameter used in the HumanEval dataset</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HUMANEVAL" target="TRAJECTORIES">
      <data key="d4">8.0</data>
      <data key="d5">Trajectories refer to the number of paths sampled in the HumanEval experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="HUMANEVAL" target="PROGRAMMING">
      <data key="d4">8.0</data>
      <data key="d5">Programming tasks are evaluated using the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="HUMANEVAL" target="FUNCTION SIGNATURE">
      <data key="d4">8.0</data>
      <data key="d5">Function signatures are part of the programming problems in the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="HUMANEVAL" target="DOCSTRING">
      <data key="d4">8.0</data>
      <data key="d5">Docstrings are part of the programming problems in the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="HUMANEVAL" target="REFERENCE IMPLEMENTATION">
      <data key="d4">8.0</data>
      <data key="d5">Reference implementations are provided for programming problems in the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="HUMANEVAL" target="UNIT TESTS">
      <data key="d4">8.0</data>
      <data key="d5">Unit tests are provided to check the correctness of function implementations in the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="HUMANEVAL" target="NATURAL LANGUAGE DESCRIPTION">
      <data key="d4">8.0</data>
      <data key="d5">Natural language descriptions explain programming tasks in the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="HUMANEVAL" target="ALGORITHMS">
      <data key="d4">8.0</data>
      <data key="d5">Algorithms are evaluated through programming tasks in the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="HUMANEVAL" target="BASIC MATHEMATICS">
      <data key="d4">8.0</data>
      <data key="d5">Basic mathematics is evaluated through programming tasks in the HumanEval dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="LATS">
      <data key="d4">25.0</data>
      <data key="d5">LATS significantly improved performance on the WebShop dataset
LATS is evaluated on the WebShop benchmark to demonstrate its reasoning and acting capabilities.
LATS is used in WebShop to improve score and success rate, surpassing RL-based training</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="WEBSHOP" target="YAO ET AL., 2022">
      <data key="d4">14.0</data>
      <data key="d5">Yao et al., 2022 is the reference for the WebShop dataset
Yao et al., 2022 is a reference related to WebShop</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="WEBSHOP" target="IL">
      <data key="d4">7.0</data>
      <data key="d5">IL is used to train models for tasks in WebShop</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="WEBSHOP" target="RL">
      <data key="d4">7.0</data>
      <data key="d5">RL is used to train models for tasks in WebShop</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="WEBSHOP" target="FINE-TUNING">
      <data key="d4">15.0</data>
      <data key="d5">Fine-tuning is used to improve model performance on WebShop
Fine-tuning is a method mentioned in the context of improving performance in WebShop</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="WEBSHOP" target="EXPERT">
      <data key="d4">7.0</data>
      <data key="d5">Expert performance is used as a benchmark for comparison on WebShop
Expert refers to human performance metrics used as a benchmark in WebShop</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="WEBSHOP" target="REFLEXION">
      <data key="d4">7.0</data>
      <data key="d5">Reflexion is a prompting method used in WebShop, similar to ReAct</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="IMPROVEMENT LEARNING (IL)">
      <data key="d4">7.0</data>
      <data key="d5">IL is a method used in WebShop for training agents, mentioned in comparison with other methods</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="REINFORCEMENT LEARNING (RL)">
      <data key="d4">7.0</data>
      <data key="d5">RL is a method used in WebShop for training agents, mentioned in comparison with other methods</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="PROMPTING">
      <data key="d4">8.0</data>
      <data key="d5">Prompting methods are used to guide the behavior of models like GPT-3.5 in WebShop</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="SEARCH AND CLICK COMMANDS">
      <data key="d4">7.0</data>
      <data key="d5">Search and click commands are part of the preconstructed action space used in WebShop</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="BROWSER FEEDBACK">
      <data key="d4">7.0</data>
      <data key="d5">Browser feedback is used in WebShop as part of the observation mechanism for agents</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="REFLECTIONS">
      <data key="d4">7.0</data>
      <data key="d5">Reflections are used in WebShop as part of the observation mechanism for agents</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="SUCCESS RATE (SR)">
      <data key="d4">8.0</data>
      <data key="d5">Success Rate (SR) is a metric used in WebShop to indicate the frequency with which the chosen product fulfills all given conditions</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="AVERAGE SCORE">
      <data key="d4">8.0</data>
      <data key="d5">Average Score is a metric used in WebShop to reflect the percentage of user-specified attributes met by the selected product</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="WEBSHOP" target="MBPP">
      <data key="d4">12.0</data>
      <data key="d5">MBPP is a benchmark used to evaluate program synthesis techniques, while WebShop evaluates agents on grounded language understanding and decision-making.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="AMAZON">
      <data key="d4">16.0</data>
      <data key="d5">Amazon is the source of over 1 million real-world products used in the WebShop environment.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="TASK SCORE">
      <data key="d4">16.0</data>
      <data key="d5">Task Score is one of the evaluation metrics used in the WebShop environment.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="SUCCESS RATE">
      <data key="d4">9.0</data>
      <data key="d5">Success Rate is one of the evaluation metrics used in the WebShop environment.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="QUERY SEARCHES">
      <data key="d4">8.0</data>
      <data key="d5">Query searches are actions in WebShop that allow agents to search for products.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="BUTTON CLICKS">
      <data key="d4">8.0</data>
      <data key="d5">Button clicks are actions in WebShop that allow agents to interact with the web interface.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="HTML MODE">
      <data key="d4">8.0</data>
      <data key="d5">HTML mode in WebShop provides pixel-level observations for agents.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="SIMPLE MODE">
      <data key="d4">8.0</data>
      <data key="d5">Simple mode in WebShop converts raw HTML into structured text observations.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="LEXICAL MATCHING">
      <data key="d4">8.0</data>
      <data key="d5">Lexical matching is used in WebShop to compare purchased products against specified attributes.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="SEMANTIC SIMILARITY">
      <data key="d4">1.0</data>
      <data key="d5">Semantic similarity is used in WebShop to compare purchased products against specified attributes based on meaning.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="WEBSHOP" target="GAME OF 24">
      <data key="d4">6.0</data>
      <data key="d5">Both WebShop and Game of 24 are used as environments for experiments involving language models</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="WEBSHOP" target="BRIGHT CITRUS DEODORANT">
      <data key="d4">8.0</data>
      <data key="d5">Webshop is a platform where users can search for and purchase Bright Citrus Deodorant</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="WEBSHOP" target="GINGER FRESH DEODORANT">
      <data key="d4">8.0</data>
      <data key="d5">Webshop is a platform where users can search for and purchase Ginger Fresh Deodorant</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="WEBSHOP" target="CEDAR &amp; PATCHOULI DEODORANT">
      <data key="d4">8.0</data>
      <data key="d5">Webshop is a platform where users can search for and purchase Cedar &amp; Patchouli Deodorant</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="WEBSHOP" target="ACTING PROMPT">
      <data key="d4">8.0</data>
      <data key="d5">The ACTING PROMPT guides actions in the WEBSHOP</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="WEBSHOP" target="SEARCH">
      <data key="d4">8.0</data>
      <data key="d5">SEARCH is an action performed in the WEBSHOP to find products</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="PROCEEDINGS OF THE 41ST INTERNATIONAL CONFERENCE ON MACHINE LEARNING" target="PMLR 235">
      <data key="d4">8.0</data>
      <data key="d5">The paper on LATS was published in volume 235 of the Proceedings of the 41st International Conference on Machine Learning.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="PROCEEDINGS OF THE 41ST INTERNATIONAL CONFERENCE ON MACHINE LEARNING" target="2024">
      <data key="d4">1.0</data>
      <data key="d5">The paper on LATS was presented in the year 2024.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="PROCEEDINGS OF THE 41ST INTERNATIONAL CONFERENCE ON MACHINE LEARNING" target="PMLR">
      <data key="d4">1.0</data>
      <data key="d5">PMLR published the Proceedings of the 41st International Conference on Machine Learning where the paper on LATS was presented.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="GITHUB" target="HTTPS://GITHUB.COM/LAPISROCKS/LANGUAGEAGENTTREESEARCH">
      <data key="d4">8.0</data>
      <data key="d5">The URL where the code for LATS is available on GitHub.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="GITHUB" target="META AGENT SEARCH">
      <data key="d4">1.0</data>
      <data key="d5">All code, prompts, and experiment results related to Meta Agent Search are available on GitHub</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="GITHUB" target="SHENGRAN HU">
      <data key="d4">15.0</data>
      <data key="d5">Shengran Hu has made the framework code available on GitHub
Shengran Hu is associated with the GitHub repository containing all agents from the experiment</data>
      <data key="d6">449db721e37968e073e3579b59e023b2,d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="WOOLDRIDGE" target="JENNINGS">
      <data key="d4">7.0</data>
      <data key="d5">Wooldridge and Jennings are co-referenced in the context of general autonomous agents capable of reasoning and decision-making in a variety of environments.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="CHOWDHERY ET AL." target="OPENAI">
      <data key="d4">7.0</data>
      <data key="d5">Chowdhery et al. and OpenAI are co-referenced in the context of the rise of language models with strong reasoning and general adaptability.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="OPENAI" target="GPT-4O-2024-05-13">
      <data key="d4">26.0</data>
      <data key="d5">GPT-4o-2024-05-13 is a model developed by OpenAI
OpenAI developed the GPT-4o-2024-05-13 model
OpenAI developed GPT-4o-2024-05-13</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="OPENAI" target="GPT-3.5-TURBO-0125">
      <data key="d4">11.0</data>
      <data key="d5">GPT-3.5-turbo-0125 is a model developed by OpenAI
OpenAI developed the GPT-3.5-turbo-0125 model
OpenAI developed GPT-3.5-turbo-0125</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,4b43decac6833d1515992f8869ecada7,84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="NALLAPATI ET AL." target="BOWMAN ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Nallapati et al. and Bowman et al. are co-referenced in the context of language models excelling in standard natural language processing tasks.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="COBBE ET AL." target="SAPAROV AND HE">
      <data key="d4">7.0</data>
      <data key="d5">Cobbe et al. and Saparov and He are co-referenced in the context of language models being adapted to tasks requiring advanced common-sense reasoning or quantitative skills.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="COBBE ET AL." target="GSM8K">
      <data key="d4">8.0</data>
      <data key="d5">Cobbe et al. are the authors of the GSM8K dataset</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="SCHICK ET AL." target="TOOL USE">
      <data key="d4">22.0</data>
      <data key="d5">Schick et al. contributed to the research on tool use in agentic systems
Schick et al. are the authors of the Tool Use method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="GAO ET AL." target="SHINN ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Gao et al. and Shinn et al. are co-referenced in the context of prompting techniques that augment language models with feedback or observations from an external environment.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="GAO ET AL." target="GSM-HARD">
      <data key="d4">8.0</data>
      <data key="d5">Gao et al. are the authors of the GSM-Hard dataset</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="SHINN ET AL." target="SELF-REFLECTION">
      <data key="d4">14.0</data>
      <data key="d5">Shinn et al. contributed to the research on self-reflection in agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="SHINN ET AL." target="REFLECTION">
      <data key="d4">8.0</data>
      <data key="d5">Shinn et al. are the authors of the Reflection method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="SLOMAN" target="EVANS">
      <data key="d4">7.0</data>
      <data key="d5">Sloman and Evans are co-referenced in the context of the limitations of reflexive methods in language models compared to humans' deliberate and thoughtful decision-making characteristics.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="XIE ET AL." target="HAO ET AL.">
      <data key="d4">7.0</data>
      <data key="d5">Xie et al. and Hao et al. are co-referenced in the context of recent search-guided language model work that addresses the issue of planning and multiple reasoning paths.</data>
      <data key="d6">93cb0d0456e0822b5fe30a3e627405f8</data>
    </edge>
    <edge source="XIE ET AL." target="BEAM SEARCH">
      <data key="d4">16.0</data>
      <data key="d5">Xie et al. contributed to the understanding of beam search in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="HAO ET AL." target="RAP">
      <data key="d4">18.0</data>
      <data key="d5">Hao et al. contributed to the development of the RAP technique</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LATS" target="SEARCH ALGORITHMS">
      <data key="d4">17.0</data>
      <data key="d5">LATS adapts search algorithms to language agents for better performance
LATS uses search algorithms to construct trajectories and incorporate external feedback</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="NODES">
      <data key="d4">14.0</data>
      <data key="d5">Nodes in LATS store and retrieve external feedback
Nodes refer to the sampled points in the search space used in the evaluation of LATS on Game of 24</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="PROMPTS">
      <data key="d4">7.0</data>
      <data key="d5">Prompts in LATS store and retrieve external feedback</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="EXTERNAL FEEDBACK">
      <data key="d4">16.0</data>
      <data key="d5">LATS incorporates external feedback to improve performance
LATS incorporates external feedback to improve performance</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26,faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="INTERNAL REASONING PERFORMANCE">
      <data key="d4">7.0</data>
      <data key="d5">LATS aims to surpass internal reasoning performance</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="PRETRAINED LMS">
      <data key="d4">8.0</data>
      <data key="d5">LATS repurposes pretrained language models for value functions and self-reflections</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="LM-POWERED VALUE FUNCTIONS">
      <data key="d4">8.0</data>
      <data key="d5">LATS uses LM-powered value functions to guide exploration</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="SELF-REFLECTIONS">
      <data key="d4">8.0</data>
      <data key="d5">LATS uses self-reflections to improve exploration</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="IN-CONTEXT LEARNING">
      <data key="d4">8.0</data>
      <data key="d5">LATS leverages in-context learning abilities of modern language models</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="ENVIRONMENTAL CONDITIONS">
      <data key="d4">8.0</data>
      <data key="d5">LATS adapts planning to environmental conditions</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="REASONING">
      <data key="d4">8.0</data>
      <data key="d5">LATS incorporates reasoning to enhance language model performance</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="ACTING">
      <data key="d4">8.0</data>
      <data key="d5">LATS incorporates acting to enhance language model performance</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="PLANNING">
      <data key="d4">16.0</data>
      <data key="d5">LATS incorporates planning to enhance language model performance
Planning in LATS involves organizing information, planning future actions, or injecting internal knowledge</data>
      <data key="d6">c234cb83764b899335af0950677ad024,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="AUTONOMOUS REASONING">
      <data key="d4">8.0</data>
      <data key="d5">LATS enhances autonomous reasoning in language models</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="DECISION-MAKING">
      <data key="d4">8.0</data>
      <data key="d5">LATS enhances decision-making in language models</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="INTERACTIVE QUESTION-ANSWERING (QA)">
      <data key="d4">8.0</data>
      <data key="d5">LATS is evaluated on interactive question-answering tasks</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="WEB NAVIGATION">
      <data key="d4">8.0</data>
      <data key="d5">LATS is evaluated on web navigation tasks</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="MATH">
      <data key="d4">8.0</data>
      <data key="d5">LATS is evaluated on math tasks</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="PROGRAMMING">
      <data key="d4">8.0</data>
      <data key="d5">LATS is evaluated on programming tasks</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LATS" target="COT">
      <data key="d4">23.0</data>
      <data key="d5">LATS incorporates designs from CoT for reasoning, acting, and planning
CoT is used as the base prompting design in LATS for the Game of 24</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LATS" target="SELF-REFINE">
      <data key="d4">16.0</data>
      <data key="d5">LATS incorporates designs from Self-Refine for reasoning, acting, and planning</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LATS" target="REFLEXION">
      <data key="d4">16.0</data>
      <data key="d5">LATS incorporates designs from Reflexion for reasoning, acting, and planning</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LATS" target="ADAPLANNER">
      <data key="d4">16.0</data>
      <data key="d5">LATS incorporates designs from AdaPlanner for reasoning, acting, and planning</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LATS" target="MCTS">
      <data key="d4">45.0</data>
      <data key="d5">LATS adopts a variant of MCTS to frame decision-making as a tree search
LATS leverages MCTS for principled search
MCTS is the search algorithm used in LATS
LATS incorporates improvements to MCTS for better performance and efficiency</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf,594449768ae2dea9b2efbe677075096b,c234cb83764b899335af0950677ad024,faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="LM AGENT">
      <data key="d4">8.0</data>
      <data key="d5">LM Agent is initialized with a language model and used within LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="P&#920;">
      <data key="d4">9.0</data>
      <data key="d5">P&#952; is repurposed as an agent, state evaluator, and feedback generator in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="SELECTION">
      <data key="d4">8.0</data>
      <data key="d5">Selection is the first operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="EXPANSION">
      <data key="d4">8.0</data>
      <data key="d5">Expansion is the second operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="SIMULATION">
      <data key="d4">8.0</data>
      <data key="d5">Simulation is an operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="BACKPROPAGATION">
      <data key="d4">8.0</data>
      <data key="d5">Backpropagation is an operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="REFLECTION">
      <data key="d4">8.0</data>
      <data key="d5">Reflection is an operation in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="TASK">
      <data key="d4">8.0</data>
      <data key="d5">A task in LATS is successfully completed when the series of operations result in a solution or a computational limit is reached</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="TRAJECTORY">
      <data key="d4">16.0</data>
      <data key="d5">A trajectory in LATS refers to a sequence of actions and observations sampled from P&#952;
Trajectory refers to the path taken through the state space in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8,c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="FEEDBACK">
      <data key="d4">8.0</data>
      <data key="d5">Feedback in LATS is the response from the environment to the agent's actions</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="REASONING TASKS">
      <data key="d4">7.0</data>
      <data key="d5">Reasoning tasks are a type of task in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="DECISION-MAKING TASKS">
      <data key="d4">7.0</data>
      <data key="d5">Decision-making tasks are a type of task in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="SAMPLING">
      <data key="d4">8.0</data>
      <data key="d5">Sampling in LATS involves selecting a diverse set of candidates at each step</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="SEARCH ALGORITHM">
      <data key="d4">8.0</data>
      <data key="d5">A search algorithm in LATS controls the problem-solving process with planning</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="PSEUDOCODE">
      <data key="d4">7.0</data>
      <data key="d5">The full pseudocode of LATS can be found in the Appendix</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="SEC. A">
      <data key="d4">7.0</data>
      <data key="d5">Section A in the Appendix contains the full pseudocode of LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="EVANS, 2010">
      <data key="d4">6.0</data>
      <data key="d5">Evans, 2010 is a reference cited in the context of sampling diverse candidates for complex decision-making tasks in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="HAO ET AL., 2023">
      <data key="d4">6.0</data>
      <data key="d5">Hao et al., 2023 is a reference cited in the context of standard MCTS and RAP in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="RAP">
      <data key="d4">29.0</data>
      <data key="d5">RAP is an algorithm that relies on internal dynamics models, unlike LATS
LATS outperforms RAP on internal reasoning tasks
RAP is a prompting method compared with LATS in HotPotQA and Game of 24
LATS outperforms RAP in terms of performance and efficiency</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,c234cb83764b899335af0950677ad024,faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="MODERN LMS">
      <data key="d4">1.0</data>
      <data key="d5">Modern LMs provide useful language representations that facilitate planning in LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="ACTION SPACE">
      <data key="d4">16.0</data>
      <data key="d5">Action space refers to the set of all possible actions an agent can take in LATS
Action space (A) is the set of possible actions in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8,c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LATS" target="VALUE FUNCTION">
      <data key="d4">34.0</data>
      <data key="d5">LATS uses a novel value function based on self-generated LM score and self-consistency score
Value function is a component of LATS that incorporates self-consistency as an additional heuristic
Value function (pV) is used to evaluate states in the LATS algorithm</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf,48e423e2baf2ed485872756f5b4d87d8,594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="LATS" target="CHEN ET AL., 2021">
      <data key="d4">7.0</data>
      <data key="d5">LATS is evaluated on programming tasks as referenced in the paper by Chen et al. (2021).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="LATS" target="AUSTIN ET AL., 2022">
      <data key="d4">7.0</data>
      <data key="d5">LATS is evaluated on programming tasks as referenced in the paper by Austin et al. (2022).</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="LATS" target="GAME OF 24">
      <data key="d4">24.0</data>
      <data key="d5">LATS is evaluated on the Game of 24 benchmark to demonstrate its reasoning and acting capabilities.
LATS is tested on Game of 24 to evaluate its reasoning ability
LATS is a method used to improve performance in the Game of 24</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,b8dd0300033963bb4a3e1bad37f8e7b9,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="LATS" target="MBPP">
      <data key="d4">9.0</data>
      <data key="d5">LATS achieves the highest performance on MBPP</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="LATS" target="TOT">
      <data key="d4">15.0</data>
      <data key="d5">ToT is a prompting method compared with LATS in HotPotQA
LATS outperforms ToT in terms of performance and efficiency</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="DFS">
      <data key="d4">7.0</data>
      <data key="d5">DFS is a search algorithm variant compared with MCTS in LATS</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="LATS" target="SELF-REFLECTION">
      <data key="d4">8.0</data>
      <data key="d5">Self-reflection is a technique used in LATS to provide additional semantic signals for the agent</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="LATS" target="MCTS (MONTE CARLO TREE SEARCH)">
      <data key="d4">9.0</data>
      <data key="d5">MCTS is the search algorithm used in LATS</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="LATS" target="A*">
      <data key="d4">7.0</data>
      <data key="d5">A* is a search algorithm variant compared with MCTS in LATS</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="LATS" target="TOKEN CONSUMPTION">
      <data key="d4">15.0</data>
      <data key="d5">Token consumption is a metric used in the ablation study of LATS
LATS requires fewer tokens compared to other methods</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="TRAJECTORIES">
      <data key="d4">10.0</data>
      <data key="d5">Trajectories refer to the sampled paths in the search space used in the evaluation of LATS on Game of 24 and HotPotQA
LATS constructs trajectories using search algorithms for enhanced decision-making</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="REVERSION PROPERTY">
      <data key="d4">7.0</data>
      <data key="d5">LATS assumes the ability to revert to earlier states in decision-making environments</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="SYSTEM-2 LM APPROACHES">
      <data key="d4">8.0</data>
      <data key="d5">LATS is an example of a System-2 LM approach that involves reasoning and planning</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="PERFORMANCE">
      <data key="d4">9.0</data>
      <data key="d5">LATS achieves better performance compared to other methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="SAMPLE COMPLEXITY">
      <data key="d4">8.0</data>
      <data key="d5">LATS has the same sample complexity as other tree-based search methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="NODES EXPANDED">
      <data key="d4">8.0</data>
      <data key="d5">LATS expands fewer nodes compared to other methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="COMPUTATIONAL COST">
      <data key="d4">7.0</data>
      <data key="d5">LATS has a higher computational cost compared to simpler prompting methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="INFERENCE-TIME COMPUTE COSTS">
      <data key="d4">7.0</data>
      <data key="d5">LATS is expected to have reduced inference-time compute costs over time</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="CONCLUSION">
      <data key="d4">8.0</data>
      <data key="d5">The conclusion section summarizes the contributions and findings related to LATS</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="LIMITATIONS AND FUTURE DIRECTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The limitations and future directions section discusses the constraints and potential future work for LATS</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LATS" target="EXPLORATION WEIGHT">
      <data key="d4">8.0</data>
      <data key="d5">Exploration weight is a parameter in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="DEPTH">
      <data key="d4">8.0</data>
      <data key="d5">Depth is a parameter in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="LM VALUE FUNCTION">
      <data key="d4">8.0</data>
      <data key="d5">The LM value function is used in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="YAO ET AL., 2023B">
      <data key="d4">7.0</data>
      <data key="d5">Yao et al., 2023b is referenced for setting the maximum depth in LATS experiments</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="WIKIPEDIA WEB API">
      <data key="d4">8.0</data>
      <data key="d5">The Wikipedia web API is used for interactive information retrieval in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="LANGUAGE MODELS">
      <data key="d4">8.0</data>
      <data key="d5">Language models are used in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="HYPERPARAMETERS">
      <data key="d4">8.0</data>
      <data key="d5">Hyperparameters are used in the value function for the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="FIG. 3">
      <data key="d4">7.0</data>
      <data key="d5">Figure 3 shows the performance of LATS over time</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="FIG. 4">
      <data key="d4">7.0</data>
      <data key="d5">Figure 4 illustrates how LATS works on an example task of HotPotQA</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="ALGORITHM 1">
      <data key="d4">1.0</data>
      <data key="d5">Algorithm 1 describes the LATS process</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="SELECTION FORMULA">
      <data key="d4">8.0</data>
      <data key="d5">Selection formula is used in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="STATE SPACE">
      <data key="d4">8.0</data>
      <data key="d5">State space refers to the complexity of the environment in which the LATS algorithm operates</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="ROLL-OUTS">
      <data key="d4">8.0</data>
      <data key="d5">Roll-outs refer to the number of iterations (K) in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="CONTEXT">
      <data key="d4">8.0</data>
      <data key="d5">Context (c) is part of the state in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="VISIT COUNTER">
      <data key="d4">8.0</data>
      <data key="d5">Visit counter (N) is used in the LATS algorithm to track the number of visits to each state</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="OBSERVATION SPACE">
      <data key="d4">8.0</data>
      <data key="d5">Observation space (O) is the set of possible observations in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="INITIAL STATE">
      <data key="d4">8.0</data>
      <data key="d5">Initial state (s) is the starting point in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="ACTION GENERATOR">
      <data key="d4">8.0</data>
      <data key="d5">Action generator (p&#952;) is used to sample actions in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="REFLECTION GENERATOR">
      <data key="d4">8.0</data>
      <data key="d5">Reflection generator (pref) is used to generate reflections in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="ENVIRONMENT">
      <data key="d4">8.0</data>
      <data key="d5">Environment provides the context and rewards for the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="REWARD">
      <data key="d4">8.0</data>
      <data key="d5">Reward (r) is the feedback from the environment in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="TERMINAL STATE">
      <data key="d4">8.0</data>
      <data key="d5">Terminal state (st) is a state where the process ends in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="SUCCESS">
      <data key="d4">8.0</data>
      <data key="d5">Success is a condition checked in the reflection phase of the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="EVALUATION OPERATION">
      <data key="d4">8.0</data>
      <data key="d5">Evaluation operation is used to score states in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="ITERATIONS">
      <data key="d4">8.0</data>
      <data key="d5">Iterations refer to the number of times the LATS algorithm is run</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="LATS" target="VALUE FUNCTION HYPERPARAMETERS">
      <data key="d4">8.0</data>
      <data key="d5">Value function hyperparameters include &#955;=0.5 for the LM score and self-consistency score in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="SELF-CONSISTENCY">
      <data key="d4">7.0</data>
      <data key="d5">Self-consistency is a method to improve Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="LEAST-TO-MOST PROMPTING">
      <data key="d4">7.0</data>
      <data key="d5">Least-to-most prompting is a method to improve Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="TREE-OF-THOUGHT (TOT) PROMPTING">
      <data key="d4">16.0</data>
      <data key="d5">Tree-of-Thought prompting is a method to improve Chain-of-Thought prompting
Tree-of-thought (ToT) prompting extends Chain-of-thought (CoT) prompting by exploring multiple reasoning paths over thoughts.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="REASONING VIA PLANNING (RAP)">
      <data key="d4">7.0</data>
      <data key="d5">Reasoning via Planning is a method to improve Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="COBBE ET AL., 2021">
      <data key="d4">16.0</data>
      <data key="d5">Cobbe et al., 2021 is the reference for reasoning in language models involving decomposing complex inputs into sequential steps</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="WEI ET AL., 2022">
      <data key="d4">16.0</data>
      <data key="d5">Wei et al., 2022 is the reference for Chain-of-Thought prompting and its variants</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="KOJIMA ET AL., 2022">
      <data key="d4">16.0</data>
      <data key="d5">Kojima et al., 2022 is the reference for a variant of Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="WANG ET AL., 2022">
      <data key="d4">16.0</data>
      <data key="d5">Wang et al., 2022 is the reference for self-consistency and Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="GUO ET AL., 2018">
      <data key="d4">16.0</data>
      <data key="d5">Guo et al., 2018 is the reference for the issue of error propagation in Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="CHEN ET AL., 2023B">
      <data key="d4">16.0</data>
      <data key="d5">Chen et al., 2023b is the reference for the issue of error propagation in Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="BESTA ET AL., 2023">
      <data key="d4">16.0</data>
      <data key="d5">Besta et al., 2023 is the reference for search algorithms in Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="YAO ET AL., 2023A">
      <data key="d4">8.0</data>
      <data key="d5">Yao et al., 2023a is the reference for Tree-of-Thought prompting and search algorithms in Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="HAO ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Hao et al., 2023 is the reference for Reasoning via Planning using Monte Carlo Tree Search</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="LANGUAGE MODEL (LM)">
      <data key="d4">8.0</data>
      <data key="d5">Chain-of-thought (CoT) prompting uses a language model (LM) to generate intermediate thoughts that act as stepping stones between the input and the output.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT (COT) PROMPTING" target="RAP (HAO ET AL., 2023)">
      <data key="d4">7.0</data>
      <data key="d5">RAP is a reasoning-based method similar to Chain-of-thought (CoT) prompting.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="TREE-OF-THOUGHT (TOT) PROMPTING" target="YAO ET AL., 2023A">
      <data key="d4">8.0</data>
      <data key="d5">Yao et al., 2023a is the reference for Tree-of-Thought prompting and search algorithms in Chain-of-Thought prompting</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="TREE-OF-THOUGHT (TOT) PROMPTING" target="LANGUAGE MODEL (LM)">
      <data key="d4">8.0</data>
      <data key="d5">Tree-of-thought (ToT) prompting uses a language model (LM) to generate thoughts and explore multiple reasoning paths.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="TREE-OF-THOUGHT (TOT) PROMPTING" target="DEPTH-FIRST SEARCH (DFS)">
      <data key="d4">8.0</data>
      <data key="d5">Depth-first search (DFS) is used in Tree-of-thought (ToT) prompting to explore the tree of reasoning paths.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="TREE-OF-THOUGHT (TOT) PROMPTING" target="BREADTH-FIRST SEARCH (BFS)">
      <data key="d4">8.0</data>
      <data key="d5">Breadth-first search (BFS) is used in Tree-of-thought (ToT) prompting to explore the tree of reasoning paths.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="TREE-OF-THOUGHT (TOT) PROMPTING" target="RAP (HAO ET AL., 2023)">
      <data key="d4">7.0</data>
      <data key="d5">RAP is a reasoning-based method similar to Tree-of-thought (ToT) prompting.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="REASONING VIA PLANNING (RAP)" target="HAO ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Hao et al., 2023 is the reference for Reasoning via Planning using Monte Carlo Tree Search</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="SELF-REFINE" target="REFLEXION">
      <data key="d4">23.0</data>
      <data key="d5">Both Self-refine and Reflexion are methods that use self-improvement techniques to enhance language model performance
Self-Refine and Reflexion both use self-improvement to enhance reasoning and decision-making</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6,f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="SELF-REFINE" target="MADAAN ET AL., 2023">
      <data key="d4">16.0</data>
      <data key="d5">Madaan et al., 2023 is the reference for the self-refine method</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="SELF-REFINE" target="ADAPLANNER">
      <data key="d4">14.0</data>
      <data key="d5">AdaPlanner and Self-Refine both aim to enhance reasoning and decision-making</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="SELF-REFINE" target="BEAM SEARCH">
      <data key="d4">16.0</data>
      <data key="d5">Beam search uses self-improvement to enhance reasoning and decision-making in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="SELF-REFINE" target="META AGENT SEARCH">
      <data key="d4">41.0</data>
      <data key="d5">Meta Agent Search uses the Self-Refine technique to perform iterations of refinement on proposals
Meta Agent Search uses Self-Refine as one of the state-of-the-art hand-designed agents
Meta Agent Search compares its discovered agents against the Self-Refine baseline
Meta Agent Search outperforms the Self-Refine method
Self-Refine is a method used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="SELF-REFINE" target="MADAAN ET AL., 2024">
      <data key="d4">28.0</data>
      <data key="d5">Self-Refine was introduced by Madaan et al., 2024
Madaan et al., 2024 discusses the Self-Refine method
The publication by Madaan et al. in 2024 is related to Self-Refine
Madaan et al., 2024 is a publication referenced for the Self-Refine method</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="SELF-REFINE" target="SHINN ET AL., 2023">
      <data key="d4">14.0</data>
      <data key="d5">Self-Refine was contributed to by Shinn et al., 2023
Shinn et al., 2023 is a publication referenced for the Self-Refine method</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="SELF-REFINE" target="MADAAN ET AL.">
      <data key="d4">14.0</data>
      <data key="d5">Madaan et al. are the authors of the Self-Refine agent
Madaan et al. are the authors of the Self-Refine method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="SELF-REFINE" target="ARC">
      <data key="d4">7.0</data>
      <data key="d5">Self-Refine is used as a baseline for experiments on ARC</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="SELF-REFINE" target="SHENGRAN HU">
      <data key="d4">7.0</data>
      <data key="d5">Shengran Hu is associated with the implementation of Self-Refine</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="REFLEXION" target="SHINN ET AL., 2023">
      <data key="d4">24.0</data>
      <data key="d5">Shinn et al., 2023 is the reference for the Reflexion method
The paper by Shinn et al. (2023) discusses the Reflexion method and its application in language models.
Shinn et al., 2023 introduced the Reflexion prompting method
Shinn et al., 2023 is a reference related to Reflexion and its performance</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,f8e7ed806916bf15245bcb4d52570c26,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="REFLEXION" target="ADAPLANNER">
      <data key="d4">14.0</data>
      <data key="d5">AdaPlanner and Reflexion both aim to enhance reasoning and decision-making</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="REFLEXION" target="LANGUAGE MODEL (LM)">
      <data key="d4">1.0</data>
      <data key="d5">Reflexion uses a language model (LM) for decision-making tasks, similar to ReAct.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="SILVER ET AL., 2017" target="HEURISTIC">
      <data key="d4">2.0</data>
      <data key="d5">Heuristic is a concept referenced in Silver et al., 2017</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="YAO ET AL., 2022" target="IL">
      <data key="d4">8.0</data>
      <data key="d5">Yao et al., 2022 discusses the IL method and its performance on WebShop</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="YAO ET AL., 2022" target="RL">
      <data key="d4">8.0</data>
      <data key="d5">Yao et al., 2022 discusses the RL method and its performance on WebShop</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="CHEN ET AL., 2021" target="META AGENT SEARCH">
      <data key="d4">16.0</data>
      <data key="d5">Chen et al., 2021 discusses safety concerns when executing untrusted model-generated code in Meta Agent Search</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="COBBE ET AL., 2021" target="GSM8K">
      <data key="d4">8.0</data>
      <data key="d5">Cobbe et al., 2021 discusses the GSM8K dataset</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="WEI ET AL., 2022" target="COT">
      <data key="d4">30.0</data>
      <data key="d5">CoT is a method referenced in Wei et al., 2022
The paper by Wei et al. (2022) discusses the CoT method and its application in language models.
Wei et al., 2022 introduced the CoT prompting method</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf,99d90aededb61e04241516ed9ec656cc,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="WEI ET AL., 2022" target="CHAIN-OF-THOUGHT (COT)">
      <data key="d4">14.0</data>
      <data key="d5">Chain-of-Thought (COT) was introduced by Wei et al., 2022
Wei et al., 2022 is a publication referenced for the Chain-of-Thought (COT) method</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="WEI ET AL., 2022" target="CHAIN-OF-THOUGHT">
      <data key="d4">14.0</data>
      <data key="d5">Wei et al., 2022 discusses the Chain-of-Thought method
The publication by Wei et al. in 2022 is related to Chain-of-Thought</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="WANG ET AL., 2022" target="SELF-CONSISTENCY SCORE">
      <data key="d4">14.0</data>
      <data key="d5">Self-consistency score is a metric referenced in Wang et al., 2022</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="WANG ET AL., 2022" target="COT-SC">
      <data key="d4">1.0</data>
      <data key="d5">The paper by Wang et al. (2022) discusses the CoT-SC method and its application in language models.</data>
      <data key="d6">fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="CHEN ET AL., 2023B" target="AGENTVERSE">
      <data key="d4">8.0</data>
      <data key="d5">Chen et al., 2023b discusses AgentVerse and its application in optimizing role definition in the prompt</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="YAO ET AL., 2023A" target="TOT">
      <data key="d4">30.0</data>
      <data key="d5">ToT is a method referenced in Yao et al., 2023a
The paper by Yao et al. (2023a) discusses the ToT method and its application in language models.
Yao et al., 2023a introduced the ToT search method</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf,99d90aededb61e04241516ed9ec656cc,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="HAO ET AL., 2023" target="RAP">
      <data key="d4">36.0</data>
      <data key="d5">RAP is a method referenced in Hao et al., 2023
The paper by Hao et al. (2023) discusses the RAP method and its application in language models.
Hao et al., 2023 introduced the RAP search method
Hao et al., 2023 is a reference related to RAP and its performance</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf,594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc,fb9cb0c0984d44c3da881886ed637e55</data>
    </edge>
    <edge source="AHN ET AL., 2022" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Ahn et al., 2022 is the reference for using language models as high-level controllers in robotics</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="HUANG ET AL., 2022" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Huang et al., 2022 is the reference for using language models as high-level controllers in robotics</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="DRIESS ET AL., 2023" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Driess et al., 2023 is the reference for using language models as high-level controllers in robotics</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="BAKER ET AL., 2022" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Baker et al., 2022 is the reference for adapting language model agents to complex multimodal games</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="WANG ET AL., 2023" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Wang et al., 2023 is the reference for adapting language model agents to complex multimodal games</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="GUSS ET AL., 2019" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Guss et al., 2019 is the reference for the game Minecraft used in adapting language model agents</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="FAN ET AL., 2022" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Fan et al., 2022 is the reference for the game Minecraft used in adapting language model agents</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LIU ET AL., 2018" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Liu et al., 2018 is the reference for using language models in text-based environments</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="SHRIDHAR ET AL., 2020" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Shridhar et al., 2020 is the reference for using language models in text-based environments</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LIU ET AL., 2024" target="LMS FOR ACTING">
      <data key="d4">16.0</data>
      <data key="d5">Liu et al., 2024 is the reference for using language models in text-based environments</data>
      <data key="d6">f8e7ed806916bf15245bcb4d52570c26</data>
    </edge>
    <edge source="LIU ET AL., 2024" target="EOH">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Liu et al. in 2024 is related to EoH</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="MADAAN ET AL., 2023" target="SELF-REFLECTION">
      <data key="d4">14.0</data>
      <data key="d5">Self-reflection is a process referenced in Madaan et al., 2023</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="SHINN ET AL., 2023" target="SELF-REFLECTION">
      <data key="d4">14.0</data>
      <data key="d5">Self-reflection is a process referenced in Shinn et al., 2023</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="SHINN ET AL., 2023" target="META AGENT">
      <data key="d4">6.0</data>
      <data key="d5">The meta agent's self-reflection process is influenced by the work of Shinn et al. (2023).</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="SEARCH ALGORITHMS" target="PLANNING">
      <data key="d4">16.0</data>
      <data key="d5">Planning refers to the use of a search algorithm to determine the best course of action in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="SEARCH ALGORITHMS" target="RAP">
      <data key="d4">16.0</data>
      <data key="d5">RAP incorporates planning and search algorithms to enhance reasoning and decision-making in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="SEARCH ALGORITHMS" target="TOT">
      <data key="d4">16.0</data>
      <data key="d5">ToT incorporates planning and search algorithms to enhance reasoning and decision-making in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="SEARCH ALGORITHMS" target="ZHUANG ET AL., 2023">
      <data key="d4">1.0</data>
      <data key="d5">Zhuang et al., 2023 is a reference related to search algorithms like A* and DFS</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b</data>
    </edge>
    <edge source="SEARCH ALGORITHMS" target="TRAJECTORY CONSTRUCTION">
      <data key="d4">9.0</data>
      <data key="d5">Trajectory construction in LATS is done using search algorithms</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PROMPTS" target="LM">
      <data key="d4">16.0</data>
      <data key="d5">Prompts are provided along with the input to improve reasoning in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="IN-CONTEXT LEARNING" target="LM">
      <data key="d4">16.0</data>
      <data key="d5">In-context learning leverages the abilities of language models to learn from the context</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="IN-CONTEXT LEARNING" target="MEMORY">
      <data key="d4">16.0</data>
      <data key="d5">In-context learning integrates stored trajectories and reflections as additional context</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="REASONING" target="LANGUAGE AGENT TREE SEARCH">
      <data key="d4">9.0</data>
      <data key="d5">LATS unifies reasoning, acting, and planning for enhanced LM problem-solving</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="REASONING" target="LANGUAGE MODELS">
      <data key="d4">8.0</data>
      <data key="d5">Language models use reasoning to solve problems</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="REASONING" target="DECISION-MAKING">
      <data key="d4">8.0</data>
      <data key="d5">Reasoning is a key component of decision-making</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="REASONING" target="EXPERIMENTS">
      <data key="d4">8.0</data>
      <data key="d5">Experiments were conducted on the Reasoning domain</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="REASONING" target="AGENTINSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct generates data covering the skill of reasoning</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="ACTING" target="LANGUAGE AGENT TREE SEARCH">
      <data key="d4">9.0</data>
      <data key="d5">LATS unifies reasoning, acting, and planning for enhanced LM problem-solving</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="ACTING" target="LANGUAGE MODELS">
      <data key="d4">8.0</data>
      <data key="d5">Language models execute actions based on decisions</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="ACTING" target="DECISION-MAKING">
      <data key="d4">8.0</data>
      <data key="d5">Acting is a key component of decision-making</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PLANNING" target="LANGUAGE AGENT TREE SEARCH">
      <data key="d4">9.0</data>
      <data key="d5">LATS unifies reasoning, acting, and planning for enhanced LM problem-solving</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PLANNING" target="LANGUAGE MODELS">
      <data key="d4">8.0</data>
      <data key="d5">Language models create plans to achieve goals</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PLANNING" target="DECISION-MAKING">
      <data key="d4">8.0</data>
      <data key="d5">Planning is a key component of decision-making</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="DECISION-MAKING" target="LM">
      <data key="d4">16.0</data>
      <data key="d5">Decision-making refers to the process of making choices based on reasoning and planning in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="DECISION-MAKING" target="LANGUAGE AGENT TREE SEARCH">
      <data key="d4">9.0</data>
      <data key="d5">LATS enhances decision-making capabilities of language models</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="DECISION-MAKING" target="LANGUAGE MODELS">
      <data key="d4">8.0</data>
      <data key="d5">Language models are used for decision-making tasks</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="DECISION-MAKING" target="INTERACTION">
      <data key="d4">8.0</data>
      <data key="d5">Interaction enhances decision-making in LATS</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="DECISION-MAKING" target="REFLECTION">
      <data key="d4">8.0</data>
      <data key="d5">Reflection enhances decision-making in LATS</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="MATH" target="MGSM">
      <data key="d4">8.0</data>
      <data key="d5">MGSM is a benchmark for math tasks</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="MATH" target="GSM8K">
      <data key="d4">8.0</data>
      <data key="d5">GSM8K is a benchmark for math tasks</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="MATH" target="GSM-HARD">
      <data key="d4">8.0</data>
      <data key="d5">GSM-Hard is a benchmark for math tasks</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="MATH" target="META AGENT SEARCH">
      <data key="d4">22.0</data>
      <data key="d5">Meta Agent Search tests agents in the Math domain
Meta Agent Search is effective in the Math domain</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="MATH" target="F1 SCORE">
      <data key="d4">7.0</data>
      <data key="d5">F1 scores are used to evaluate agents in the Math domain</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="MATH" target="ACCURACY">
      <data key="d4">1.0</data>
      <data key="d5">Accuracy rates are used to evaluate agents in the Math domain</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="MATH" target="AGENTINSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct generates data covering the skill of math</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="PROGRAMMING" target="MBPP">
      <data key="d4">8.0</data>
      <data key="d5">Programming tasks are evaluated using the MBPP dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="COT" target="WEI ET AL.">
      <data key="d4">18.0</data>
      <data key="d5">Wei et al. contributed to the development of Chain-of-thought (CoT) prompting</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="COT" target="LM AGENT">
      <data key="d4">7.0</data>
      <data key="d5">CoT is used as the base prompting framework for LM Agent in environments without feedback</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="COT" target="FM">
      <data key="d4">7.0</data>
      <data key="d5">FM is prompted to think step by step in the COT method</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="COT" target="MIRAGE">
      <data key="d4">8.0</data>
      <data key="d5">CoT shows the performance of models on MIRAGE datasets when answering directly without using RAG</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="HUANG ET AL." target="EXTERNAL TOOLS">
      <data key="d4">16.0</data>
      <data key="d5">Huang et al. suggested the use of external tools to enhance the reasoning and practical abilities of language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="EXTERNAL TOOLS" target="SHICK ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Schick et al. contributed to the understanding of using external tools to enhance language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="EXTERNAL TOOLS" target="SHEN ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Shen et al. contributed to the understanding of using external tools to enhance language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="EXTERNAL TOOLS" target="SURIS ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Suris et al. contributed to the understanding of using external tools to enhance language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="EXTERNAL TOOLS" target="REASONING TASKS">
      <data key="d4">7.0</data>
      <data key="d5">External tools are tools or APIs used by an agent in reasoning tasks</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="MCTS">
      <data key="d4">18.0</data>
      <data key="d5">Tree-based search and MCTS are both search algorithms used to explore multiple branches of outcomes</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="REINFORCEMENT LEARNING">
      <data key="d4">16.0</data>
      <data key="d5">Reinforcement learning and tree-based search are both used for their good exploration-exploitation trade-off in planning algorithms</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="VODOPIVEC ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Vodopivec et al. contributed to the understanding of tree-based search in reinforcement learning</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="SWIECHOWSKI ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Swiechowski et al. contributed to the understanding of tree-based search in planning algorithms</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="LAVALLE">
      <data key="d4">16.0</data>
      <data key="d5">LaValle contributed to the understanding of tree-based search in planning algorithms</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="HAFNER ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Hafner et al. contributed to the understanding of tree-based search in reinforcement learning</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="DU ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Du et al. contributed to the understanding of tree-based search in reinforcement learning</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="TREE-BASED SEARCH" target="WU ET AL.">
      <data key="d4">16.0</data>
      <data key="d5">Wu et al. contributed to the understanding of tree-based search in reinforcement learning</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="MCTS" target="LM TASKS">
      <data key="d4">7.0</data>
      <data key="d5">LM tasks do not have the limitation of requiring an environment model, unlike MCTS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="MCTS" target="EXPLORATION WEIGHT">
      <data key="d4">8.0</data>
      <data key="d5">Exploration weight is a parameter used in MCTS to balance exploration and exploitation</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="MCTS" target="PARENT NODE">
      <data key="d4">8.0</data>
      <data key="d5">Parent node is a node in the tree structure from which child nodes are derived in MCTS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="MCTS" target="EPISODE">
      <data key="d4">7.0</data>
      <data key="d5">An episode refers to a complete sequence of actions and observations in MCTS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="MCTS" target="RETURN">
      <data key="d4">8.0</data>
      <data key="d5">Return is the reward or feedback used for updating the value function in MCTS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="MCTS" target="VALUE FUNCTION">
      <data key="d4">8.0</data>
      <data key="d5">Value function is used to estimate the expected return of a state in MCTS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="MCTS" target="LM-BASED HEURISTIC">
      <data key="d4">7.0</data>
      <data key="d5">MCTS incorporates the LM-based heuristic used in ToT to improve performance</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LM" target="SELF-REFLECTION">
      <data key="d4">16.0</data>
      <data key="d5">Self-reflection uses language model-generated feedback to improve reasoning and decision-making</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="EXTERNAL MEMORY">
      <data key="d4">16.0</data>
      <data key="d5">External memory is used to store past text context for future updates of the solution in language models</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="LM TASKS">
      <data key="d4">16.0</data>
      <data key="d5">LM tasks refer to the various tasks that language models can perform</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="LM INTERNAL REASONING">
      <data key="d4">16.0</data>
      <data key="d5">LM internal reasoning refers to the reasoning capabilities of language models without external inputs</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="PROMPT IO">
      <data key="d4">16.0</data>
      <data key="d5">Prompt IO refers to the process where an input prompt is transformed into an output by a language model</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="INPUT X">
      <data key="d4">16.0</data>
      <data key="d5">Input X refers to the initial input provided to a language model for reasoning or decision-making</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="OUTPUT Y">
      <data key="d4">16.0</data>
      <data key="d5">Output Y refers to the final output generated by a language model based on the given input</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="P&#920;(X)">
      <data key="d4">16.0</data>
      <data key="d5">P&#952;(X) refers to the pre-trained language model parameterized by &#952; used to generate outputs based on given inputs</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="LM" target="AUTOREGRESSIVE DECODING">
      <data key="d4">2.0</data>
      <data key="d5">Autoregressive decoding is a method where the language model generates text sequentially</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="YA0 ET AL." target="TOT">
      <data key="d4">18.0</data>
      <data key="d5">Yao et al. contributed to the development of the ToT technique</data>
      <data key="d6">c95e02c0dca4a4a36b701cbc7dd14da6</data>
    </edge>
    <edge source="SELF-REFLECTION" target="REFLECTION">
      <data key="d4">18.0</data>
      <data key="d5">Reflection leverages self-reflection to refine decision-making</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="SELF-REFLECTION" target="MEMORY">
      <data key="d4">16.0</data>
      <data key="d5">Failed trajectories and reflections are stored in memory</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="SELF-REFLECTION" target="AI PYTHON ASSISTANT">
      <data key="d4">9.0</data>
      <data key="d5">The AI Python assistant performs self-reflection to identify errors and improve code</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="SELF-REFLECTION" target="FUNCTION IMPLEMENTATION">
      <data key="d4">8.0</data>
      <data key="d5">Self-reflection is used to review and improve function implementations</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="SELF-REFLECTION" target="REFLECTION PROMPT">
      <data key="d4">8.0</data>
      <data key="d5">The REFLECTION PROMPT guides the AI Python assistant in performing SELF-REFLECTION</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="SELF-REFLECTION" target="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)">
      <data key="d4">14.0</data>
      <data key="d5">Self-Reflection is a technique used as a building block in agentic systems within the research area of Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="SELF-REFLECTION" target="MADAAN ET AL.">
      <data key="d4">14.0</data>
      <data key="d5">Madaan et al. contributed to the research on self-reflection in agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="SELF-REFLECTION" target="META AGENT">
      <data key="d4">9.0</data>
      <data key="d5">The meta agent performs self-reflection to review and improve its proposed architecture and implementation.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="SELF-REFLECTION" target="FM MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The FM Module is used in the self-reflection process to improve task-solving</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="SELF-REFLECTION" target="COT_INITIAL_INSTRUCTION">
      <data key="d4">7.0</data>
      <data key="d5">Self-reflection uses the COT_INITIAL_INSTRUCTION for initial reasoning</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="SELF-REFLECTION" target="COT_REFLECT_INSTRUCTION">
      <data key="d4">7.0</data>
      <data key="d5">Self-reflection uses the COT_REFLECT_INSTRUCTION for reflecting on previous attempts and feedback</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="SELF-REFLECTION" target="CRITIC_INSTRUCTION">
      <data key="d4">7.0</data>
      <data key="d5">Self-reflection uses the CRITIC_INSTRUCTION for providing feedback and correcting the answer</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="EXTERNAL MEMORY" target="META AGENT SEARCH">
      <data key="d4">6.0</data>
      <data key="d5">External Memory is used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="REINFORCEMENT LEARNING" target="SEARCH ALGORITHM">
      <data key="d4">7.0</data>
      <data key="d5">Reinforcement Learning is a method used in search algorithms to explore the search space in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="WEI ET AL." target="CHAIN-OF-THOUGHT">
      <data key="d4">28.0</data>
      <data key="d5">Wei et al. contributed to the research on chain-of-thought planning and reasoning
Wei et al. are the authors of the Chain-of-Thought agent
Wei et al. are the authors of the Chain-of-Thought method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b,c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="DU ET AL." target="LLM DEBATE">
      <data key="d4">14.0</data>
      <data key="d5">Du et al. are the authors of the LLM Debate agent
Du et al. are the authors of the LLM Debate method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="WU ET AL." target="ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES AND ENABLING THEM TO COLLABORATE">
      <data key="d4">8.0</data>
      <data key="d5">Wu et al. are the authors of the method for assigning FM modules in the agentic system with different roles and enabling them to collaborate</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="LM TASKS" target="RESET">
      <data key="d4">8.0</data>
      <data key="d5">Reset refers to the ability to revert to a previous state or step in LM tasks</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="RAP" target="LANGUAGE MODEL (LM)">
      <data key="d4">7.0</data>
      <data key="d5">RAP relies on the internal representations of the language model (LM) for reasoning and planning.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="RAP" target="PERFORMANCE">
      <data key="d4">7.0</data>
      <data key="d5">RAP is evaluated for performance in the study</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="RAP" target="TOKEN CONSUMPTION">
      <data key="d4">7.0</data>
      <data key="d5">RAP is evaluated for token consumption in the study</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="TOT" target="LM-BASED HEURISTIC">
      <data key="d4">8.0</data>
      <data key="d5">ToT uses the LM-based heuristic to prune branches with low values</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="INPUT X" target="PROMPT">
      <data key="d4">8.0</data>
      <data key="d5">A prompt is provided along with input x to improve reasoning in language models.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="INPUT X" target="OUTPUT Y">
      <data key="d4">9.0</data>
      <data key="d5">Input x is transformed into output y by the language model.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="INPUT X" target="THOUGHT Z">
      <data key="d4">8.0</data>
      <data key="d5">Thought z is created as an intermediate step between input x and output y in Chain-of-thought (CoT) prompting.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="OUTPUT Y" target="THOUGHT Z">
      <data key="d4">8.0</data>
      <data key="d5">Thought z is created as an intermediate step between input x and output y in Chain-of-thought (CoT) prompting.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="UPPER CONFIDENCE BOUNDS APPLIED TO TREES (UCT)" target="KOCSIS AND SZEPESV&#193;RI (2006)">
      <data key="d4">9.0</data>
      <data key="d5">Kocsis and Szepesv&#225;ri developed the Upper Confidence bounds applied to Trees (UCT) metric used in MCTS.</data>
      <data key="d6">9bb90746134619cad9a3e649b8b35f24</data>
    </edge>
    <edge source="PROMPT" target="THOUGHT">
      <data key="d4">8.0</data>
      <data key="d5">Prompt guides the Thought process in HotPotQA</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="PROMPT" target="META AGENT">
      <data key="d4">9.0</data>
      <data key="d5">The meta agent uses the prompt to format its output, including sections for thought process, agent name, and code implementation.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="PROMPT" target="FM MODULE">
      <data key="d4">9.0</data>
      <data key="d5">The FM Module generates a prompt by concatenating input Info objects</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="PROMPT" target="SYSTEM PROMPT">
      <data key="d4">8.0</data>
      <data key="d5">The prompt generated by the FM Module includes a system prompt</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="PROMPT" target="USER PROMPT">
      <data key="d4">8.0</data>
      <data key="d5">The prompt generated by the FM Module includes a user prompt</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="LM AGENT" target="ENVIRONMENT">
      <data key="d4">8.0</data>
      <data key="d5">The environment provides observations to the LM Agent and receives actions from it</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LM AGENT" target="P&#920;">
      <data key="d4">9.0</data>
      <data key="d5">P&#952; is the language model used to initialize the LM Agent</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="LM AGENT" target="POLICY">
      <data key="d4">8.0</data>
      <data key="d5">Policy is a strategy followed by the LM Agent to decide actions based on observations and past actions</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="ENVIRONMENT" target="OBSERVATION">
      <data key="d4">8.0</data>
      <data key="d5">Observation is the feedback received from the environment after an action is taken by the agent</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="P&#920;" target="VALUE FUNCTION">
      <data key="d4">16.0</data>
      <data key="d5">P&#952; is repurposed into a value function by reasoning about a given state</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="UCT ALGORITHM" target="SELECTION">
      <data key="d4">8.0</data>
      <data key="d5">The UCT algorithm is used in the selection operation of LATS to balance exploration and exploitation</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="EXPANSION" target="LONG-TERM MEMORY STRUCTURE">
      <data key="d4">7.0</data>
      <data key="d5">The expanded tree in LATS is stored in a long-term memory structure</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="EXPANSION" target="AGENTINSTRUCT FLOW">
      <data key="d4">7.0</data>
      <data key="d5">The AgentInstruct Flow includes expansion as one of the text modification tasks.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SIMULATION" target="TERMINAL STATE">
      <data key="d4">16.0</data>
      <data key="d5">Simulation expands the selected node until a terminal state is reached</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="BACKPROPAGATION" target="TRAJECTORY">
      <data key="d4">18.0</data>
      <data key="d5">Backpropagation updates the values of the tree based on the outcome of a trajectory</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="BACKPROPAGATION" target="REWARD">
      <data key="d4">16.0</data>
      <data key="d5">Reward is used in backpropagation to update the values of the tree</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="BACKPROPAGATION" target="UCT FORMULA">
      <data key="d4">16.0</data>
      <data key="d5">The UCT formula uses updated values from backpropagation to guide node selection</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="REFLECTION" target="LANGUAGE AGENT TREE SEARCH">
      <data key="d4">8.0</data>
      <data key="d5">LATS incorporates reflection to enhance decision-making</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="REFLECTION" target="DAIRY FREE AND APPLE VARIETY PACK OF CHIPS">
      <data key="d4">6.0</data>
      <data key="d5">The reflection action is used to review the decision to buy the dairy-free and apple variety pack of chips</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="REFLECTION" target="ACTION">
      <data key="d4">7.0</data>
      <data key="d5">Reflection is a type of action taken by the user</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="REFLECTION" target="META AGENT SEARCH">
      <data key="d4">6.0</data>
      <data key="d5">Reflection is used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="TASK" target="COMPUTATIONAL LIMIT">
      <data key="d4">8.0</data>
      <data key="d5">A task in LATS is completed when a solution is found or a computational limit is reached</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="TRAJECTORY" target="TERMINAL STATE">
      <data key="d4">16.0</data>
      <data key="d5">A trajectory is the sequence of states from the root to the terminal state</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="TRAJECTORY" target="PREVIOUS TRIAL">
      <data key="d4">1.0</data>
      <data key="d5">Trajectory refers to the sequence of actions in the previous trial</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="FEEDBACK" target="META AGENT SEARCH">
      <data key="d4">8.0</data>
      <data key="d5">Feedback is provided by critics and experts to refine answers in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="FEEDBACK" target="CRITIC_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The critic_module provides feedback</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="FEEDBACK" target="THOUGHTS">
      <data key="d4">14.0</data>
      <data key="d5">Thoughts are evaluated to generate feedback</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FEEDBACK" target="CORRECT EXAMPLES">
      <data key="d4">14.0</data>
      <data key="d5">Feedback includes correct examples where the code produced the correct output</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FEEDBACK" target="WRONG EXAMPLES">
      <data key="d4">14.0</data>
      <data key="d5">Feedback includes wrong examples where the code produced the incorrect output</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FEEDBACK" target="CORRECT_COUNT">
      <data key="d4">1.0</data>
      <data key="d5">Feedback includes the correct_count of examples</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FEEDBACK" target="CODE">
      <data key="d4">9.0</data>
      <data key="d5">Feedback is provided based on the performance of the code</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="DECISION-MAKING TASKS" target="COMMANDS">
      <data key="d4">7.0</data>
      <data key="d5">Commands are specific actions taken by an agent in decision-making tasks</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="SAMPLING" target="STOCHASTIC NATURE">
      <data key="d4">8.0</data>
      <data key="d5">Sampling in LATS involves selecting a diverse set of candidates to mitigate the stochastic nature of LM text generation</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="SEARCH ALGORITHM" target="ADAS">
      <data key="d4">16.0</data>
      <data key="d5">The search algorithm is a key component of ADAS that defines how the search space is explored</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH ALGORITHM" target="SUTTON">
      <data key="d4">10.0</data>
      <data key="d5">Sutton is referenced in the context of the exploration-exploitation trade-off in search algorithms</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH ALGORITHM" target="BARTO">
      <data key="d4">6.0</data>
      <data key="d5">Barto is referenced in the context of the exploration-exploitation trade-off in search algorithms</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH ALGORITHM" target="EXPLORATION-EXPLOITATION TRADE-OFF">
      <data key="d4">7.0</data>
      <data key="d5">The exploration-exploitation trade-off is a concept in search algorithms that balances discovering high-performance agentic systems and avoiding local optima</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="VALUE FUNCTION" target="VOLD(S)">
      <data key="d4">7.0</data>
      <data key="d5">Vold(s) is the old value function before it is updated with the new return in MCTS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="VALUE FUNCTION" target="N(S)">
      <data key="d4">7.0</data>
      <data key="d5">N(s) is the number of times a state has been visited, used in the value function update in MCTS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="VALUE FUNCTION" target="ENVIRONMENTAL FEEDBACK">
      <data key="d4">14.0</data>
      <data key="d5">Environmental feedback is used to improve value assignment</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="VALUE FUNCTION" target="SELF-CONSISTENCY SCORE">
      <data key="d4">8.0</data>
      <data key="d5">Value Function uses Self-Consistency Score to improve performance in the Game of 24</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="ACTION SPACE" target="PERMISSIBLE ACTIONS">
      <data key="d4">7.0</data>
      <data key="d5">Permissible actions are the actions that an agent is allowed to take in the action space of LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="ACTION SPACE" target="REASONING TRACES">
      <data key="d4">7.0</data>
      <data key="d5">Reasoning traces are the language-based thoughts used in the action space of LATS</data>
      <data key="d6">c234cb83764b899335af0950677ad024</data>
    </edge>
    <edge source="OBSERVATION" target="THOUGHT">
      <data key="d4">8.0</data>
      <data key="d5">Observation leads to the next Thought in HotPotQA</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="OBSERVATION" target="ACTION">
      <data key="d4">31.0</data>
      <data key="d5">Action leads to Observation in HotPotQA
Actions lead to observations that provide feedback or information
Observation is a type of action taken by the user</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f,5d356b8ff719763a38cecff22c4e17b7,b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="OBSERVATION" target="TRAJECTORIES">
      <data key="d4">16.0</data>
      <data key="d5">Trajectories include observations that provide feedback or information</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="OBSERVATION" target="SEARCH">
      <data key="d4">8.0</data>
      <data key="d5">OBSERVATION is the result received after performing a SEARCH</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="OBSERVATION" target="THINK">
      <data key="d4">7.0</data>
      <data key="d5">THINK is the process of reasoning about the OBSERVATION before taking further action</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="HEURISTIC" target="CAMPBELL ET AL., 2002">
      <data key="d4">14.0</data>
      <data key="d5">Heuristic is a concept referenced in Campbell et al., 2002</data>
      <data key="d6">02ef0185bbeaaef92c3a8ee18b7a38cf</data>
    </edge>
    <edge source="AGENT" target="META AGENT SEARCH">
      <data key="d4">9.0</data>
      <data key="d5">Meta Agent Search discovers agents that outperform state-of-the-art hand-designed baselines</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="AGENT" target="SUGGESTER AGENT">
      <data key="d4">7.0</data>
      <data key="d5">Suggester agents are a type of agent that propose various approaches to increase the intricacy of instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENT" target="EDITOR AGENT">
      <data key="d4">7.0</data>
      <data key="d5">Editor agents are a type of agent that modify instructions based on suggestions from Suggester agents.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENT" target="SEARCH API">
      <data key="d4">7.0</data>
      <data key="d5">Agents can use search APIs as tools to perform search operations as part of their tasks.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENT" target="CODE INTERPRETER">
      <data key="d4">7.0</data>
      <data key="d5">Agents can use code interpreters as tools to interpret and execute code as part of their tasks.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENT" target="CALCULATOR">
      <data key="d4">7.0</data>
      <data key="d5">Agents can use calculators as tools to perform calculations as part of their tasks.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENT" target="SEED INSTRUCTION GENERATION FLOW">
      <data key="d4">8.0</data>
      <data key="d5">The Seed Instruction Generation Flow uses multiple agents to generate questions from a given text.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="AGENT" target="CONTENT TRANSFORMATION AGENT">
      <data key="d4">7.0</data>
      <data key="d5">The Content Transformation Agent determines which subset of agents to engage in the Seed Instruction Generation Flow.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="REWARD" target="SUCCESS RATE">
      <data key="d4">8.0</data>
      <data key="d5">Reward and Success Rate are metrics used to evaluate performance in WebShop</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="COT-SC" target="PERFORMANCE">
      <data key="d4">7.0</data>
      <data key="d5">CoT-SC is evaluated for performance in the study</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="COT-SC" target="TOKEN CONSUMPTION">
      <data key="d4">7.0</data>
      <data key="d5">CoT-SC is evaluated for token consumption in the study</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="COT-SC" target="META AGENT SEARCH">
      <data key="d4">27.0</data>
      <data key="d5">Meta Agent Search compares its discovered agents against the COT-SC baseline
Meta Agent Search outperforms the COT-SC method
COT-SC is a method used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="COT-SC" target="WANG ET AL.">
      <data key="d4">14.0</data>
      <data key="d5">Wang et al. are the authors of the COT-SC agent
Wang et al. are the authors of the COT-SC method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="COT-SC" target="FM">
      <data key="d4">7.0</data>
      <data key="d5">FM is used to sample answers and perform an ensemble in the COT-SC method</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="AUSTIN ET AL., 2022" target="MBPP">
      <data key="d4">9.0</data>
      <data key="d5">Austin et al., 2022 introduced the MBPP dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="MBPP" target="TAB. 4">
      <data key="d4">8.0</data>
      <data key="d5">Tab. 4 shows the performance of various methods on programming tasks using the MBPP dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="MBPP" target="TAB. 5">
      <data key="d4">8.0</data>
      <data key="d5">Tab. 5 shows the GPT-3.5 Pass@1 accuracy on MBPP for various prompting methods</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="MBPP" target="APPENDIX SEC. D">
      <data key="d4">7.0</data>
      <data key="d5">Appendix Sec. D provides additional details on the evaluation of LATS and other methods on programming tasks using the MBPP dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="MBPP" target="CHEN ET AL., 2023A">
      <data key="d4">8.0</data>
      <data key="d5">Chen et al., 2023a discusses the use of an LM to generate a synthetic test suite for evaluating programming tasks using the MBPP dataset</data>
      <data key="d6">99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="MBPP" target="NATURAL LANGUAGE DESCRIPTION">
      <data key="d4">8.0</data>
      <data key="d5">Natural language descriptions explain programming tasks in the MBPP dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="MBPP" target="CROWDSOURCING">
      <data key="d4">8.0</data>
      <data key="d5">Crowdsourcing was used to create the MBPP dataset.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="FINE-TUNING" target="FURUTA ET AL., 2024">
      <data key="d4">7.0</data>
      <data key="d5">Furuta et al., 2024 discusses the fine-tuning method and its performance on WebShop
Furuta et al., 2024 is a reference related to fine-tuning methods</data>
      <data key="d6">594449768ae2dea9b2efbe677075096b,99d90aededb61e04241516ed9ec656cc</data>
    </edge>
    <edge source="FINE-TUNING" target="AGENTINSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct generates synthetic datasets for fine-tuning AI models</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="DFS" target="ZHUANG ET AL., 2023">
      <data key="d4">6.0</data>
      <data key="d5">Zhuang et al., 2023 is a reference that mentions DFS</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="ZHUANG ET AL., 2023" target="A*">
      <data key="d4">6.0</data>
      <data key="d5">Zhuang et al., 2023 is a reference that mentions A*</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="TOKEN CONSUMPTION" target="TAB. 9">
      <data key="d4">7.0</data>
      <data key="d5">Tab. 9 shows token consumption comparisons of different methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="TRAJECTORIES" target="THOUGHT">
      <data key="d4">16.0</data>
      <data key="d5">Trajectories include thoughts that reason about the current situation</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="TRAJECTORIES" target="ACTION">
      <data key="d4">16.0</data>
      <data key="d5">Trajectories include actions that can be performed to progress in the task</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="TRAJECTORIES" target="REFLECTION PROMPT">
      <data key="d4">18.0</data>
      <data key="d5">The Reflection Prompt involves analyzing the trajectories of a solution to a question-answering task</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="PROMPTING TECHNIQUES">
      <data key="d4">8.0</data>
      <data key="d5">LATS addresses key limitations of prior prompting techniques</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="TRAJECTORY CONSTRUCTION">
      <data key="d4">9.0</data>
      <data key="d5">LATS constructs trajectories with search algorithms to improve decision-making</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="INTERACTION">
      <data key="d4">8.0</data>
      <data key="d5">LATS incorporates interaction to enhance decision-making</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="AUTONOMOUS DECISION-MAKING">
      <data key="d4">9.0</data>
      <data key="d5">LATS enables autonomous decision-making in language models</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="COMPUTATIONAL BUDGET">
      <data key="d4">8.0</data>
      <data key="d5">LATS manages computational budget efficiently compared to other methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="GROUND-TRUTH FEEDBACK">
      <data key="d4">8.0</data>
      <data key="d5">LATS uses ground-truth feedback to improve performance</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="SYSTEM-2 LM APPROACHES">
      <data key="d4">8.0</data>
      <data key="d5">LATS is an example of a System-2 LM approach</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE AGENT TREE SEARCH" target="SYSTEM-1 LM APPROACHES">
      <data key="d4">7.0</data>
      <data key="d5">LATS has a higher computational cost compared to System-1 LM approaches</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="SYSTEM-2 LM APPROACHES" target="PROMPTING TECHNIQUES">
      <data key="d4">7.0</data>
      <data key="d5">Advanced prompting techniques like LATS are examples of System-2 LM approaches</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="SYSTEM-2 LM APPROACHES" target="COMPUTATIONAL COST">
      <data key="d4">1.0</data>
      <data key="d5">System-2 LM approaches have higher computational cost compared to System-1 approaches</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PERFORMANCE" target="TAB. 8">
      <data key="d4">7.0</data>
      <data key="d5">Tab. 8 shows performance comparisons of different methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PERFORMANCE" target="GROUND-TRUTH FEEDBACK">
      <data key="d4">8.0</data>
      <data key="d5">Ground-truth feedback improves the performance of language models</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PERFORMANCE" target="EVALUATION FUNCTION">
      <data key="d4">7.0</data>
      <data key="d5">Performance is a metric used in the evaluation function to assess an agent's effectiveness</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SAMPLE COMPLEXITY" target="TAB. 9">
      <data key="d4">7.0</data>
      <data key="d5">Tab. 9 shows sample complexity comparisons of different methods</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="COMPUTATIONAL COST" target="COMPUTATIONAL BUDGET">
      <data key="d4">8.0</data>
      <data key="d5">Managing computational budget is crucial for controlling computational cost</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="COMPUTATIONAL COST" target="SYSTEM-1 LM APPROACHES">
      <data key="d4">7.0</data>
      <data key="d5">System-1 LM approaches have lower computational cost compared to System-2 approaches</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="LANGUAGE MODELS" target="AUTONOMOUS DECISION-MAKING">
      <data key="d4">8.0</data>
      <data key="d5">Language models can make autonomous decisions</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PROMPTING TECHNIQUES" target="SYSTEM-1 LM APPROACHES">
      <data key="d4">7.0</data>
      <data key="d5">Simpler prompting techniques like ReAct and Reflexion are examples of System-1 LM approaches</data>
      <data key="d6">faa2bd677c7f052136479e0175da3e5b</data>
    </edge>
    <edge source="PROMPTING TECHNIQUES" target="META AGENT SEARCH">
      <data key="d4">6.0</data>
      <data key="d5">Prompting Techniques are used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="PROMPTING TECHNIQUES" target="CHEN ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Chen et al. are the authors of the Prompting Techniques method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="PROMPTING TECHNIQUES" target="SCHULHOFF ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Schulhoff et al. are the authors of the Prompting Techniques method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="CHELSEA FINN" target="RAFAEL RAFAILOV">
      <data key="d4">8.0</data>
      <data key="d5">Rafael Rafailov and Chelsea Finn co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="CHELSEA FINN" target="ARCHIT SHARMA">
      <data key="d4">8.0</data>
      <data key="d5">Archit Sharma and Chelsea Finn co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="CHELSEA FINN" target="ERIC MITCHELL">
      <data key="d4">8.0</data>
      <data key="d5">Eric Mitchell and Chelsea Finn co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="CHELSEA FINN" target="CHRISTOPHER D MANNING">
      <data key="d4">8.0</data>
      <data key="d5">Christopher D Manning and Chelsea Finn co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="CHELSEA FINN" target="STEFANO ERMON">
      <data key="d4">8.0</data>
      <data key="d5">Stefano Ermon and Chelsea Finn co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="QUOC LE" target="XUEZHI WANG">
      <data key="d4">8.0</data>
      <data key="d5">Xuezhi Wang and Quoc Le co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="QUOC LE" target="JASON WEI">
      <data key="d4">8.0</data>
      <data key="d5">Jason Wei and Quoc Le co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="QUOC LE" target="DALE SCHUURMANS">
      <data key="d4">8.0</data>
      <data key="d5">Dale Schuurmans and Quoc Le co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="QUOC LE" target="ED CHI">
      <data key="d4">16.0</data>
      <data key="d5">Quoc Le and Ed Chi co-authored a paper
Quoc Le and Ed Chi co-authored the paper "Least-to-most prompting enables complex reasoning in large language models"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7,8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="QUOC LE" target="DENNY ZHOU">
      <data key="d4">8.0</data>
      <data key="d5">Quoc Le and Denny Zhou co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="QUOC LE" target="OLIVIER BOUSQUET">
      <data key="d4">8.0</data>
      <data key="d5">Olivier Bousquet and Quoc Le co-authored the paper "Least-to-most prompting enables complex reasoning in large language models"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="JEFF CLUNE" target="SHENGRAN HU">
      <data key="d4">16.0</data>
      <data key="d5">Shengran Hu and Jeff Clune co-authored the study on Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="JEFF CLUNE" target="CONG LU">
      <data key="d4">16.0</data>
      <data key="d5">Cong Lu and Jeff Clune co-authored the study on Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="JEFF CLUNE" target="JENNY ZHANG">
      <data key="d4">16.0</data>
      <data key="d5">Jenny Zhang and Jeff Clune co-authored the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JEFF CLUNE" target="JOEL LEHMAN">
      <data key="d4">16.0</data>
      <data key="d5">Joel Lehman and Jeff Clune co-authored the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JEFF CLUNE" target="KENNETH STANLEY">
      <data key="d4">16.0</data>
      <data key="d5">Kenneth Stanley and Jeff Clune co-authored the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="CHRISTOPHER D MANNING" target="RAFAEL RAFAILOV">
      <data key="d4">8.0</data>
      <data key="d5">Rafael Rafailov and Christopher D Manning co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="CHRISTOPHER D MANNING" target="ARCHIT SHARMA">
      <data key="d4">8.0</data>
      <data key="d5">Archit Sharma and Christopher D Manning co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="CHRISTOPHER D MANNING" target="ERIC MITCHELL">
      <data key="d4">8.0</data>
      <data key="d5">Eric Mitchell and Christopher D Manning co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="CHRISTOPHER D MANNING" target="STEFANO ERMON">
      <data key="d4">8.0</data>
      <data key="d5">Christopher D Manning and Stefano Ermon co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="AMANDA ASKELL" target="ANTHROPIC">
      <data key="d4">12.0</data>
      <data key="d5">Amanda Askell is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ILYA SUTSKEVER" target="DAVID SILVER">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Ilya Sutskever co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="AAKANKSHA CHOWDHERY" target="MIRAC SUZGUN">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Aakanksha Chowdhery co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="AAKANKSHA CHOWDHERY" target="NATHAN SCALES">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Aakanksha Chowdhery co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="AAKANKSHA CHOWDHERY" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Aakanksha Chowdhery co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="AAKANKSHA CHOWDHERY" target="SEBASTIAN GEHRMANN">
      <data key="d4">8.0</data>
      <data key="d5">Sebastian Gehrmann and Aakanksha Chowdhery co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="AAKANKSHA CHOWDHERY" target="YI TAY">
      <data key="d4">8.0</data>
      <data key="d5">Yi Tay and Aakanksha Chowdhery co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="HYUNG WON CHUNG" target="MIRAC SUZGUN">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Hyung Won Chung co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="HYUNG WON CHUNG" target="NATHAN SCALES">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Hyung Won Chung co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="HYUNG WON CHUNG" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Hyung Won Chung co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="HYUNG WON CHUNG" target="SEBASTIAN GEHRMANN">
      <data key="d4">8.0</data>
      <data key="d5">Sebastian Gehrmann and Hyung Won Chung co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="HYUNG WON CHUNG" target="YI TAY">
      <data key="d4">8.0</data>
      <data key="d5">Yi Tay and Hyung Won Chung co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="MIRAC SUZGUN">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Sebastian Gehrmann co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="NATHAN SCALES">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Sebastian Gehrmann co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Sebastian Gehrmann co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="YI TAY">
      <data key="d4">8.0</data>
      <data key="d5">Sebastian Gehrmann and Yi Tay co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="QUOC V LE">
      <data key="d4">8.0</data>
      <data key="d5">Sebastian Gehrmann and Quoc V Le co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="ED H CHI">
      <data key="d4">8.0</data>
      <data key="d5">Sebastian Gehrmann and Ed H Chi co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="DENNY ZHOU">
      <data key="d4">8.0</data>
      <data key="d5">Sebastian Gehrmann and Denny Zhou co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="SEBASTIAN GEHRMANN" target="JASON WEI">
      <data key="d4">8.0</data>
      <data key="d5">Sebastian Gehrmann and Jason Wei co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YI TAY" target="MIRAC SUZGUN">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Yi Tay co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YI TAY" target="NATHAN SCALES">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Yi Tay co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YI TAY" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Yi Tay co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YI TAY" target="QUOC V LE">
      <data key="d4">8.0</data>
      <data key="d5">Yi Tay and Quoc V Le co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YI TAY" target="ED H CHI">
      <data key="d4">8.0</data>
      <data key="d5">Yi Tay and Ed H Chi co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="DENNY ZHOU" target="XUEZHI WANG">
      <data key="d4">8.0</data>
      <data key="d5">Xuezhi Wang and Denny Zhou co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="DENNY ZHOU" target="JASON WEI">
      <data key="d4">8.0</data>
      <data key="d5">Jason Wei and Denny Zhou co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="DENNY ZHOU" target="DALE SCHUURMANS">
      <data key="d4">8.0</data>
      <data key="d5">Dale Schuurmans and Denny Zhou co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="DENNY ZHOU" target="ED CHI">
      <data key="d4">8.0</data>
      <data key="d5">Ed Chi and Denny Zhou co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="DENNY ZHOU" target="MIRAC SUZGUN">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Denny Zhou co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="DENNY ZHOU" target="NATHAN SCALES">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Denny Zhou co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="DENNY ZHOU" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Denny Zhou co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="XUEZHI WANG" target="JASON WEI">
      <data key="d4">8.0</data>
      <data key="d5">Xuezhi Wang and Jason Wei co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="XUEZHI WANG" target="DALE SCHUURMANS">
      <data key="d4">8.0</data>
      <data key="d5">Xuezhi Wang and Dale Schuurmans co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="XUEZHI WANG" target="ED CHI">
      <data key="d4">8.0</data>
      <data key="d5">Xuezhi Wang and Ed Chi co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="JASON WEI" target="DALE SCHUURMANS">
      <data key="d4">8.0</data>
      <data key="d5">Jason Wei and Dale Schuurmans co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="JASON WEI" target="ED CHI">
      <data key="d4">8.0</data>
      <data key="d5">Jason Wei and Ed Chi co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="JASON WEI" target="MIRAC SUZGUN">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Jason Wei co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="JASON WEI" target="NATHAN SCALES">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Jason Wei co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="JASON WEI" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Jason Wei co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="DALE SCHUURMANS" target="ED CHI">
      <data key="d4">8.0</data>
      <data key="d5">Dale Schuurmans and Ed Chi co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="PIETER ABBEEL" target="PHILIPP WU">
      <data key="d4">8.0</data>
      <data key="d5">Philipp Wu and Pieter Abbeel co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="PIETER ABBEEL" target="ALEJANDRO ESCONTRELA">
      <data key="d4">8.0</data>
      <data key="d5">Alejandro Escontrela and Pieter Abbeel co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="PIETER ABBEEL" target="YOSHUA BENGIO">
      <data key="d4">8.0</data>
      <data key="d5">Yoshua Bengio and Pieter Abbeel co-authored the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="LINXI FAN" target="GUANZHI WANG">
      <data key="d4">8.0</data>
      <data key="d5">Guanzhi Wang and Linxi Fan co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="GUANZHI WANG" target="YUQI XIE">
      <data key="d4">8.0</data>
      <data key="d5">Guanzhi Wang and Yuqi Xie co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="GUANZHI WANG" target="YUNFAN JIANG">
      <data key="d4">8.0</data>
      <data key="d5">Guanzhi Wang and Yunfan Jiang co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="GUANZHI WANG" target="AJAY MANDLEKAR">
      <data key="d4">8.0</data>
      <data key="d5">Guanzhi Wang and Ajay Mandlekar co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="GUANZHI WANG" target="CHAOWEI XIAO">
      <data key="d4">8.0</data>
      <data key="d5">Guanzhi Wang and Chaowei Xiao co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="GUANZHI WANG" target="YUKE ZHU">
      <data key="d4">8.0</data>
      <data key="d5">Guanzhi Wang and Yuke Zhu co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="GUANZHI WANG" target="ANIMA ANANDKUMAR">
      <data key="d4">8.0</data>
      <data key="d5">Guanzhi Wang and Anima Anandkumar co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="PENGFEI LIU" target="YIXIN LIU">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Pengfei Liu co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="DANIJAR HAFNER" target="PHILIPP WU">
      <data key="d4">8.0</data>
      <data key="d5">Philipp Wu and Danijar Hafner co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="DANIJAR HAFNER" target="ALEJANDRO ESCONTRELA">
      <data key="d4">8.0</data>
      <data key="d5">Alejandro Escontrela and Danijar Hafner co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="PERCY LIANG" target="XUECHEN LI">
      <data key="d4">8.0</data>
      <data key="d5">Xuechen Li and Percy Liang co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="PERCY LIANG" target="TIANYI ZHANG">
      <data key="d4">8.0</data>
      <data key="d5">Tianyi Zhang and Percy Liang co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="PERCY LIANG" target="YANN DUBOIS">
      <data key="d4">8.0</data>
      <data key="d5">Yann Dubois and Percy Liang co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="PERCY LIANG" target="ROHAN TAORI">
      <data key="d4">8.0</data>
      <data key="d5">Rohan Taori and Percy Liang co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="PERCY LIANG" target="ISHAAN GULRAJANI">
      <data key="d4">8.0</data>
      <data key="d5">Ishaan Gulrajani and Percy Liang co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="PERCY LIANG" target="CARLOS GUESTRIN">
      <data key="d4">8.0</data>
      <data key="d5">Carlos Guestrin and Percy Liang co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="PERCY LIANG" target="TATSUNORI B. HASHIMOTO">
      <data key="d4">8.0</data>
      <data key="d5">Percy Liang and Tatsunori B. Hashimoto co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="PETER CLARK" target="ISAAC COWHEY">
      <data key="d4">8.0</data>
      <data key="d5">Peter Clark and Isaac Cowhey co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="PETER CLARK" target="OREN ETZIONI">
      <data key="d4">8.0</data>
      <data key="d5">Peter Clark and Oren Etzioni co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="PETER CLARK" target="TUSHAR KHOT">
      <data key="d4">8.0</data>
      <data key="d5">Peter Clark and Tushar Khot co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="PETER CLARK" target="ASHISH SABHARWAL">
      <data key="d4">8.0</data>
      <data key="d5">Peter Clark and Ashish Sabharwal co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="PETER CLARK" target="CARISSA SCHOENICK">
      <data key="d4">8.0</data>
      <data key="d5">Peter Clark and Carissa Schoenick co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="PETER CLARK" target="OYVIND TAFJORD">
      <data key="d4">8.0</data>
      <data key="d5">Peter Clark and Oyvind Tafjord co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="DAVID SILVER" target="AJA HUANG">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and Aja Huang co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"David Silver and Aja Huang co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="CHRIS J. MADDISON">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and Chris J. Maddison co-authored the paper "Mastering the game of Go with deep neural networks and tree search"David Silver and Chris J. Maddison co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="ARTHUR GUEZ">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and Arthur Guez co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"David Silver and Arthur Guez co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="L. SIFRE">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and L. Sifre co-authored the paper "Mastering the game of Go with deep neural networks and tree search"David Silver and L. Sifre co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="GEORGE VAN DEN DRIESSCHE">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and George van den Driessche co-authored the paper "Mastering the game of Go with deep neural networks and tree search"David Silver and George van den Driessche co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="JULIAN SCHRITTWIESER">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and Julian Schrittwieser co-authored the paper "Mastering the game of Go with deep neural networks and tree search"David Silver and Julian Schrittwieser co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="IOANNIS ANTONOGLOU">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and Ioannis Antonoglou co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"David Silver and Ioannis Antonoglou co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="VEDAVYAS PANNEERSHELVAM">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and Vedavyas Panneershelvam co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"David Silver and Vedavyas Panneershelvam co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="MARC LANCTOT">
      <data key="d4">16.0</data>
      <data key="d5">David Silver and Marc Lanctot co-authored the paper "Mastering chess and Shogi by self-play with a general reinforcement learning algorithm"David Silver and Marc Lanctot co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="SANDER DIELEMAN">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Sander Dieleman co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="DOMINIK GREWE">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Dominik Grewe co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="JOHN NHAM">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and John Nham co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="NAL KALCHBRENNER">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Nal Kalchbrenner co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="TIMOTHY P. LILLICRAP">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Timothy P. Lillicrap co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="MADELEINE LEACH">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Madeleine Leach co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="KORAY KAVUKCUOGLU">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Koray Kavukcuoglu co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="THORE GRAEPEL">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Thore Graepel co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="DAVID SILVER" target="DEMIS HASSABIS">
      <data key="d4">8.0</data>
      <data key="d5">David Silver and Demis Hassabis co-authored the paper "Mastering the game of Go with deep neural networks and tree search"</data>
      <data key="d6">2d4672dfb7bd4283f0b5f23ab4f26653</data>
    </edge>
    <edge source="YUCHEN ZHUANG" target="XIANG CHEN">
      <data key="d4">8.0</data>
      <data key="d5">Yuchen Zhuang and Xiang Chen co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="YUCHEN ZHUANG" target="TONG YU">
      <data key="d4">8.0</data>
      <data key="d5">Yuchen Zhuang and Tong Yu co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="YUCHEN ZHUANG" target="SAAYAN MITRA">
      <data key="d4">8.0</data>
      <data key="d5">Yuchen Zhuang and Saayan Mitra co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="YUCHEN ZHUANG" target="VICTOR BURSZTYN">
      <data key="d4">8.0</data>
      <data key="d5">Yuchen Zhuang and Victor Bursztyn co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="YUCHEN ZHUANG" target="RYAN A. ROSSI">
      <data key="d4">8.0</data>
      <data key="d5">Yuchen Zhuang and Ryan A. Rossi co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="YUCHEN ZHUANG" target="SOMDEB SARKHEL">
      <data key="d4">8.0</data>
      <data key="d5">Yuchen Zhuang and Somdeb Sarkhel co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="YUCHEN ZHUANG" target="CHAO ZHANG">
      <data key="d4">8.0</data>
      <data key="d5">Yuchen Zhuang and Chao Zhang co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="CHAO ZHANG" target="XIANG CHEN">
      <data key="d4">8.0</data>
      <data key="d5">Xiang Chen and Chao Zhang co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="CHAO ZHANG" target="TONG YU">
      <data key="d4">8.0</data>
      <data key="d5">Tong Yu and Chao Zhang co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="CHAO ZHANG" target="SAAYAN MITRA">
      <data key="d4">8.0</data>
      <data key="d5">Saayan Mitra and Chao Zhang co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="CHAO ZHANG" target="VICTOR BURSZTYN">
      <data key="d4">8.0</data>
      <data key="d5">Victor Bursztyn and Chao Zhang co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="CHAO ZHANG" target="RYAN A. ROSSI">
      <data key="d4">8.0</data>
      <data key="d5">Ryan A. Rossi and Chao Zhang co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="CHAO ZHANG" target="SOMDEB SARKHEL">
      <data key="d4">1.0</data>
      <data key="d5">Somdeb Sarkhel and Chao Zhang co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="TOM VODOPIVEC" target="SPYRIDON SAMOTHRAKIS">
      <data key="d4">8.0</data>
      <data key="d5">Tom Vodopivec and Spyridon Samothrakis co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="TOM VODOPIVEC" target="BRANKO STER">
      <data key="d4">8.0</data>
      <data key="d5">Tom Vodopivec and Branko Ster co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="SPYRIDON SAMOTHRAKIS" target="BRANKO STER">
      <data key="d4">8.0</data>
      <data key="d5">Spyridon Samothrakis and Branko Ster co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="ED CHI" target="OLIVIER BOUSQUET">
      <data key="d4">8.0</data>
      <data key="d5">Olivier Bousquet and Ed Chi co-authored the paper "Least-to-most prompting enables complex reasoning in large language models"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="MICHAEL WOOLDRIDGE" target="NICHOLAS R JENNINGS">
      <data key="d4">8.0</data>
      <data key="d5">Michael Wooldridge and Nicholas R Jennings co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="PHILIPP WU" target="ALEJANDRO ESCONTRELA">
      <data key="d4">8.0</data>
      <data key="d5">Philipp Wu and Alejandro Escontrela co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="PHILIPP WU" target="KEN GOLDBERG">
      <data key="d4">8.0</data>
      <data key="d5">Philipp Wu and Ken Goldberg co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="ALEJANDRO ESCONTRELA" target="KEN GOLDBERG">
      <data key="d4">8.0</data>
      <data key="d5">Alejandro Escontrela and Ken Goldberg co-authored a paper</data>
      <data key="d6">8180bf20b7577f3eee40df5991e2886d</data>
    </edge>
    <edge source="YOSHUA BENGIO" target="GEOFFREY HINTON">
      <data key="d4">16.0</data>
      <data key="d5">Yoshua Bengio and Geoffrey Hinton co-authored the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="YOSHUA BENGIO" target="ANDREW YAO">
      <data key="d4">16.0</data>
      <data key="d5">Yoshua Bengio and Andrew Yao co-authored the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="YOSHUA BENGIO" target="DAWN SONG">
      <data key="d4">16.0</data>
      <data key="d5">Yoshua Bengio and Dawn Song co-authored the paper "Managing Extreme AI Risks Amid Rapid Progress"</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="NATHAN SCALES" target="MIRAC SUZGUN">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Nathan Scales co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="NATHAN SCALES" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Nathanael Sch&#228;rli co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="NATHAN SCALES" target="QUOC V LE">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Quoc V Le co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="NATHAN SCALES" target="ED H CHI">
      <data key="d4">8.0</data>
      <data key="d5">Nathan Scales and Ed H Chi co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="XIANG CHEN" target="TONG YU">
      <data key="d4">8.0</data>
      <data key="d5">Xiang Chen and Tong Yu co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="XIANG CHEN" target="SAAYAN MITRA">
      <data key="d4">8.0</data>
      <data key="d5">Xiang Chen and Saayan Mitra co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="XIANG CHEN" target="VICTOR BURSZTYN">
      <data key="d4">8.0</data>
      <data key="d5">Xiang Chen and Victor Bursztyn co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="XIANG CHEN" target="RYAN A. ROSSI">
      <data key="d4">8.0</data>
      <data key="d5">Xiang Chen and Ryan A. Rossi co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="XIANG CHEN" target="SOMDEB SARKHEL">
      <data key="d4">8.0</data>
      <data key="d5">Xiang Chen and Somdeb Sarkhel co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="TONG YU" target="SAAYAN MITRA">
      <data key="d4">8.0</data>
      <data key="d5">Tong Yu and Saayan Mitra co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="TONG YU" target="VICTOR BURSZTYN">
      <data key="d4">8.0</data>
      <data key="d5">Tong Yu and Victor Bursztyn co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="TONG YU" target="RYAN A. ROSSI">
      <data key="d4">8.0</data>
      <data key="d5">Tong Yu and Ryan A. Rossi co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="TONG YU" target="SOMDEB SARKHEL">
      <data key="d4">8.0</data>
      <data key="d5">Tong Yu and Somdeb Sarkhel co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="SAAYAN MITRA" target="VICTOR BURSZTYN">
      <data key="d4">8.0</data>
      <data key="d5">Saayan Mitra and Victor Bursztyn co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="SAAYAN MITRA" target="RYAN A. ROSSI">
      <data key="d4">8.0</data>
      <data key="d5">Saayan Mitra and Ryan A. Rossi co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="SAAYAN MITRA" target="SOMDEB SARKHEL">
      <data key="d4">8.0</data>
      <data key="d5">Saayan Mitra and Somdeb Sarkhel co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="VICTOR BURSZTYN" target="RYAN A. ROSSI">
      <data key="d4">8.0</data>
      <data key="d5">Victor Bursztyn and Ryan A. Rossi co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="VICTOR BURSZTYN" target="SOMDEB SARKHEL">
      <data key="d4">8.0</data>
      <data key="d5">Victor Bursztyn and Somdeb Sarkhel co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="RYAN A. ROSSI" target="SOMDEB SARKHEL">
      <data key="d4">8.0</data>
      <data key="d5">Ryan A. Rossi and Somdeb Sarkhel co-authored the paper "ToolChain*: Efficient action space navigation in large language models with A* search"</data>
      <data key="d6">42de130f5b6144472a86a4c8260a87c7</data>
    </edge>
    <edge source="WIKIPEDIA WEB API" target="SEARCH [ENTITY]">
      <data key="d4">8.0</data>
      <data key="d5">Search [entity] is an action type in the Wikipedia web API used in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="WIKIPEDIA WEB API" target="LOOKUP [STRING]">
      <data key="d4">1.0</data>
      <data key="d5">Lookup [string] is an action type in the Wikipedia web API used in the LATS algorithm</data>
      <data key="d6">48e423e2baf2ed485872756f5b4d87d8</data>
    </edge>
    <edge source="ITERATIONS" target="INSTRUCTION REFINEMENT FLOW">
      <data key="d4">8.0</data>
      <data key="d5">Instruction Refinement Flow involves iterations to enhance the complexity and quality of instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="SEARCH" target="LOOKUP">
      <data key="d4">7.0</data>
      <data key="d5">Search and Lookup are actions used in HotPotQA</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="SEARCH" target="DAIRY FREE AND APPLE VARIETY PACK OF CHIPS">
      <data key="d4">8.0</data>
      <data key="d5">The search action is used to look for the dairy-free and apple variety pack of chips</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="SEARCH" target="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON">
      <data key="d4">8.0</data>
      <data key="d5">The search action is used to look for gluten-free vegetarian smoked peppered bacon</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="SEARCH" target="REFINE SEARCH">
      <data key="d4">7.0</data>
      <data key="d5">Refine Search is an action taken to improve the results of a Search</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="SEARCH" target="VEGETARIAN BACON">
      <data key="d4">7.0</data>
      <data key="d5">Vegetarian Bacon is the product being searched for</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="LOOKUP" target="INTERACTIVE INFORMATION RETRIEVAL">
      <data key="d4">16.0</data>
      <data key="d5">Lookup is one of the commands used in interactive information retrieval to return the next sentence containing a specified string.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="FINISH" target="INTERACTIVE INFORMATION RETRIEVAL">
      <data key="d4">16.0</data>
      <data key="d5">Finish is one of the commands used in interactive information retrieval to complete the current task.</data>
      <data key="d6">fb2b4544aedd793e4d4ec3147320a51c</data>
    </edge>
    <edge source="FINISH" target="THOUGHT">
      <data key="d4">8.0</data>
      <data key="d5">Thought can lead to Finish in HotPotQA</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="UNIT TESTS" target="MINIMUM SUBARRAY SUM">
      <data key="d4">9.0</data>
      <data key="d5">Unit tests are used to validate the correctness of the Minimum Subarray Sum function</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="BAUER MEDIA GROUP" target="FIRST FOR WOMEN">
      <data key="d4">8.0</data>
      <data key="d5">First for Women is published by Bauer Media Group</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="ARTHUR'S MAGAZINE" target="GODEY'S LADY'S BOOK">
      <data key="d4">8.0</data>
      <data key="d5">Arthur's Magazine was merged into Godey's Lady's Book in May 1846</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="ARTHUR'S MAGAZINE" target="FIRST FOR WOMEN">
      <data key="d4">1.0</data>
      <data key="d5">Arthur's Magazine and First for Women are compared in a question answering task</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="QUERY" target="RESULTS">
      <data key="d4">8.0</data>
      <data key="d5">Query leads to Results in WebShop</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="RESULTS" target="ITEM">
      <data key="d4">8.0</data>
      <data key="d5">Results lead to Item in WebShop</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="ITEM" target="ITEM-DETAIL">
      <data key="d4">15.0</data>
      <data key="d5">Item leads to Item-Detail in WebShopItem-Detail can lead back to Item in WebShop</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="ITEM" target="EPISODE END">
      <data key="d4">8.0</data>
      <data key="d5">Item can lead to Episode End in WebShop</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="THOUGHT" target="ACTION">
      <data key="d4">24.0</data>
      <data key="d5">Thought leads to Action in HotPotQA
The thought process determines the actions to be taken</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f,b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="ACTION" target="THINK">
      <data key="d4">7.0</data>
      <data key="d5">Think is a type of action taken by the user</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="ACTION" target="CLICK">
      <data key="d4">7.0</data>
      <data key="d5">Click is a type of action taken by the user</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="COLORADO OROGENY" target="HIGH PLAINS">
      <data key="d4">1.0</data>
      <data key="d5">Colorado Orogeny extends into the High Plains</data>
      <data key="d6">b8dd0300033963bb4a3e1bad37f8e7b9</data>
    </edge>
    <edge source="VALUE FUNCTION PROMPT" target="WEB SHOP">
      <data key="d4">16.0</data>
      <data key="d5">The Value Function Prompt involves analyzing a purchasing trajectory on a web shop</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ARTHUR&#8217;S MAGAZINE" target="GODEY&#8217;S LADY&#8217;S BOOK">
      <data key="d4">8.0</data>
      <data key="d5">Arthur&#8217;s Magazine was merged into Godey&#8217;s Lady&#8217;s Book in May 1846</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="ARTHUR&#8217;S MAGAZINE" target="TIMOTHY SHAY ARTHUR">
      <data key="d4">9.0</data>
      <data key="d5">Timothy Shay Arthur was the editor of Arthur&#8217;s Magazine</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="ARTHUR&#8217;S MAGAZINE" target="EDGAR A. POE">
      <data key="d4">7.0</data>
      <data key="d5">Edgar A. Poe was a contributor to Arthur&#8217;s Magazine</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="ARTHUR&#8217;S MAGAZINE" target="J.H. INGRAHAM">
      <data key="d4">7.0</data>
      <data key="d5">J.H. Ingraham was a contributor to Arthur&#8217;s Magazine</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="ARTHUR&#8217;S MAGAZINE" target="SARAH JOSEPHA HALE">
      <data key="d4">7.0</data>
      <data key="d5">Sarah Josepha Hale was a contributor to Arthur&#8217;s Magazine</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="ARTHUR&#8217;S MAGAZINE" target="THOMAS G. SPEAR">
      <data key="d4">1.0</data>
      <data key="d5">Thomas G. Spear was a contributor to Arthur&#8217;s Magazine</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="REFLECTION PROMPT" target="WEB SHOP">
      <data key="d4">9.0</data>
      <data key="d5">The Reflection Prompt involves diagnosing a failure in a purchasing trial on a web shop</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="PROGRAMMING PROMPTS" target="HUMANEVAL FUNCTION IMPLEMENTATION EXAMPLE">
      <data key="d4">14.0</data>
      <data key="d5">Programming Prompts include examples like the HumanEval function implementation</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="HUMANEVAL FUNCTION IMPLEMENTATION EXAMPLE" target="MINIMUM SUBARRAY SUM">
      <data key="d4">16.0</data>
      <data key="d5">The HumanEval function implementation example includes a sample implementation of the Minimum Subarray Sum function</data>
      <data key="d6">357f3442ba581c9d2bdf84d90509056f</data>
    </edge>
    <edge source="AI PYTHON ASSISTANT" target="FUNCTION IMPLEMENTATION">
      <data key="d4">9.0</data>
      <data key="d5">The AI Python assistant helps in writing and defining function implementations</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="AI PYTHON ASSISTANT" target="UNIT TEST">
      <data key="d4">9.0</data>
      <data key="d5">The AI Python assistant runs unit tests to validate the function implementations</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="AI PYTHON ASSISTANT" target="BASE ACTING/REASONING PROMPT">
      <data key="d4">8.0</data>
      <data key="d5">The BASE ACTING/REASONING PROMPT guides the AI Python assistant in implementing a function</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="FUNCTION IMPLEMENTATION" target="UNIT TEST">
      <data key="d4">8.0</data>
      <data key="d5">Unit tests are used to validate the correctness of function implementations</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="FUNCTION IMPLEMENTATION" target="IMPROVED IMPLEMENTATION">
      <data key="d4">8.0</data>
      <data key="d5">IMPROVED IMPLEMENTATION is the revised version of a FUNCTION IMPLEMENTATION</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="UNIT TEST" target="TEST CASE GENERATION PROMPT">
      <data key="d4">8.0</data>
      <data key="d5">The TEST CASE GENERATION PROMPT guides the AI coding assistant in writing UNIT TESTS</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="BRIGHT CITRUS DEODORANT" target="EARTH MAMA">
      <data key="d4">26.0</data>
      <data key="d5">Bright Citrus Deodorant is a product by the brand Earth Mama
Bright Citrus Deodorant is a product made by the brand Earth Mama</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad,785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="BRIGHT CITRUS DEODORANT" target="PRICE">
      <data key="d4">7.0</data>
      <data key="d5">PRICE is an attribute of the BRIGHT CITRUS DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="BRIGHT CITRUS DEODORANT" target="SIZE">
      <data key="d4">7.0</data>
      <data key="d5">SIZE is an attribute of the BRIGHT CITRUS DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="BRIGHT CITRUS DEODORANT" target="SCENT">
      <data key="d4">7.0</data>
      <data key="d5">SCENT is an attribute of the BRIGHT CITRUS DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="EARTH MAMA" target="GINGER FRESH DEODORANT">
      <data key="d4">17.0</data>
      <data key="d5">Ginger Fresh Deodorant is a product by the brand Earth Mama
Ginger Fresh Deodorant is a product made by the brand Earth Mama</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad,785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="EARTH MAMA" target="CALMING LAVENDER DEODORANT">
      <data key="d4">9.0</data>
      <data key="d5">Calming Lavender Deodorant is a product made by the brand Earth Mama</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="EARTH MAMA" target="SIMPLY NON-SCENTS DEODORANT">
      <data key="d4">9.0</data>
      <data key="d5">Simply Non-Scents Deodorant is a product made by the brand Earth Mama</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="EARTH MAMA" target="TRAVEL SET (4-PACK)">
      <data key="d4">9.0</data>
      <data key="d5">Travel Set (4-Pack) is a product made by the brand Earth Mama</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="EARTH MAMA" target="3 OUNCE (PACK OF 1)">
      <data key="d4">9.0</data>
      <data key="d5">3 Ounce (Pack of 1) is a product made by the brand Earth Mama</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="EARTH MAMA" target="3-OUNCE (2-PACK)">
      <data key="d4">9.0</data>
      <data key="d5">3-Ounce (2-Pack) is a product made by the brand Earth Mama</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="GINGER FRESH DEODORANT" target="PRICE">
      <data key="d4">7.0</data>
      <data key="d5">PRICE is an attribute of the GINGER FRESH DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="GINGER FRESH DEODORANT" target="SIZE">
      <data key="d4">7.0</data>
      <data key="d5">SIZE is an attribute of the GINGER FRESH DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="GINGER FRESH DEODORANT" target="SCENT">
      <data key="d4">7.0</data>
      <data key="d5">SCENT is an attribute of the GINGER FRESH DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="BARREL AND OAK" target="CEDAR &amp; PATCHOULI DEODORANT">
      <data key="d4">1.0</data>
      <data key="d5">Cedar &amp; Patchouli Deodorant is a product by the brand Barrel and Oak</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="CEDAR &amp; PATCHOULI DEODORANT" target="PRICE">
      <data key="d4">7.0</data>
      <data key="d5">PRICE is an attribute of the CEDAR &amp; PATCHOULI DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="CEDAR &amp; PATCHOULI DEODORANT" target="SIZE">
      <data key="d4">7.0</data>
      <data key="d5">SIZE is an attribute of the CEDAR &amp; PATCHOULI DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="CEDAR &amp; PATCHOULI DEODORANT" target="SCENT">
      <data key="d4">1.0</data>
      <data key="d5">SCENT is an attribute of the CEDAR &amp; PATCHOULI DEODORANT</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="MIN SUM" target="CURRENT SUM">
      <data key="d4">7.0</data>
      <data key="d5">MIN SUM is updated based on the value of CURRENT SUM during iteration</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="CURRENT SUM" target="NUMS">
      <data key="d4">7.0</data>
      <data key="d5">CURRENT SUM is calculated by iterating over the elements in NUMS</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="THINK" target="CLICK">
      <data key="d4">7.0</data>
      <data key="d5">CLICK is the action taken after the THINK process</data>
      <data key="d6">785ad59c6a37896a4676ec5c1689735f</data>
    </edge>
    <edge source="THINK" target="DAIRY FREE AND APPLE VARIETY PACK OF CHIPS">
      <data key="d4">7.0</data>
      <data key="d5">The think action is used to reflect on whether the dairy-free and apple variety pack of chips meets the criteria</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="THINK" target="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON">
      <data key="d4">7.0</data>
      <data key="d5">The think action is used to reflect on whether the gluten-free vegetarian smoked peppered bacon meets the criteria</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="CLICK" target="DAIRY FREE AND APPLE VARIETY PACK OF CHIPS">
      <data key="d4">8.0</data>
      <data key="d5">The click action is used to select the dairy-free and apple variety pack of chips</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="CLICK" target="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON">
      <data key="d4">8.0</data>
      <data key="d5">The click action is used to select the gluten-free vegetarian smoked peppered bacon</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="DAIRY FREE AND APPLE VARIETY PACK OF CHIPS">
      <data key="d4">27.0</data>
      <data key="d5">The dairy-free and apple variety pack of chips is a product made by Enjoy Life Foods
The dairy-free and apple variety pack of chips is a product offered by Enjoy Life Foods</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd,6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="ENJOY LIFE FOODS SOFT BAKED OVALS">
      <data key="d4">9.0</data>
      <data key="d5">Enjoy Life Foods Soft Baked Ovals are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="ENJOY LIFE SOFT BAKED CHEWY BARS">
      <data key="d4">9.0</data>
      <data key="d5">Enjoy Life Soft Baked Chewy Bars are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="ENJOY LIFE LENTIL CHIPS VARIETY PACK">
      <data key="d4">9.0</data>
      <data key="d5">Enjoy Life Lentil Chips Variety Pack is a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="DAIRY FREE CHIPS">
      <data key="d4">18.0</data>
      <data key="d5">Dairy-Free Chips are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="NUT FREE BARS">
      <data key="d4">9.0</data>
      <data key="d5">Nut-Free Bars are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="SOY FREE BARS">
      <data key="d4">9.0</data>
      <data key="d5">Soy-Free Bars are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="GLUTEN FREE BARS">
      <data key="d4">9.0</data>
      <data key="d5">Gluten-Free Bars are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="VEGAN BARS">
      <data key="d4">9.0</data>
      <data key="d5">Vegan Bars are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="NON GMO BARS">
      <data key="d4">9.0</data>
      <data key="d5">Non-GMO Bars are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="SOY FREE CHIPS">
      <data key="d4">9.0</data>
      <data key="d5">Soy-Free Chips are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="NUT FREE CHIPS">
      <data key="d4">9.0</data>
      <data key="d5">Nut-Free Chips are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="NON GMO CHIPS">
      <data key="d4">9.0</data>
      <data key="d5">Non-GMO Chips are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="VEGAN CHIPS">
      <data key="d4">9.0</data>
      <data key="d5">Vegan Chips are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="GLUTEN FREE CHIPS">
      <data key="d4">1.0</data>
      <data key="d5">Gluten-Free Chips are a product made by Enjoy Life Foods</data>
      <data key="d6">6f486e20e3102c7a285e357d356417ad</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="SOFT BAKED OVALS">
      <data key="d4">8.0</data>
      <data key="d5">Soft Baked Ovals are a product offered by Enjoy Life Foods</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="SOFT BAKED CHEWY BARS">
      <data key="d4">8.0</data>
      <data key="d5">Soft Baked Chewy Bars are a product offered by Enjoy Life Foods</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="ENJOY LIFE FOODS" target="LENTIL CHIPS">
      <data key="d4">8.0</data>
      <data key="d5">Lentil Chips are a product offered by Enjoy Life Foods</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="ENJOY LIFE FOODS SOFT BAKED OVALS" target="SOFT BAKED OVALS">
      <data key="d4">8.0</data>
      <data key="d5">Enjoy Life Foods Soft Baked Ovals is a specific product that includes breakfast bars</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="ENJOY LIFE SOFT BAKED CHEWY BARS" target="SOFT BAKED CHEWY BARS">
      <data key="d4">8.0</data>
      <data key="d5">Enjoy Life Soft Baked Chewy Bars is a specific product that includes chewy bars</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="LENTIL CHIPS" target="VARIETY PACK">
      <data key="d4">7.0</data>
      <data key="d5">The variety pack is an option for the Enjoy Life Lentil Chips</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="LENTIL CHIPS" target="0.8 OUNCE (PACK OF 24)">
      <data key="d4">7.0</data>
      <data key="d5">The 0.8 ounce (pack of 24) is a size option for the Enjoy Life Lentil Chips</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="LENTIL CHIPS" target="BUY NOW">
      <data key="d4">6.0</data>
      <data key="d5">The Buy Now action is used to purchase the Enjoy Life Lentil Chips</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="LENTIL CHIPS" target="ENJOY LIFE LENTIL CHIPS">
      <data key="d4">8.0</data>
      <data key="d5">Enjoy Life Lentil Chips is a specific product that includes lentil chips</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON" target="SMOKED BACON SEA SALT">
      <data key="d4">5.0</data>
      <data key="d5">Both products are gluten-free and have a smoked bacon flavor</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON" target="SPICY HOT PEPPER SEA SALT">
      <data key="d4">5.0</data>
      <data key="d5">Both products are gluten-free and contain pepper flavors</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON" target="LOUISVILLE VEGAN JERKY">
      <data key="d4">1.0</data>
      <data key="d5">Both products are gluten-free and vegetarian</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON" target="PREVIOUS TRIAL INSTRUCTION">
      <data key="d4">7.0</data>
      <data key="d5">The previous trial instruction was to search for gluten-free vegetarian smoked peppered bacon</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="SMOKED BACON SEA SALT" target="SMOKED BACON CHIPOTLE">
      <data key="d4">7.0</data>
      <data key="d5">Smoked Bacon Chipotle is a flavor option in the Smoked Bacon Sea Salt 3-Pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="SMOKED BACON SEA SALT" target="SMOKED BACON AND ONION">
      <data key="d4">7.0</data>
      <data key="d5">Smoked Bacon and Onion is a flavor option in the Smoked Bacon Sea Salt 3-Pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="SPICY HOT PEPPER SEA SALT" target="GHOST PEPPER">
      <data key="d4">7.0</data>
      <data key="d5">Ghost Pepper is a flavor option in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="SPICY HOT PEPPER SEA SALT" target="JALAPENO">
      <data key="d4">7.0</data>
      <data key="d5">Jalapeno is a flavor option in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="SPICY HOT PEPPER SEA SALT" target="HABANERO">
      <data key="d4">7.0</data>
      <data key="d5">Habanero is a flavor option in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd</data>
    </edge>
    <edge source="LOUISVILLE VEGAN JERKY" target="BLACK PEPPER">
      <data key="d4">15.0</data>
      <data key="d5">Black Pepper is a flavor option in the Louisville Vegan Jerky 5-Flavor Variety Pack
Black Pepper is one of the flavors in the Louisville Vegan Jerky variety pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="LOUISVILLE VEGAN JERKY" target="BUFFALO DILL">
      <data key="d4">15.0</data>
      <data key="d5">Buffalo Dill is a flavor option in the Louisville Vegan Jerky 5-Flavor Variety Pack
Buffalo Dill is one of the flavors in the Louisville Vegan Jerky variety pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="LOUISVILLE VEGAN JERKY" target="PEPPERONI">
      <data key="d4">15.0</data>
      <data key="d5">Pepperoni is a flavor option in the Louisville Vegan Jerky 5-Flavor Variety Pack
Pepperoni is one of the flavors in the Louisville Vegan Jerky variety pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="LOUISVILLE VEGAN JERKY" target="MAPLE BACON">
      <data key="d4">15.0</data>
      <data key="d5">Maple Bacon is a flavor option in the Louisville Vegan Jerky 5-Flavor Variety Pack
Maple Bacon is one of the flavors in the Louisville Vegan Jerky variety pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="LOUISVILLE VEGAN JERKY" target="CAROLINA BBQ">
      <data key="d4">9.0</data>
      <data key="d5">Carolina BBQ is a flavor option in the Louisville Vegan Jerky 5-Flavor Variety Pack
Carolina BBQ is one of the flavors in the Louisville Vegan Jerky variety pack</data>
      <data key="d6">4ed5aa10872b585d02aa2daf4ff8f7fd,5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="LOUISVILLE VEGAN JERKY" target="SOY PROTEIN">
      <data key="d4">9.0</data>
      <data key="d5">Soy Protein is the main ingredient in Louisville Vegan Jerky</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="GHOST PEPPER" target="SPICY HOT PEPPER SEA SALT 3-PACK">
      <data key="d4">9.0</data>
      <data key="d5">Ghost Pepper is one of the ingredients in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="JALAPENO" target="SPICY HOT PEPPER SEA SALT 3-PACK">
      <data key="d4">9.0</data>
      <data key="d5">Jalapeno is one of the ingredients in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="HABANERO" target="SPICY HOT PEPPER SEA SALT 3-PACK">
      <data key="d4">9.0</data>
      <data key="d5">Habanero is one of the ingredients in the Spicy Hot Pepper Sea Salt 3-Pack</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="REFINE SEARCH" target="GLUTEN-FREE AND 4 OUNCE PACK OF 2">
      <data key="d4">1.0</data>
      <data key="d5">Refine Search is done to meet the constraints of Gluten-Free and 4 Ounce Pack of 2</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="STATUS" target="FAIL">
      <data key="d4">8.0</data>
      <data key="d5">Fail is a status indicating an unsuccessful attempt</data>
      <data key="d6">5d356b8ff719763a38cecff22c4e17b7</data>
    </edge>
    <edge source="SHENGRAN HU" target="CONG LU">
      <data key="d4">16.0</data>
      <data key="d5">Shengran Hu and Cong Lu co-authored the study on Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="SHENGRAN HU" target="SHENGRAN HU'S GITHUB">
      <data key="d4">14.0</data>
      <data key="d5">Shengran Hu's GitHub contains the code related to the research conducted by Shengran Hu</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="SHENGRAN HU" target="META AGENT SEARCH">
      <data key="d4">9.0</data>
      <data key="d5">Shengran Hu is the author of the paper on Automated Design of Agentic Systems and the creator of the Meta Agent Search algorithm</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="SHENGRAN HU" target="ADAS">
      <data key="d4">1.0</data>
      <data key="d5">Shengran Hu is associated with the ADAS repository</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="UNIVERSITY OF BRITISH COLUMBIA" target="VECTOR INSTITUTE">
      <data key="d4">14.0</data>
      <data key="d5">Researchers from the University of British Columbia and the Vector Institute collaborated on the study of Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="UNIVERSITY OF BRITISH COLUMBIA" target="CANADA CIFAR AI CHAIR">
      <data key="d4">12.0</data>
      <data key="d5">Jeff Clune, who holds a Canada CIFAR AI Chair, is affiliated with the University of British Columbia</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="VECTOR INSTITUTE" target="CANADA CIFAR AI CHAIR">
      <data key="d4">12.0</data>
      <data key="d5">Jeff Clune, who holds a Canada CIFAR AI Chair, is affiliated with the Vector Institute</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="VECTOR INSTITUTE" target="ADAS">
      <data key="d4">16.0</data>
      <data key="d5">Vector Institute supported the work on ADAS</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)" target="META AGENT SEARCH">
      <data key="d4">18.0</data>
      <data key="d5">Meta Agent Search is an algorithm used within the research area of Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)" target="FOUNDATION MODELS (FMS)">
      <data key="d4">16.0</data>
      <data key="d5">Foundation Models are used as modules within agentic systems in the research area of Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)" target="CHAIN-OF-THOUGHT">
      <data key="d4">14.0</data>
      <data key="d5">Chain-of-Thought is a technique used as a building block in agentic systems within the research area of Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)" target="TOOLFORMER">
      <data key="d4">14.0</data>
      <data key="d5">Toolformer is a technique used as a building block in agentic systems within the research area of Automated Design of Agentic Systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)" target="CLUNE (2019)">
      <data key="d4">14.0</data>
      <data key="d5">Clune (2019) contributed to the research on the history of machine learning and the replacement of hand-designed solutions with learned solutions</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="META AGENT SEARCH" target="META AGENT">
      <data key="d4">18.0</data>
      <data key="d5">Meta Agent is an agent that is programmed and iteratively improved by the Meta Agent Search algorithm</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="META AGENT SEARCH" target="AGENT ARCHIVE">
      <data key="d4">16.0</data>
      <data key="d5">Agent Archive is a repository used by the Meta Agent Search algorithm to store discovered agents</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="META AGENT SEARCH" target="MULTI-STEP PEER REVIEW AGENT">
      <data key="d4">14.0</data>
      <data key="d5">Multi-Step Peer Review Agent is an example of an agent discovered by the Meta Agent Search algorithm</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="META AGENT SEARCH" target="VERIFIED MULTIMODAL AGENT">
      <data key="d4">14.0</data>
      <data key="d5">Verified Multimodal Agent is an example of an agent discovered by the Meta Agent Search algorithm</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="META AGENT SEARCH" target="DIVIDE AND CONQUER AGENT">
      <data key="d4">14.0</data>
      <data key="d5">Divide and Conquer Agent is an example of an agent discovered by the Meta Agent Search algorithm</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ADAS">
      <data key="d4">61.0</data>
      <data key="d5">Meta Agent Search is an algorithm used to demonstrate the approach of defining and searching for agents in ADAS
Meta Agent Search is an approach studied within the field of ADAS
Meta Agent Search is a method used within the ADAS process to discover superior agents
ADAS is the research area that proposes Meta Agent Search
Meta Agent Search is a proposed approach to ADAS</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,4884e8429ca1e567dadf5e22b4b68274,7de66b94cf868b37b1df51dc545c415f,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="FM">
      <data key="d4">9.0</data>
      <data key="d5">Meta Agent Search uses FMs as meta agents to iteratively program new agents</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="FUNSEARCH">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search uses a similar practice to FunSearch by programming a "forward" function to define a new agentic system</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ARC">
      <data key="d4">24.0</data>
      <data key="d5">Meta Agent Search is evaluated on the ARC logic puzzle task
Meta Agent Search evaluates the performance of discovered agents on the ARC dataset</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="DROP">
      <data key="d4">15.0</data>
      <data key="d5">Meta Agent Search improves performance on reading comprehension tasks in the DROP dataset
Meta Agent Search uses the DROP benchmark to evaluate reading comprehension</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="MGSM">
      <data key="d4">31.0</data>
      <data key="d5">Meta Agent Search improves performance on math tasks in the MGSM dataset
Meta Agent Search uses the MGSM benchmark to evaluate math capability
Meta Agent Search evaluates the performance of discovered agents on the MGSM dataset</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="GSM8K">
      <data key="d4">31.0</data>
      <data key="d5">Meta Agent Search improves performance on math tasks in the GSM8K dataset
Meta Agent Search evaluates the performance of discovered agents on the GSM8K dataset
Meta Agent Search improves accuracy on the GSM8K dataset</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="GSM-HARD">
      <data key="d4">31.0</data>
      <data key="d5">Meta Agent Search improves performance on math tasks in the GSM-Hard dataset
Meta Agent Search evaluates the performance of discovered agents on the GSM-Hard dataset
Meta Agent Search improves accuracy on the GSM-Hard dataset</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="CHAIN-OF-THOUGHT">
      <data key="d4">34.0</data>
      <data key="d5">Meta Agent Search uses the Chain-of-Thought technique to generate, refine, and ensemble answers
Meta Agent Search compares its discovered agents against the Chain-of-Thought baseline
Meta Agent Search outperforms the Chain-of-Thought method
Chain-of-Thought is a method used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="LLM DEBATE">
      <data key="d4">34.0</data>
      <data key="d5">Meta Agent Search uses the LLM Debate technique to enhance refinement through multiple critics
Meta Agent Search compares its discovered agents against the LLM Debate baseline
Meta Agent Search outperforms the LLM Debate method
LLM Debate is a method used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="QUALITY-DIVERSITY">
      <data key="d4">41.0</data>
      <data key="d5">Meta Agent Search uses the Quality-Diversity technique to explore new agents based on an archive of previous discoveries
Meta Agent Search uses Quality-Diversity as one of the state-of-the-art hand-designed agents
Meta Agent Search compares its discovered agents against the Quality-Diversity baseline
Meta Agent Search outperforms the Quality-Diversity method
Quality-Diversity is a method used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,1a6353c9d196dc2debad7c27c902bcd7,24d7b89ae9522ae60d2317984951355b,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="LU">
      <data key="d4">6.0</data>
      <data key="d5">Lu is an author mentioned in relation to open-endedness algorithms that leverage human notions of interestingness, similar to Meta Agent Search</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ZHANG">
      <data key="d4">6.0</data>
      <data key="d5">Zhang is an author mentioned in relation to open-endedness algorithms that leverage human notions of interestingness, similar to Meta Agent Search</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="MADAAN">
      <data key="d4">6.0</data>
      <data key="d5">Madaan is an author mentioned in relation to self-reflection iterations in the meta agent</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="SHINN">
      <data key="d4">6.0</data>
      <data key="d5">Shinn is an author mentioned in relation to self-reflection iterations in the meta agent</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ARC CHALLENGE">
      <data key="d4">9.0</data>
      <data key="d5">Meta Agent Search is used to discover novel agentic systems that outperform existing state-of-the-art agents in the ARC challenge</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="CHAIN-OF-THOUGHT (COT)">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search uses Chain-of-Thought (COT) as one of the state-of-the-art hand-designed agents</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT (COT-SC)">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search uses Self-Consistency with Chain-of-Thought (COT-SC) as one of the state-of-the-art hand-designed agents</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="LLM-DEBATE">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search uses LLM-Debate as one of the state-of-the-art hand-designed agents</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ABSTRACTION AND REASONING CORPUS (ARC)">
      <data key="d4">9.0</data>
      <data key="d5">Meta Agent Search is applied to the Abstraction and Reasoning Corpus (ARC) dataset</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="FALDOR ET AL., 2024">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search builds on prior works on open-endedness and AI-GAs by Faldor et al., 2024</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="LEHMAN &amp; STANLEY, 2011">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search builds on prior works on open-endedness and AI-GAs by Lehman &amp; Stanley, 2011</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="WANG ET AL., 2019">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search builds on prior works on open-endedness and AI-GAs by Wang et al., 2019</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="WANG ET AL., 2020">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search builds on prior works on open-endedness and AI-GAs by Wang et al., 2020</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ZHANG ET AL., 2024A">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search builds on prior works on open-endedness and AI-GAs by Zhang et al., 2024a</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="APPENDIX C">
      <data key="d4">6.0</data>
      <data key="d5">Appendix C provides detailed implementation of the best agent discovered by Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="APPENDIX E">
      <data key="d4">12.0</data>
      <data key="d5">Appendix E provides more details about the baselines used in Meta Agent Search
Appendix E contains more details about the baselines used in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="FIGURE 3">
      <data key="d4">6.0</data>
      <data key="d5">Figure 3 shows the results of Meta Agent Search on the ARC challenge</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="PUBLIC TRAINING SET (EASY)">
      <data key="d4">6.0</data>
      <data key="d5">Meta Agent Search uses the Public Training Set (Easy) for training agents</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="VALIDATION SET">
      <data key="d4">6.0</data>
      <data key="d5">Meta Agent Search uses a validation set of 20 questions for validating agents</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="TEST SET">
      <data key="d4">1.0</data>
      <data key="d5">Meta Agent Search uses a test set of 60 questions for testing agents</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="DYNAMIC MEMORY">
      <data key="d4">7.0</data>
      <data key="d5">Dynamic memory is introduced in Meta Agent Search for doing more refinements</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="MULTIPLE CRITICS">
      <data key="d4">7.0</data>
      <data key="d5">Multiple critics are introduced in Meta Agent Search for enhanced refinement</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="META-AGENT">
      <data key="d4">8.0</data>
      <data key="d5">The meta-agent in Meta Agent Search uses GPT-4</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="CRITIC">
      <data key="d4">7.0</data>
      <data key="d5">Critics provide feedback for refining answers in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="EFFICIENCY EXPERT">
      <data key="d4">7.0</data>
      <data key="d5">Efficiency experts evaluate the efficiency of answers in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="READABILITY EXPERT">
      <data key="d4">7.0</data>
      <data key="d5">Readability experts evaluate the readability of answers in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="SIMPLICITY EXPERT">
      <data key="d4">7.0</data>
      <data key="d5">Simplicity experts evaluate the simplicity of answers in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ENSEMBLE">
      <data key="d4">8.0</data>
      <data key="d5">Ensembling is used to combine the best answers in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="HUMAN-LIKE FEEDBACK">
      <data key="d4">7.0</data>
      <data key="d5">Human-like feedback is simulated to refine answers in Meta Agent Search</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ITERATION 3">
      <data key="d4">7.0</data>
      <data key="d5">Iteration 3 in Meta Agent Search uses multiple COTs to generate possible answers, refine them, and ensemble the best answers</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ITERATION 5">
      <data key="d4">7.0</data>
      <data key="d5">Iteration 5 in Meta Agent Search incorporates diverse feedback</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ITERATION 11">
      <data key="d4">7.0</data>
      <data key="d5">Iteration 11 in Meta Agent Search evaluates for various specific traits via experts</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ITERATION 12">
      <data key="d4">1.0</data>
      <data key="d5">Iteration 12 in Meta Agent Search simulates human-like feedback</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="META AGENT SEARCH" target="MMLU">
      <data key="d4">8.0</data>
      <data key="d5">Meta Agent Search uses the MMLU benchmark to evaluate multi-task problem solving</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="GPQA">
      <data key="d4">8.0</data>
      <data key="d5">Meta Agent Search uses the GPQA benchmark to evaluate the capability of solving hard science questions</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="STEP-BACK ABSTRACTION">
      <data key="d4">13.0</data>
      <data key="d5">Meta Agent Search compares its discovered agents against the Step-back Abstraction baseline
Step-back Abstraction is a method used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ROLE ASSIGNMENT">
      <data key="d4">13.0</data>
      <data key="d5">Meta Agent Search compares its discovered agents against the Role Assignment baseline
Role Assignment is a method used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="READING COMPREHENSION">
      <data key="d4">29.0</data>
      <data key="d5">Meta Agent Search tests agents in the Reading Comprehension domain
Meta Agent Search is effective in the Reading Comprehension domain
Meta Agent Search outperforms state-of-the-art baselines in Reading Comprehension</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="MULTI-TASK PROBLEM SOLVING">
      <data key="d4">8.0</data>
      <data key="d5">Meta Agent Search tests agents in the Multi-task Problem Solving domain</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="F1 SCORE">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search reports F1 scores for agents in the Reading Comprehension and Math domains</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ACCURACY">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search reports accuracy rates for agents in the Reading Comprehension and Math domains</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="BOOTSTRAP CONFIDENCE INTERVAL">
      <data key="d4">7.0</data>
      <data key="d5">Meta Agent Search reports the 95% bootstrap confidence interval for performance comparison</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="EXPERIMENT SETTINGS">
      <data key="d4">6.0</data>
      <data key="d5">Meta Agent Search details about datasets and experiment settings can be found in Appendix D</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="BASELINES">
      <data key="d4">8.0</data>
      <data key="d5">Meta Agent Search compares discovered agents against state-of-the-art hand-designed baselines</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="APPENDIX D">
      <data key="d4">6.0</data>
      <data key="d5">Appendix D contains more details about datasets and experiment settings used in Meta Agent Search</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="FMS">
      <data key="d4">24.0</data>
      <data key="d5">Meta Agent Search uses Foundation Models in the process
Meta Agent Search is used to discover agents that can leverage the knowledge in FMs</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="META AGENT SEARCH" target="MULTI-TASK">
      <data key="d4">19.0</data>
      <data key="d5">Meta Agent Search is used in the Multi-task domain
Meta Agent Search outperforms state-of-the-art baselines in Multi-task</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="CLAUDE-HAIKU">
      <data key="d4">16.0</data>
      <data key="d5">Meta Agent Search uses Claude-Haiku to evaluate the performance of discovered agents</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="CLAUDE-SONNET">
      <data key="d4">16.0</data>
      <data key="d5">Meta Agent Search uses Claude-Sonnet to evaluate the performance of discovered agents</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="STRUCTURED FEEDBACK AND ENSEMBLE AGENT">
      <data key="d4">16.0</data>
      <data key="d5">Structured Feedback and Ensemble Agent is one of the top agents discovered by Meta Agent Search</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="HIERARCHICAL COMMITTEE REINFORCEMENT AGENT">
      <data key="d4">16.0</data>
      <data key="d5">Hierarchical Committee Reinforcement Agent is one of the top agents discovered by Meta Agent Search</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="DYNAMIC MEMORY AND REFINEMENT AGENT">
      <data key="d4">16.0</data>
      <data key="d5">Dynamic Memory and Refinement Agent is one of the top agents discovered by Meta Agent Search</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="SVAMP">
      <data key="d4">8.0</data>
      <data key="d5">Meta Agent Search evaluates the performance of discovered agents on the SVAMP dataset</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ASDIV">
      <data key="d4">8.0</data>
      <data key="d5">Meta Agent Search evaluates the performance of discovered agents on the ASDiv dataset</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="META AGENT SEARCH" target="DYNAMIC ROLE-PLAYING ARCHITECTURE">
      <data key="d4">7.0</data>
      <data key="d5">Dynamic Role-Playing Architecture is a top agent discovered by Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="META AGENT SEARCH" target="STRUCTURED MULTIMODAL FEEDBACK LOOP">
      <data key="d4">7.0</data>
      <data key="d5">Structured Multimodal Feedback Loop is a top agent discovered by Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="META AGENT SEARCH" target="INTERACTIVE MULTIMODAL FEEDBACK LOOP">
      <data key="d4">7.0</data>
      <data key="d5">Interactive Multimodal Feedback Loop is a top agent discovered by Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="META AGENT SEARCH" target="TOOL USE">
      <data key="d4">6.0</data>
      <data key="d5">Tool Use is used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="META AGENT SEARCH" target="FM MODULES">
      <data key="d4">6.0</data>
      <data key="d5">FM Modules are used in Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="META AGENT SEARCH" target="AI-GENERATING ALGORITHMS">
      <data key="d4">6.0</data>
      <data key="d5">AI-Generating Algorithms are related to Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="META AGENT SEARCH" target="AUTOML">
      <data key="d4">1.0</data>
      <data key="d5">AutoML is related to Meta Agent Search</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ROKON ET AL., 2020">
      <data key="d4">16.0</data>
      <data key="d5">Rokon et al., 2020 discusses safety concerns when executing untrusted model-generated code in Meta Agent Search</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="META AGENT SEARCH" target="YEE ET AL., 2010">
      <data key="d4">16.0</data>
      <data key="d5">Yee et al., 2010 discusses the use of sandbox environments to safely run untrusted model-generated code in Meta Agent Search</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="META AGENT SEARCH" target="EXPERIMENT">
      <data key="d4">8.0</data>
      <data key="d5">The experiment used Meta Agent Search to discover the best agent on ARC</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="META AGENT SEARCH" target="GPT-4O-MINI">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4o-Mini is used in Meta Agent Search to improve results and reduce costs</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="META AGENT SEARCH" target="ADAS ALGORITHMS">
      <data key="d4">1.0</data>
      <data key="d5">ADAS algorithms are used in the Meta Agent Search process</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="META AGENT SEARCH" target="EVALUATION FUNCTION">
      <data key="d4">7.0</data>
      <data key="d5">The evaluation function is used in the Meta Agent Search process</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="CLAUDE">
      <data key="d4">16.0</data>
      <data key="d5">Claude is an example of a Foundation Model used in agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="WANG ET AL.">
      <data key="d4">14.0</data>
      <data key="d5">Wang et al. contributed to the research on Foundation Models</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="ROCKT&#196;SCHEL">
      <data key="d4">14.0</data>
      <data key="d5">Rockt&#228;schel contributed to the research on compound agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="ZAHARIA ET AL.">
      <data key="d4">14.0</data>
      <data key="d5">Zaharia et al. contributed to the research on compound agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="ADAS">
      <data key="d4">16.0</data>
      <data key="d5">Foundation Models (FMs) are used as modules in the control flow of agentic systems in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="CHASE">
      <data key="d4">10.0</data>
      <data key="d5">Chase is referenced in the context of defining agentic systems involving Foundation Models</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="NG">
      <data key="d4">10.0</data>
      <data key="d5">Ng is referenced in the context of defining agentic systems involving Foundation Models</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="AGENTIC SYSTEMS">
      <data key="d4">8.0</data>
      <data key="d5">Agentic systems involve Foundation Models as modules to solve tasks</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="FRAMEWORK">
      <data key="d4">7.0</data>
      <data key="d5">The framework includes querying Foundation Models to assist the meta agent in generating and improving agent architectures.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="FOUNDATION MODELS (FMS)" target="INFO OBJECT">
      <data key="d4">6.0</data>
      <data key="d5">Foundation Models' responses are encapsulated in Info objects within the framework.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="CLAUDE" target="ANTHROPIC">
      <data key="d4">16.0</data>
      <data key="d5">Anthropic is the organization that developed the Claude Foundation Model</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT" target="HU &amp; CLUNE">
      <data key="d4">14.0</data>
      <data key="d5">Hu &amp; Clune contributed to the research on chain-of-thought planning and reasoning</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="META AGENT" target="AGENTIC SYSTEMS">
      <data key="d4">8.0</data>
      <data key="d5">A meta agent creates other agents within agentic systems</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="META AGENT" target="ADAS">
      <data key="d4">8.0</data>
      <data key="d5">Meta Agent is a concept where a meta agent programs better agents in code, enhancing the ADAS algorithm's ability to discover new agentic systems</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="META AGENT" target="FRAMEWORK">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent uses the framework to implement basic functions and format prompts.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="IMPLEMENTATION MISTAKES">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent identifies and corrects implementation mistakes during the self-reflection process.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="IMPROVEMENT">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent suggests and implements improvements to increase the performance or effectiveness of the proposed architecture.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="RUNTIME ERROR">
      <data key="d4">1.0</data>
      <data key="d5">The meta agent debugs and corrects the code when a runtime error is encountered during execution.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="APPENDIX B">
      <data key="d4">7.0</data>
      <data key="d5">The meta agent uses the framework code provided in Appendix B.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="APPENDICES C AND D">
      <data key="d4">6.0</data>
      <data key="d5">The meta agent references information available in Appendices C and D.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="OUTPUT INSTRUCTION AND EXAMPLE">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent follows the output instructions and examples provided in this section.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="MADAAN ET AL., 2024">
      <data key="d4">6.0</data>
      <data key="d5">The meta agent's self-reflection process is influenced by the work of Madaan et al. (2024).</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="ARCHIVE">
      <data key="d4">7.0</data>
      <data key="d5">The meta agent compares its proposed architecture against existing methods in the archive during self-reflection.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="WRONG IMPLEMENTATION EXAMPLES">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent uses the "WRONG Implementation examples" section to identify and correct mistakes during self-reflection.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="INFO OBJECT">
      <data key="d4">1.0</data>
      <data key="d5">The meta agent uses Info objects to encapsulate and combine different types of information within the framework.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="META AGENT" target="ARC CHALLENGE">
      <data key="d4">9.0</data>
      <data key="d5">The meta agent is designed to perform well on the ARC challenge</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="META AGENT" target="GPT-4O-2024-05-13">
      <data key="d4">17.0</data>
      <data key="d5">The meta agent uses GPT-4o-2024-05-13
The meta agent uses GPT-4o-2024-05-13 to find optimal agents for various benchmarks</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="META AGENT" target="GPT-3.5-TURBO-0125">
      <data key="d4">7.0</data>
      <data key="d5">Discovered agents and baselines are evaluated using GPT-3.5-turbo-0125</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="META AGENT" target="ARC">
      <data key="d4">9.0</data>
      <data key="d5">The meta agent is designed to perform well on the ARC challenge</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="META AGENT" target="AUTOMATED DESIGN OF AGENTIC SYSTEMS">
      <data key="d4">7.0</data>
      <data key="d5">Meta agent is discussed in the Automated Design of Agentic Systems document</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="META AGENT" target="DROP">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent aims to find an optimal agent performing well on the DROP benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="META AGENT" target="GPQA">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent aims to find an optimal agent performing well on the GPQA benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="META AGENT" target="MGSM">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent aims to find an optimal agent performing well on the MGSM benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="META AGENT" target="MMLU">
      <data key="d4">8.0</data>
      <data key="d5">The meta agent aims to find an optimal agent performing well on the MMLU benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="MULTI-STEP PEER REVIEW AGENT" target="GPQA">
      <data key="d4">8.0</data>
      <data key="d5">Multi-Step Peer Review Agent was discovered in the GPQA domain</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="MULTI-STEP PEER REVIEW AGENT" target="REIN ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Multi-Step Peer Review Agent is discussed in the publication by Rein et al. in 2023</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="MULTI-STEP PEER REVIEW AGENT" target="PHYSICS CRITIC">
      <data key="d4">6.0</data>
      <data key="d5">Physics Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="MULTI-STEP PEER REVIEW AGENT" target="CHEMISTRY CRITIC">
      <data key="d4">6.0</data>
      <data key="d5">Chemistry Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="MULTI-STEP PEER REVIEW AGENT" target="BIOLOGY CRITIC">
      <data key="d4">6.0</data>
      <data key="d5">Biology Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="MULTI-STEP PEER REVIEW AGENT" target="GENERAL CRITIC">
      <data key="d4">6.0</data>
      <data key="d5">General Critic is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="MULTI-STEP PEER REVIEW AGENT" target="FINAL DECISION">
      <data key="d4">6.0</data>
      <data key="d5">Final Decision is a role assigned in the Multi-Step Peer Review Agent</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="VERIFIED MULTIMODAL AGENT" target="MGSM">
      <data key="d4">8.0</data>
      <data key="d5">Verified Multimodal Agent was discovered in the MGSM domain</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="VERIFIED MULTIMODAL AGENT" target="SHI ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Verified Multimodal Agent is discussed in the publication by Shi et al. in 2023</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="VERIFIED MULTIMODAL AGENT" target="VISUAL REPRESENTATION MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Verified Multimodal Agent uses the Visual Representation Module to generate visual aids</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="VERIFIED MULTIMODAL AGENT" target="VERIFICATION MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Verified Multimodal Agent uses the Verification Module to verify visual aids</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="VERIFIED MULTIMODAL AGENT" target="CHAIN-OF-THOUGHT MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Verified Multimodal Agent uses the Chain-of-Thought Module to solve problems</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="DIVIDE AND CONQUER AGENT" target="GPQA">
      <data key="d4">8.0</data>
      <data key="d5">Divide and Conquer Agent was discovered in the GPQA domain</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="DIVIDE AND CONQUER AGENT" target="REIN ET AL., 2023">
      <data key="d4">8.0</data>
      <data key="d5">Divide and Conquer Agent is discussed in the publication by Rein et al. in 2023</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="DIVIDE AND CONQUER AGENT" target="DECOMPOSITION MODULE">
      <data key="d4">6.0</data>
      <data key="d5">Decomposition Module is a role in the Divide and Conquer Agent</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="DIVIDE AND CONQUER AGENT" target="SPECIALIZED EXPERT">
      <data key="d4">1.0</data>
      <data key="d5">Specialized Expert is a role in the Divide and Conquer Agent</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="HOG" target="CONVOLUTIONAL NEURAL NETWORKS (CNNS)">
      <data key="d4">16.0</data>
      <data key="d5">HOG features were replaced by learned features from Convolutional Neural Networks</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="HOG" target="DALAL &amp; TRIGGS (2005)">
      <data key="d4">14.0</data>
      <data key="d5">Dalal &amp; Triggs (2005) contributed to the research on hand-designed features like HOG</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="CONVOLUTIONAL NEURAL NETWORKS (CNNS)" target="NEURAL ARCHITECTURE SEARCH">
      <data key="d4">16.0</data>
      <data key="d5">Neural Architecture Search is a method that led to the best-performing Convolutional Neural Networks</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="CONVOLUTIONAL NEURAL NETWORKS (CNNS)" target="KRIZHEVSKY ET AL. (2012)">
      <data key="d4">14.0</data>
      <data key="d5">Krizhevsky et al. (2012) contributed to the research on Convolutional Neural Networks</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="ELSKEN">
      <data key="d4">2.0</data>
      <data key="d5">Elsken contributed to the research on Neural Architecture Search</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="ELSKEN ET AL. (2019)">
      <data key="d4">8.0</data>
      <data key="d5">Elsken et al. (2019) discusses Neural Architecture Search</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="SHEN ET AL. (2023)">
      <data key="d4">8.0</data>
      <data key="d5">Shen et al. (2023) discusses Neural Architecture Search</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="ADAS">
      <data key="d4">7.0</data>
      <data key="d5">Neural Architecture Search is a research area related to ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="AI-GENERATING ALGORITHMS">
      <data key="d4">7.0</data>
      <data key="d5">Neural Architecture Search is a technique under the first pillar of AI-Generating Algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="ELSKEN ET AL., 2019">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Elsken et al. in 2019 is related to Neural Architecture Search</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="HU ET AL., 2021">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Hu et al. in 2021 is related to Neural Architecture Search</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="NEURAL ARCHITECTURE SEARCH" target="LU ET AL., 2019">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Lu et al. in 2019 is related to Neural Architecture Search</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="AUTOML" target="AI-GENERATING ALGORITHMS (AI-GAS)">
      <data key="d4">16.0</data>
      <data key="d5">AutoML and AI-Generating Algorithms are methods that demonstrate the superiority of learned AI systems over hand-designed ones</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="AUTOML" target="HUTTER ET AL. (2019)">
      <data key="d4">22.0</data>
      <data key="d5">Hutter et al. (2019) contributed to the research on AutoML methods
Hutter et al. (2019) discusses AutoML methods</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba,c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="AUTOML" target="ADAS">
      <data key="d4">15.0</data>
      <data key="d5">AutoML is a research area related to ADAS
ADAS aims to invent novel building blocks and design powerful agentic systems, which aligns with the goals of AutoML</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274,7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="AUTOML" target="HUTTER ET AL., 2019">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Hutter et al. in 2019 is related to AutoML</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="AI-GENERATING ALGORITHMS (AI-GAS)" target="CLUNE (2019)">
      <data key="d4">8.0</data>
      <data key="d5">Clune (2019) discusses AI-Generating Algorithms</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="ANTHROPIC" target="CLAUDE-HAIKU">
      <data key="d4">8.0</data>
      <data key="d5">Anthropic is the organization behind the Claude-Haiku model</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="ANTHROPIC" target="CLAUDE-SONNET">
      <data key="d4">8.0</data>
      <data key="d5">Anthropic is the organization behind the Claude-Sonnet model</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="ANTHROPIC" target="YUNTAO BAI">
      <data key="d4">12.0</data>
      <data key="d5">Yuntao Bai is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="SAURAV KADAVATH">
      <data key="d4">12.0</data>
      <data key="d5">Saurav Kadavath is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="SANDIPAN KUNDU">
      <data key="d4">12.0</data>
      <data key="d5">Sandipan Kundu is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="JACKSON KERNION">
      <data key="d4">12.0</data>
      <data key="d5">Jackson Kernion is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="ANDY JONES">
      <data key="d4">12.0</data>
      <data key="d5">Andy Jones is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="ANNA CHEN">
      <data key="d4">12.0</data>
      <data key="d5">Anna Chen is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="ANNA GOLDIE">
      <data key="d4">12.0</data>
      <data key="d5">Anna Goldie is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="AZALIA MIRHOSEINI">
      <data key="d4">12.0</data>
      <data key="d5">Azalia Mirhoseini is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ANTHROPIC" target="CAMERON MCKINNON">
      <data key="d4">12.0</data>
      <data key="d5">Cameron McKinnon is associated with Anthropic</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="WANG ET AL." target="DEVELOPING NEW SKILLS FOR EMBODIED AGENTS IN CODE">
      <data key="d4">8.0</data>
      <data key="d5">Wang et al. are the authors of the method for developing new skills for embodied agents in code</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="HU &amp; CLUNE" target="CHAIN-OF-THOUGHT-BASED PLANNING AND REASONING METHODS">
      <data key="d4">8.0</data>
      <data key="d5">Hu &amp; Clune are the authors of the Chain-of-Thought-based planning and reasoning methods</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="ZHANG ET AL." target="MEMORY STRUCTURES">
      <data key="d4">14.0</data>
      <data key="d5">Zhang et al. contributed to the research on memory structures in agentic systems</data>
      <data key="d6">c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="ZHANG ET AL." target="EXTERNAL MEMORY AND RAG">
      <data key="d4">8.0</data>
      <data key="d5">Zhang et al. are the authors of the External Memory and RAG methods</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="QU ET AL." target="TOOL USE">
      <data key="d4">22.0</data>
      <data key="d5">Qu et al. contributed to the research on tool use in agentic systems
Qu et al. are the authors of the Tool Use method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,c3d0436082aada237ee4bee645f16059</data>
    </edge>
    <edge source="KRIZHEVSKY ET AL. (2012)" target="CNN">
      <data key="d4">8.0</data>
      <data key="d5">Krizhevsky et al. (2012) discusses Convolutional Neural Networks</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="ELSKEN" target="ADAS">
      <data key="d4">10.0</data>
      <data key="d5">Elsken is referenced in the context of Neural Architecture Search related to ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="TOOL USE" target="AGENTIC SYSTEMS">
      <data key="d4">7.0</data>
      <data key="d5">Tool use is a component of agentic systems that involves using external tools to accomplish tasks</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="TOOL USE" target="NAKANO ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Nakano et al. are the authors of the Tool Use method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="TOOL USE" target="AGENTINSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct generates data covering the skill of tool use</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="TOOL USE" target="READING COMPREHENSION">
      <data key="d4">6.0</data>
      <data key="d5">Both Reading Comprehension and Tool Use are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="TOOL USE" target="TEXT MODIFICATION">
      <data key="d4">6.0</data>
      <data key="d5">Both Text Modification and Tool Use are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="TOOL USE" target="OPEN DOMAIN QUESTION ANSWERING">
      <data key="d4">6.0</data>
      <data key="d5">Both Open Domain Question Answering and Tool Use are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="TOOL USE" target="WEB AGENT">
      <data key="d4">7.0</data>
      <data key="d5">Web Agent is a type of tool use skill implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="BUILDING BLOCKS" target="AGENTIC SYSTEMS">
      <data key="d4">8.0</data>
      <data key="d5">Building blocks are fundamental components used to construct agentic systems</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="LEARNED LOSS FUNCTIONS" target="LU ET AL. (2024A)">
      <data key="d4">8.0</data>
      <data key="d5">Lu et al. (2024a) discusses learned loss functions in LLM alignment</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="DPO" target="RAFAILOV ET AL. (2024)">
      <data key="d4">8.0</data>
      <data key="d5">Rafailov et al. (2024) discusses DPO</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="AI SCIENTIST" target="LU ET AL. (2024B)">
      <data key="d4">8.0</data>
      <data key="d5">Lu et al. (2024b) discusses the AI Scientist</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="OMNI-EPIC" target="FALDOR ET AL. (2024)">
      <data key="d4">8.0</data>
      <data key="d5">Faldor et al. (2024) discusses OMNI-EPIC</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="OMNI-EPIC" target="AI-GENERATING ALGORITHMS">
      <data key="d4">7.0</data>
      <data key="d5">OMNI-EPIC is a technique under the third pillar of AI-Generating Algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="OMNI-EPIC" target="FOUNDATION MODELS">
      <data key="d4">1.0</data>
      <data key="d5">OMNI-EPIC enables Foundation Models to create robotics learning environments</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="OMNI-EPIC" target="FALDOR ET AL., 2024">
      <data key="d4">14.0</data>
      <data key="d5">The publication by Faldor et al. in 2024 is related to OMNI-EPIC
Faldor et al., 2024 discusses OMNI-EPIC and its application in enabling FMs to create robotics learning environments by programming in code</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="OMNI-EPIC" target="FMS">
      <data key="d4">8.0</data>
      <data key="d5">OMNI-EPIC enables FMs to create robotics learning environments by programming in code</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="ADAS" target="FERNANDO ET AL. (2024)">
      <data key="d4">16.0</data>
      <data key="d5">Fernando et al. (2024) discusses ADAS methods focusing on designing prompts</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="ADAS" target="YANG ET AL. (2024)">
      <data key="d4">16.0</data>
      <data key="d5">Yang et al. (2024) discusses ADAS methods focusing on designing prompts</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="ADAS" target="SEARCH SPACE">
      <data key="d4">16.0</data>
      <data key="d5">The search space is a key component of ADAS that defines which agentic systems can be represented</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="EVALUATION FUNCTION">
      <data key="d4">16.0</data>
      <data key="d5">The evaluation function is a key component of ADAS that defines how to evaluate a candidate agent</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="CLUNE">
      <data key="d4">10.0</data>
      <data key="d5">Clune is referenced in the context of research areas related to ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="HUTTER">
      <data key="d4">10.0</data>
      <data key="d5">Hutter is referenced in the context of research areas related to ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="FERNANDO">
      <data key="d4">10.0</data>
      <data key="d5">Fernando is referenced in the context of ADAS-related works</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="ZHUGE">
      <data key="d4">10.0</data>
      <data key="d5">Zhuge is referenced in the context of search spaces and reinforcement learning in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="LIU">
      <data key="d4">10.0</data>
      <data key="d5">Liu is referenced in the context of search spaces such as feed-forward networks in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="AI-GAS">
      <data key="d4">7.0</data>
      <data key="d5">AI-GAs is a research area related to ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="MATHEMATICS">
      <data key="d4">6.0</data>
      <data key="d5">Mathematics is a domain where agentic systems can be applied and evaluated in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="READING COMPREHENSION">
      <data key="d4">6.0</data>
      <data key="d5">Reading comprehension is a domain where agentic systems can be applied and evaluated in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="AI SAFETY">
      <data key="d4">7.0</data>
      <data key="d5">AI safety is a concept that involves ensuring the safe operation of AI systems, which can be enhanced by using readable program code in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="DEBUGGING">
      <data key="d4">7.0</data>
      <data key="d5">Debugging is a process that involves identifying and fixing issues in program code, which is made easier by using readable code in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="EXISTING HUMAN EFFORTS">
      <data key="d4">7.0</data>
      <data key="d5">Existing human efforts refer to the prior work and knowledge that can be built upon in ADAS, especially when using programming languages as the search space</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="ADAS" target="MECHANISM">
      <data key="d4">8.0</data>
      <data key="d5">The sophisticated feedback mechanism is part of the ADAS process</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="ADAS" target="MEYERSON ET AL.">
      <data key="d4">6.0</data>
      <data key="d5">Meyerson et al. discussed the concept of crossover in evolution via LLMs, which is related to ADAS</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="ADAS" target="AI-GENERATING ALGORITHMS">
      <data key="d4">8.0</data>
      <data key="d5">ADAS aims to invent novel building blocks and design powerful agentic systems, which aligns with the goals of AI-Generating Algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="ADAS" target="AGENTIC SYSTEM">
      <data key="d4">18.0</data>
      <data key="d5">ADAS aims to design powerful agentic systems</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="CANADA CIFAR AI CHAIRS PROGRAM">
      <data key="d4">16.0</data>
      <data key="d5">Canada CIFAR AI Chairs program supported the work on ADAS</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="SCHMIDT FUTURES">
      <data key="d4">16.0</data>
      <data key="d5">Schmidt Futures provided grants for the work on ADAS</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="OPEN PHILANTHROPY">
      <data key="d4">16.0</data>
      <data key="d5">Open Philanthropy provided grants for the work on ADAS</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="NSERC DISCOVERY GRANT">
      <data key="d4">16.0</data>
      <data key="d5">NSERC Discovery Grant supported the work on ADAS</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="RAFAEL COSMAN">
      <data key="d4">16.0</data>
      <data key="d5">Rafael Cosman made a generous donation to support the work on ADAS</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="JENNY ZHANG">
      <data key="d4">14.0</data>
      <data key="d5">Jenny Zhang provided insightful discussions and feedback on the work</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="RACH PRADHAN">
      <data key="d4">14.0</data>
      <data key="d5">Rach Pradhan provided insightful discussions and feedback on the work</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="RUIYU GOU">
      <data key="d4">14.0</data>
      <data key="d5">Ruiyu Gou provided insightful discussions and feedback on the work</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="NICHOLAS IOANNIDIS">
      <data key="d4">14.0</data>
      <data key="d5">Nicholas Ioannidis provided insightful discussions and feedback on the work</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ADAS" target="EUNJEONG HWANG">
      <data key="d4">14.0</data>
      <data key="d5">Eunjeong Hwang provided insightful discussions and feedback on the work</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="ARC LOGIC PUZZLE TASK" target="CHOLLET (2019)">
      <data key="d4">8.0</data>
      <data key="d5">Chollet (2019) discusses the ARC logic puzzle task</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="CHOLLET (2019)" target="ARC">
      <data key="d4">8.0</data>
      <data key="d5">Chollet (2019) discusses the ARC logic puzzle task</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="DROP" target="DUA ET AL. (2019)">
      <data key="d4">8.0</data>
      <data key="d5">Dua et al. (2019) discusses the DROP benchmark</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="DROP" target="READING COMPREHENSION">
      <data key="d4">16.0</data>
      <data key="d5">DROP is a benchmark for reading comprehension tasks
DROP is a reading comprehension benchmark</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba,86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="DROP" target="DUA">
      <data key="d4">6.0</data>
      <data key="d5">Dua is an author mentioned in relation to the DROP dataset</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="DROP" target="DUA ET AL.">
      <data key="d4">6.0</data>
      <data key="d5">Dua et al. are the authors of the DROP benchmark</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="DROP" target="ONE-SHOT STYLE QUESTIONS">
      <data key="d4">16.0</data>
      <data key="d5">DROP uses one-shot style questions for evaluationDROP uses one-shot style questions</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="DROP" target="AUTOMATED DESIGN OF AGENTIC SYSTEMS">
      <data key="d4">7.0</data>
      <data key="d5">DROP is discussed in the Automated Design of Agentic Systems document</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="DROP" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">DROP is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="DROP" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">DROP is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="DROP" target="ORCA-3">
      <data key="d4">7.0</data>
      <data key="d5">DROP is a benchmark used to evaluate the performance of Orca-3</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="DROP" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">DROP is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="DROP" target="GPT-3.5-TURBO">
      <data key="d4">7.0</data>
      <data key="d5">DROP is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="DROP" target="EXACT MATCH/SPAN EXTRACTION PROBLEMS">
      <data key="d4">8.0</data>
      <data key="d5">DROP is a dataset used for problems where a ground-truth answer value is given in exact match/span extraction problems</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="MGSM" target="SHI ET AL. (2023)">
      <data key="d4">8.0</data>
      <data key="d5">Shi et al. (2023) discusses the MGSM benchmark</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="MGSM" target="SHI">
      <data key="d4">6.0</data>
      <data key="d5">Shi is an author mentioned in relation to the MGSM dataset</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="MGSM" target="SHI ET AL.">
      <data key="d4">6.0</data>
      <data key="d5">Shi et al. are the authors of the MGSM benchmark</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="GSM8K" target="COBBE ET AL. (2021)">
      <data key="d4">16.0</data>
      <data key="d5">Cobbe et al. (2021) discusses the GSM8K benchmark</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="GSM8K" target="COBBE">
      <data key="d4">6.0</data>
      <data key="d5">Cobbe is an author mentioned in relation to the GSM8K dataset</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="GSM8K" target="ORCA-3">
      <data key="d4">36.0</data>
      <data key="d5">Orca-3 showed a 54% improvement on the GSM8K benchmark
Orca-3 shows 54% improvement on GSM8K benchmark
GSM8K is a benchmark used to evaluate the performance of Orca-3
Orca-3 shows significant improvements on the GSM8K math benchmark</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GSM8K" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">GSM8K is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GSM8K" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">GSM8K is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GSM8K" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">GSM8K is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GSM8K" target="GPT-3.5-TURBO">
      <data key="d4">13.0</data>
      <data key="d5">GSM8K is a benchmark used to evaluate the performance of GPT-3.5-turbo
GPT-3.5-turbo scores for GSM8K are taken from a specific reference</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GSM8K" target="GRADE SCHOOL MATH">
      <data key="d4">8.0</data>
      <data key="d5">GSM8K consists of grade school math word problems</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="GSM8K" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B shows significant improvements on the GSM8K math benchmark</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="GSM8K" target="EXACT MATCH/SPAN EXTRACTION PROBLEMS">
      <data key="d4">8.0</data>
      <data key="d5">GSM8K is a dataset used for math-based questions in exact match/span extraction problems</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="GSM-HARD" target="GAO ET AL. (2023)">
      <data key="d4">16.0</data>
      <data key="d5">Gao et al. (2023) discusses the GSM-Hard benchmark</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="GSM-HARD" target="GAO">
      <data key="d4">1.0</data>
      <data key="d5">Gao is an author mentioned in relation to the GSM-Hard dataset</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="FMS" target="LU ET AL. (2024C)">
      <data key="d4">16.0</data>
      <data key="d5">Lu et al. (2024c) discusses open-endedness algorithms leveraging human notions of interestingness</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="FMS" target="ZHANG ET AL. (2024A)">
      <data key="d4">2.0</data>
      <data key="d5">Zhang et al. (2024a) discusses open-endedness algorithms leveraging human notions of interestingness</data>
      <data key="d6">81c504ffbcc5ed882e234802135295ba</data>
    </edge>
    <edge source="FMS" target="LANGUAGE-TO-REWARD">
      <data key="d4">8.0</data>
      <data key="d5">Language-to-reward enables FMs to write reward functions for reinforcement learning in robotics</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGENTIC SYSTEMS" target="CONTROL FLOW">
      <data key="d4">7.0</data>
      <data key="d5">Control flow is a component of agentic systems that dictates the sequence of operations or steps an agent follows</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="AGENTIC SYSTEMS" target="TEXT PROMPTS">
      <data key="d4">7.0</data>
      <data key="d5">Text prompts are components of agentic systems that can be mutated to create new agents</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="AGENTIC SYSTEMS" target="STRUCTURED FEEDBACK AND ENSEMBLE AGENT">
      <data key="d4">7.0</data>
      <data key="d5">The structured feedback and ensemble agent is a type of agentic system</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="ARC" target="CHOLLET">
      <data key="d4">6.0</data>
      <data key="d5">Chollet is an author mentioned in relation to the ARC logic puzzle task</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="ARC" target="BEST AGENT">
      <data key="d4">16.0</data>
      <data key="d5">The best agent was evaluated on the ARC dataset</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="ARC" target="CHAIN-OF-THOUGHT (COT)">
      <data key="d4">7.0</data>
      <data key="d5">Chain-of-Thought (COT) is used as a baseline for experiments on ARC</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="ARC" target="SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT (COT-SC)">
      <data key="d4">7.0</data>
      <data key="d5">Self-Consistency with Chain-of-Thought (COT-SC) is used as a baseline for experiments on ARC</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="ARC" target="LLM-DEBATE">
      <data key="d4">7.0</data>
      <data key="d5">LLM-Debate is used as a baseline for experiments on ARC</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="ARC" target="QUALITY-DIVERSITY">
      <data key="d4">7.0</data>
      <data key="d5">Quality-Diversity is used as a baseline for experiments on ARC</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="ARC" target="GPT-3.5-TURBO-0125">
      <data key="d4">7.0</data>
      <data key="d5">ARC dataset is evaluated using GPT-3.5-Turbo-0125</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="ARC" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">ARC is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ARC" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">ARC is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ARC" target="ORCA-3">
      <data key="d4">7.0</data>
      <data key="d5">ARC is a benchmark used to evaluate the performance of Orca-3</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ARC" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">ARC is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ARC" target="GPT-3.5-TURBO">
      <data key="d4">7.0</data>
      <data key="d5">ARC is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ARC" target="MMLU">
      <data key="d4">6.0</data>
      <data key="d5">Both MMLU and ARC are benchmarks that measure various capabilities of language models</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="ARC" target="ALLENAI">
      <data key="d4">8.0</data>
      <data key="d5">AllenAI developed the AI2 Reasoning Challenge (ARC) benchmark</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="READING COMPREHENSION" target="F1 SCORE">
      <data key="d4">7.0</data>
      <data key="d5">F1 scores are used to evaluate agents in the Reading Comprehension domain</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="READING COMPREHENSION" target="ACCURACY">
      <data key="d4">7.0</data>
      <data key="d5">Accuracy rates are used to evaluate agents in the Reading Comprehension domain</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="READING COMPREHENSION" target="TEXT MODIFICATION">
      <data key="d4">6.0</data>
      <data key="d5">Both Reading Comprehension and Text Modification are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="READING COMPREHENSION" target="OPEN DOMAIN QUESTION ANSWERING">
      <data key="d4">6.0</data>
      <data key="d5">Both Open Domain Question Answering and Reading Comprehension are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="READING COMPREHENSION" target="MULTIPLE CHOICE QUESTIONS">
      <data key="d4">6.0</data>
      <data key="d5">Both Multiple Choice Questions and Reading Comprehension are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="READING COMPREHENSION" target="READING COMPREHENSION TESTS">
      <data key="d4">7.0</data>
      <data key="d5">Reading comprehension tests assess the reader&#8217;s understanding of text passages.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="READING COMPREHENSION" target="AGENTINSTRUCT FLOW">
      <data key="d4">9.0</data>
      <data key="d5">AgentInstruct Flow includes a specific flow for reading comprehension to facilitate learning and understanding.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="READING COMPREHENSION" target="CONTENT TRANSFORMATION FLOW">
      <data key="d4">8.0</data>
      <data key="d5">Content Transformation Flow is used to generate materials that support reading comprehension.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="READING COMPREHENSION" target="LSAT">
      <data key="d4">9.0</data>
      <data key="d5">LSAT is known for its difficult reading comprehension sections</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="READING COMPREHENSION" target="LLMS">
      <data key="d4">8.0</data>
      <data key="d5">Reading comprehension is a crucial capability for LLMs</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="READING COMPREHENSION" target="SLMS">
      <data key="d4">8.0</data>
      <data key="d5">Reading comprehension is arguably even more important for Small Language Models (SLMs)</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="SEARCH SPACE" target="PROMPTBREEDER">
      <data key="d4">14.0</data>
      <data key="d5">PromptBreeder mutates only the text prompts of an agent within the search space</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH SPACE" target="FEED-FORWARD NETWORKS">
      <data key="d4">7.0</data>
      <data key="d5">Feed-forward networks are a type of search space explored in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH SPACE" target="GRAPH STRUCTURES">
      <data key="d4">7.0</data>
      <data key="d5">Graph structures are a type of search space explored in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH SPACE" target="PROGRAMMING LANGUAGES">
      <data key="d4">7.0</data>
      <data key="d5">Programming languages are used as a search space in ADAS to define and discover new agentic systems</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH SPACE" target="OPEN-SOURCE AGENT FRAMEWORKS">
      <data key="d4">7.0</data>
      <data key="d5">Open-source agent frameworks like LangChain are used to build upon existing building blocks in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="SEARCH SPACE" target="EXISTING BUILDING BLOCKS">
      <data key="d4">7.0</data>
      <data key="d5">Existing building blocks are components like RAG and search engine tools that can be used in open-source agent frameworks in ADAS</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="EVALUATION FUNCTION" target="ACCURACY RATE">
      <data key="d4">7.0</data>
      <data key="d5">Accuracy rate is a metric used in the evaluation function to assess an agent's performance on validation data</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="EVALUATION FUNCTION" target="COST">
      <data key="d4">7.0</data>
      <data key="d5">Cost is a metric used in the evaluation function to assess the expense of running an agent</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="EVALUATION FUNCTION" target="LATENCY">
      <data key="d4">7.0</data>
      <data key="d5">Latency is a metric used in the evaluation function to assess the response time of an agent</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="EVALUATION FUNCTION" target="SAFETY">
      <data key="d4">7.0</data>
      <data key="d5">Safety is a metric used in the evaluation function to assess the risk associated with an agent</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="EVALUATION FUNCTION" target="VALIDATION DATA">
      <data key="d4">7.0</data>
      <data key="d5">Validation data is used to assess the performance of an agent on unseen future data in the evaluation function</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="EVALUATION FUNCTION" target="UNSEEN FUTURE DATA">
      <data key="d4">7.0</data>
      <data key="d5">Unseen future data is data that an agent has not encountered before, used to evaluate its performance in the evaluation function</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="EVALUATION FUNCTION" target="EXPERIMENT COST">
      <data key="d4">1.0</data>
      <data key="d5">Optimizing the evaluation function can reduce the cost of experiments</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="PROMPTBREEDER" target="FERNANDO">
      <data key="d4">10.0</data>
      <data key="d5">Fernando is referenced in the context of PromptBreeder</data>
      <data key="d6">4884e8429ca1e567dadf5e22b4b68274</data>
    </edge>
    <edge source="PROMPTBREEDER" target="OPRO">
      <data key="d4">7.0</data>
      <data key="d5">Both OPRO and PromptBreeder adopt FMs to automate prompt engineering for agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="PROMPTBREEDER" target="SELF-DISCOVER">
      <data key="d4">7.0</data>
      <data key="d5">Both PromptBreeder and Self-Discover adopt FMs to automate prompt engineering for agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="PROMPTBREEDER" target="FERNANDO ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Fernando et al., 2024 discusses PromptBreeder and its application in automating prompt engineering for agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="COST" target="AGENTINSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">Cost is a limitation of AgentInstruct and synthetic data generation</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="FUNSEARCH" target="ROMERA-PAREDES">
      <data key="d4">6.0</data>
      <data key="d5">Romera-Paredes is an author mentioned in relation to the FunSearch algorithm</data>
      <data key="d6">24d7b89ae9522ae60d2317984951355b</data>
    </edge>
    <edge source="FUNSEARCH" target="FOUNDATION MODELS">
      <data key="d4">6.0</data>
      <data key="d5">FunSearch uses Foundation Models to write code for discovering better optimization algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="FUNSEARCH" target="ROMERA-PAREDES ET AL., 2024">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Romera-Paredes et al. in 2024 is related to FunSearch</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="LLM DEBATE" target="DU ET AL., 2023">
      <data key="d4">14.0</data>
      <data key="d5">Du et al., 2023 discusses the LLM Debate method
The publication by Du et al. in 2023 is related to LLM Debate</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="QUALITY-DIVERSITY" target="LU ET AL., 2024C">
      <data key="d4">28.0</data>
      <data key="d5">Quality-Diversity was introduced by Lu et al., 2024c
Lu et al., 2024c discusses the Quality-Diversity method
The publication by Lu et al. in 2024 is related to Quality-Diversity
Lu et al., 2024c is a publication referenced for the Quality-Diversity method</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7,2901d5e2711fa4f32d39cd8eea36cd71,7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="QUALITY-DIVERSITY" target="LU ET AL.">
      <data key="d4">14.0</data>
      <data key="d5">Lu et al. are the authors of the Quality-Diversity agent
Lu et al. are the authors of the Quality-Diversity method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="ARC CHALLENGE" target="TRANSFORMATION RULE">
      <data key="d4">18.0</data>
      <data key="d5">The ARC challenge requires AI systems to learn the transformation rule of grid patterns
The ARC challenge involves learning a transformation rule from examples</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7,4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="NUMBER COUNTING">
      <data key="d4">8.0</data>
      <data key="d5">Number counting is a capability required by AI systems in the ARC challenge</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="GEOMETRY">
      <data key="d4">8.0</data>
      <data key="d5">Geometry is a capability required by AI systems in the ARC challenge</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="TOPOLOGY">
      <data key="d4">8.0</data>
      <data key="d5">Topology is a capability required by AI systems in the ARC challenge</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="TOOL FUNCTIONS">
      <data key="d4">7.0</data>
      <data key="d5">Tool functions are provided in the framework to evaluate the generated transformation code in the ARC challenge</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="STOCHASTIC SAMPLING OF FMS">
      <data key="d4">7.0</data>
      <data key="d5">Stochastic sampling of FMs is used to reduce variance in the validation and test accuracy of agents in the ARC challenge</data>
      <data key="d6">1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="INPUT GRID">
      <data key="d4">9.0</data>
      <data key="d5">The ARC challenge involves input grids</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="OUTPUT GRID">
      <data key="d4">9.0</data>
      <data key="d5">The ARC challenge involves output grids</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="EXACT MATCH">
      <data key="d4">1.0</data>
      <data key="d5">The accuracy rate in the ARC challenge is calculated using the Exact Match metric</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="EXPERIMENT DETAILS">
      <data key="d4">7.0</data>
      <data key="d5">The experiment details section provides information about the ARC challenge</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="DEMONSTRATION EXAMPLES">
      <data key="d4">9.0</data>
      <data key="d5">The ARC challenge provides demonstration examples to illustrate the transformation rule</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="TEST EXAMPLE">
      <data key="d4">9.0</data>
      <data key="d5">The ARC challenge includes a test example where the output grid needs to be predicted</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="ARC CHALLENGE" target="EXAMPLE 0">
      <data key="d4">8.0</data>
      <data key="d5">Example 0 is a specific task from the ARC challenge</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="LLM-DEBATE" target="DU ET AL., 2023">
      <data key="d4">14.0</data>
      <data key="d5">LLM-Debate was introduced by Du et al., 2023
Du et al., 2023 is a publication referenced for the LLM-Debate method</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,1a6353c9d196dc2debad7c27c902bcd7</data>
    </edge>
    <edge source="LLM-DEBATE" target="PHYSICS EXPERT">
      <data key="d4">6.0</data>
      <data key="d5">Physics Expert is a role assigned in the LLM-Debate method</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="LLM-DEBATE" target="CHEMISTRY EXPERT">
      <data key="d4">6.0</data>
      <data key="d5">Chemistry Expert is a role assigned in the LLM-Debate method</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="LLM-DEBATE" target="BIOLOGY EXPERT">
      <data key="d4">6.0</data>
      <data key="d5">Biology Expert is a role assigned in the LLM-Debate method</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="LLM-DEBATE" target="SCIENCE GENERALIST">
      <data key="d4">6.0</data>
      <data key="d5">Science Generalist is a role assigned in the LLM-Debate method</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="WANG ET AL., 2019" target="POET">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Wang et al. in 2019 is related to POET</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="WANG ET AL., 2020" target="POET">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Wang et al. in 2020 is related to POET</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="EFFICIENCY EXPERT" target="EXPERT_ADVISORS">
      <data key="d4">8.0</data>
      <data key="d5">Efficiency Expert is one of the expert advisors</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="READABILITY EXPERT" target="EXPERT_ADVISORS">
      <data key="d4">8.0</data>
      <data key="d5">Readability Expert is one of the expert advisors</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="SIMPLICITY EXPERT" target="EXPERT_ADVISORS">
      <data key="d4">8.0</data>
      <data key="d5">Simplicity Expert is one of the expert advisors</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="ITERATION 5" target="MECHANISM">
      <data key="d4">7.0</data>
      <data key="d5">The idea of incorporating diverse feedback emerged in Iteration 5</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="ITERATION 11" target="MECHANISM">
      <data key="d4">7.0</data>
      <data key="d5">The idea of evaluating for various specific traits emerged in Iteration 11</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="ITERATION 12" target="MECHANISM">
      <data key="d4">7.0</data>
      <data key="d5">The idea of simulating human-like feedback emerged in Iteration 12</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="MMLU" target="HENDRYCKS ET AL.">
      <data key="d4">6.0</data>
      <data key="d5">Hendrycks et al. are the authors of the MMLU benchmark</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="MMLU" target="STEM">
      <data key="d4">8.0</data>
      <data key="d5">STEM is one of the subject areas included in the MMLU benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="MMLU" target="SOCIAL SCIENCES">
      <data key="d4">8.0</data>
      <data key="d5">Social Sciences is one of the subject areas included in the MMLU benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="MMLU" target="HUMANITIES">
      <data key="d4">8.0</data>
      <data key="d5">Humanities is one of the subject areas included in the MMLU benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="MMLU" target="ORCA-3">
      <data key="d4">36.0</data>
      <data key="d5">Orca-3 showed a 19% improvement on the MMLU benchmark
Orca-3 shows 19% improvement on MMLU benchmark
MMLU is a benchmark used to evaluate the performance of Orca-3
Orca-3 shows significant improvements on the MMLU mathematical benchmarks</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MMLU" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">MMLU is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MMLU" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">MMLU is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MMLU" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">MMLU is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MMLU" target="GPT-3.5-TURBO">
      <data key="d4">7.0</data>
      <data key="d5">MMLU is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MMLU" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B shows significant improvements on the MMLU mathematical benchmarks</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="MMLU" target="MIRAGE">
      <data key="d4">8.0</data>
      <data key="d5">MMLU is one of the datasets included in the MIRAGE collection</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="GPQA" target="REIN ET AL.">
      <data key="d4">6.0</data>
      <data key="d5">Rein et al. are the authors of the GPQA benchmark</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="GPQA" target="VALIDATION_SET">
      <data key="d4">8.0</data>
      <data key="d5">GPQA uses a validation set for evaluation</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="GPQA" target="TEST_SET">
      <data key="d4">8.0</data>
      <data key="d5">GPQA uses a test set for evaluation</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="GPQA" target="ZERO-SHOT STYLE QUESTIONS">
      <data key="d4">8.0</data>
      <data key="d5">GPQA uses zero-shot style questions</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="GPQA" target="AUTOMATED DESIGN OF AGENTIC SYSTEMS">
      <data key="d4">7.0</data>
      <data key="d5">GPQA is discussed in the Automated Design of Agentic Systems document</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="GPQA" target="BIOLOGY">
      <data key="d4">8.0</data>
      <data key="d5">Biology is one of the domains included in the GPQA benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="GPQA" target="PHYSICS">
      <data key="d4">8.0</data>
      <data key="d5">Physics is one of the domains included in the GPQA benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="GPQA" target="CHEMISTRY">
      <data key="d4">8.0</data>
      <data key="d5">Chemistry is one of the domains included in the GPQA benchmark</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="GPQA" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">GPQA is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPQA" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">GPQA is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPQA" target="ORCA-3">
      <data key="d4">7.0</data>
      <data key="d5">GPQA is a benchmark used to evaluate the performance of Orca-3</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPQA" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">GPQA is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPQA" target="GPT-3.5-TURBO">
      <data key="d4">7.0</data>
      <data key="d5">GPQA is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPQA" target="DOMAIN EXPERTS">
      <data key="d4">8.0</data>
      <data key="d5">GPQA questions are created by domain experts pursuing PhDs in their respective fields</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="STEP-BACK ABSTRACTION" target="ZHENG ET AL.">
      <data key="d4">14.0</data>
      <data key="d5">Zheng et al. are the authors of the Step-back Abstraction agent
Zheng et al. are the authors of the Step-back Abstraction method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="STEP-BACK ABSTRACTION" target="ZHENG ET AL., 2023">
      <data key="d4">20.0</data>
      <data key="d5">The publication by Zheng et al. in 2023 is related to Step-back Abstraction
Zheng et al., 2023 is a publication referenced for the Step-back Abstraction method
Step-back Abstraction is discussed in the publication by Zheng et al. in 2023</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="STEP-BACK ABSTRACTION" target="REASONING AND PROBLEM-SOLVING DOMAINS">
      <data key="d4">7.0</data>
      <data key="d5">Step-back Abstraction is used as a baseline for experiments on Reasoning and Problem-Solving domains</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="ROLE ASSIGNMENT" target="XU ET AL.">
      <data key="d4">9.0</data>
      <data key="d5">Xu et al. are the authors of the Role Assignment agent
Xu et al. are the authors of the Role Assignment method</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64,bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="ROLE ASSIGNMENT" target="XU ET AL., 2023">
      <data key="d4">10.0</data>
      <data key="d5">The publication by Xu et al. in 2023 is related to Role Assignment
Xu et al., 2023 is a publication referenced for the Role Assignment method
Role Assignment is discussed in the publication by Xu et al. in 2023</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959,7c08d98f503d722d7de13be55375c8cb,97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="ROLE ASSIGNMENT" target="REASONING AND PROBLEM-SOLVING DOMAINS">
      <data key="d4">7.0</data>
      <data key="d5">Role Assignment is used as a baseline for experiments on Reasoning and Problem-Solving domains</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="MECHANISM" target="EXPERTS">
      <data key="d4">6.0</data>
      <data key="d5">Experts evaluate various specific traits such as efficiency and simplicity</data>
      <data key="d6">bc26e68b0b2783ba912b9e5606d9eb0b</data>
    </edge>
    <edge source="ACCURACY" target="AGENTINSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">Accuracy is a limitation of AgentInstruct and synthetic data generation</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="BASELINES" target="GPT-3.5-TURBO-0125">
      <data key="d4">1.0</data>
      <data key="d5">Baselines use the GPT-3.5-turbo-0125 model</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="BASELINES" target="AUTOMATED DESIGN OF AGENTIC SYSTEMS">
      <data key="d4">1.0</data>
      <data key="d5">Baselines are discussed in the Automated Design of Agentic Systems document</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="STRUCTURED FEEDBACK AND ENSEMBLE AGENT" target="BEST AGENT">
      <data key="d4">16.0</data>
      <data key="d5">The best agent uses structured feedback and ensemble methods</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="STRUCTURED FEEDBACK AND ENSEMBLE AGENT" target="FM_MODULE">
      <data key="d4">16.0</data>
      <data key="d5">The agent uses FM_Module to generate initial candidate solutions</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="STRUCTURED FEEDBACK AND ENSEMBLE AGENT" target="ENSEMBLE METHODS">
      <data key="d4">8.0</data>
      <data key="d5">The structured feedback and ensemble agent uses ensemble methods</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="SVAMP" target="PATEL ET AL., 2021">
      <data key="d4">8.0</data>
      <data key="d5">Patel et al., 2021 discusses the SVAMP dataset</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="SVAMP" target="PATEL ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Patel et al. are the authors of the SVAMP dataset</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="ASDIV" target="MIAO ET AL., 2020">
      <data key="d4">8.0</data>
      <data key="d5">Miao et al., 2020 discusses the ASDiv dataset</data>
      <data key="d6">2901d5e2711fa4f32d39cd8eea36cd71</data>
    </edge>
    <edge source="ASDIV" target="MIAO ET AL.">
      <data key="d4">8.0</data>
      <data key="d5">Miao et al. are the authors of the ASDiv dataset</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="AI-GENERATING ALGORITHMS" target="MAML">
      <data key="d4">7.0</data>
      <data key="d5">MAML is a technique under the second pillar of AI-Generating Algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="AI-GENERATING ALGORITHMS" target="META-RL">
      <data key="d4">7.0</data>
      <data key="d5">Meta-RL is a technique under the second pillar of AI-Generating Algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="AI-GENERATING ALGORITHMS" target="POET">
      <data key="d4">7.0</data>
      <data key="d5">POET is a technique under the third pillar of AI-Generating Algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="AI-GENERATING ALGORITHMS" target="CLUNE, 2019">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Clune in 2019 is related to AI-Generating Algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="VEMPRALA ET AL." target="DEVELOPING NEW SKILLS FOR EMBODIED AGENTS IN CODE">
      <data key="d4">8.0</data>
      <data key="d5">Vemprala et al. are the authors of the method for developing new skills for embodied agents in code</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="HONG ET AL." target="ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES AND ENABLING THEM TO COLLABORATE">
      <data key="d4">8.0</data>
      <data key="d5">Hong et al. are the authors of the method for assigning FM modules in the agentic system with different roles and enabling them to collaborate</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="QIAN ET AL." target="ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES AND ENABLING THEM TO COLLABORATE">
      <data key="d4">8.0</data>
      <data key="d5">Qian et al. are the authors of the method for assigning FM modules in the agentic system with different roles and enabling them to collaborate</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="RICHARDS" target="ENABLING THE AGENT TO INSTRUCT ITSELF FOR THE NEXT ACTION">
      <data key="d4">1.0</data>
      <data key="d5">Richards is the author of the method for enabling the agent to instruct itself for the next action</data>
      <data key="d6">0b6b4880e77d40e284702da16be4ef64</data>
    </edge>
    <edge source="MAML" target="FINN ET AL., 2017">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Finn et al. in 2017 is related to MAML</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="META-RL" target="DUAN ET AL., 2017">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Duan et al. in 2017 is related to Meta-RL</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="META-RL" target="NORMAN &amp; CLUNE, 2023">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Norman &amp; Clune in 2023 is related to Meta-RL</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="META-RL" target="WANG ET AL., 2016">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Wang et al. in 2016 is related to Meta-RL</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="META-RL" target="ZINTGRAF ET AL., 2021A">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Zintgraf et al. in 2021 is related to Meta-RL</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="META-RL" target="ZINTGRAF ET AL., 2021B">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Zintgraf et al. in 2021 is related to Meta-RL</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="POET" target="DHARNA ET AL., 2020">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Dharna et al. in 2020 is related to POET</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="FOUNDATION MODELS" target="EOH">
      <data key="d4">6.0</data>
      <data key="d5">EoH uses Foundation Models to write code for discovering better optimization algorithms</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="FOUNDATION MODELS" target="DISCOPOP">
      <data key="d4">6.0</data>
      <data key="d5">DiscoPOP uses Foundation Models to program the loss function for preference learning</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="FOUNDATION MODELS" target="EUREKA">
      <data key="d4">6.0</data>
      <data key="d5">Eureka uses Foundation Models to write reward functions for reinforcement learning in robotics</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="FOUNDATION MODELS" target="LANGUAGE-TO-REWARD">
      <data key="d4">6.0</data>
      <data key="d5">Language-to-Reward uses Foundation Models to write reward functions for reinforcement learning in robotics</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="DISCOPOP" target="LU ET AL., 2024A">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Lu et al. in 2024 is related to DiscoPOP</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="DISCOPOP" target="RAFAILOV ET AL., 2024">
      <data key="d4">6.0</data>
      <data key="d5">The publication by Rafailov et al. in 2024 is related to FM alignment training, which is part of DiscoPOP</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb</data>
    </edge>
    <edge source="EUREKA" target="MA ET AL., 2023">
      <data key="d4">14.0</data>
      <data key="d5">The publication by Ma et al. in 2023 is related to Eureka
Ma et al., 2023 discusses Eureka and its application in enabling FMs to write reward functions for reinforcement learning in robotics</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="LANGUAGE-TO-REWARD" target="YU ET AL., 2023">
      <data key="d4">14.0</data>
      <data key="d5">The publication by Yu et al. in 2023 is related to language-to-reward
Yu et al., 2023 discusses language-to-reward and its application in enabling FMs to write reward functions for reinforcement learning in robotics</data>
      <data key="d6">7c08d98f503d722d7de13be55375c8cb,dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="CLUNE, 2019" target="AGI">
      <data key="d4">14.0</data>
      <data key="d5">Clune, 2019 discusses the pursuit of AGI</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="XU ET AL., 2023" target="AGENTVERSE">
      <data key="d4">7.0</data>
      <data key="d5">Xu et al., 2023 discusses the benefits of assigning personas or roles to agents, which is a concept used in AgentVerse</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="OPRO" target="SELF-DISCOVER">
      <data key="d4">7.0</data>
      <data key="d5">Both OPRO and Self-Discover adopt FMs to automate prompt engineering for agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="OPRO" target="YANG ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Yang et al., 2024 discusses OPRO and its application in automating prompt engineering for agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="SELF-DISCOVER" target="ZHOU ET AL., 2024A">
      <data key="d4">8.0</data>
      <data key="d5">Zhou et al., 2024a discusses Self-Discover and its application in automating prompt engineering for agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="EVOAGENT" target="AGENTVERSE">
      <data key="d4">7.0</data>
      <data key="d5">Both EvoAgent and AgentVerse optimize role definition in the prompt</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="EVOAGENT" target="YUAN ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Yuan et al., 2024 discusses EvoAgent and its application in optimizing role definition in the prompt</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="DYLAN" target="DSPY">
      <data key="d4">7.0</data>
      <data key="d5">Both DyLAN and DSPy involve learning more components than just prompts in agentic systems</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="DYLAN" target="GPT-SWARM">
      <data key="d4">7.0</data>
      <data key="d5">Both DyLAN and GPT-Swarm involve learning more components than just prompts in agentic systems</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="DSPY" target="GPT-SWARM">
      <data key="d4">7.0</data>
      <data key="d5">Both DSPy and GPT-Swarm involve learning more components than just prompts in agentic systems</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="DSPY" target="KHATTAB ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Khattab et al., 2024 discusses DSPy and its application in generating a set of possible nodes and optimizing across the Cartesian product of these nodes</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="GPT-SWARM" target="ZHUGE ET AL., 2024">
      <data key="d4">8.0</data>
      <data key="d5">Zhuge et al., 2024 discusses GPT-Swarm and its application in representing an agentic system in a graph with a predefined set of nodes</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGENTOPTIMIZER" target="AGENT SYMBOLIC LEARNING">
      <data key="d4">7.0</data>
      <data key="d5">Both AgentOptimizer and Agent Symbolic Learning learn tools used in agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGENTOPTIMIZER" target="ZHANG ET AL., 2024B">
      <data key="d4">8.0</data>
      <data key="d5">Zhang et al., 2024b discusses AgentOptimizer and its application in learning the tools used in agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGENT SYMBOLIC LEARNING" target="ZHOU ET AL., 2024B">
      <data key="d4">8.0</data>
      <data key="d5">Zhou et al., 2024b discusses Agent Symbolic Learning and its application in learning prompts, tools, and control flow together in agents</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGI" target="AI-GA">
      <data key="d4">16.0</data>
      <data key="d5">AI-GA could potentially contribute to creating AGI faster than the current manual approach</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGI" target="BENGIO ET AL., 2024">
      <data key="d4">14.0</data>
      <data key="d5">Bengio et al., 2024 discusses the pursuit of AGI</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGI" target="BOSTROM, 2002">
      <data key="d4">14.0</data>
      <data key="d5">Bostrom, 2002 discusses the pursuit of AGI</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGI" target="ECOFFET ET AL., 2020">
      <data key="d4">14.0</data>
      <data key="d5">Ecoffet et al., 2020 discusses the pursuit of AGI</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="AGI" target="YUDKOWSKY ET AL., 2008">
      <data key="d4">2.0</data>
      <data key="d5">Yudkowsky et al., 2008 discusses the pursuit of AGI</data>
      <data key="d6">dc55f071b95dec721a9820d39cdb3ccd</data>
    </edge>
    <edge source="COMPLEXITY" target="INSTRUCTION REFINEMENT FLOW">
      <data key="d4">8.0</data>
      <data key="d5">Instruction Refinement Flow aims to enhance the complexity of the generated instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="INSTRUCTION" target="TASKS">
      <data key="d4">7.0</data>
      <data key="d5">Instructions are the tasks or guidelines that agents follow to perform their roles in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="INSTRUCTION" target="OBJECTIVES">
      <data key="d4">7.0</data>
      <data key="d5">Instructions are tailored to achieve specific objectives in the Content Transformation Flow.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AS" target="HUMAN ORGANIZATION">
      <data key="d4">14.0</data>
      <data key="d5">AS sheds light on the origins of complexity emerging from human organization</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="AS" target="HUMAN SOCIETY">
      <data key="d4">14.0</data>
      <data key="d5">AS sheds light on the origins of complexity emerging from human society</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="HUMAN ORGANIZATION" target="AGENTIC SYSTEM">
      <data key="d4">16.0</data>
      <data key="d5">Agentic systems operate over natural language, which is used by humans in constructing our organization</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="HUMAN SOCIETY" target="AGENTIC SYSTEM">
      <data key="d4">16.0</data>
      <data key="d5">Agentic systems operate over natural language, which is used by humans in constructing our society</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="AGENTIC SYSTEM" target="HONG ET AL., 2023">
      <data key="d4">12.0</data>
      <data key="d5">Hong et al. (2023) incorporated the organizational structure for human companies in agents</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="AGENTIC SYSTEM" target="PARK ET AL., 2023">
      <data key="d4">12.0</data>
      <data key="d5">Park et al. (2023) simulated a human town with agents</data>
      <data key="d6">7de66b94cf868b37b1df51dc545c415f</data>
    </edge>
    <edge source="JENNY ZHANG" target="JOEL LEHMAN">
      <data key="d4">16.0</data>
      <data key="d5">Jenny Zhang and Joel Lehman co-authored the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JENNY ZHANG" target="KENNETH STANLEY">
      <data key="d4">16.0</data>
      <data key="d5">Jenny Zhang and Kenneth Stanley co-authored the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JOEL LEHMAN" target="KENNETH STANLEY">
      <data key="d4">16.0</data>
      <data key="d5">Joel Lehman and Kenneth Stanley co-authored the paper "OMNI: Open-endedness via models of human notions of interestingness"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="CHATGPT" target="ORCA-2.5">
      <data key="d4">6.0</data>
      <data key="d5">Orca-2.5 and ChatGPT are both baseline models evaluated on the Orca-Bench dataset</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="CHATGPT" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">6.0</data>
      <data key="d5">Mistral-Instruct-7B and ChatGPT are both baseline models evaluated on the Orca-Bench dataset</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="CHATGPT" target="ORCA-3">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3 shows notable enhancement in capabilities during post-training compared to ChatGPT</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="CHATGPT" target="TABLE 2">
      <data key="d4">7.0</data>
      <data key="d5">Table 2 encapsulates the average scores of ChatGPT</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="CHATGPT" target="FIGURE 4">
      <data key="d4">7.0</data>
      <data key="d5">Figure 4 illustrates the performance comparison of ChatGPT</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="QIANG WANG" target="DAWEI YIN">
      <data key="d4">8.0</data>
      <data key="d5">Qiang Wang and Dawei Yin co-authored the paper "Tool learning with large language models: A survey"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="QIANG WANG" target="JUN XU">
      <data key="d4">8.0</data>
      <data key="d5">Qiang Wang and Jun Xu co-authored the paper "Tool learning with large language models: A survey"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="QIANG WANG" target="JI-RONG WEN">
      <data key="d4">8.0</data>
      <data key="d5">Qiang Wang and Ji-Rong Wen co-authored the paper "Tool learning with large language models: A survey"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAWEI YIN" target="JUN XU">
      <data key="d4">8.0</data>
      <data key="d5">Dawei Yin and Jun Xu co-authored the paper "Tool learning with large language models: A survey"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAWEI YIN" target="JI-RONG WEN">
      <data key="d4">8.0</data>
      <data key="d5">Dawei Yin and Ji-Rong Wen co-authored the paper "Tool learning with large language models: A survey"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="JUN XU" target="JI-RONG WEN">
      <data key="d4">8.0</data>
      <data key="d5">Jun Xu and Ji-Rong Wen co-authored the paper "Tool learning with large language models: A survey"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="RAFAEL RAFAILOV" target="ARCHIT SHARMA">
      <data key="d4">8.0</data>
      <data key="d5">Rafael Rafailov and Archit Sharma co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="RAFAEL RAFAILOV" target="ERIC MITCHELL">
      <data key="d4">8.0</data>
      <data key="d5">Rafael Rafailov and Eric Mitchell co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="RAFAEL RAFAILOV" target="STEFANO ERMON">
      <data key="d4">8.0</data>
      <data key="d5">Rafael Rafailov and Stefano Ermon co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ARCHIT SHARMA" target="ERIC MITCHELL">
      <data key="d4">8.0</data>
      <data key="d5">Archit Sharma and Eric Mitchell co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ARCHIT SHARMA" target="STEFANO ERMON">
      <data key="d4">8.0</data>
      <data key="d5">Archit Sharma and Stefano Ermon co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ERIC MITCHELL" target="STEFANO ERMON">
      <data key="d4">8.0</data>
      <data key="d5">Eric Mitchell and Stefano Ermon co-authored the paper "Direct preference optimization: Your language model is secretly a reward model"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAVID REIN" target="BETTY LI HOU">
      <data key="d4">8.0</data>
      <data key="d5">David Rein and Betty Li Hou co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAVID REIN" target="ASA COOPER STICKLAND">
      <data key="d4">8.0</data>
      <data key="d5">David Rein and Asa Cooper Stickland co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAVID REIN" target="JACKSON PETTY">
      <data key="d4">8.0</data>
      <data key="d5">David Rein and Jackson Petty co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAVID REIN" target="RICHARD YUANZHE PANG">
      <data key="d4">8.0</data>
      <data key="d5">David Rein and Richard Yuanzhe Pang co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAVID REIN" target="JULIEN DIRANI">
      <data key="d4">8.0</data>
      <data key="d5">David Rein and Julien Dirani co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAVID REIN" target="JULIAN MICHAEL">
      <data key="d4">8.0</data>
      <data key="d5">David Rein and Julian Michael co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="DAVID REIN" target="SAMUEL R. BOWMAN">
      <data key="d4">8.0</data>
      <data key="d5">David Rein and Samuel R. Bowman co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="BETTY LI HOU" target="ASA COOPER STICKLAND">
      <data key="d4">8.0</data>
      <data key="d5">Betty Li Hou and Asa Cooper Stickland co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="BETTY LI HOU" target="JACKSON PETTY">
      <data key="d4">8.0</data>
      <data key="d5">Betty Li Hou and Jackson Petty co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="BETTY LI HOU" target="RICHARD YUANZHE PANG">
      <data key="d4">8.0</data>
      <data key="d5">Betty Li Hou and Richard Yuanzhe Pang co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="BETTY LI HOU" target="JULIEN DIRANI">
      <data key="d4">8.0</data>
      <data key="d5">Betty Li Hou and Julien Dirani co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="BETTY LI HOU" target="JULIAN MICHAEL">
      <data key="d4">8.0</data>
      <data key="d5">Betty Li Hou and Julian Michael co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="BETTY LI HOU" target="SAMUEL R. BOWMAN">
      <data key="d4">8.0</data>
      <data key="d5">Betty Li Hou and Samuel R. Bowman co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ASA COOPER STICKLAND" target="JACKSON PETTY">
      <data key="d4">8.0</data>
      <data key="d5">Asa Cooper Stickland and Jackson Petty co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ASA COOPER STICKLAND" target="RICHARD YUANZHE PANG">
      <data key="d4">8.0</data>
      <data key="d5">Asa Cooper Stickland and Richard Yuanzhe Pang co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ASA COOPER STICKLAND" target="JULIEN DIRANI">
      <data key="d4">8.0</data>
      <data key="d5">Asa Cooper Stickland and Julien Dirani co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ASA COOPER STICKLAND" target="JULIAN MICHAEL">
      <data key="d4">8.0</data>
      <data key="d5">Asa Cooper Stickland and Julian Michael co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="ASA COOPER STICKLAND" target="SAMUEL R. BOWMAN">
      <data key="d4">8.0</data>
      <data key="d5">Asa Cooper Stickland and Samuel R. Bowman co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="JACKSON PETTY" target="RICHARD YUANZHE PANG">
      <data key="d4">8.0</data>
      <data key="d5">Jackson Petty and Richard Yuanzhe Pang co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="JACKSON PETTY" target="JULIEN DIRANI">
      <data key="d4">8.0</data>
      <data key="d5">Jackson Petty and Julien Dirani co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="JACKSON PETTY" target="JULIAN MICHAEL">
      <data key="d4">8.0</data>
      <data key="d5">Jackson Petty and Julian Michael co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="JACKSON PETTY" target="SAMUEL R. BOWMAN">
      <data key="d4">8.0</data>
      <data key="d5">Jackson Petty and Samuel R. Bowman co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="RICHARD YUANZHE PANG" target="JULIEN DIRANI">
      <data key="d4">8.0</data>
      <data key="d5">Richard Yuanzhe Pang and Julien Dirani co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="RICHARD YUANZHE PANG" target="JULIAN MICHAEL">
      <data key="d4">8.0</data>
      <data key="d5">Richard Yuanzhe Pang and Julian Michael co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="RICHARD YUANZHE PANG" target="SAMUEL R. BOWMAN">
      <data key="d4">8.0</data>
      <data key="d5">Richard Yuanzhe Pang and Samuel R. Bowman co-authored the paper "Gpqa: A graduate-level google-proof q&amp;a benchmark"</data>
      <data key="d6">34d0bb2211fc795fe1096442e086a2b3</data>
    </edge>
    <edge source="MIRAC SUZGUN" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Nathanael Sch&#228;rli co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="MIRAC SUZGUN" target="QUOC V LE">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Quoc V Le co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="MIRAC SUZGUN" target="ED H CHI">
      <data key="d4">8.0</data>
      <data key="d5">Mirac Suzgun and Ed H Chi co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="QUOC V LE" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Quoc V Le co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="QINGYUN WU" target="SHAOKUN ZHANG">
      <data key="d4">16.0</data>
      <data key="d5">Shaokun Zhang and Qingyun Wu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="QINGYUN WU" target="JIEYU ZHANG">
      <data key="d4">16.0</data>
      <data key="d5">Jieyu Zhang and Qingyun Wu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="QINGYUN WU" target="JIALE LIU">
      <data key="d4">16.0</data>
      <data key="d5">Jiale Liu and Qingyun Wu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="QINGYUN WU" target="LINXIN SONG">
      <data key="d4">16.0</data>
      <data key="d5">Linxin Song and Qingyun Wu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="QINGYUN WU" target="CHI WANG">
      <data key="d4">16.0</data>
      <data key="d5">Chi Wang and Qingyun Wu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="QINGYUN WU" target="RANJAY KRISHNA">
      <data key="d4">1.0</data>
      <data key="d5">Ranjay Krishna and Qingyun Wu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JIEYU ZHANG" target="SHAOKUN ZHANG">
      <data key="d4">16.0</data>
      <data key="d5">Shaokun Zhang and Jieyu Zhang co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JIEYU ZHANG" target="JIALE LIU">
      <data key="d4">16.0</data>
      <data key="d5">Jieyu Zhang and Jiale Liu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JIEYU ZHANG" target="LINXIN SONG">
      <data key="d4">16.0</data>
      <data key="d5">Jieyu Zhang and Linxin Song co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JIEYU ZHANG" target="CHI WANG">
      <data key="d4">16.0</data>
      <data key="d5">Jieyu Zhang and Chi Wang co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JIEYU ZHANG" target="RANJAY KRISHNA">
      <data key="d4">16.0</data>
      <data key="d5">Jieyu Zhang and Ranjay Krishna co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="SHAOKUN ZHANG" target="JIALE LIU">
      <data key="d4">16.0</data>
      <data key="d5">Shaokun Zhang and Jiale Liu co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="SHAOKUN ZHANG" target="LINXIN SONG">
      <data key="d4">16.0</data>
      <data key="d5">Shaokun Zhang and Linxin Song co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="SHAOKUN ZHANG" target="CHI WANG">
      <data key="d4">16.0</data>
      <data key="d5">Shaokun Zhang and Chi Wang co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="SHAOKUN ZHANG" target="RANJAY KRISHNA">
      <data key="d4">16.0</data>
      <data key="d5">Shaokun Zhang and Ranjay Krishna co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="CHI WANG" target="JIALE LIU">
      <data key="d4">16.0</data>
      <data key="d5">Jiale Liu and Chi Wang co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="CHI WANG" target="LINXIN SONG">
      <data key="d4">16.0</data>
      <data key="d5">Linxin Song and Chi Wang co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="CHI WANG" target="RANJAY KRISHNA">
      <data key="d4">16.0</data>
      <data key="d5">Chi Wang and Ranjay Krishna co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JIALE LIU" target="LINXIN SONG">
      <data key="d4">16.0</data>
      <data key="d5">Jiale Liu and Linxin Song co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="JIALE LIU" target="RANJAY KRISHNA">
      <data key="d4">16.0</data>
      <data key="d5">Jiale Liu and Ranjay Krishna co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="LINXIN SONG" target="RANJAY KRISHNA">
      <data key="d4">16.0</data>
      <data key="d5">Linxin Song and Ranjay Krishna co-authored the paper "Offline training of language model agents with functions as learnable weights"</data>
      <data key="d6">cc802d9b841fde55e9c0c2ba0ef7869d</data>
    </edge>
    <edge source="ED H CHI" target="NATHANAEL SCH&#196;RLI">
      <data key="d4">8.0</data>
      <data key="d5">Nathanael Sch&#228;rli and Ed H Chi co-authored the paper "Challenging big-bench tasks and whether chain-of-thought can solve them"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YARIN GAL" target="ILIA SHUMAILOV">
      <data key="d4">8.0</data>
      <data key="d5">Ilia Shumailov and Yarin Gal co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YARIN GAL" target="ZAKHAR SHUMAYLOV">
      <data key="d4">8.0</data>
      <data key="d5">Zakhar Shumaylov and Yarin Gal co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YARIN GAL" target="YIREN ZHAO">
      <data key="d4">8.0</data>
      <data key="d5">Yiren Zhao and Yarin Gal co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YARIN GAL" target="NICOLAS PAPERNOT">
      <data key="d4">8.0</data>
      <data key="d5">Yarin Gal and Nicolas Papernot co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YARIN GAL" target="ROSS ANDERSON">
      <data key="d4">8.0</data>
      <data key="d5">Yarin Gal and Ross Anderson co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="CODE" target="THOUGHTS">
      <data key="d4">14.0</data>
      <data key="d5">Thoughts include the generated code</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="CODE" target="THINKING">
      <data key="d4">8.0</data>
      <data key="d5">Thinking is applied to the code during evaluation and feedback generation</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="CODE" target="CORRECT_EXAMPLES">
      <data key="d4">8.0</data>
      <data key="d5">Correct examples are instances where the code produced the correct output</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="CODE" target="WRONG_EXAMPLES">
      <data key="d4">8.0</data>
      <data key="d5">Wrong examples are instances where the code produced incorrect output</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="CODE" target="INITIAL_SOLUTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Initial solutions are the first set of code solutions generated</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FRAMEWORK" target="NAMEDTUPLE INFO OBJECT">
      <data key="d4">7.0</data>
      <data key="d5">The framework uses the namedtuple Info object to encapsulate and combine different types of information.</data>
      <data key="d6">282313a8340c6792e8c35f53ed157cd0</data>
    </edge>
    <edge source="APPENDIX B" target="BENCHMARKS">
      <data key="d4">7.0</data>
      <data key="d5">Appendix B specifies the types of tasks/benchmarks and the corresponding methods used to extract answers and generate metrics</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="FM MODULE" target="INFO">
      <data key="d4">9.0</data>
      <data key="d5">The FM Module constructs prompts by concatenating input Info objects</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="FORMAT_INST">
      <data key="d4">8.0</data>
      <data key="d5">The FM Module uses FORMAT_INST to format instructions for FM responses</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="ROLE_DESC">
      <data key="d4">7.0</data>
      <data key="d5">The FM Module uses ROLE_DESC to describe its role</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="GET_JSON_RESPONSE_FROM_GPT">
      <data key="d4">8.0</data>
      <data key="d5">The FM Module uses the function GET_JSON_RESPONSE_FROM_GPT to get JSON responses from a GPT model</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="AGENT SYSTEM">
      <data key="d4">7.0</data>
      <data key="d5">The Agent System can use the FM Module to process task information</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="META-AGENT SEARCH">
      <data key="d4">8.0</data>
      <data key="d5">The FM Module is part of the Meta-Agent Search system</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="FM RESPONSES">
      <data key="d4">8.0</data>
      <data key="d5">The FM Module generates FM responses based on the constructed prompts</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="RESULTS FROM TOOL FUNCTION CALLS">
      <data key="d4">7.0</data>
      <data key="d5">The FM Module may use results from tool function calls in its operations</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="FM MODULE" target="TASK DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The FM Module uses task descriptions to facilitate communication between different modules</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="INFO" target="REFINEMENT_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">Info is used by the Refinement Module to provide structured feedback</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FM_MODULE" target="INITIAL INSTRUCTION">
      <data key="d4">14.0</data>
      <data key="d5">FM_Module uses the initial instruction to generate candidate solutions</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FM_MODULE" target="TASKINFO">
      <data key="d4">14.0</data>
      <data key="d5">FM_Module uses TaskInfo as input data to generate solutions</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FM_MODULE" target="INITIAL SOLUTIONS">
      <data key="d4">16.0</data>
      <data key="d5">FM_Module generates initial solutions based on the initial instruction and task information</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FM_MODULE" target="NUM_CANDIDATES">
      <data key="d4">12.0</data>
      <data key="d5">FM_Module generates a specified number of initial candidate solutions</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FM_MODULE" target="TEMPERATURE">
      <data key="d4">7.0</data>
      <data key="d5">FM_Module uses the temperature parameter to control the randomness of the generated solutions</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FM_MODULE" target="INITIAL CANDIDATE SOLUTIONS">
      <data key="d4">7.0</data>
      <data key="d5">FM_Module generates initial candidate solutions</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="FM_MODULE" target="HUMAN_LIKE_FEEDBACK_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">Human-like Feedback Module is an instance of FM_Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FM_MODULE" target="EXPERT_ADVISORS">
      <data key="d4">8.0</data>
      <data key="d5">Expert advisors are instances of FM_Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FM_MODULE" target="REFINEMENT_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">Refinement Module is an instance of FM_Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FM_MODULE" target="FINAL_DECISION_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">Final Decision Module is an instance of FM_Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FM_MODULE" target="DECOMPOSITION MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Decomposition Module is an instance of FM_Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="FM_MODULE" target="SPECIALIZED EXPERT">
      <data key="d4">8.0</data>
      <data key="d5">Specialized Experts are instances of FM_Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="FM_MODULE" target="INTEGRATION MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Integration Module is an instance of FM_Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="FM_MODULE" target="VISUAL REPRESENTATION MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Visual Representation Module is an instance of FM_Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="FM_MODULE" target="VERIFICATION MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Verification Module is an instance of FM_Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="FM_MODULE" target="CHAIN-OF-THOUGHT MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The Chain-of-Thought Module is an instance of FM_Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="META-AGENT SEARCH" target="CODE 1">
      <data key="d4">7.0</data>
      <data key="d5">Code 1 shows the simple framework used in Meta-Agent Search</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="META-AGENT SEARCH" target="CODE 2">
      <data key="d4">7.0</data>
      <data key="d5">Code 2 shows an example of implementing self-reflection using the framework</data>
      <data key="d6">d66dc9ce4a9545b44f7486ea057b5937</data>
    </edge>
    <edge source="COT_INITIAL_INSTRUCTION" target="COT_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The cot_module uses the cot_initial_instruction to start the task-solving process</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="COT_REFLECT_INSTRUCTION" target="COT_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The cot_module uses the cot_reflect_instruction to refine the answer</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="CRITIC_INSTRUCTION" target="CRITIC_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">The critic_module uses the critic_instruction to review and correct the answer</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="COT_MODULE" target="TASKINFO">
      <data key="d4">8.0</data>
      <data key="d5">The cot_module uses taskInfo as initial input</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="COT_MODULE" target="THINKING">
      <data key="d4">8.0</data>
      <data key="d5">The cot_module generates the thinking process</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="CRITIC_MODULE" target="TASKINFO">
      <data key="d4">8.0</data>
      <data key="d5">The critic_module uses taskInfo as input</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="CRITIC_MODULE" target="THINKING">
      <data key="d4">8.0</data>
      <data key="d5">The critic_module reviews the thinking process</data>
      <data key="d6">4b43decac6833d1515992f8869ecada7</data>
    </edge>
    <edge source="TASKINFO" target="INITIAL_INSTRUCTION">
      <data key="d4">9.0</data>
      <data key="d5">TaskInfo provides the initial instruction for the code evaluation process</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="TASKINFO" target="DECOMPOSITION MODULE">
      <data key="d4">8.0</data>
      <data key="d5">TaskInfo is the input for the Decomposition Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="GPT-4O-2024-05-13" target="META_AGENT">
      <data key="d4">9.0</data>
      <data key="d5">Meta agent uses the GPT-4o-2024-05-13 model</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="GPT-3.5-TURBO-0125" target="DISCOVERED_AGENTS">
      <data key="d4">9.0</data>
      <data key="d5">Discovered agents use the GPT-3.5-turbo-0125 model</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="GPT-3.5-TURBO-0125" target="OPENAI API">
      <data key="d4">8.0</data>
      <data key="d5">GPT-3.5-Turbo-0125 is queried using the OpenAI API</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="INITIAL SOLUTIONS" target="THOUGHTS">
      <data key="d4">14.0</data>
      <data key="d5">Initial solutions include thoughts, which consist of thinking and code</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="EXPERIMENT" target="REPOSITORY">
      <data key="d4">7.0</data>
      <data key="d5">The experiment results are stored in a repository on GitHub</data>
      <data key="d6">449db721e37968e073e3579b59e023b2</data>
    </edge>
    <edge source="HUMAN_LIKE_FEEDBACK_MODULE" target="HUMAN_FEEDBACK_INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">Human-like Feedback Module uses the human feedback instruction to generate feedback</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="HUMAN_LIKE_FEEDBACK_MODULE" target="HUMAN_FEEDBACK">
      <data key="d4">9.0</data>
      <data key="d5">Human feedback is generated by the Human-like Feedback Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="EXPERT_ROLES" target="EXPERT_ADVISORS">
      <data key="d4">8.0</data>
      <data key="d5">Expert advisors are assigned specific expert roles</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="EXPERT_ADVISORS" target="EXPERT_INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">Expert advisors use the expert instruction to evaluate code</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="EXPERT_ADVISORS" target="EXPERT_FEEDBACK">
      <data key="d4">9.0</data>
      <data key="d5">Expert feedback is provided by expert advisors</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="REFINEMENT_MODULE" target="REFINEMENT_INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">Refinement Module uses the refinement instruction to refine code solutions</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="REFINEMENT_MODULE" target="REFINEMENT_THINKING">
      <data key="d4">8.0</data>
      <data key="d5">Refinement thinking is applied by the Refinement Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="REFINEMENT_MODULE" target="REFINED_CODE">
      <data key="d4">9.0</data>
      <data key="d5">Refined code is generated by the Refinement Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="REFINED_CODE" target="REFINED_SOLUTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Refined solutions include the refined code</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="REFINED_SOLUTIONS" target="SORTED_SOLUTIONS">
      <data key="d4">8.0</data>
      <data key="d5">Sorted solutions are derived from refined solutions</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="SORTED_SOLUTIONS" target="TOP_SOLUTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Top solutions are selected from sorted solutions</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FINAL_DECISION_INSTRUCTION" target="FINAL_DECISION_MODULE">
      <data key="d4">8.0</data>
      <data key="d5">Final Decision Module uses the final decision instruction to make the final decision</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FINAL_DECISION_MODULE" target="FINAL_THOUGHTS">
      <data key="d4">8.0</data>
      <data key="d5">Final thoughts are generated by the Final Decision Module</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="FINAL_THOUGHTS" target="FINAL_CODE">
      <data key="d4">9.0</data>
      <data key="d5">Final code is derived from final thoughts</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="DISCOVERED AGENTS" target="AUTOMATED DESIGN OF AGENTIC SYSTEMS">
      <data key="d4">7.0</data>
      <data key="d5">Discovered agents are discussed in the Automated Design of Agentic Systems document</data>
      <data key="d6">84317ae35cc75d612287186d93461447</data>
    </edge>
    <edge source="BAHRAIN" target="INDIANS">
      <data key="d4">7.0</data>
      <data key="d5">Indians are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="BAHRAIN" target="BANGLADESHIS">
      <data key="d4">7.0</data>
      <data key="d5">Bangladeshis are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="BAHRAIN" target="PAKISTANIS">
      <data key="d4">7.0</data>
      <data key="d5">Pakistanis are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="BAHRAIN" target="FILIPINOS">
      <data key="d4">7.0</data>
      <data key="d5">Filipinos are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="BAHRAIN" target="INDONESIANS">
      <data key="d4">7.0</data>
      <data key="d5">Indonesians are one of the nationalities with a significant population living in Bahrain between 2005-2009</data>
      <data key="d6">10fda605f670bcfccfc13c2ca0dde959</data>
    </edge>
    <edge source="PROBLEM-SOLVING" target="EXPERIMENTS">
      <data key="d4">8.0</data>
      <data key="d5">Experiments were conducted on the Problem-Solving domain</data>
      <data key="d6">97457e990eb6e3c88c11c862f9e3265b</data>
    </edge>
    <edge source="DECOMPOSITION MODULE" target="SUB_PROBLEMS">
      <data key="d4">9.0</data>
      <data key="d5">The Decomposition Module generates sub-problems from the main task</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="DECOMPOSITION MODULE" target="DECOMPOSITION INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">The Decomposition Module uses the decomposition instruction to break down the main task</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="SPECIALIZED EXPERT" target="SUB_SOLUTIONS">
      <data key="d4">9.0</data>
      <data key="d5">Specialized Experts generate solutions to the sub-problems</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="SPECIALIZED EXPERT" target="SUB_PROBLEMS">
      <data key="d4">8.0</data>
      <data key="d5">Sub-problems are solved by Specialized Experts</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="SPECIALIZED EXPERT" target="SUB_PROBLEM INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">Specialized Experts use the sub-problem instruction to solve sub-problems</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="INTEGRATION MODULE" target="SUB_SOLUTIONS">
      <data key="d4">8.0</data>
      <data key="d5">Sub-solutions are integrated by the Integration Module</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="INTEGRATION MODULE" target="INTEGRATION INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">The Integration Module uses the integration instruction to combine sub-solutions</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="VISUAL REPRESENTATION MODULE" target="VISUAL INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">The Visual Representation Module uses the visual instruction to create visual aids</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="VERIFICATION MODULE" target="VERIFICATION INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">The Verification Module uses the verification instruction to verify visual aids</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="CHAIN-OF-THOUGHT MODULE" target="COT INSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">The Chain-of-Thought Module uses the CoT instruction to solve problems</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="GPT-4O-MINI" target="OPENAI API">
      <data key="d4">8.0</data>
      <data key="d5">GPT-4o-Mini is queried using the OpenAI API</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="OPENAI API" target="EXPERIMENT COST">
      <data key="d4">9.0</data>
      <data key="d5">The cost of experiments is primarily due to querying the OpenAI API</data>
      <data key="d6">ef75d2c866bee783577ed9f65707cf13</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="GENERATIVE TEACHING">
      <data key="d4">41.0</data>
      <data key="d5">AgentInstruct is used to facilitate Generative Teaching by creating synthetic data
AgentInstruct is an agentic solution for Generative Teaching
AgentInstruct is effective for Generative Teaching, enhancing model performance across various datasets</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="MISTRAL-7B">
      <data key="d4">24.0</data>
      <data key="d5">AgentInstruct was used to generate data for post-training Mistral-7b
AgentInstruct created a synthetic dataset used to fine-tune the Mistral-7B model</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="MULTI-AGENT WORKFLOWS">
      <data key="d4">8.0</data>
      <data key="d5">AgentInstruct can utilize multi-agent workflows to generate high-quality synthetic data</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="SYNTHETIC-DATA-GENERATION-AS-A-SERVICE">
      <data key="d4">16.0</data>
      <data key="d5">AgentInstruct can enable the creation of Synthetic-Data-Generation-As-A-Service</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="CONTENT TRANSFORMATION AGENTS">
      <data key="d4">16.0</data>
      <data key="d5">AgentInstruct uses Content Transformation Agents to transform raw seeds</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="REFINEMENT AGENTS">
      <data key="d4">16.0</data>
      <data key="d5">AgentInstruct uses Refinement Agents to refine seed instructions</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="RAW SEEDS">
      <data key="d4">16.0</data>
      <data key="d5">AgentInstruct uses raw seeds as input to generate diverse data</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="CREATIVE WRITING">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct generates data covering the skill of creative writing</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="DATA FILTERING">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct applies data filtering to ensure the quality of generated data</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="VERIFICATION">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct applies verification to ensure the quality of generated data</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="DEMONSTRATION DATA">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct creates demonstration data to teach AI models specific skills</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="FEEDBACK DATA">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct creates feedback data to teach AI models specific skills</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="POST-TRAINING">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct generates synthetic datasets for post-training AI models</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="CONTINUAL LEARNING">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct enables continual learning of AI models</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="SELF-IMPROVEMENT">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct enables self-improvement of AI models</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="WEB DATA">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct uses web data as raw material for generating synthetic datasets</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="DOMAIN SPECIFIC DATA">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct uses domain specific data as raw material for generating synthetic datasets</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="TEXTBOOK CHAPTERS">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct uses textbook chapters as raw seeds</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="WEB ARTICLES">
      <data key="d4">7.0</data>
      <data key="d5">AgentInstruct uses web articles as raw seeds</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="CODE SNIPPETS">
      <data key="d4">1.0</data>
      <data key="d5">AgentInstruct uses code snippets as raw seeds</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="CONTENT TRANSFORMATION FLOW">
      <data key="d4">9.0</data>
      <data key="d5">Content Transformation Flow is one of the three flows defined by AgentInstruct to automate the generation process.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="SEED INSTRUCTION GENERATION FLOW">
      <data key="d4">9.0</data>
      <data key="d5">Seed Instruction Generation Flow is one of the three flows defined by AgentInstruct to automate the generation process.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="INSTRUCTION REFINEMENT FLOW">
      <data key="d4">9.0</data>
      <data key="d5">Instruction Refinement Flow is one of the three flows defined by AgentInstruct to automate the generation process.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="ORCA-3">
      <data key="d4">9.0</data>
      <data key="d5">Orca-3 is trained using the AgentInstruct dataset</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="KNOWLEDGEPILE">
      <data key="d4">7.0</data>
      <data key="d5">KnowledgePile is one of the data sources used to create the AgentInstruct dataset</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="AUTOMATHTEXT">
      <data key="d4">7.0</data>
      <data key="d5">AutoMathText is one of the data sources used to create the AgentInstruct dataset</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="OPENSTAX">
      <data key="d4">7.0</data>
      <data key="d5">Openstax is one of the data sources used to create the AgentInstruct dataset</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="APACHE-2.0 LICENSED SOURCE CODE">
      <data key="d4">7.0</data>
      <data key="d5">Apache-2.0 licensed source code is one of the data sources used to create the AgentInstruct dataset</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="ORCA-BENCH">
      <data key="d4">8.0</data>
      <data key="d5">Orca-Bench dataset is created using data curated from AgentInstruct</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="ORCA-2.5">
      <data key="d4">8.0</data>
      <data key="d5">AgentInstruct data led to a performance augmentation of 33.94% over the Orca 2.5 baseline</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">8.0</data>
      <data key="d5">AgentInstruct data led to an enhancement of 14.92% over Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="MISTRAL">
      <data key="d4">16.0</data>
      <data key="d5">AgentInstruct is used to improve Mistral&#8217;s reading comprehension capabilities</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="MISTRAL-7B-INSTRUCT">
      <data key="d4">14.0</data>
      <data key="d5">AgentInstruct is used to enhance Mistral-7B-Instruct's proficiency in math</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="ORCA-3-7B">
      <data key="d4">15.0</data>
      <data key="d5">AgentInstruct approach is used to reduce hallucinations in Orca-3-7B
Orca-3-7B is fine-tuned with AgentInstruct data based on the Mistral model family</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="LIMITATIONS">
      <data key="d4">8.0</data>
      <data key="d5">The Limitations section discusses the limitations of AgentInstruct and synthetic data generation</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="VALIDATION">
      <data key="d4">7.0</data>
      <data key="d5">Validation is a limitation of AgentInstruct and synthetic data generation</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="DEPENDENCY ON SEED DATA">
      <data key="d4">7.0</data>
      <data key="d5">Dependency on Seed Data is a limitation of AgentInstruct and synthetic data generation</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="EXTENSIBILITY">
      <data key="d4">7.0</data>
      <data key="d5">Extensibility is a limitation of AgentInstruct and synthetic data generation</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="AGENTINSTRUCT" target="BIAS">
      <data key="d4">7.0</data>
      <data key="d5">Bias is a limitation of AgentInstruct and synthetic data generation</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="SYNTHETIC DATA" target="LLMS">
      <data key="d4">9.0</data>
      <data key="d5">Synthetic data has significantly accelerated the development of Large Language Models (LLMs)</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="SYNTHETIC DATA" target="SLMS">
      <data key="d4">8.0</data>
      <data key="d5">Synthetic data has significantly accelerated the development of Small Language Models (SLMs)</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="SYNTHETIC DATA" target="RLHF">
      <data key="d4">7.0</data>
      <data key="d5">Synthetic data is used in the process of Reinforcement Learning from Human Feedback (RLHF)</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MISTRAL-7B" target="ORCA-3">
      <data key="d4">27.0</data>
      <data key="d5">Orca-3 is the result of post-training Mistral-7b with data generated by AgentInstruct
Orca-3 is the fine-tuned version of the Mistral-7B model</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="ORCA-3" target="AGIEVAL">
      <data key="d4">36.0</data>
      <data key="d5">Orca-3 showed a 40% improvement on the AGIEval benchmark
Orca-3 shows 40% improvement on AGIEval benchmark
AGIEval is a benchmark used to evaluate the performance of Orca-3
Orca-3 is evaluated using the AGIEval benchmark</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="BBH">
      <data key="d4">28.0</data>
      <data key="d5">Orca-3 showed a 38% improvement on the BBH benchmark
Orca-3 shows 38% improvement on BBH benchmark
BBH is a benchmark used to evaluate the performance of Orca-3</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="ALPACAEVAL">
      <data key="d4">21.0</data>
      <data key="d5">Orca-3 showed a 45% improvement on the AlpacaEval benchmark
Orca-3 shows 45% improvement on AlpacaEval benchmark</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="ORCA-3" target="LLAMA-8B-INSTRUCT">
      <data key="d4">24.0</data>
      <data key="d5">Orca-3 consistently outperformed LLAMA-8B-instruct
Orca-3 outperforms LLAMA-8B-instruct on multiple benchmarks</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="ORCA-3" target="GPT-3.5-TURBO">
      <data key="d4">15.0</data>
      <data key="d5">Orca-3 consistently outperformed GPT-3.5-turbo
Orca-3 is evaluated against GPT-3.5-turbo on various benchmarks</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="MISTRAL-7B-INSTRUCT">
      <data key="d4">34.0</data>
      <data key="d5">Orca-3 showed significant improvements compared to Mistral-7b-Instruct
Orca-3 shows relative improvement over Mistral-7b-Instruct in various benchmarks
Orca-3 shows significant improvements over Mistral-7B-Instruct in various benchmarks</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,86f77e15d41cbd0cb33f635ccb2cb66b,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="ORCA-3" target="FOFO">
      <data key="d4">22.0</data>
      <data key="d5">Orca-3 showed improvements on the FOFO benchmark
FOFO is a benchmark used to evaluate the performance of Orca-3
Orca-3 shows significant improvements on the FoFo format-following benchmark</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c,bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="MIRAGE-RAG">
      <data key="d4">1.0</data>
      <data key="d5">Orca-3 showed improvements on the MIRAGE-RAG benchmark</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="ORCA-3" target="ORCA-2.5-DATASET">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3 is trained using the Orca-2.5-dataset</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-3" target="MISTRAL-7B-V0.1">
      <data key="d4">9.0</data>
      <data key="d5">Orca-3 is finetuned on Mistral-7b-v0.1</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-3" target="NVIDIA A100">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3 is trained using 152 NVIDIA A100 GPUs</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-3" target="ADAMW OPTIMIZER">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3 is trained using the AdamW optimizer</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-3" target="ORCA-2.5">
      <data key="d4">17.0</data>
      <data key="d5">Orca-3 shows notable enhancement in capabilities during post-training compared to Orca-2.5
Orca-3 shows significant improvements over Orca-2.5 in various benchmarks</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3 shows notable enhancement in capabilities during post-training compared to Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">Orca-3 is evaluated against LLAMA3-8B-Instruct on various benchmarks</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="ORCA-3 CHECKPOINT EPOCH 1">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3 checkpoint epoch 1 is a specific checkpoint of the Orca-3 model</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="ORCA-3 CHECKPOINT EPOCH 2">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3 checkpoint epoch 2 is a specific checkpoint of the Orca-3 model</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="TABLE 2">
      <data key="d4">7.0</data>
      <data key="d5">Table 2 encapsulates the average scores of Orca-3</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="FIGURE 4">
      <data key="d4">7.0</data>
      <data key="d5">Figure 4 illustrates the performance comparison of Orca-3</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="BENCHMARK RESULTS">
      <data key="d4">7.0</data>
      <data key="d5">Benchmark results section evaluates Orca-3</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="METRIC-V2">
      <data key="d4">14.0</data>
      <data key="d5">Metric-v2 is a benchmark used to evaluate the performance of Orca-3
Orca-3's performance was evaluated using Metric-v2</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="METRIC-V1">
      <data key="d4">14.0</data>
      <data key="d5">Metric-v1 is a benchmark used to evaluate the performance of Orca-3
Orca-3's performance was evaluated using Metric-v1</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-3" target="LSAT">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3's performance on the LSAT reading comprehension sections matches that of GPT-4</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="ORCA-3" target="GEMINI PRO">
      <data key="d4">7.0</data>
      <data key="d5">Orca-3 surpasses Gemini Pro in format-following capabilities</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="AGIEVAL" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">AGIEval is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="AGIEVAL" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">AGIEval is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="AGIEVAL" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">AGIEval is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="AGIEVAL" target="GPT-3.5-TURBO">
      <data key="d4">7.0</data>
      <data key="d5">AGIEval is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="AGIEVAL" target="SAT">
      <data key="d4">7.0</data>
      <data key="d5">AGIEval evaluates models on tasks pertinent to human cognition, including standardized exams like SAT</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="AGIEVAL" target="LSAT">
      <data key="d4">7.0</data>
      <data key="d5">AGIEval evaluates models on tasks pertinent to human cognition, including standardized exams like LSAT</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="AGIEVAL" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B is evaluated using the AGIEval benchmark</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="BBH" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">BBH is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="BBH" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">BBH is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="BBH" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">BBH is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="BBH" target="GPT-3.5-TURBO">
      <data key="d4">7.0</data>
      <data key="d5">BBH is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="BBH" target="BIG-BENCH">
      <data key="d4">16.0</data>
      <data key="d5">BBH consists of tasks selected from the broader Big-Bench benchmark</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="ALPACAEVAL" target="ALPACA WEB DEMO">
      <data key="d4">7.0</data>
      <data key="d5">AlpacaEval consists of instructions representative of user interactions on the Alpaca web demo</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="ALPACAEVAL" target="OPEN-ENDED GENERATION">
      <data key="d4">7.0</data>
      <data key="d5">AlpacaEval is a benchmark under the category of Open-Ended Generation tasks.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="FOFO">
      <data key="d4">7.0</data>
      <data key="d5">FOFO is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="BENCHMARK RESULTS">
      <data key="d4">7.0</data>
      <data key="d5">Benchmark results section evaluates GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="METRIC-V2">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v2 is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="METRIC-V1">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v1 is a benchmark used to evaluate the performance of GPT-3.5-turbo</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="PHI3">
      <data key="d4">2.0</data>
      <data key="d5">The accuracy scores for GPT-3.5-turbo on the GSM8K benchmark are reported in the Phi3 paper</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B's performance is compared to GPT-3.5-turbo in various benchmarks</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="MIRAGE">
      <data key="d4">8.0</data>
      <data key="d5">GPT-3.5-turbo is used in the evaluation of MIRAGE datasets</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="GPT-3.5-TURBO" target="AZURE">
      <data key="d4">2.0</data>
      <data key="d5">Azure is recommended for reviewing transparency notes related to GPT-3.5-turbo</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="AGENTIC WORKFLOWS" target="MULTI-AGENT WORKFLOWS">
      <data key="d4">8.0</data>
      <data key="d5">Multi-agent workflows are a type of agentic workflow that involve multiple agents working together</data>
      <data key="d6">6fe27f9eb76cf2ddf712a2cee5783d1c</data>
    </edge>
    <edge source="MISTRAL-7B-INSTRUCT" target="ORCA-3-7B">
      <data key="d4">17.0</data>
      <data key="d5">Orca-3-7B shows significant improvements over Mistral-7B-Instruct in various benchmarks
Orca-3-7B's performance is compared to Mistral-7B-Instruct in various benchmarks</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="FOFO" target="ORCA-2.5">
      <data key="d4">7.0</data>
      <data key="d5">FOFO is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="FOFO" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">7.0</data>
      <data key="d5">FOFO is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="FOFO" target="LLAMA3-8B-INSTRUCT">
      <data key="d4">7.0</data>
      <data key="d5">FOFO is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="FOFO" target="DOMAIN-SPECIFIC FORMATS">
      <data key="d4">8.0</data>
      <data key="d5">FoFo evaluates a model&#8217;s ability to follow complex, domain-specific formats</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="FOFO" target="AI-HUMAN COLLABORATION">
      <data key="d4">8.0</data>
      <data key="d5">FoFo benchmark tests format following on a diverse range of real-world formats and instructions created using AI-Human collaboration</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="FOFO" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B shows significant improvements on the FoFo format-following benchmark</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="FOFO" target="OPEN-ENDED GENERATION">
      <data key="d4">7.0</data>
      <data key="d5">FOFO is a benchmark under the category of Open-Ended Generation tasks.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION AGENTS" target="CONTENT TRANSFORMATION FLOW">
      <data key="d4">14.0</data>
      <data key="d5">Content Transformation Agents are used in the Content Transformation Flow</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="REFINEMENT AGENTS" target="REFINEMENT FLOW">
      <data key="d4">8.0</data>
      <data key="d5">Refinement Agents are used in the Refinement Flow</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="SEED INSTRUCTION CREATION FLOW">
      <data key="d4">14.0</data>
      <data key="d5">Content Transformation Flow is followed by Seed Instruction Creation Flow</data>
      <data key="d6">b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="AGENTIC FLOWS">
      <data key="d4">9.0</data>
      <data key="d5">Content Transformation Flow is a part of the agentic flows used to convert raw seeds into intermediate representations.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="SEED INSTRUCTION GENERATION FLOW">
      <data key="d4">8.0</data>
      <data key="d5">Seed Instruction Generation Flow takes transformed content from the Content Transformation Flow to generate diverse instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="INTERMEDIATE REPRESENTATION">
      <data key="d4">8.0</data>
      <data key="d5">Content Transformation Flow converts raw seeds into an intermediate representation to simplify the creation of instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="ARGUMENT PASSAGE GENERATOR">
      <data key="d4">8.0</data>
      <data key="d5">Argument Passage Generator is a tool within the Content Transformation Flow.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="LSAT LOGICAL REASONING TEST">
      <data key="d4">8.0</data>
      <data key="d5">The Content Transformation Flow aims to generate materials that support the creation of diverse question types required for comprehensive reading comprehension evaluation, including those found in the LSAT Logical Reasoning test.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="AGENTINSTRUCT FLOW">
      <data key="d4">6.0</data>
      <data key="d5">Both the AgentInstruct Flow and Content Transformation Flow involve the use of agents to perform specific tasks</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="API RETRIEVAL AGENT">
      <data key="d4">8.0</data>
      <data key="d5">The API Retrieval Agent is used in the Content Transformation Flow to expand the API list</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="CONTENT TRANSFORMATION FLOW" target="LIBRARY RECONSTRUCTION">
      <data key="d4">8.0</data>
      <data key="d5">Library Reconstruction is a scenario in the Content Transformation Flow</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SEED INSTRUCTION CREATION FLOW" target="REFINEMENT FLOW">
      <data key="d4">21.0</data>
      <data key="d5">Seed Instruction Creation Flow is followed by Refinement Flow
Both processes are involved in creating and refining tasks for the AI assistant to perform.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229,b88745a13b69cecbc0ee9c3af41389bf</data>
    </edge>
    <edge source="SEED INSTRUCTION CREATION FLOW" target="AGENT-INSTRUCT FLOW">
      <data key="d4">8.0</data>
      <data key="d5">The Agent-Instruct flow uses the tasks created by the Seed Instruction Creation Flow to generate multi-turn conversations.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="AGENTIC FLOWS" target="SEED INSTRUCTION GENERATION FLOW">
      <data key="d4">9.0</data>
      <data key="d5">Seed Instruction Generation Flow is a part of the agentic flows used to generate diverse instructions from transformed content.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENTIC FLOWS" target="INSTRUCTION REFINEMENT FLOW">
      <data key="d4">9.0</data>
      <data key="d5">Instruction Refinement Flow is a part of the agentic flows used to iteratively enhance the complexity and quality of instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENTIC FLOWS" target="RAW ARTICLES">
      <data key="d4">8.0</data>
      <data key="d5">Raw articles are used as seeds in agentic flows to foster diversity and ensure broad coverage of generated problems.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENTIC FLOWS" target="SKILLS">
      <data key="d4">1.0</data>
      <data key="d5">Agentic flows are implemented for 17 different skills, each having multiple subcategories.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="INSTRUCTION REFINEMENT FLOW">
      <data key="d4">8.0</data>
      <data key="d5">Instruction Refinement Flow takes instructions from the Seed Instruction Generation Flow and enhances their complexity and quality.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="TAXONOMY">
      <data key="d4">8.0</data>
      <data key="d5">Seed Instruction Generation Flow uses a pre-defined, but extensible, taxonomy to introduce diversity in the generated instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="PASSAGE-QUESTION PAIRS">
      <data key="d4">8.0</data>
      <data key="d5">The Seed Instruction Generation Flow produces passage-question pairs as its output.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="LITERAL COMPREHENSION QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">The Seed Instruction Generation Flow includes literal comprehension questions as one of the types of reading comprehension questions.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="CRITICAL COMPREHENSION QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">The Seed Instruction Generation Flow includes critical comprehension questions as one of the types of reading comprehension questions.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="EVALUATIVE COMPREHENSION QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">The Seed Instruction Generation Flow includes evaluative comprehension questions as one of the types of reading comprehension questions.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="REASONING QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">The Seed Instruction Generation Flow includes reasoning questions as one of the types of reading comprehension questions.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="IDENTIFYING ASSUMPTIONS QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">The Seed Instruction Generation Flow includes identifying assumptions questions as one of the types of reading comprehension questions.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="IDENTIFYING INFORMATION THAT STRENGTHENS/WEAKENS AN ARGUMENT QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">The Seed Instruction Generation Flow includes questions that identify information that strengthens or weakens an argument.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="ORDERING EVENTS QUESTIONS">
      <data key="d4">7.0</data>
      <data key="d5">The Seed Instruction Generation Flow includes ordering events questions as one of the types of reading comprehension questions.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="APPENDIX A">
      <data key="d4">7.0</data>
      <data key="d5">Appendix A lists the types of reading comprehension questions included in the Seed Instruction Generation Flow.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="SEED INSTRUCTION GENERATION FLOW" target="INSTRUCTION TAXONOMY">
      <data key="d4">8.0</data>
      <data key="d5">The Instruction Taxonomy is used in the Seed Instruction Generation Flow</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="INSTRUCTION REFINEMENT FLOW" target="QUALITY">
      <data key="d4">8.0</data>
      <data key="d5">Instruction Refinement Flow aims to boost the quality of the generated instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="INSTRUCTION REFINEMENT FLOW" target="SUGGESTER AGENT">
      <data key="d4">8.0</data>
      <data key="d5">The Instruction Refinement Flow involves the Suggester Agent to provide suggestions for modifying passage-question pairs.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="INSTRUCTION REFINEMENT FLOW" target="EDITOR AGENT">
      <data key="d4">8.0</data>
      <data key="d5">The Instruction Refinement Flow involves the Editor Agent to implement modifications suggested by the Suggester Agent.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="INSTRUCTION REFINEMENT FLOW" target="HYPOTHETICAL STUDY">
      <data key="d4">7.0</data>
      <data key="d5">The Instruction Refinement Flow suggests introducing a hypothetical study to strengthen an argument.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="INSTRUCTION REFINEMENT FLOW" target="GENETIC PREDISPOSITION">
      <data key="d4">7.0</data>
      <data key="d5">The Instruction Refinement Flow suggests adding complexity by considering genetic predisposition to hyperuricemia.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="INSTRUCTION REFINEMENT FLOW" target="DISTRACTOR OPTION">
      <data key="d4">7.0</data>
      <data key="d5">The Instruction Refinement Flow suggests including a distractor option to test the test-taker's ability to discern relevant from irrelevant information.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="INSTRUCTION REFINEMENT FLOW" target="SUGGESTER-EDITOR PAIR">
      <data key="d4">8.0</data>
      <data key="d5">The Instruction Refinement Flow involves a Suggester-Editor pair that increases the complexity of generated instructions</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SUGGESTER AGENT" target="EDITOR AGENT">
      <data key="d4">8.0</data>
      <data key="d5">Suggester agents propose approaches to increase the intricacy of instructions, which are then modified by Editor agents.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="SUGGESTER AGENT" target="SUGGESTIONS">
      <data key="d4">8.0</data>
      <data key="d5">Suggester agents propose various suggestions to increase the intricacy of the initial instructions.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="EDITOR AGENT" target="EDITING">
      <data key="d4">8.0</data>
      <data key="d5">Editor agents perform editing to modify instructions based on the suggestions from Suggester agents.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="TEXT MODIFICATION" target="OPEN DOMAIN QUESTION ANSWERING">
      <data key="d4">6.0</data>
      <data key="d5">Both Open Domain Question Answering and Text Modification are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="OPEN DOMAIN QUESTION ANSWERING" target="MULTIPLE-CHOICE QUESTIONS FLOWS">
      <data key="d4">14.0</data>
      <data key="d5">Both processes are used to generate math problems for evaluating AI models</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="BRAIN TEASER" target="ANALYTICAL REASONING">
      <data key="d4">6.0</data>
      <data key="d5">Both Brain Teaser and Analytical Reasoning are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="ANALYTICAL REASONING" target="FERMI PROBLEMS">
      <data key="d4">6.0</data>
      <data key="d5">Both Fermi Problems and Analytical Reasoning are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="MULTIPLE CHOICE QUESTIONS" target="EVALUATION DETAILS">
      <data key="d4">9.0</data>
      <data key="d5">Evaluation Details include the method used to extract answers and generate metrics for Multiple Choice Questions</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="MULTIPLE CHOICE QUESTIONS" target="OPEN-ENDED GENERATION SETTING">
      <data key="d4">8.0</data>
      <data key="d5">Multiple Choice Questions are evaluated in an open-ended generation setting</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="MULTIPLE CHOICE QUESTIONS" target="REGEX BASED EXTRACTION">
      <data key="d4">7.0</data>
      <data key="d5">Regex based extraction was previously used for extracting options selected by models in Multiple Choice Questions</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="MULTIPLE CHOICE QUESTIONS" target="SYSTEM MESSAGE">
      <data key="d4">8.0</data>
      <data key="d5">A system message is used to guide the GPT-4 model in extracting student responses for Multiple Choice Questions</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="DATA TO TEXT" target="TEXT EXTRACTION">
      <data key="d4">6.0</data>
      <data key="d5">Both Data to Text and Text Extraction are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="FERMI PROBLEMS" target="ENRICO FERMI">
      <data key="d4">18.0</data>
      <data key="d5">Fermi problems are named after physicist Enrico Fermi.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="CODING" target="TEXT EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">Both Coding and Text Extraction are skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="TEXT EXTRACTION" target="TEXT CLASSIFICATION">
      <data key="d4">7.0</data>
      <data key="d5">Both text extraction and text classification are processes used in handling and analyzing text documents.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="TEXT EXTRACTION" target="RETRIEVAL AUGMENTED GENERATION">
      <data key="d4">8.0</data>
      <data key="d5">Retrieval Augmented Generation involves retrieving relevant documents, which is a form of text extraction.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="TABLE 1" target="SKILLS">
      <data key="d4">7.0</data>
      <data key="d5">Table 1 provides a full list of the 17 different skills implemented in the agentic flows.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="CASE STUDIES" target="SKILLS">
      <data key="d4">7.0</data>
      <data key="d5">Case studies explain how the workflows work for generating data for specific skills.</data>
      <data key="d6">f7eb89a70f544664546a510e46d5febd</data>
    </edge>
    <edge source="AGENTINSTRUCT FLOW" target="TEXT MODIFICATION TASKS">
      <data key="d4">8.0</data>
      <data key="d5">The AgentInstruct Flow involves performing various text modification tasks.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="AGENTINSTRUCT FLOW" target="PARAPHRASING AGENT">
      <data key="d4">1.0</data>
      <data key="d5">The AgentInstruct Flow includes the Paraphrasing Agent to create paraphrased versions of text.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="AGENTINSTRUCT FLOW" target="PARAPHRASING">
      <data key="d4">7.0</data>
      <data key="d5">The AgentInstruct Flow includes paraphrasing as one of the text modification tasks.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="AGENTINSTRUCT FLOW" target="SIMPLIFICATION">
      <data key="d4">7.0</data>
      <data key="d5">The AgentInstruct Flow includes simplification as one of the text modification tasks.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="AGENTINSTRUCT FLOW" target="REDACTING">
      <data key="d4">7.0</data>
      <data key="d5">The AgentInstruct Flow includes redacting as one of the text modification tasks.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="AGENTINSTRUCT FLOW" target="STYLING">
      <data key="d4">7.0</data>
      <data key="d5">The AgentInstruct Flow includes styling as one of the text modification tasks.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="AGENTINSTRUCT FLOW" target="CODE SWITCHING">
      <data key="d4">1.0</data>
      <data key="d5">The AgentInstruct Flow includes code switching as one of the text modification tasks.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="URIC ACID" target="HYPERURICEMIA">
      <data key="d4">18.0</data>
      <data key="d5">Hyperuricemia is a condition caused by high levels of uric acid in the blood.
High levels of uric acid in the blood lead to the condition known as hyperuricemia.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="URIC ACID" target="HYPOURICEMIA">
      <data key="d4">10.0</data>
      <data key="d5">Hypouricemia is a condition caused by low levels of uric acid in the blood.
Low levels of uric acid in the blood lead to the condition known as hypouricemia.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="URIC ACID" target="CARDIOVASCULAR DISEASE">
      <data key="d4">17.0</data>
      <data key="d5">High levels of uric acid in the blood, known as hyperuricemia, may increase the risk of cardiovascular disease.
High levels of uric acid are associated with an increased risk of cardiovascular disease.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="URIC ACID" target="KIDNEY DISEASES">
      <data key="d4">8.0</data>
      <data key="d5">Kidney diseases can be caused by an imbalance of uric acid in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="LEUKEMIA">
      <data key="d4">8.0</data>
      <data key="d5">Leukemia can cause an imbalance of uric acid in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="OBESITY">
      <data key="d4">8.0</data>
      <data key="d5">Obesity can cause an imbalance of uric acid in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="ANEMIA">
      <data key="d4">8.0</data>
      <data key="d5">Anemia can cause an imbalance of uric acid in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="ALCOHOL">
      <data key="d4">8.0</data>
      <data key="d5">Alcohol consumption is a lifestyle factor that can contribute to high levels of uric acid in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="PROCESSED FOODS">
      <data key="d4">8.0</data>
      <data key="d5">Processed foods are a lifestyle factor that can contribute to high levels of uric acid in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="RED MEAT">
      <data key="d4">15.0</data>
      <data key="d5">Red meat is a dietary source of purines, which can contribute to high levels of uric acid in the body.
Red meat contains uric acid, which can influence its levels in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="URIC ACID" target="SEAFOOD">
      <data key="d4">15.0</data>
      <data key="d5">Seafood is a dietary source of purines, which can contribute to high levels of uric acid in the body.
Seafood contains uric acid, which can influence its levels in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e,1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="URIC ACID" target="LABORATORY BLOOD TESTS">
      <data key="d4">9.0</data>
      <data key="d5">Laboratory blood tests are used to diagnose conditions related to uric acid levels in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="URINE TESTS">
      <data key="d4">1.0</data>
      <data key="d5">Urine tests are used to diagnose conditions related to uric acid levels in the body.</data>
      <data key="d6">0c212c1467564ad33330b1f655a8e27e</data>
    </edge>
    <edge source="URIC ACID" target="ALCOHOL CONSUMPTION">
      <data key="d4">7.0</data>
      <data key="d5">Alcohol consumption can influence uric acid levels in the body.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="URIC ACID" target="PHYSICAL INACTIVITY">
      <data key="d4">7.0</data>
      <data key="d5">Physical inactivity can influence uric acid levels in the body.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="HYPERURICEMIA" target="CARDIOVASCULAR DISEASE">
      <data key="d4">8.0</data>
      <data key="d5">Hyperuricemia is associated with an increased risk of cardiovascular disease.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="HYPERURICEMIA" target="LABORATORY TESTS">
      <data key="d4">7.0</data>
      <data key="d5">Laboratory tests are required to diagnose hyperuricemia.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="HYPOURICEMIA" target="LABORATORY TESTS">
      <data key="d4">7.0</data>
      <data key="d5">Laboratory tests are required to diagnose hypouricemia.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="HYPOURICEMIA" target="KIDNEY ISSUES">
      <data key="d4">7.0</data>
      <data key="d5">Hypouricemia can indicate underlying kidney issues.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="HYPOURICEMIA" target="LIVER ISSUES">
      <data key="d4">7.0</data>
      <data key="d5">Hypouricemia can indicate underlying liver issues.</data>
      <data key="d6">1d8835c0ce90e56be22873bcf2740a5d</data>
    </edge>
    <edge source="PARAPHRASING AGENT" target="NATASCHA VAN DER ZWAN">
      <data key="d4">5.0</data>
      <data key="d5">The Paraphrasing Agent could be used to modify text related to Natascha van der Zwan's research on financialization</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="PARAPHRASING AGENT" target="SUGGESTER-EDITOR PAIR">
      <data key="d4">7.0</data>
      <data key="d5">The Suggester-Editor pair provides suggestions and edits to increase the complexity of instructions generated by the Paraphrasing Agent</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="PARAPHRASING AGENT" target="RANDOM SEED">
      <data key="d4">7.0</data>
      <data key="d5">The Paraphrasing Agent uses a random seed to create text modification tasks</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="NATASCHA VAN DER ZWAN" target="FINANCIALIZATION">
      <data key="d4">9.0</data>
      <data key="d5">Natascha van der Zwan identifies three distinct research streams that have approached financialization</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="FINANCIALIZATION" target="FINANCE">
      <data key="d4">8.0</data>
      <data key="d5">Financialization is a broad concept that encompasses the increasing social impact and interconnection of financial discourses, markets, actors, and institutions</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="AMERICAN ANTHROPOLOGICAL ASSOCIATION (AAA)" target="SEA 2017 ANNUAL MEETING">
      <data key="d4">8.0</data>
      <data key="d5">The American Anthropological Association hosts the SEA 2017 Annual Meeting</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SEA 2017 ANNUAL MEETING" target="UNIVERSITY OF IOWA">
      <data key="d4">8.0</data>
      <data key="d5">The SEA 2017 Annual Meeting was held at the University of Iowa</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SEA 2017 ANNUAL MEETING" target="APRIL 6-8, 2017">
      <data key="d4">9.0</data>
      <data key="d5">The SEA 2017 Annual Meeting took place on April 6-8, 2017</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SEA 2017 ANNUAL MEETING" target="DECEMBER 1, 2016">
      <data key="d4">9.0</data>
      <data key="d5">The abstract submission deadline for the SEA 2017 Annual Meeting was December 1, 2016</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SUGGESTER-EDITOR PAIR" target="SUGGESTION 1">
      <data key="d4">7.0</data>
      <data key="d5">Suggestion 1 is a suggestion provided by the Suggester-Editor pair</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SUGGESTER-EDITOR PAIR" target="SUGGESTION 2">
      <data key="d4">7.0</data>
      <data key="d5">Suggestion 2 is a suggestion provided by the Suggester-Editor pair</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SUGGESTER-EDITOR PAIR" target="SUGGESTION 3">
      <data key="d4">7.0</data>
      <data key="d5">Suggestion 3 is a suggestion provided by the Suggester-Editor pair</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SUGGESTER-EDITOR PAIR" target="MODIFIED INSTRUCTION 1">
      <data key="d4">7.0</data>
      <data key="d5">Modified Instruction 1 is an edit provided by the Suggester-Editor pair</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SUGGESTER-EDITOR PAIR" target="MODIFIED INSTRUCTION 2">
      <data key="d4">7.0</data>
      <data key="d5">Modified Instruction 2 is an edit provided by the Suggester-Editor pair</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SUGGESTER-EDITOR PAIR" target="MODIFIED INSTRUCTION 3">
      <data key="d4">7.0</data>
      <data key="d5">Modified Instruction 3 is an edit provided by the Suggester-Editor pair</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="VIEW ALL FOOD ITEMS" target="SEARCH FOOD ITEMS">
      <data key="d4">1.0</data>
      <data key="d5">Both "View All Food Items" and "Search Food Items" are APIs related to food item data</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="VIEW ALL FOOD ITEMS" target="FOOD ITEMS">
      <data key="d4">8.0</data>
      <data key="d5">The "View All Food Items" API provides a detailed list of food items</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SEARCH FOOD ITEMS" target="FOOD ITEMS">
      <data key="d4">1.0</data>
      <data key="d5">The "Search Food Items" API allows clients to search for food items</data>
      <data key="d6">427e98b00e49b6a8f8649054122dd45b</data>
    </edge>
    <edge source="SEARCH FOOD ITEMS" target="GET FOOD ITEM DETAILS">
      <data key="d4">7.0</data>
      <data key="d5">Both APIs are related to managing and retrieving information about food items.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="CREATE MEAL PLAN" target="TRACK USER MEAL">
      <data key="d4">8.0</data>
      <data key="d5">Both APIs are used in the process of managing a user's diet and meal tracking.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="CREATE MEAL PLAN" target="ASSISTANT">
      <data key="d4">9.0</data>
      <data key="d5">The assistant uses the Create Meal Plan API to generate a meal plan for the user.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="UPDATE FOOD ITEM" target="ADD NEW FOOD ITEM">
      <data key="d4">8.0</data>
      <data key="d5">Both APIs are used for managing the food database, either by adding new items or updating existing ones.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="UPDATE FOOD ITEM" target="DELETE FOOD ITEM">
      <data key="d4">8.0</data>
      <data key="d5">Both APIs are used for managing the food database, either by deleting or updating existing items.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="UPDATE FOOD ITEM" target="ASSISTANT">
      <data key="d4">9.0</data>
      <data key="d5">The assistant uses the Update Food Item API to update the nutritional information of an existing food item.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="UPDATE FOOD ITEM" target="CHANA MASALA">
      <data key="d4">9.0</data>
      <data key="d5">The user wants to update the nutritional information for Chana Masala using the Update Food Item API.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="TRACK USER MEAL" target="GET USER NUTRITIONAL STATS">
      <data key="d4">8.0</data>
      <data key="d5">Both APIs are used for tracking and analyzing a user's nutritional intake.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="TRACK USER MEAL" target="ASSISTANT">
      <data key="d4">9.0</data>
      <data key="d5">The assistant uses the Track User Meal API to help the user track their daily meals.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="GET DIETARY RECOMMENDATIONS" target="ASSISTANT">
      <data key="d4">9.0</data>
      <data key="d5">The assistant uses the Get Dietary Recommendations API to provide new food recommendations to the user.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="ADD NEW FOOD ITEM" target="ASSISTANT">
      <data key="d4">9.0</data>
      <data key="d5">The assistant uses the Add New Food Item API to add a new recipe to the database.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="ADD NEW FOOD ITEM" target="QUINOA SALAD">
      <data key="d4">9.0</data>
      <data key="d5">The user wants to add the Quinoa Salad recipe to the database using the Add New Food Item API.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="DELETE FOOD ITEM" target="ASSISTANT">
      <data key="d4">1.0</data>
      <data key="d5">The assistant uses the Delete Food Item API to remove a food item from the database.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="DELETE FOOD ITEM" target="BUTTER CHICKEN">
      <data key="d4">9.0</data>
      <data key="d5">The user wants to remove Butter Chicken from the database using the Delete Food Item API.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="GET USER NUTRITIONAL STATS" target="ASSISTANT">
      <data key="d4">9.0</data>
      <data key="d5">The assistant uses the Get User Nutritional Stats API to generate a nutritional summary for the user.</data>
      <data key="d6">0922646b93a124514ce2a267d961d229</data>
    </edge>
    <edge source="USER" target="ASSISTANT">
      <data key="d4">17.0</data>
      <data key="d5">The user interacts with the assistant to achieve their dietary and nutritional goals.
User and assistant are participants in the multi-turn interaction in Orca-Bench</data>
      <data key="d6">0922646b93a124514ce2a267d961d229,bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="USER" target="QUINOA SALAD">
      <data key="d4">16.0</data>
      <data key="d5">The user wants to add the Quinoa Salad recipe to the database.
The user requests the assistant to add the Quinoa Salad recipe to the database</data>
      <data key="d6">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="USER" target="CHANA MASALA">
      <data key="d4">16.0</data>
      <data key="d5">The user wants to update the calorie count for Chana Masala.
The user requests the assistant to update the Chana Masala recipe in the database</data>
      <data key="d6">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="USER" target="BUTTER CHICKEN">
      <data key="d4">9.0</data>
      <data key="d5">The user wants to remove Butter Chicken from the database.
The user requests the assistant to remove the Butter Chicken recipe from the database</data>
      <data key="d6">0922646b93a124514ce2a267d961d229,09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="USER" target="SYSTEM MESSAGE">
      <data key="d4">7.0</data>
      <data key="d5">System message is part of the multi-turn interaction involving the user in Orca-Bench</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ASSISTANT" target="VEGETARIAN MEAL PLAN">
      <data key="d4">9.0</data>
      <data key="d5">The assistant creates and provides an overview of the vegetarian meal plan</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ASSISTANT" target="SYSTEM MESSAGE">
      <data key="d4">7.0</data>
      <data key="d5">System message is part of the multi-turn interaction involving the assistant in Orca-Bench</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-2.5-DATASET" target="ORCA-1">
      <data key="d4">14.0</data>
      <data key="d5">Orca-2.5-dataset includes instructions sourced from Orca-1</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-2.5-DATASET" target="ORCA-2">
      <data key="d4">14.0</data>
      <data key="d5">Orca-2.5-dataset includes instructions sourced from Orca-2</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-2.5-DATASET" target="ORCA-MATH">
      <data key="d4">14.0</data>
      <data key="d5">Orca-2.5-dataset includes instructions sourced from Orca-Math</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-BENCH" target="OPEN DOMAIN QUESTION ANSWERING (ODQA)">
      <data key="d4">8.0</data>
      <data key="d5">Orca-Bench includes a test set for Open Domain Question Answering (ODQA)</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-BENCH" target="COMPLEX ODQA">
      <data key="d4">1.0</data>
      <data key="d5">Orca-Bench includes a test set for Complex ODQA</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ALMOND MILK" target="DAY 1">
      <data key="d4">8.0</data>
      <data key="d5">Almond milk is included in the breakfast for Day 1</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="CHICKPEA SALAD" target="DAY 1">
      <data key="d4">8.0</data>
      <data key="d5">Chickpea salad is included in the lunch for Day 1</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="WHOLE WHEAT BREAD" target="DAY 1">
      <data key="d4">8.0</data>
      <data key="d5">Whole wheat bread is included in the lunch for Day 1</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="MIXED VEGETABLE STIR FRY" target="DAY 1">
      <data key="d4">8.0</data>
      <data key="d5">Mixed vegetable stir fry is included in the dinner for Day 1</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="BROWN RICE" target="DAY 1">
      <data key="d4">8.0</data>
      <data key="d5">Brown rice is included in the dinner for Day 1</data>
      <data key="d6">09cb89de3b77d765983cff25b7d74a1a</data>
    </edge>
    <edge source="ORCA-2.5" target="MISTRAL-INSTRUCT-7B">
      <data key="d4">6.0</data>
      <data key="d5">Orca-2.5 and Mistral-Instruct-7B are both baseline models evaluated on the Orca-Bench dataset</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-2.5" target="TABLE 2">
      <data key="d4">7.0</data>
      <data key="d5">Table 2 encapsulates the average scores of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-2.5" target="FIGURE 4">
      <data key="d4">7.0</data>
      <data key="d5">Figure 4 illustrates the performance comparison of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-2.5" target="BENCHMARK RESULTS">
      <data key="d4">7.0</data>
      <data key="d5">Benchmark results section evaluates Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-2.5" target="METRIC-V2">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v2 is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-2.5" target="METRIC-V1">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v1 is a benchmark used to evaluate the performance of Orca-2.5</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="ORCA-2.5" target="MISTRAL">
      <data key="d4">8.0</data>
      <data key="d5">Mistral showed an 18% improvement over Orca-2.5 in reading comprehension capabilities</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="MISTRAL-INSTRUCT-7B" target="TABLE 2">
      <data key="d4">7.0</data>
      <data key="d5">Table 2 encapsulates the average scores of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MISTRAL-INSTRUCT-7B" target="FIGURE 4">
      <data key="d4">7.0</data>
      <data key="d5">Figure 4 illustrates the performance comparison of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MISTRAL-INSTRUCT-7B" target="BENCHMARK RESULTS">
      <data key="d4">7.0</data>
      <data key="d5">Benchmark results section evaluates Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MISTRAL-INSTRUCT-7B" target="METRIC-V2">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v2 is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="MISTRAL-INSTRUCT-7B" target="METRIC-V1">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v1 is a benchmark used to evaluate the performance of Mistral-Instruct-7B</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="LLAMA3-8B-INSTRUCT" target="BENCHMARK RESULTS">
      <data key="d4">7.0</data>
      <data key="d5">Benchmark results section evaluates LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="LLAMA3-8B-INSTRUCT" target="METRIC-V2">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v2 is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="LLAMA3-8B-INSTRUCT" target="METRIC-V1">
      <data key="d4">7.0</data>
      <data key="d5">Metric-v1 is a benchmark used to evaluate the performance of LLAMA3-8B-Instruct</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="LLAMA3-8B-INSTRUCT" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B's performance is compared to LLAMA3-8B-Instruct in various benchmarks</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="IFEVAL" target="INSTRUCTION-FOLLOWING">
      <data key="d4">8.0</data>
      <data key="d5">IFEval measures a model&#8217;s ability to follow natural language instructions</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="IFEVAL" target="OPEN-ENDED GENERATION">
      <data key="d4">7.0</data>
      <data key="d5">IFEval is a benchmark under the category of Open-Ended Generation tasks.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="INFOBENCH" target="DRFR">
      <data key="d4">8.0</data>
      <data key="d5">InFoBench uses the Decomposed Requirements Following Ratio (DRFR) metric</data>
      <data key="d6">86f77e15d41cbd0cb33f635ccb2cb66b</data>
    </edge>
    <edge source="INFOBENCH" target="OPEN-ENDED GENERATION">
      <data key="d4">7.0</data>
      <data key="d5">InfoBench is a benchmark under the category of Open-Ended Generation tasks.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="EQBENCH" target="EQBENCH GPT-4 EXTRACTION SYSTEM MESSAGE">
      <data key="d4">1.0</data>
      <data key="d5">The EQBench GPT-4 Extraction System Message is used for extracting emotion scores from evaluated model responses in EQBench tasks</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="EQBENCH" target="EMOTION SCORES">
      <data key="d4">8.0</data>
      <data key="d5">EQBench tasks involve generating and evaluating emotion scores</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="EQBENCH" target="CRITIQUE">
      <data key="d4">1.0</data>
      <data key="d5">Critique is part of the process in EQBench tasks to revise emotion scores</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="TEACHER" target="STUDENT">
      <data key="d4">9.0</data>
      <data key="d5">Teacher (GPT-4) and student (model being evaluated) are roles in the multi-turn interaction in Orca-Bench</data>
      <data key="d6">bd4eb9459bc29b4c2da4658914fd4635</data>
    </edge>
    <edge source="LSAT" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B's performance on the LSAT reading comprehension sections matches that of GPT-4</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="MISTRAL" target="ORCA-3-7B">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B is fine-tuned with AgentInstruct data based on the Mistral model family</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="GEMINI PRO" target="ORCA-3-7B">
      <data key="d4">7.0</data>
      <data key="d5">Orca-3-7B surpasses Gemini Pro in format-following capabilities</data>
      <data key="d6">bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="ORCA-3-7B" target="ORCA-2.5-7B">
      <data key="d4">17.0</data>
      <data key="d5">Orca-3-7B shows significant improvements over Orca-2.5-7B in various benchmarks
Orca-3-7B's performance is compared to Orca-2.5-7B in various benchmarks</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b,bb87f82e6a9f1d4da6480ec78a0e3701</data>
    </edge>
    <edge source="ORCA-3-7B" target="FOFO BENCHMARK">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B is evaluated on the FoFo benchmark</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="ORCA-3-7B" target="ACI-BENCH">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B is evaluated on the ACI-Bench for summarization abilities</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="ORCA-3-7B" target="INSTRUSUM">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B is evaluated on the InstruSum for summarization abilities</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="ORCA-3-7B" target="ORCA-SUM">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B is evaluated on the Orca-Sum for summarization abilities</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="ORCA-3-7B" target="MIRAGE">
      <data key="d4">8.0</data>
      <data key="d5">Orca-3-7B is used in the evaluation of MIRAGE datasets</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="ORCA-3-7B" target="AGENTINSTRUCT RAG FLOW">
      <data key="d4">9.0</data>
      <data key="d5">Orca-3-7B shows substantial improvement due to training with AgentInstruct RAG flow</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="ORCA-3-7B" target="DATA BIASES">
      <data key="d4">7.0</data>
      <data key="d5">Orca-3-7B retains many limitations, including data biases</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="ORCA-3-7B" target="LACK OF TRANSPARENCY">
      <data key="d4">7.0</data>
      <data key="d5">Orca-3-7B retains many limitations, including lack of transparency</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="ORCA-3-7B" target="CONTENT HARMS">
      <data key="d4">7.0</data>
      <data key="d5">Orca-3-7B retains many limitations, including content harms</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="ORCA-2.5-7B" target="MIRAGE">
      <data key="d4">8.0</data>
      <data key="d5">Orca-2.5-7B is used in the evaluation of MIRAGE datasets</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="ORCA-SUM" target="HUGGING FACE">
      <data key="d4">7.0</data>
      <data key="d5">Orca-Sum benchmark uses datasets collected from Hugging Face</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="MIRAGE" target="MMLU-MED">
      <data key="d4">7.0</data>
      <data key="d5">MMLU-Med is a dataset used in the MIRAGE benchmark</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="MIRAGE" target="MEDQA-US">
      <data key="d4">7.0</data>
      <data key="d5">MedQA-US is a dataset used in the MIRAGE benchmark</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="MIRAGE" target="MEDMCQA">
      <data key="d4">7.0</data>
      <data key="d5">MedMCQA is a dataset used in the MIRAGE benchmark</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b</data>
    </edge>
    <edge source="MIRAGE" target="PUBMEDQA">
      <data key="d4">16.0</data>
      <data key="d5">PubMedQA is a dataset used in the MIRAGE benchmark
PubMedQA is one of the datasets included in the MIRAGE collection and is considered an effective testbed for assessing models' ability to do RAG</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="MIRAGE" target="BIOASQ">
      <data key="d4">9.0</data>
      <data key="d5">BioASQ is a dataset used in the MIRAGE benchmark
BioASQ is one of the datasets included in the MIRAGE collection</data>
      <data key="d6">8ee9617c145e19fa95f1f9349bfbe69b,ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="MIRAGE" target="MEDMEDQA">
      <data key="d4">8.0</data>
      <data key="d5">MedMedQA is one of the datasets included in the MIRAGE collection</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="MIRAGE" target="USMEDMCQA">
      <data key="d4">8.0</data>
      <data key="d5">USMedMCQA is one of the datasets included in the MIRAGE collection</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="MIRAGE" target="MISTRAL-7B-INSTRUCT-V0.1">
      <data key="d4">8.0</data>
      <data key="d5">Mistral-7B-Instruct-v0.1 is used in the evaluation of MIRAGE datasets</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="MIRAGE" target="MEDRAG">
      <data key="d4">8.0</data>
      <data key="d5">MedRAG is the retrieval mechanism used across all models on MIRAGE</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="MIRAGE" target="TABLE 8">
      <data key="d4">8.0</data>
      <data key="d5">Table 8 shows the evaluation results of RAG skill on MIRAGE datasets</data>
      <data key="d6">ab04427ae0415a1c812a35cf8d3ee1a2</data>
    </edge>
    <edge source="AZURE" target="TRANSPARENCY NOTES">
      <data key="d4">8.0</data>
      <data key="d5">Azure provides transparency notes to explain the rationale behind specific outputs or decisions</data>
      <data key="d6">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </edge>
    <edge source="CONTENT HARMS" target="CONTENT MODERATION SERVICES">
      <data key="d4">9.0</data>
      <data key="d5">Content moderation services are recommended to prevent content harms caused by large language models</data>
      <data key="d6">dd9a46950237e49ef9b1c7ef08e08d42</data>
    </edge>
    <edge source="PHILIPP WITTE" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Philipp Witte co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="HAIPING WU" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Haiping Wu co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="MICHAEL WYATT" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Michael Wyatt co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="BIN XIAO" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Bin Xiao co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="CAN XU" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Can Xu co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="JIAHANG XU" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Jiahang Xu co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="WEIJIAN XU" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Weijian Xu co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="SONALI YADAV" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Sonali Yadav co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="FAN YANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Fan Yang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="JIANWEI YANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Jianwei Yang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="ZIYI YANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Ziyi Yang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="YIFAN YANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Yifan Yang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="DONGHAN YU" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Donghan Yu co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="LU YUAN" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Lu Yuan co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="CHENGRUIDONG ZHANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Chengruidong Zhang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="CYRIL ZHANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Cyril Zhang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="JIANWEN ZHANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Jianwen Zhang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="LI LYNA ZHANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Li Lyna Zhang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="YI ZHANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Yi Zhang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="YUE ZHANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Yue Zhang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="YUNAN ZHANG" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Yunan Zhang co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="XIREN ZHOU" target="WANG">
      <data key="d4">8.0</data>
      <data key="d5">Wang and Xiren Zhou co-authored the paper "Phi-3 technical report: A highly capable language model locally on your phone"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="ISAAC COWHEY" target="OREN ETZIONI">
      <data key="d4">8.0</data>
      <data key="d5">Isaac Cowhey and Oren Etzioni co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="ISAAC COWHEY" target="TUSHAR KHOT">
      <data key="d4">8.0</data>
      <data key="d5">Isaac Cowhey and Tushar Khot co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="ISAAC COWHEY" target="ASHISH SABHARWAL">
      <data key="d4">8.0</data>
      <data key="d5">Isaac Cowhey and Ashish Sabharwal co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="ISAAC COWHEY" target="CARISSA SCHOENICK">
      <data key="d4">8.0</data>
      <data key="d5">Isaac Cowhey and Carissa Schoenick co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="ISAAC COWHEY" target="OYVIND TAFJORD">
      <data key="d4">8.0</data>
      <data key="d5">Isaac Cowhey and Oyvind Tafjord co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="OREN ETZIONI" target="TUSHAR KHOT">
      <data key="d4">8.0</data>
      <data key="d5">Oren Etzioni and Tushar Khot co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="OREN ETZIONI" target="ASHISH SABHARWAL">
      <data key="d4">8.0</data>
      <data key="d5">Oren Etzioni and Ashish Sabharwal co-authored the paper "Think you have solved question answering? try arc, the ai2 reasoning challenge"</data>
      <data key="d6">cc20c99cad8edecc66b82ac751ff7172</data>
    </edge>
    <edge source="BERNARD GHANEM" target="RII KHIZBULLIN">
      <data key="d4">8.0</data>
      <data key="d5">Rii Khizbullin and Bernard Ghanem co-authored the paper "Camel: Communicative agents for 'mind' exploration of large language model society"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="XUECHEN LI" target="TIANYI ZHANG">
      <data key="d4">8.0</data>
      <data key="d5">Xuechen Li and Tianyi Zhang co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="XUECHEN LI" target="YANN DUBOIS">
      <data key="d4">8.0</data>
      <data key="d5">Xuechen Li and Yann Dubois co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="XUECHEN LI" target="ROHAN TAORI">
      <data key="d4">8.0</data>
      <data key="d5">Xuechen Li and Rohan Taori co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="XUECHEN LI" target="ISHAAN GULRAJANI">
      <data key="d4">8.0</data>
      <data key="d5">Xuechen Li and Ishaan Gulrajani co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="XUECHEN LI" target="CARLOS GUESTRIN">
      <data key="d4">8.0</data>
      <data key="d5">Xuechen Li and Carlos Guestrin co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="XUECHEN LI" target="TATSUNORI B. HASHIMOTO">
      <data key="d4">8.0</data>
      <data key="d5">Xuechen Li and Tatsunori B. Hashimoto co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="TIANYI ZHANG" target="YANN DUBOIS">
      <data key="d4">8.0</data>
      <data key="d5">Tianyi Zhang and Yann Dubois co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="TIANYI ZHANG" target="ROHAN TAORI">
      <data key="d4">8.0</data>
      <data key="d5">Tianyi Zhang and Rohan Taori co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="TIANYI ZHANG" target="ISHAAN GULRAJANI">
      <data key="d4">8.0</data>
      <data key="d5">Tianyi Zhang and Ishaan Gulrajani co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="TIANYI ZHANG" target="CARLOS GUESTRIN">
      <data key="d4">8.0</data>
      <data key="d5">Tianyi Zhang and Carlos Guestrin co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="TIANYI ZHANG" target="TATSUNORI B. HASHIMOTO">
      <data key="d4">8.0</data>
      <data key="d5">Tianyi Zhang and Tatsunori B. Hashimoto co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YANN DUBOIS" target="ROHAN TAORI">
      <data key="d4">8.0</data>
      <data key="d5">Yann Dubois and Rohan Taori co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YANN DUBOIS" target="ISHAAN GULRAJANI">
      <data key="d4">8.0</data>
      <data key="d5">Yann Dubois and Ishaan Gulrajani co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YANN DUBOIS" target="CARLOS GUESTRIN">
      <data key="d4">8.0</data>
      <data key="d5">Yann Dubois and Carlos Guestrin co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YANN DUBOIS" target="TATSUNORI B. HASHIMOTO">
      <data key="d4">8.0</data>
      <data key="d5">Yann Dubois and Tatsunori B. Hashimoto co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ROHAN TAORI" target="ISHAAN GULRAJANI">
      <data key="d4">8.0</data>
      <data key="d5">Rohan Taori and Ishaan Gulrajani co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ROHAN TAORI" target="CARLOS GUESTRIN">
      <data key="d4">8.0</data>
      <data key="d5">Rohan Taori and Carlos Guestrin co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ROHAN TAORI" target="TATSUNORI B. HASHIMOTO">
      <data key="d4">8.0</data>
      <data key="d5">Rohan Taori and Tatsunori B. Hashimoto co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ISHAAN GULRAJANI" target="CARLOS GUESTRIN">
      <data key="d4">8.0</data>
      <data key="d5">Ishaan Gulrajani and Carlos Guestrin co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ISHAAN GULRAJANI" target="TATSUNORI B. HASHIMOTO">
      <data key="d4">8.0</data>
      <data key="d5">Ishaan Gulrajani and Tatsunori B. Hashimoto co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="CARLOS GUESTRIN" target="TATSUNORI B. HASHIMOTO">
      <data key="d4">8.0</data>
      <data key="d5">Carlos Guestrin and Tatsunori B. Hashimoto co-authored the paper "Alpacaeval: An automatic evaluator of instruction-following models"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="ALEXANDER R. FABBRI">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Alexander R. Fabbri co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="JIAWEN CHEN">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Jiawen Chen co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="YILUN ZHAO">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Yilun Zhao co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="SIMENG HAN">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Simeng Han co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="SHAFIQ JOTY">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Shafiq Joty co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="DRAGOMIR RADEV">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Dragomir Radev co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="CHIEN-SHENG WU">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Chien-Sheng Wu co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="YIXIN LIU" target="ARMAN COHAN">
      <data key="d4">8.0</data>
      <data key="d5">Yixin Liu and Arman Cohan co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ALEXANDER R. FABBRI" target="JIAWEN CHEN">
      <data key="d4">8.0</data>
      <data key="d5">Alexander R. Fabbri and Jiawen Chen co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ALEXANDER R. FABBRI" target="YILUN ZHAO">
      <data key="d4">8.0</data>
      <data key="d5">Alexander R. Fabbri and Yilun Zhao co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ALEXANDER R. FABBRI" target="SIMENG HAN">
      <data key="d4">8.0</data>
      <data key="d5">Alexander R. Fabbri and Simeng Han co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="ALEXANDER R. FABBRI" target="SHAFIQ JOTY">
      <data key="d4">8.0</data>
      <data key="d5">Alexander R. Fabbri and Shafiq Joty co-authored the paper "Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization"</data>
      <data key="d6">3d1f6634f93f8a4c296dc8df7e59859e</data>
    </edge>
    <edge source="MOHAMMED LATIF SIDDIQ" target="JIAHAO ZHANG">
      <data key="d4">8.0</data>
      <data key="d5">Mohammed Latif Siddiq and Jiahao Zhang co-authored the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="MOHAMMED LATIF SIDDIQ" target="LINDSAY RONEY">
      <data key="d4">8.0</data>
      <data key="d5">Mohammed Latif Siddiq and Lindsay Roney co-authored the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="MOHAMMED LATIF SIDDIQ" target="JOANNA C. S. SANTOS">
      <data key="d4">8.0</data>
      <data key="d5">Mohammed Latif Siddiq and Joanna C. S. Santos co-authored the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="JIAHAO ZHANG" target="LINDSAY RONEY">
      <data key="d4">8.0</data>
      <data key="d5">Jiahao Zhang and Lindsay Roney co-authored the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="JIAHAO ZHANG" target="JOANNA C. S. SANTOS">
      <data key="d4">8.0</data>
      <data key="d5">Jiahao Zhang and Joanna C. S. Santos co-authored the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="LINDSAY RONEY" target="JOANNA C. S. SANTOS">
      <data key="d4">8.0</data>
      <data key="d5">Lindsay Roney and Joanna C. S. Santos co-authored the paper "Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="ILIA SHUMAILOV" target="ZAKHAR SHUMAYLOV">
      <data key="d4">8.0</data>
      <data key="d5">Ilia Shumailov and Zakhar Shumaylov co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="ILIA SHUMAILOV" target="YIREN ZHAO">
      <data key="d4">8.0</data>
      <data key="d5">Ilia Shumailov and Yiren Zhao co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="ILIA SHUMAILOV" target="NICOLAS PAPERNOT">
      <data key="d4">8.0</data>
      <data key="d5">Ilia Shumailov and Nicolas Papernot co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="ILIA SHUMAILOV" target="ROSS ANDERSON">
      <data key="d4">8.0</data>
      <data key="d5">Ilia Shumailov and Ross Anderson co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="ZAKHAR SHUMAYLOV" target="YIREN ZHAO">
      <data key="d4">8.0</data>
      <data key="d5">Zakhar Shumaylov and Yiren Zhao co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="ZAKHAR SHUMAYLOV" target="NICOLAS PAPERNOT">
      <data key="d4">8.0</data>
      <data key="d5">Zakhar Shumaylov and Nicolas Papernot co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="ZAKHAR SHUMAYLOV" target="ROSS ANDERSON">
      <data key="d4">8.0</data>
      <data key="d5">Zakhar Shumaylov and Ross Anderson co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YIREN ZHAO" target="NICOLAS PAPERNOT">
      <data key="d4">8.0</data>
      <data key="d5">Yiren Zhao and Nicolas Papernot co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="YIREN ZHAO" target="ROSS ANDERSON">
      <data key="d4">8.0</data>
      <data key="d5">Yiren Zhao and Ross Anderson co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="NICOLAS PAPERNOT" target="ROSS ANDERSON">
      <data key="d4">8.0</data>
      <data key="d5">Nicolas Papernot and Ross Anderson co-authored the paper "The curse of recursion: Training on generated data makes models forget"</data>
      <data key="d6">f4e98ee0b7fb42428f3312f29cb444dd</data>
    </edge>
    <edge source="TEXT MODIFICATION FLOW" target="INSTRUCTION TAXONOMY">
      <data key="d4">8.0</data>
      <data key="d5">The Text Modification Flow is part of the Instruction Taxonomy for generating seed instructions</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="STUDENT RESPONSE" target="PARSED STUDENT ANSWER">
      <data key="d4">18.0</data>
      <data key="d5">The student response is parsed to extract the final answer
The parsed student answer is extracted from the student response</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50,5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="STUDENT RESPONSE" target="ANSWER OPTIONS">
      <data key="d4">8.0</data>
      <data key="d5">The student response includes selecting an option from the provided answer options</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="ANSWER OPTIONS" target="PARSED STUDENT ANSWER">
      <data key="d4">1.0</data>
      <data key="d5">The parsed student answer is represented by the alphabet ID of the selected option from the answer options</data>
      <data key="d6">5819b66e04fd77fa705574edc49395bb</data>
    </edge>
    <edge source="EXACT MATCH/SPAN EXTRACTION PROBLEMS" target="MATHS GPT-4 EXTRACTION SYSTEM MESSAGE">
      <data key="d4">7.0</data>
      <data key="d5">The Maths GPT-4 Extraction System Message is used for evaluating student answers to math word problems in exact match/span extraction problems</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="EXACT MATCH/SPAN EXTRACTION PROBLEMS" target="GENERAL EXTRACTION SYSTEM MESSAGE">
      <data key="d4">7.0</data>
      <data key="d5">The General Extraction System Message is used for parsing student responses and matching them with the correct answer in exact match/span extraction problems</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="FINAL ANSWER" target="ERROR ANALYSIS">
      <data key="d4">8.0</data>
      <data key="d5">Error analysis involves comparing the final answer with the student's answer</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="ERROR ANALYSIS" target="FINAL VERDICT">
      <data key="d4">8.0</data>
      <data key="d5">The final verdict is the result of the error analysis</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="EMOTION SCORES" target="CRITIQUE">
      <data key="d4">7.0</data>
      <data key="d5">Critique is used to revise the emotion scores</data>
      <data key="d6">103d98395c393552cc954c89d4e59f50</data>
    </edge>
    <edge source="RESIGNED" target="ELLIOT">
      <data key="d4">8.0</data>
      <data key="d5">Elliot feels resigned after confessing his feelings to Alex, with a score of 7.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="ANGRY" target="ELLIOT">
      <data key="d4">7.0</data>
      <data key="d5">Elliot feels angry at himself for putting himself in this situation, with a score of 3.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="HOPEFUL" target="ELLIOT">
      <data key="d4">7.0</data>
      <data key="d5">Elliot feels hopeful that Alex might reciprocate his feelings, with a score of 5.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="EMBARRASSED" target="ELLIOT">
      <data key="d4">8.0</data>
      <data key="d5">Elliot feels embarrassed for putting Alex in an awkward position, with a score of 8.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="QUALITY JUDGE" target="PROMPT TEMPLATE">
      <data key="d4">8.0</data>
      <data key="d5">The Quality Judge process uses a specific prompt template to evaluate the quality of AI-generated summaries.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="REVISED SCORES" target="EMOTIONS">
      <data key="d4">8.0</data>
      <data key="d5">Revised scores provide numerical values for each of the emotions experienced by Elliot.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="TEXT SUMMARIZATION" target="HALLUCINATION DETECTION">
      <data key="d4">8.0</data>
      <data key="d5">Hallucination Detection is a part of the Text Summarization task, focusing on identifying hallucinated content.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
    <edge source="TEXT SUMMARIZATION" target="SUMMARIZATION QUALITY">
      <data key="d4">8.0</data>
      <data key="d5">Summarization Quality is a part of the Text Summarization task, focusing on evaluating the quality of the generated summaries.</data>
      <data key="d6">0cf2e43f324fa4175b9b00b90e5e90ba</data>
    </edge>
  </graph>
</graphml>